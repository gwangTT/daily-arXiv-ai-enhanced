<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 48]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.CL](#cs.CL) [Total: 60]
- [cs.CV](#cs.CV) [Total: 124]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.LG](#cs.LG) [Total: 125]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.PF](#cs.PF) [Total: 3]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 55]
- [cs.SE](#cs.SE) [Total: 7]
- [q-bio.NC](#q-bio.NC) [Total: 6]
- [stat.ML](#stat.ML) [Total: 8]
- [cs.MM](#cs.MM) [Total: 1]
- [math.CO](#math.CO) [Total: 1]
- [quant-ph](#quant-ph) [Total: 5]
- [cs.ET](#cs.ET) [Total: 3]
- [cs.SD](#cs.SD) [Total: 7]
- [cs.CC](#cs.CC) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [math.OC](#math.OC) [Total: 7]
- [cs.CY](#cs.CY) [Total: 9]
- [eess.AS](#eess.AS) [Total: 8]
- [cs.IR](#cs.IR) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [eess.SY](#eess.SY) [Total: 5]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.GR](#cs.GR) [Total: 4]
- [math.AP](#math.AP) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.NI](#cs.NI) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 8]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [stat.AP](#stat.AP) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services](https://arxiv.org/abs/2509.18101)
*Guanzhong Pan,Haibo Wang*

Main category: cs.AI

TL;DR: The paper analyzes the cost-effectiveness of deploying open-source LLMs locally versus using commercial cloud services.


<details>
  <summary>Details</summary>
Motivation: Organizations face challenges in deciding between cloud-based subscriptions and local deployment of LLMs due to factors like cost, scalability, and privacy.

Method: The authors perform a cost-benefit analysis comparing hardware, operational costs, and performance benchmarks of open-source models against cloud subscription fees.

Result: The paper identifies a breakeven point for local deployments, considering usage levels and performance requirements.

Conclusion: Organizations can use the findings to make informed decisions about their LLM strategies based on economic feasibility and operational needs.

Abstract: Large language models (LLMs) are becoming increasingly widespread.
Organizations that want to use AI for productivity now face an important
decision. They can subscribe to commercial LLM services or deploy models on
their own infrastructure. Cloud services from providers such as OpenAI,
Anthropic, and Google are attractive because they provide easy access to
state-of-the-art models and are easy to scale. However, concerns about data
privacy, the difficulty of switching service providers, and long-term operating
costs have driven interest in local deployment of open-source models. This
paper presents a cost-benefit analysis framework to help organizations
determine when on-premise LLM deployment becomes economically viable compared
to commercial subscription services. We consider the hardware requirements,
operational expenses, and performance benchmarks of the latest open-source
models, including Qwen, Llama, Mistral, and etc. Then we compare the total cost
of deploying these models locally with the major cloud providers subscription
fee. Our findings provide an estimated breakeven point based on usage levels
and performance needs. These results give organizations a practical framework
for planning their LLM strategies.

</details>


### [2] [SPADE: A Large Language Model Framework for Soil Moisture Pattern Recognition and Anomaly Detection in Precision Agriculture](https://arxiv.org/abs/2509.18123)
*Yeonju Lee,Rui Qi Chen,Joseph Oboamah,Po Nien Su,Wei-zhen Liang,Yeyin Shi,Lu Gan,Yongsheng Chen,Xin Qiao,Jing Li*

Main category: cs.AI

TL;DR: The paper introduces SPADE, a framework using large language models (LLMs) like ChatGPT-4.1 to analyze soil moisture time-series data for irrigation patterns and anomaly detection, achieving superior performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current methods for soil moisture analysis are either rule-based or dependent on data-intensive machine learning models, which lack adaptability and interpretability.

Method: SPADE leverages ChatGPT-4.1 for zero-shot analysis by transforming time-series data into textual representations and using domain-informed prompts to analyze irrigation and anomalies.

Result: Experiments on real-world soil moisture data demonstrated SPADE's superior performance in anomaly detection (higher recall and F1 scores) and irrigation event detection with high precision and recall.

Conclusion: SPADE validates the potential of LLMs in precision agriculture, offering scalable, adaptable, and interpretable approaches to soil moisture monitoring and irrigation management.

Abstract: Accurate interpretation of soil moisture patterns is critical for irrigation
scheduling and crop management, yet existing approaches for soil moisture
time-series analysis either rely on threshold-based rules or data-hungry
machine learning or deep learning models that are limited in adaptability and
interpretability. In this study, we introduce SPADE (Soil moisture Pattern and
Anomaly DEtection), an integrated framework that leverages large language
models (LLMs) to jointly detect irrigation patterns and anomalies in soil
moisture time-series data. SPADE utilizes ChatGPT-4.1 for its advanced
reasoning and instruction-following capabilities, enabling zero-shot analysis
without requiring task-specific annotation or fine-tuning. By converting
time-series data into a textual representation and designing domain-informed
prompt templates, SPADE identifies irrigation events, estimates net irrigation
gains, detects, classifies anomalies, and produces structured, interpretable
reports. Experiments were conducted on real-world soil moisture sensor data
from commercial and experimental farms cultivating multiple crops across the
United States. Results demonstrate that SPADE outperforms the existing method
in anomaly detection, achieving higher recall and F1 scores and accurately
classifying anomaly types. Furthermore, SPADE achieved high precision and
recall in detecting irrigation events, indicating its strong capability to
capture irrigation patterns accurately. SPADE's reports provide
interpretability and usability of soil moisture analytics. This study
highlights the potential of LLMs as scalable, adaptable tools for precision
agriculture, which is capable of integrating qualitative knowledge and
data-driven reasoning to produce actionable insights for accurate soil moisture
monitoring and improved irrigation scheduling from soil moisture time-series
data.

</details>


### [3] [Position Paper: Integrating Explainability and Uncertainty Estimation in Medical AI](https://arxiv.org/abs/2509.18132)
*Xiuyi Fan*

Main category: cs.AI

TL;DR: This paper introduces Explainable Uncertainty Estimation (XUE) for medical AI, combining explainability with uncertainty quantification to improve clinical trust and usability.


<details>
  <summary>Details</summary>
Motivation: The paper addresses a gap where current medical AI systems fail to merge explainability and uncertainty estimation, creating a barrier for adoption in clinical settings.

Method: The authors propose XUE by systematically mapping medical uncertainty to AI uncertainty concepts, identifying challenges, and suggesting technical advancements such as multimodal quantification and uncertainty-aware decision systems.

Result: The framework of XUE is outlined, revealing how it can integrate explainability with confidence measures to align AI output with clinical reasoning.

Conclusion: XUE promises to enhance trust in medical AI by providing both reliable predictions and meaningful confidence explanations, aligning AI systems with clinical needs and complexities.

Abstract: Uncertainty is a fundamental challenge in medical practice, but current
medical AI systems fail to explicitly quantify or communicate uncertainty in a
way that aligns with clinical reasoning. Existing XAI works focus on
interpreting model predictions but do not capture the confidence or reliability
of these predictions. Conversely, uncertainty estimation (UE) techniques
provide confidence measures but lack intuitive explanations. The disconnect
between these two areas limits AI adoption in medicine. To address this gap, we
propose Explainable Uncertainty Estimation (XUE) that integrates explainability
with uncertainty quantification to enhance trust and usability in medical AI.
We systematically map medical uncertainty to AI uncertainty concepts and
identify key challenges in implementing XUE. We outline technical directions
for advancing XUE, including multimodal uncertainty quantification,
model-agnostic visualization techniques, and uncertainty-aware decision support
systems. Lastly, we propose guiding principles to ensure effective XUE
realisation. Our analysis highlights the need for AI systems that not only
generate reliable predictions but also articulate confidence levels in a
clinically meaningful way. This work contributes to the development of
trustworthy medical AI by bridging explainability and uncertainty, paving the
way for AI systems that are aligned with real-world clinical complexities.

</details>


### [4] [HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics](https://arxiv.org/abs/2509.18168)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: The paper introduces Hierarchical Segment-Graph Memory (HSGM), a framework to efficiently parse long documents by combining local semantic graphs and global summarization.


<details>
  <summary>Details</summary>
Motivation: There is a need to address the computational and memory challenges of semantic parsing for long documents, given the quadratic growth in pairwise composition and resource usage.

Method: The HSGM framework decomposes long inputs into smaller meaningful segments, constructs local semantic graphs, summarizes nodes into a global graph memory, and processes queries hierarchically.

Result: HSGM achieves 2–4× faster inference speed, reduces memory usage by over 60%, and retains ≥95% of baseline accuracy across tasks like AMR parsing, semantic role labeling, and legal event extraction.

Conclusion: HSGM enables scalable and accurate semantic modeling for ultra-long texts, making it applicable for real-time and resource-constrained NLP scenarios.

Abstract: Semantic parsing of long documents remains challenging due to quadratic
growth in pairwise composition and memory requirements. We introduce
\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that
decomposes an input of length $N$ into $M$ meaningful segments, constructs
\emph{Local Semantic Graphs} on each segment, and extracts compact
\emph{summary nodes} to form a \emph{Global Graph Memory}. HSGM supports
\emph{incremental updates} -- only newly arrived segments incur local graph
construction and summary-node integration -- while \emph{Hierarchical Query
Processing} locates relevant segments via top-$K$ retrieval over summary nodes
and then performs fine-grained reasoning within their local graphs.
  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to
$O\!\left(N\,k + (N/k)^2\right)$, with segment size $k \ll N$, and we derive
Frobenius-norm bounds on the approximation error introduced by node
summarization and sparsification thresholds. Empirically, on three benchmarks
-- long-document AMR parsing, segment-level semantic role labeling (OntoNotes),
and legal event extraction -- HSGM achieves \emph{2--4$\times$ inference
speedup}, \emph{$>60\%$ reduction} in peak memory, and \emph{$\ge 95\%$} of
baseline accuracy. Our approach unlocks scalable, accurate semantic modeling
for ultra-long texts, enabling real-time and resource-constrained NLP
applications.

</details>


### [5] [Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM](https://arxiv.org/abs/2509.18178)
*Ling Yue,Nithin Somasekharan,Tingwen Zhang,Yadi Cao,Shaowu Pan*

Main category: cs.AI

TL;DR: Foam-Agent simplifies and automates the OpenFOAM simulation pipeline using natural language prompts, achieving a notable 88.2% success rate compared to existing frameworks.


<details>
  <summary>Details</summary>
Motivation: The steep learning curve and complexity in manual CFD setups prevent many users from efficiently working with OpenFOAM.

Method: Foam-Agent utilizes a multi-agent framework that automates pre-processing, meshing, simulation execution, HPC script creation, and post-simulation visualization.

Result: Foam-Agent achieves an 88.2% success rate in 110 benchmark tasks, surpassing the 55.5% success rate of MetaOpenFOAM.

Conclusion: Foam-Agent lowers expertise barriers, democratizing CFD workflows and showcasing the potential of specialized multi-agent systems in scientific computing.

Abstract: Computational Fluid Dynamics (CFD) is an essential simulation tool in
engineering, yet its steep learning curve and complex manual setup create
significant barriers. To address these challenges, we introduce Foam-Agent, a
multi-agent framework that automates the entire end-to-end OpenFOAM workflow
from a single natural language prompt. Our key innovations address critical
gaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation:
Foam-Agent is the first system to manage the full simulation pipeline,
including advanced pre-processing with a versatile Meshing Agent capable of
handling external mesh files and generating new geometries via Gmsh, automatic
generation of HPC submission scripts, and post-simulation visualization via
ParaView. 2. Composable Service Architecture: Going beyond a monolithic agent,
the framework uses Model Context Protocol (MCP) to expose its core functions as
discrete, callable tools. This allows for flexible integration and use by other
agentic systems, such as Claude-code, for more exploratory workflows. 3.
High-Fidelity Configuration Generation: We achieve superior accuracy through a
Hierarchical Multi-Index RAG for precise context retrieval and a
dependency-aware generation process that ensures configuration consistency.
Evaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2%
success rate with Claude 3.5 Sonnet, significantly outperforming existing
frameworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the
expertise barrier for CFD, demonstrating how specialized multi-agent systems
can democratize complex scientific computing. The code is public at
https://github.com/csml-rpi/Foam-Agent.

</details>


### [6] [Large Language Models and Operations Research: A Structured Survey](https://arxiv.org/abs/2509.18180)
*Yang Wang,Kai Li*

Main category: cs.AI

TL;DR: The paper reviews how large language models (LLMs) can enhance operations research (OR) by enabling automatic modeling, auxiliary optimization, and direct solving of problems.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional OR methods, such as difficulty in handling large-scale, dynamic, and multi-constraint problems, by leveraging the capabilities of LLMs.

Method: The paper surveys existing research on integrating LLMs into OR and organizes methods into categories such as automatic modeling, auxiliary optimization, and direct solving. It also reviews benchmarks, applications, and identifies key challenges.

Result: The paper highlights how LLMs can contribute to OR by translating natural language into models, evolving algorithms, and tackling optimization tasks, despite current limitations in generalization and evaluation.

Conclusion: LLMs hold significant potential to advance OR by overcoming traditional challenges, but more research is needed to handle semantic mapping instability, fragmented progress, and lack of robust evaluation systems.

Abstract: Operations research (OR) provides fundamental methodologies for complex
system decision-making, with established applications in transportation, supply
chain management, and production scheduling. Traditional approaches, which
depend on expert-based modeling and manual parameter adjustment, often face
challenges in handling large-scale, dynamic, and multi-constraint problems.
Recently, large language models (LLMs) have shown potential to address these
limitations through semantic understanding, structured generation, and
reasoning control. LLMs can translate natural language descriptions into
mathematical models or executable code, generate heuristics, evolve algorithms,
and directly tackle optimization tasks. This paper surveys recent progress on
the integration of LLMs into OR, organizing methods into three main directions:
automatic modeling, auxiliary optimization, and direct solving. It further
reviews evaluation benchmarks and domain-specific applications, and summarizes
key open issues such as unstable semantic-to-structure mapping, fragmented
research progress, limited generalization, and insufficient evaluation systems.
Finally, the survey outlines possible research avenues for advancing the role
of LLMs in OR.

</details>


### [7] [Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling](https://arxiv.org/abs/2509.18181)
*Mustafa Sameen,Xiaojian Zhang,Xilei Zhao*

Main category: cs.AI

TL;DR: The paper introduces the SAPA framework, a hierarchical model leveraging Large Language Models (LLMs) to improve predictions of ridesourcing mode choices by synthesizing theory-driven latent attitudes.


<details>
  <summary>Details</summary>
Motivation: Current models for predicting ridesourcing mode choices face challenges like limited predictive accuracy due to their inability to capture psychological factors and issues with class imbalance, given the rarity of ridesourcing trips.

Method: The SAPA framework utilizes an LLM to generate traveler personas from travel survey data, enrich demographic and behavioral features, assign scores to latent variables, and integrate them into a classifier for mode choice prediction.

Result: SAPA outperformed state-of-the-art baselines by up to 75.9% in PR-AUC on a multi-year travel survey test dataset, proving its effectiveness.

Conclusion: By substantially improving prediction accuracy, the SAPA framework advances modeling of ridesourcing mode choices and offers a versatile methodology applicable to broader domains.

Abstract: Accurate modeling of ridesourcing mode choices is essential for designing and
implementing effective traffic management policies for reducing congestion,
improving mobility, and allocating resources more efficiently. Existing models
for predicting ridesourcing mode choices often suffer from limited predictive
accuracy due to their inability to capture key psychological factors, and are
further challenged by severe class imbalance, as ridesourcing trips comprise
only a small fraction of individuals' daily travel. To address these
limitations, this paper introduces the Synthesizing Attitudes, Predicting
Actions (SAPA) framework, a hierarchical approach that uses Large Language
Models (LLMs) to synthesize theory-grounded latent attitudes to predict
ridesourcing choices. SAPA first uses an LLM to generate qualitative traveler
personas from raw travel survey data and then trains a propensity-score model
on demographic and behavioral features, enriched by those personas, to produce
an individual-level score. Next, the LLM assigns quantitative scores to
theory-driven latent variables (e.g., time and cost sensitivity), and a final
classifier integrates the propensity score, latent-variable scores (with their
interaction terms), and observable trip attributes to predict ridesourcing mode
choice. Experiments on a large-scale, multi-year travel survey show that SAPA
significantly outperforms state-of-the-art baselines, improving ridesourcing
choice predictions by up to 75.9% in terms of PR-AUC on a held-out test set.
This study provides a powerful tool for accurately predicting ridesourcing mode
choices, and provides a methodology that is readily transferable to various
applications.

</details>


### [8] [An Outcome-Based Educational Recommender System](https://arxiv.org/abs/2509.18186)
*Nursultan Askarbekuly,Timur Fayzrakhmanov,Sladjan Babarogić,Ivan Luković*

Main category: cs.AI

TL;DR: This paper introduces OBER, an educational recommender that evaluates systems based on learning outcomes instead of just engagement such as clicks or ratings.


<details>
  <summary>Details</summary>
Motivation: Current educational recommendation systems prioritize engagement metrics like clicks or ratings without addressing their actual impact on learning outcomes.

Method: Developed OBER, a system embedding learning outcomes and assessments into its schema, using a minimalist model and a plug-in architecture, and tested it through randomized trials with 5,700 learners.

Result: Collaborative filtering (CF) improved retention, whereas the fixed expert trajectory achieved better mastery. OBER allows simultaneous analysis of multiple metrics from the same logs without additional testing.

Conclusion: OBER is a versatile, method-agnostic framework that balances engagement and learning outcomes, adaptable for future intelligent recommendation systems.

Abstract: Most educational recommender systems are tuned and judged on click- or
rating-based relevance, leaving their true pedagogical impact unclear. We
introduce OBER-an Outcome-Based Educational Recommender that embeds learning
outcomes and assessment items directly into the data schema, so any algorithm
can be evaluated on the mastery it fosters. OBER uses a minimalist
entity-relation model, a log-driven mastery formula, and a plug-in
architecture. Integrated into an e-learning system in non-formal domain, it was
evaluated trough a two-week randomized split test with over 5 700 learners
across three methods: fixed expert trajectory, collaborative filtering (CF),
and knowledge-based (KB) filtering. CF maximized retention, but the fixed path
achieved the highest mastery. Because OBER derives business, relevance, and
learning metrics from the same logs, it lets practitioners weigh relevance and
engagement against outcome mastery with no extra testing overhead. The
framework is method-agnostic and readily extensible to future adaptive or
context-aware recommenders.

</details>


### [9] [MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation](https://arxiv.org/abs/2509.18198)
*Rui Liu,Zikang Wang,Peng Gao,Yu Shen,Pratap Tokekar,Ming Lin*

Main category: cs.AI

TL;DR: The paper introduces MMCD, a framework enhancing decision-making in autonomous systems using multi-modal data fusion and cross-modal knowledge distillation to address sensor failures and missing data modalities.


<details>
  <summary>Details</summary>
Motivation: Challenges in accident-prone environments make robust and reliable decision-making crucial for autonomous systems, especially when relying on limited sensor data.

Method: MMCD integrates multi-modal observations from ego and collaborative vehicles using cross-modal knowledge distillation with a teacher-student model structure to ensure performance even when some data modalities are unavailable.

Result: Experimental results demonstrate a safety improvement of up to 20.7% in connected autonomous driving scenarios, outperforming existing methods in detecting accidents and ensuring safe decisions.

Conclusion: MMCD provides a novel approach to overcoming limitations in multi-modal autonomous systems, enhancing decision-making robustness and safety under real-world conditions.

Abstract: Autonomous systems have advanced significantly, but challenges persist in
accident-prone environments where robust decision-making is crucial. A single
vehicle's limited sensor range and obstructed views increase the likelihood of
accidents. Multi-vehicle connected systems and multi-modal approaches,
leveraging RGB images and LiDAR point clouds, have emerged as promising
solutions. However, existing methods often assume the availability of all data
modalities and connected vehicles during both training and testing, which is
impractical due to potential sensor failures or missing connected vehicles. To
address these challenges, we introduce a novel framework MMCD (Multi-Modal
Collaborative Decision-making) for connected autonomy. Our framework fuses
multi-modal observations from ego and collaborative vehicles to enhance
decision-making under challenging conditions. To ensure robust performance when
certain data modalities are unavailable during testing, we propose an approach
based on cross-modal knowledge distillation with a teacher-student model
structure. The teacher model is trained with multiple data modalities, while
the student model is designed to operate effectively with reduced modalities.
In experiments on $\textit{connected autonomous driving with ground vehicles}$
and $\textit{aerial-ground vehicles collaboration}$, our method improves
driving safety by up to ${\it 20.7}\%$, surpassing the best-existing baseline
in detecting potential accidents and making safe driving decisions. More
information can be found on our website https://ruiiu.github.io/mmcd.

</details>


### [10] [Change in Quantitative Bipolar Argumentation: Sufficient, Necessary, and Counterfactual Explanations](https://arxiv.org/abs/2509.18215)
*Timotheus Kampik,Kristijonas Čyras,José Ruiz Alarcón*

Main category: cs.AI

TL;DR: The paper proposes a method to explain changes in inference strength within Quantitative Bipolar Argumentation Frameworks during updates.


<details>
  <summary>Details</summary>
Motivation: Understanding why inference strength changes within Quantitative Bipolar Argumentation Frameworks after updates is crucial for improved reasoning and transparency in argumentation models.

Method: The study introduces a formalized approach to trace strength inconsistencies, identifies explanations (sufficient, necessary, and counterfactual), and implements a heuristic-based tool to locate these explanations.

Result: Strength inconsistency explanations are shown to exist if and only if updates in the framework cause inconsistencies, and the heuristic-based implementation facilitates finding these explanations.

Conclusion: The work enhances understanding of inference changes in argumentation frameworks and provides an effective way to identify and explain related inconsistencies.

Abstract: This paper presents a formal approach to explaining change of inference in
Quantitative Bipolar Argumentation Frameworks (QBAFs). When drawing conclusions
from a QBAF and updating the QBAF to then again draw conclusions (and so on),
our approach traces changes -- which we call strength inconsistencies -- in the
partial order over argument strengths that a semantics establishes on some
arguments of interest, called topic arguments. We trace the causes of strength
inconsistencies to specific arguments, which then serve as explanations. We
identify sufficient, necessary, and counterfactual explanations for strength
inconsistencies and show that strength inconsistency explanations exist if and
only if an update leads to strength inconsistency. We define a heuristic-based
approach to facilitate the search for strength inconsistency explanations, for
which we also provide an implementation.

</details>


### [11] [nDNA -- the Semantic Helix of Artificial Cognition](https://arxiv.org/abs/2509.18216)
*Amitava Das*

Main category: cs.AI

TL;DR: This paper introduces Neural DNA (nDNA), a novel framework to understand and represent the latent identity of AI models via their internal geometry, enabling analysis of model evolution, inheritance, and cognition.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the 'identity' of AI models beyond their visible output by analyzing their intrinsic latent geometry, which reflects deeper cognitive structures and semantic flow.

Method: The authors define Neural DNA using three core principles: spectral curvature (conceptual flow across layers), thermodynamic length (semantic effort for transitions), and belief vector field (semantic torsion in belief orientations).

Result: They present nDNA as a stable, geometry-first fingerprint of how models process meaning and establish its utility in tracing model lineage, fine-tuning, checkpoint inheritance, and measuring cognitive evolution.

Conclusion: The work establishes the foundation for 'Neural Genomics,' a new field to study AI models as semantic organisms, facilitating their comparison, risk diagnosis, and governance.

Abstract: As AI foundation models grow in capability, a deeper question emerges: What
shapes their internal cognitive identity -- beyond fluency and output?
Benchmarks measure behavior, but the soul of a model resides in its latent
geometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic
representation that captures this latent identity through the intrinsic
geometry of belief. At its core, nDNA is synthesized from three principled and
indispensable dimensions of latent geometry: spectral curvature, which reveals
the curvature of conceptual flow across layers; thermodynamic length, which
quantifies the semantic effort required to traverse representational
transitions through layers; and belief vector field, which delineates the
semantic torsion fields that guide a model's belief directional orientations.
Like biological DNA, it encodes ancestry, mutation, and semantic inheritance,
found in finetuning and alignment scars, cultural imprints, and architectural
drift. In naming it, we open a new field: Neural Genomics, where models are not
just tools, but digital semantic organisms with traceable inner cognition.
  Modeling statement. We read AI foundation models as semantic fluid--dynamics:
meaning is transported through layers like fluid in a shaped conduit; nDNA is
the physics-grade readout of that flow -- a geometry-first measure of how
meaning is bent, paid for, and pushed -- yielding a stable, coordinate-free
neural DNA fingerprint tied to on-input behavior; with this fingerprint we
cross into biology: tracing lineages across pretraining, fine-tuning,
alignment, pruning, distillation, and merges; measuring inheritance between
checkpoints; detecting drift as traits shift under new data or objectives; and,
ultimately, studying the evolution of artificial cognition to compare models,
diagnose risks, and govern change over time.

</details>


### [12] [Similarity Field Theory: A Mathematical Framework for Intelligence](https://arxiv.org/abs/2509.18218)
*Kei-Sing Ng*

Main category: cs.AI

TL;DR: This paper introduces Similarity Field Theory as a mathematical framework for understanding dynamic systems through similarity relations among entities, focusing on their evolution and generative intelligence.


<details>
  <summary>Details</summary>
Motivation: The authors aim to create a mathematical foundation to analyze and interpret dynamic systems and intelligent processes through similarity relationships between entities.

Method: The authors define a similarity field over a set of entities, investigate the evolution of these fields, propose the concept of fibers to categorize entities, and introduce a generative operator $G$ to define intelligence. They also prove key theorems relating to asymmetry and stability.

Result: The framework provides structural constraints and interpretability for the evolution of similarity fields, making it applicable to understanding large language models as probes of societal cognition.

Conclusion: Similarity Field Theory acts as a foundational tool to characterize and construct intelligent systems, furthering the interpretability and analysis of complex dynamic systems.

Abstract: We posit that persisting and transforming similarity relations form the
structural basis of any comprehensible dynamic system. This paper introduces
Similarity Field Theory, a mathematical framework that formalizes the
principles governing similarity values among entities and their evolution. We
define: (1) a similarity field $S: U \times U \to [0,1]$ over a universe of
entities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed
relational field (asymmetry and non-transitivity are allowed); (2) the
evolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by
$p=0,1,2,\ldots$; (3) concepts $K$ as entities that induce fibers
$F_{\alpha}(K) = { E \in U \mid S(E,K) \ge \alpha }$, i.e., superlevel sets of
the unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that
produces new entities. Within this framework, we formalize a generative
definition of intelligence: an operator $G$ is intelligent with respect to a
concept $K$ if, given a system containing entities belonging to the fiber of
$K$, it generates new entities that also belong to that fiber. Similarity Field
Theory thus offers a foundational language for characterizing, comparing, and
constructing intelligent systems. We prove two theorems: (i) asymmetry blocks
mutual inclusion; and (ii) stability requires either an anchor coordinate or
eventual confinement within a level set of $f$. These results ensure that the
evolution of similarity fields is both constrained and interpretable,
culminating in an exploration of how the framework allows us to interpret large
language models and use them as experimental probes into societal cognition.

</details>


### [13] [Multimodal Health Risk Prediction System for Chronic Diseases via Vision-Language Fusion and Large Language Models](https://arxiv.org/abs/2509.18221)
*Dingxin Lu,Shurui Wu,Xinyi Huang*

Main category: cs.AI

TL;DR: The paper introduces a novel AI framework named VL-RiskFormer designed for predicting health risks using multimodal data like medical imaging, textual records, and wearable sensor data. It outperformed benchmarks with high accuracy and reliable calibration.


<details>
  <summary>Details</summary>
Motivation: With the increase in chronic diseases worldwide and the availability of diverse clinical data types, there is a pressing demand for a unified AI system that can analyze such data to predict health risks effectively.

Method: The proposed system, VL-RiskFormer, is a hierarchical multimodal Transformer with key innovations: cross-modal pre-training using momentum encoders and loss functions, adaptive time-coding for handling irregular clinical visits, and integration of disease codes with graph attention. It is equipped with a large language model at its top layer.

Result: The model showed superior performance on the MIMIC-IV longitudinal dataset, achieving an AUROC score of 0.90 and an expected calibration error of 2.7 percent.

Conclusion: The VL-RiskFormer demonstrates the potential of multimodal AI systems for precise health risk prediction, paving the way for advancements in personalized medicine.

Abstract: With the rising global burden of chronic diseases and the multimodal and
heterogeneous clinical data (medical imaging, free-text recordings, wearable
sensor streams, etc.), there is an urgent need for a unified multimodal AI
framework that can proactively predict individual health risks. We propose
VL-RiskFormer, a hierarchical stacked visual-language multimodal Transformer
with a large language model (LLM) inference head embedded in its top layer. The
system builds on the dual-stream architecture of existing visual-linguistic
models (e.g., PaLM-E, LLaVA) with four key innovations: (i) pre-training with
cross-modal comparison and fine-grained alignment of radiological images,
fundus maps, and wearable device photos with corresponding clinical narratives
using momentum update encoders and debiased InfoNCE losses; (ii) a time fusion
block that integrates irregular visit sequences into the causal Transformer
decoder through adaptive time interval position coding; (iii) a disease
ontology map adapter that injects ICD-10 codes into visual and textual channels
in layers and infers comorbid patterns with the help of a graph attention
mechanism. On the MIMIC-IV longitudinal cohort, VL-RiskFormer achieved an
average AUROC of 0.90 with an expected calibration error of 2.7 percent.

</details>


### [14] [From "What to Eat?" to Perfect Recipe: ChefMind's Chain-of-Exploration for Ambiguous User Intent in Recipe Recommendation](https://arxiv.org/abs/2509.18226)
*Yu Fu,Linyue Cai,Ruoyu Wu,Yong Zhao*

Main category: cs.AI

TL;DR: The paper introduces ChefMind, a system enhancing recipe recommendations with hybrid AI techniques, achieving superior accuracy and clarity in handling user queries.


<details>
  <summary>Details</summary>
Motivation: The study addresses challenges in personalized recipe recommendations, particularly ambiguous user intent, semantic correctness, and ensuring detailed responses.

Method: ChefMind employs a hybrid framework combining Chain of Exploration (CoE) for refining queries, Knowledge Graph (KG) for semantic reasoning, Retrieval-Augmented Generation (RAG) for adding culinary context, and a Large Language Model (LLM) to integrate and generate coherent recommendations.

Result: ChefMind outperformed LLM-only, KG-only, and RAG-only models in key metrics and significantly reduced unprocessed queries to just 1.6%.

Conclusion: The hybrid approach in ChefMind effectively improves the performance and robustness of recipe recommendation systems by tackling ambiguity and ensuring detail-rich outputs.

Abstract: Personalized recipe recommendation faces challenges in handling fuzzy user
intent, ensuring semantic accuracy, and providing sufficient detail coverage.
We propose ChefMind, a hybrid architecture combining Chain of Exploration
(CoE), Knowledge Graph (KG), Retrieval-Augmented Generation (RAG), and a Large
Language Model (LLM). CoE refines ambiguous queries into structured conditions,
KG offers semantic reasoning and interpretability, RAG supplements contextual
culinary details, and LLM integrates outputs into coherent recommendations. We
evaluate ChefMind on the Xiachufang dataset and manually annotated queries,
comparing it with LLM-only, KG-only, and RAG-only baselines. Results show that
ChefMind achieves superior performance in accuracy, relevance, completeness,
and clarity, with an average score of 8.7 versus 6.4-6.7 for ablation models.
Moreover, it reduces unprocessed queries to 1.6%, demonstrating robustness in
handling fuzzy demands.

</details>


### [15] [An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems](https://arxiv.org/abs/2509.18229)
*Anthony Patera,Rohan Abeyaratne*

Main category: cs.AI

TL;DR: The paper addresses the reliability issues of Generative AI (like GPT) in solving mechanical engineering problems and introduces a methodology using multiple GPT agents for higher accuracy.


<details>
  <summary>Details</summary>
Motivation: The unreliability of GPT in consistently solving mechanical engineering problems, with an 85% success rate, limits its applicability in education and professional engineering.

Method: The authors propose an "N-Plus-1" GPT Agency that runs multiple instances of Agent Solve and aggregates their outputs using Agent Compare to identify the most accurate solution.

Result: Using Condorcet's Jury Theorem, the methodology demonstrates improved reliability in identifying the correct solution when individual problem-solving success rates exceed 50%.

Conclusion: The multi-agent approach provides higher transparency and pedagogical value compared to commercial models, making it suitable for preliminary engineering analyses.

Abstract: Generative AI, and specifically GPT, can produce a remarkable solution to a
mechanical engineering analysis problem - but also, on occasion, a flawed
solution. For example, an elementary mechanics problem is solved flawlessly in
one GPT instance and incorrectly in a subsequent GPT instance, with a success
probability of only 85%. This unreliability renders "out-of-the-box" GPT
unsuitable for deployment in education or engineering practice. We introduce an
"N-Plus-1" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering
Problem Statements. Agency first launches N instantiations of Agent Solve to
yield N independent Proposed Problem Solution Realizations; Agency then invokes
Agent Compare to summarize and compare the N Proposed Problem Solution
Realizations and to provide a Recommended Problem Solution. We argue from
Condorcet's Jury Theorem that, for a Problem Statement characterized by
per-Solve success probability greater than 1/2 (and N sufficiently large), the
Predominant (Agent Compare) Proposed Problem Solution will, with high
probability, correspond to a Correct Proposed Problem Solution. Furthermore,
Agent Compare can also incorporate aspects of Secondary (Agent Compare)
Proposed Problem Solutions, in particular when the latter represent alternative
Problem Statement interpretations - different Mathematical Models - or
alternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a
commercial multi-agent model, show similarities in design and performance, but
also important differences in emphasis: our Agency focuses on transparency and
pedagogical value.

</details>


### [16] [Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces](https://arxiv.org/abs/2509.18230)
*Zihan Dong,Xinyu Fan,Zixiang Tang,Yunqing Li*

Main category: cs.AI

TL;DR: The paper introduces ComputerAgent, a lightweight hierarchical reinforcement learning method for desktop control, achieving high task success rates while being highly efficient compared to large language models.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal large language models for desktop control suffer from high latency, inefficiency in complex tasks, and impracticality for on-device deployment.

Method: The paper proposes a two-level hierarchical RL framework with a triple-modal state encoder, meta-actions with early-stop mechanisms, and a compact vision backbone for efficient on-device execution.

Result: ComputerAgent achieves 92.1% success on simple tasks and 58.8% on hard tasks, matching or surpassing MLLM baselines while being more efficient and compact.

Conclusion: Hierarchical reinforcement learning offers a viable and scalable approach to desktop application control, outperforming monolithic MLLM-based solutions in efficiency and practicality.

Abstract: Controlling desktop applications via software remains a fundamental yet
under-served problem. Existing multi-modal large language models (MLLMs) ingest
screenshots and task instructions to generate keystrokes and mouse events, but
they suffer from prohibitive inference latency, poor sample efficiency on
long-horizon sparse-reward tasks, and infeasible on-device deployment. We
introduce a lightweight hierarchical reinforcement learning framework,
ComputerAgent, that formulates OS control as a two-level option process
(manager and subpolicy), employs a triple-modal state encoder (screenshot, task
ID, numeric state) to handle visual and contextual diversity, integrates
meta-actions with an early-stop mechanism to reduce wasted interactions, and
uses a compact vision backbone plus small policy networks for on-device
inference (15M parameters). On a suite of 135 real-world desktop tasks,
ComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on
hard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on
simple scenarios while reducing model size by over four orders of magnitude and
halving inference time. These results demonstrate that hierarchical RL offers a
practical, scalable alternative to monolithic MLLM-based automation for
computer control.

</details>


### [17] [The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks](https://arxiv.org/abs/2509.18234)
*Yu Gu,Jingjing Fu,Xiaodong Liu,Jeya Maria Jose Valanarasu,Noel Codella,Reuben Tan,Qianchu Liu,Ying Jin,Sheng Zhang,Jinyu Wang,Rui Wang,Lei Song,Guanghui Qin,Naoto Usuyama,Cliff Wong,Cheng Hao,Hohin Lee,Praneeth Sanapathi,Sarah Hilado,Bian Jiang,Javier Alvarez-Valle,Mu Wei,Jianfeng Gao,Eric Horvitz,Matt Lungren,Hoifung Poon,Paul Vozila*

Main category: cs.AI

TL;DR: Despite high scores on medical benchmarks, large models like GPT-5 face significant flaws such as brittleness, shortcut learning, and unreliable reasoning in medical contexts.


<details>
  <summary>Details</summary>
Motivation: To address the gap between high benchmark scores and the real-world medical readiness of AI systems.

Method: Stress testing six flagship models across six benchmarks, along with clinician-guided rubric evaluation to analyze robustness and reasoning.

Result: High leaderboard scores were found to mask serious issues such as brittleness and shortcut-based learning.

Conclusion: Medical benchmarks should not be treated as direct indicators of real-world readiness; systems must focus on robustness, sound reasoning, and alignment with medical requirements to build trust.

Abstract: Large frontier models like GPT-5 now achieve top scores on medical
benchmarks. But our stress tests tell a different story. Leading systems often
guess correctly even when key inputs like images are removed, flip answers
under trivial prompt changes, and fabricate convincing yet flawed reasoning.
These aren't glitches; they expose how today's benchmarks reward test-taking
tricks over medical understanding. We evaluate six flagship models across six
widely used benchmarks and find that high leaderboard scores hide brittleness
and shortcut learning. Through clinician-guided rubric evaluation, we show that
benchmarks vary widely in what they truly measure yet are treated
interchangeably, masking failure modes. We caution that medical benchmark
scores do not directly reflect real-world readiness. If we want AI to earn
trust in healthcare, we must demand more than leaderboard wins and must hold
systems accountable for robustness, sound reasoning, and alignment with real
medical demands.

</details>


### [18] [Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints](https://arxiv.org/abs/2509.18382)
*Adarsha Balaji,Le Chen,Rajeev Thakur,Franck Cappello,Sandeep Madireddy*

Main category: cs.AI

TL;DR: The paper explores strategies for reducing computational costs of reasoning language models while maintaining safety performance by employing reasoning length constraints and model quantization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the increased computational cost associated with utilizing longer chain-of-thought sequences for language model reasoning, balancing efficiency and safety.

Method: The paper investigates two methods: length-controlled policy optimization through reinforcement learning to manage reasoning length and quantization techniques to generate CoT sequences within specific compute constraints.

Result: Two strategies are proposed that are effective in applying compute constraints while studying their trade-offs, particularly focusing on computational efficiency and safety of the reasoning models.

Conclusion: Constrained compute resources can be efficiently managed using the proposed methods, which also allow the exploration of trade-offs between computational efficiency and model safety.

Abstract: Test-time compute scaling has demonstrated the ability to improve the
performance of reasoning language models by generating longer chain-of-thought
(CoT) sequences. However, this increase in performance comes with a significant
increase in computational cost. In this work, we investigate two compute
constraint strategies: (1) reasoning length constraint and (2) model
quantization, as methods to reduce the compute demand of reasoning models and
study their impact on their safety performance. Specifically, we explore two
approaches to apply compute constraints to reasoning models: (1) fine-tuning
reasoning models using a length controlled policy optimization (LCPO) based
reinforcement learning method to satisfy a user-defined CoT reasoning length,
and (2) applying quantization to maximize the generation of CoT sequences
within a user-defined compute constraint. Furthermore, we study the trade-off
between the computational efficiency and the safety of the model.

</details>


### [19] [Gödel Test: Can Large Language Models Solve Easy Conjectures?](https://arxiv.org/abs/2509.18383)
*Moran Feldman,Amin Karbasi*

Main category: cs.AI

TL;DR: The paper investigates whether GPT-5 can solve simple, previously unsolved conjectures in combinatorial optimization, producing mixed results with meaningful reasoning progress and clear limitations in synthesis across sources.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the capability of large language models, specifically GPT-5, to solve new, unsolved conjectures in advanced mathematics, as opposed to merely excelling in competitions or routine problems.

Method: The authors propose a 'Gödel Test' to evaluate GPT-5 by assessing its ability to produce proofs for five conjectures in combinatorial optimization, using source papers as references and analyzing its reasoning critically.

Result: GPT-5 performed nearly correct solutions for three simpler problems and refuted one conjecture via a valid alternative solution; however, it struggled with cross-paper synthesis and rigorous proof analysis in harder cases.

Conclusion: While GPT-5 shows signs of progress in mathematical reasoning and occasional originality, it still faces limitations in handling complex proofs and synthesizing information across multiple sources, indicating an early stage towards passing the 'Gödel Test.'

Abstract: Recent announcements from frontier AI model labs have highlighted strong
results on high-school and undergraduate math competitions. Yet it remains
unclear whether large language models can solve new, simple conjectures in more
advanced areas of mathematics. We propose the G\"odel Test: evaluating whether
a model can produce correct proofs for very simple, previously unsolved
conjectures. To this end, we study the performance of GPT-5 on five conjectures
in combinatorial optimization. For each problem, we provided one or two source
papers from which the conjecture arose, withheld our own conjecture, and then
assessed the model's reasoning in detail. On the three easier problems, GPT-5
produced nearly correct solutions; for Problem 2 it even derived a different
approximation guarantee that, upon checking, refuted our conjecture while
providing a valid solution. The model failed on Problem 4, which required
combining results from two papers. On Problem 5, a harder case without a
validated conjecture, GPT-5 proposed the same algorithm we had in mind but
failed in the analysis, suggesting the proof is more challenging than expected.
Although our sample is small, the results point to meaningful progress on
routine reasoning, occasional flashes of originality, and clear limitations
when cross-paper synthesis is required. GPT-5 may represent an early step
toward frontier models eventually passing the G\"odel Test.

</details>


### [20] [ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff Code Classification](https://arxiv.org/abs/2509.18400)
*Pritish Yuvraj,Siva Devarakonda*

Main category: cs.AI

TL;DR: The paper introduces the first benchmark for classifying products under the Harmonized Tariff Schedule (HTS) using machine learning and releases both a model and dataset to improve this classification process.


<details>
  <summary>Details</summary>
Motivation: HTS code misclassification can halt shipments and disrupt global trade, with little prior attention paid by the machine learning community.

Method: The authors benchmarked several large language models (LLMs) and developed a fine-tuned Atlas model to improve HTS classification accuracy.

Result: The Atlas model achieved 40% accuracy for 10-digit classifications and 57.5% for 6-digit classifications, outperforming leading LLMs, and proved cost-effective.

Conclusion: The research offers a strong baseline for HTS classification, encourages future machine learning advancements, and provides resources for better global trade documentation.

Abstract: Accurate classification of products under the Harmonized Tariff Schedule
(HTS) is a critical bottleneck in global trade, yet it has received little
attention from the machine learning community. Misclassification can halt
shipments entirely, with major postal operators suspending deliveries to the
U.S. due to incomplete customs documentation. We introduce the first benchmark
for HTS code classification, derived from the U.S. Customs Rulings Online
Search System (CROSS). Evaluating leading LLMs, we find that our fine-tuned
Atlas model (LLaMA-3.3-70B) achieves 40 percent fully correct 10-digit
classifications and 57.5 percent correct 6-digit classifications, improvements
of 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking.
Beyond accuracy, Atlas is roughly five times cheaper than GPT-5-Thinking and
eight times cheaper than Gemini-2.5-Pro-Thinking, and can be self-hosted to
guarantee data privacy in high-stakes trade and compliance workflows. While
Atlas sets a strong baseline, the benchmark remains highly challenging, with
only 40 percent 10-digit accuracy. By releasing both dataset and model, we aim
to position HTS classification as a new community benchmark task and invite
future work in retrieval, reasoning, and alignment.

</details>


### [21] [Instruction-Following Evaluation in Function Calling for Large Language Models](https://arxiv.org/abs/2509.18420)
*Nikolai Skripko*

Main category: cs.AI

TL;DR: IFEval-FC improves evaluation of function calling abilities by assessing adherence to format instructions embedded in JSON schema descriptions, exposing limitations in state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks do not test adherence to specific format instructions in function calling, a critical aspect for real-world AI applications.

Method: IFEval-FC integrates format adherence verifications into JSON schema descriptions and uses 750 test cases with algorithmic evaluation for objectivity.

Result: State-of-the-art models such as GPT-5 and Claude 4.1 Opus fail to consistently follow formatting instructions during function calling.

Conclusion: Current AI models exhibit deficiencies in adhering to precise instructions for function formatting, thus requiring improved methodologies for real-world functionality evaluations.

Abstract: Function calling is a core capability of large language models, essential for
AI agents. Existing benchmarks such as the Berkeley Function Calling
Leaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench
(arXiv:2501.12851) evaluate argument correctness but do not test adherence to
format instructions embedded in parameter descriptions, such as enclosing
values in double quotes or using ISO date formats.
  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)
that assesses precise instruction following in function calling. IFEval-FC
encodes verifiable formats directly within JSON schema descriptions, for
example specifying that a value must not contain punctuation. It includes 750
test cases, each consisting of a function with an embedded format for one of
its input parameters and a corresponding user query. Evaluation is fully
algorithmic, ensuring objectivity, reproducibility, and scalability.
  Our results show that even state-of-the-art proprietary models, including
GPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,
highlighting a practical limitation for real-world agent systems. The complete
codebase and data are publicly available at
https://github.com/Skripkon/IFEval-FC.

</details>


### [22] [Memory-QA: Answering Recall Questions Based on Multimodal Memories](https://arxiv.org/abs/2509.18436)
*Hongda Jiang,Xinyuan Zhang,Siddhant Garg,Rishab Arora,Shiun-Zu Kuo,Jiayang Xu,Christopher Brossman,Yue Liu,Aaron Colak,Ahmed Aly,Anuj Kumar,Xin Luna Dong*

Main category: cs.AI

TL;DR: Memory-QA is a new task requiring answering recall questions about visual content stored in multimodal memories. Pensieve, a novel pipeline, outperforms current methods in dealing with these challenges by up to 14% in QA accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of answering recall questions based on visual content stored in multimodal memories, especially utilizing temporal and location cues and accessing multiple memories.

Method: This paper introduces the Pensieve pipeline, which includes memory-specific augmentation, time- and location-aware multi-signal retrieval, and multi-memory QA fine-tuning.

Result: Pensieve achieves up to 14% higher QA accuracy compared to state-of-the-art solutions on a newly created multimodal benchmark.

Conclusion: The proposed task and solution successfully push the boundaries of utilizing multimodal memories, demonstrating superior recall question-answering capabilities in complex real-world scenarios.

Abstract: We introduce Memory-QA, a novel real-world task that involves answering
recall questions about visual content from previously stored multimodal
memories. This task poses unique challenges, including the creation of
task-oriented memories, the effective utilization of temporal and location
information within memories, and the ability to draw upon multiple memories to
answer a recall question. To address these challenges, we propose a
comprehensive pipeline, Pensieve, integrating memory-specific augmentation,
time- and location-aware multi-signal retrieval, and multi-memory QA
fine-tuning. We created a multimodal benchmark to illustrate various real
challenges in this task, and show the superior performance of Pensieve over
state-of-the-art solutions (up to 14% on QA accuracy).

</details>


### [23] [FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move Recognition and Rule Reasoning](https://arxiv.org/abs/2509.18527)
*Ziwen Chen,Zhong Wang*

Main category: cs.AI

TL;DR: FERA, an AI prototype, addresses refereeing challenges in foil fencing by using pose-based action recognition and rule-based reasoning to determine decisions and provide explanations.


<details>
  <summary>Details</summary>
Motivation: To address inaccuracies and biases in human refereeing of foil fencing, and enhance practice environments with consistent, unbiased AI assistance.

Method: FERA utilizes 2D joint position extraction, kinematic features, a Transformer for action/blade classification, and a distilled language model for rule-based decision-making.

Result: With limited labeled data, FERA outperformed baselines, achieving a macro-F1 score of 0.549 via 5-fold cross-validation, showcasing effective multi-label recognition and reasoning.

Conclusion: FERA is still in developmental stages, but demonstrates the feasibility of AI-assisted refereeing in fencing, opening doors for automated coaching and fairer gameplay.

Abstract: The sport of fencing, like many other sports, faces challenges in refereeing:
subjective calls, human errors, bias, and limited availability in practice
environments. We present FERA (Fencing Referee Assistant), a prototype AI
referee for foil fencing which integrates pose-based multi-label action
recognition and rule-based reasoning. FERA extracts 2D joint positions from
video, normalizes them, computes a 101-dimensional kinematic feature set, and
applies a Transformer for multi-label move and blade classification. To
determine priority and scoring, FERA applies a distilled language model with
encoded right-of-way rules, producing both a decision and an explanation for
each exchange. With limited hand-labeled data, a 5-fold cross-validation
achieves an average macro-F1 score of 0.549, outperforming multiple baselines,
including a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla
Transformer. While not ready for deployment, these results demonstrate a
promising path towards automated referee assistance in foil fencing and new
opportunities for AI applications, such as coaching in the field of fencing.

</details>


### [24] [LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs](https://arxiv.org/abs/2509.18557)
*Tom Pawelek,Raj Patel,Charlotte Crowell,Noorbakhsh Amiri,Sudip Mittal,Shahram Rahimi,Andy Perkins*

Main category: cs.AI

TL;DR: The paper proposes a novel security approach for agentic large language models (LLMs) by introducing "LLMZ+" which employs prompt whitelisting to restrict interactions to safe and predefined use cases, effectively addressing jailbreak attacks.


<details>
  <summary>Details</summary>
Motivation: Agentic AI presents significant security risks due to its nondeterministic behavior and privileged access to data and API tools that traditional software lacks. Current defenses rely on detecting malicious intent, which may be inadequate.

Method: The authors introduce "LLMZ+," a prompt whitelisting system that ensures only contextually safe and appropriate messages interact with agentic LLMs, preventing jailbreak attacks while maintaining operational integrity.

Result: Experiments demonstrate that LLMZ+ eliminates false positive and false negative rates, maintains seamless authorized traffic flow, and safeguards against common jailbreak prompts.

Conclusion: LLMZ+ enhances security for agentic LLMs by defining strict message boundaries, offering a streamlined and resilient solution against prevalent threats without disrupting legitimate communications.

Abstract: Compared to traditional models, agentic AI represents a highly valuable
target for potential attackers as they possess privileged access to data
sources and API tools, which are traditionally not incorporated into classical
agents. Unlike a typical software application residing in a Demilitarized Zone
(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI
(only defining a final goal, leaving the path selection to LLM). This
characteristic introduces substantial security risk to both operational
security and information security. Most common existing defense mechanism rely
on detection of malicious intent and preventing it from reaching the LLM agent,
thus protecting against jailbreak attacks such as prompt injection. In this
paper, we present an alternative approach, LLMZ+, which moves beyond
traditional detection-based approaches by implementing prompt whitelisting.
Through this method, only contextually appropriate and safe messages are
permitted to interact with the agentic LLM. By leveraging the specificity of
context, LLMZ+ guarantees that all exchanges between external users and the LLM
conform to predefined use cases and operational boundaries. Our approach
streamlines the security framework, enhances its long-term resilience, and
reduces the resources required for sustaining LLM information security. Our
empirical evaluation demonstrates that LLMZ+ provides strong resilience against
the most common jailbreak prompts. At the same time, legitimate business
communications are not disrupted, and authorized traffic flows seamlessly
between users and the agentic LLM. We measure the effectiveness of approach
using false positive and false negative rates, both of which can be reduced to
0 in our experimental setting.

</details>


### [25] [Solving Math Word Problems Using Estimation Verification and Equation Generation](https://arxiv.org/abs/2509.18565)
*Mitchell Piehl,Dillon Wilson,Ananya Kalita,Jugal Kalita*

Main category: cs.AI

TL;DR: Large Language Models (LLMs) struggle with Math Word Problems (MWPs) due to their demanding reasoning requirements. This paper introduces a method utilizing decomposition, external equation solvers, and iterative verification to significantly improve LLM performance on MWPs.


<details>
  <summary>Details</summary>
Motivation: Although LLMs perform well in many scenarios, they often have difficulty solving MWPs due to their need for advanced reasoning and mathematical understanding. The paper seeks to address these limitations by enhancing the problem-solving capabilities of LLMs.

Method: The approach first prompts the LLM to decompose the question and generate equations, then relies on an external symbolic equation solver for answers. Additionally, the LLM solves the problem a second time to estimate the answer independently, which is used for verification. Iterative rectification takes place if verification fails.

Result: The method achieved state-of-the-art performance on numeric and algebraic MWPs, improving prior results by nearly 2% on average, while also demonstrating success on trigonometric MWPs—a previously unexplored area.

Conclusion: The proposed technique not only enhances the reasoning abilities of LLMs but also sets new performance benchmarks for MWPs. The introduction of two new datasets further contributes to advancing this field of research.

Abstract: Large Language Models (LLMs) excel at various tasks, including
problem-solving and question-answering. However, LLMs often find Math Word
Problems (MWPs) challenging because solving them requires a range of reasoning
and mathematical abilities with which LLMs seem to struggle. Recent efforts
have helped LLMs solve more complex MWPs with improved prompts. This study
proposes a novel method that initially prompts an LLM to create equations from
a decomposition of the question, followed by using an external symbolic
equation solver to produce an answer. To ensure the accuracy of the obtained
answer, inspired by an established recommendation of math teachers, the LLM is
instructed to solve the MWP a second time, but this time with the objective of
estimating the correct answer instead of solving it exactly. The estimation is
then compared to the generated answer to verify. If verification fails, an
iterative rectification process is employed to ensure the correct answer is
eventually found. This approach achieves new state-of-the-art results on
datasets used by prior published research on numeric and algebraic MWPs,
improving the previous best results by nearly two percent on average. In
addition, the approach obtains satisfactory results on trigonometric MWPs, a
task not previously attempted to the authors' best knowledge. This study also
introduces two new datasets, SVAMPClean and Trig300, to further advance the
testing of LLMs' reasoning abilities.

</details>


### [26] [Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents](https://arxiv.org/abs/2509.18633)
*Yara Mohajerani*

Main category: cs.AI

TL;DR: The paper introduces a geospatial agent-based model combining climate hazard data and economic adaptation strategies to assess climate risks and adaptation costs.


<details>
  <summary>Details</summary>
Motivation: To better understand the systemic risks posed by climate hazards and enable adaptive strategies in economic systems.

Method: An agent-based model integrating Mesa spatial modeling, CLIMADA impact assessment, and evolutionary learning for economic agents.

Result: Firms adapt, restoring production levels despite climate stress, but systemic risks cause supply chain disruptions and price increases.

Conclusion: This open-source framework aids financial institutions and firms in quantifying climate risks and exploring adaptation strategies effectively.

Abstract: Climate risk assessment requires modelling complex interactions between
spatially heterogeneous hazards and adaptive economic systems. We present a
novel geospatial agent-based model that integrates climate hazard data with
evolutionary learning for economic agents. Our framework combines Mesa-based
spatial modelling with CLIMADA climate impact assessment, introducing adaptive
learning behaviours that allow firms to evolve strategies for budget
allocation, pricing, wages, and risk adaptation through fitness-based selection
and mutation. We demonstrate the framework using riverine flood projections
under RCP8.5 until 2100, showing that evolutionary adaptation enables firms to
converge with baseline (no hazard) production levels after decades of
disruption due to climate stress. Our results reveal systemic risks where even
agents that are not directly exposed to floods face impacts through supply
chain disruptions, with the end-of-century average price of goods 5.6% higher
under RCP8.5 compared to the baseline. This open-source framework provides
financial institutions and companies with tools to quantify both direct and
cascading climate risks while evaluating cost-effective adaptation strategies.

</details>


### [27] [TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2509.18667)
*Qiao Xiao,Hong Ting Tsang,Jiaxin Bai*

Main category: cs.AI

TL;DR: The paper introduces TERAG, a cost-effective graph-based RAG framework that uses Personalized PageRank to reduce token usage while maintaining 80% of the accuracy of existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based RAG systems often suffer from high costs associated with LLM token usage during graph construction, limiting large-scale adoption.

Method: TERAG incorporates Personalized PageRank during the retrieval phase to construct informative graphs efficiently and with significantly lower token usage.

Result: TERAG achieves at least 80% of the accuracy of popular graph-based RAG methods while using a mere 3%-11% of their token output.

Conclusion: TERAG offers an effective and economical alternative for graph-based RAG, making it more accessible for large-scale applications.

Abstract: Graph-based Retrieval-augmented generation (RAG) has become a widely studied
approach for improving the reasoning, accuracy, and factuality of Large
Language Models. However, many existing graph-based RAG systems overlook the
high cost associated with LLM token usage during graph construction, hindering
large-scale adoption. To address this, we propose TERAG, a simple yet effective
framework designed to build informative graphs at a significantly lower cost.
Inspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the
retrieval phase, and we achieve at least 80% of the accuracy of widely used
graph-based RAG methods while consuming only 3%-11% of the output tokens.

</details>


### [28] [Implementation of airborne ML models with semantics preservation](https://arxiv.org/abs/2509.18681)
*Nicolas Valot,Louis Fabre,Benjamin Lesage,Ammar Mechouche,Claire Pagetti*

Main category: cs.AI

TL;DR: The paper explores the use of Machine Learning (ML) in airborne systems while ensuring compliance with European safety regulations. It introduces a concept called Machine Learning Model Description (MLMD) to maintain semantics preservation and evaluates this in real-world industrial scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address the safety and regulatory compliance of ML-based systems in airborne applications, ensuring their capabilities fulfill their intended functions in operational environments.

Method: The paper presents a concept called Machine Learning Model Description (MLMD) and refines the notion of semantics preservation to support accurate model replication. It demonstrates these contributions through industrial use cases.

Result: The study showcases the effectiveness of MLMD in building and comparing target models across several use cases, ensuring semantic accuracy during replication.

Conclusion: The paper concludes that MLMD and semantics preservation are critical for ensuring safe and reliable integration of ML-based systems in the aerospace industry, making them compliant with regulatory standards.

Abstract: Machine Learning (ML) may offer new capabilities in airborne systems.
However, as any piece of airborne systems, ML-based systems will be required to
guarantee their safe operation. Thus, their development will have to be
demonstrated to be compliant with the adequate guidance. So far, the European
Union Aviation Safety Agency (EASA) has published a concept paper and an
EUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level
objectives to confirm the ML model achieves its intended function and maintains
training performance in the target environment. The paper aims to clarify the
difference between an ML model and its corresponding unambiguous description,
referred to as the Machine Learning Model Description (MLMD). It then refines
the essential notion of semantics preservation to ensure the accurate
replication of the model. We apply our contributions to several industrial use
cases to build and compare several target models.

</details>


### [29] [Advances in Large Language Models for Medicine](https://arxiv.org/abs/2509.18690)
*Zhiyu Kan,Wensheng Gan,Zhenlian Qi,Philip S. Yu*

Main category: cs.AI

TL;DR: This paper reviews the application of large language models (LLMs) in the medical field, covering training methodologies, adaptations, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the growing impact of large language models in medicine and systematically examine their developments, applications, and limitations.

Method: The study uses a systematic review to analyze existing research on medical LLMs, categorizing models by training approach and evaluation methods.

Result: It categorizes medical LLMs into three types, divides evaluation approaches into two categories, identifies current challenges, and suggests solutions and future research directions.

Conclusion: The research underscores the importance of developing medical LLMs, outlines their advancements, and provides actionable insights for future studies.

Abstract: Artificial intelligence (AI) technology has advanced rapidly in recent years,
with large language models (LLMs) emerging as a significant breakthrough. LLMs
are increasingly making an impact across various industries, with the medical
field standing out as the most prominent application area. This paper
systematically reviews the up-to-date research progress of LLMs in the medical
field, providing an in-depth analysis of training techniques for large medical
models, their adaptation in healthcare settings, related applications, as well
as their strengths and limitations. Furthermore, it innovatively categorizes
medical LLMs into three distinct types based on their training methodologies
and classifies their evaluation approaches into two categories. Finally, the
study proposes solutions to existing challenges and outlines future research
directions based on identified issues in the field of medical LLMs. By
systematically reviewing previous and advanced research findings, we aim to
highlight the necessity of developing medical LLMs, provide a deeper
understanding of their current state of development, and offer clear guidance
for subsequent research.

</details>


### [30] [Autonomous Data Agents: A New Opportunity for Smart Data](https://arxiv.org/abs/2509.18710)
*Yanjie Fu,Dongjie Wang,Wangyang Ying,Xiangliang Zhang,Huan Liu,Jian Pei*

Main category: cs.AI

TL;DR: The paper introduces DataAgents, autonomous systems integrating large language model (LLM) reasoning, task decomposition, and tool usage to transform unstructured data into actionable knowledge. It highlights their role in automating and scaling complex data operations.


<details>
  <summary>Details</summary>
Motivation: Growing scale and complexity of data make conventional methods for data preparation, transformation, and analysis cumbersome, labor-intensive, and difficult to scale. Aligning AI with data workflows is crucial to leverage its learning capabilities.

Method: Proposes the concept of Autonomous DataAgents combining LLM reasoning with task decomposition, action reasoning, grounding actions to code/tools, and executing workflows. It analyzes their architecture, training, and operational strategies.

Result: DataAgents demonstrate capabilities for diverse data operations such as preprocessing, transformation, augmentation, repairs, and retrieval—empowering efficient data-to-knowledge systems.

Conclusion: DataAgents represent a paradigm shift towards scalable autonomous systems capable of opaque data management and knowledge extraction. The paper advocates further research into action workflow optimization, benchmarks, privacy safeguards, scalability tradeoffs, and trustworthy operations.

Abstract: As data continues to grow in scale and complexity, preparing, transforming,
and analyzing it remains labor-intensive, repetitive, and difficult to scale.
Since data contains knowledge and AI learns knowledge from it, the alignment
between AI and data is essential. However, data is often not structured in ways
that are optimal for AI utilization. Moreover, an important question arises:
how much knowledge can we pack into data through intensive data operations?
Autonomous data agents (DataAgents), which integrate LLM reasoning with task
decomposition, action reasoning and grounding, and tool calling, can
autonomously interpret data task descriptions, decompose tasks into subtasks,
reason over actions, ground actions into python code or tool calling, and
execute operations. Unlike traditional data management and engineering tools,
DataAgents dynamically plan workflows, call powerful tools, and adapt to
diverse data tasks at scale. This report argues that DataAgents represent a
paradigm shift toward autonomous data-to-knowledge systems. DataAgents are
capable of handling collection, integration, preprocessing, selection,
transformation, reweighing, augmentation, reprogramming, repairs, and
retrieval. Through these capabilities, DataAgents transform complex and
unstructured data into coherent and actionable knowledge. We first examine why
the convergence of agentic AI and data-to-knowledge systems has emerged as a
critical trend. We then define the concept of DataAgents and discuss their
architectural design, training strategies, as well as the new skills and
capabilities they enable. Finally, we call for concerted efforts to advance
action workflow optimization, establish open datasets and benchmark ecosystems,
safeguard privacy, balance efficiency with scalability, and develop trustworthy
DataAgent guardrails to prevent malicious actions.

</details>


### [31] [Experience Scaling: Post-Deployment Evolution For Large Language Models](https://arxiv.org/abs/2509.18771)
*Xingkun Yin,Kaibin Huang,Dong In Kim,Hongyang Du*

Main category: cs.AI

TL;DR: The paper introduces "experience scaling," a framework allowing large language models (LLMs) to continuously improve post-deployment via autonomous interactions and knowledge sharing.


<details>
  <summary>Details</summary>
Motivation: Current advances in large language models are facing limits due to the exhaustion of human-generated text and diminishing returns from scaling traditional factors like data and model size.

Method: The proposed framework emphasizes autonomous interaction with the environment, collaborative sharing of accumulated knowledge, and periodic refinement of stored content to enhance efficiency and relevance.

Result: Experience scaling shows improved accuracy, sustained performance over time, and adaptability to new tasks through simulations in realistic scenarios.

Conclusion: Structured post-deployment learning through experience scaling can overcome the limits of static human-generated data and support scalable progress in LLMs' intelligence.

Abstract: Scaling model size, training data, and compute power have driven advances in
large language models (LLMs), but these approaches are reaching saturation as
human-generated text is exhausted and further gains diminish. We propose
experience scaling, a framework for continuous post-deployment evolution for
LLMs through autonomous interaction with the environment and collaborative
sharing of accumulated experience. The framework captures raw interactions,
distills them into compact, reusable knowledge, and periodically refines stored
content to preserve relevance and efficiency. We validate the framework in
simulated real-world scenarios involving generalization to previously unseen
but related tasks, repetitive queries, and over-saturated knowledge stores.
Across all settings, experience scaling improves accuracy, sustains performance
over time, and maintains gains when applied to novel situations. These results
demonstrate that structured post-deployment learning can extend LLM
capabilities beyond the limits of static human-generated data, offering a
scalable path for continued intelligence progress.

</details>


### [32] [The AGNTCY Agent Directory Service: Architecture and Implementation](https://arxiv.org/abs/2509.18787)
*Luca Muscariello,Vijoy Pandey,Ramiz Polic*

Main category: cs.AI

TL;DR: The paper introduces the Agent Directory Service (ADS), a distributed system for discovering AI agent capabilities using modern technologies like content-addressed storage and cryptographic signing.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for efficient, verifiable, and scalable discovery of agent capabilities in Multi-Agent Systems (MAS).

Method: ADS employs a Kademlia-based Distributed Hash Table (DHT) for indexing, integrates Sigstore for provenance, uses OCI/ORAS for artifact distribution, and operates under the Open Agentic Schema Framework (OASF), ensuring schema-driven extensibility.

Result: The paper establishes the architectural design, explains its storage, discovery, security strategies, and situates ADS as a key player in agent registry and interoperability initiatives.

Conclusion: ADS enhances the discovery and interoperability of AI agents, establishing an efficient framework addressing security, performance, and extensibility within heterogeneous MAS.

Abstract: The Agent Directory Service (ADS) is a distributed directory for the
discovery of AI agent capabilities, metadata, and provenance. It leverages
content-addressed storage, hierarchical taxonomies, and cryptographic signing
to enable efficient, verifiable, and multi-dimensional discovery across
heterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema
Framework (OASF), ADS decouples capability indexing from content location
through a two-level mapping realized over a Kademlia-based Distributed Hash
Table (DHT). It reuses mature OCI / ORAS infrastructure for artifact
distribution, integrates Sigstore for provenance, and supports schema-driven
extensibility for emerging agent modalities (LLM prompt agents, MCP servers,
A2A-enabled components). This paper formalizes the architectural model,
describes storage and discovery layers, explains security and performance
properties, and positions ADS within the broader landscape of emerging agent
registry and interoperability initiatives.

</details>


### [33] [Bounded PCTL Model Checking of Large Language Model Outputs](https://arxiv.org/abs/2509.18836)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb*

Main category: cs.AI

TL;DR: The paper introduces LLMCHECKER for verifying PCTL properties in LLM text generation via a model-checking approach focusing on maximal cumulative probabilities among selected tokens.


<details>
  <summary>Details</summary>
Motivation: LLMs generate text based on probabilistic logic, but the consistency and quality of the text often vary. Current methodologies lack formal verification tools to check properties of text generation processes.

Method: The authors propose a model-checking-based verification called LLMCHECKER. It uses $\alpha$-$k$-bounded text generation to focus on the top-$k$ tokens with a cumulative probability above a threshold $\alpha$, allowing evaluation of various quantification aspects like quality or bias.

Result: The paper demonstrates LLMCHECKER’s effectiveness with PCTL-based verification on multiple LLMs such as Llama, Gemma, and BERT, revealing the potential for deeper insight into LLM behavior.

Conclusion: LLMCHECKER is an innovative tool enabling formal verification of text generation properties, establishing a precedent for incorporating PCTL-based methods in LLM evaluation.

Abstract: In this paper, we introduce LLMCHECKER, a model-checking-based verification
method to verify the probabilistic computation tree logic (PCTL) properties of
an LLM text generation process. We empirically show that only a limited number
of tokens are typically chosen during text generation, which are not always the
same. This insight drives the creation of $\alpha$-$k$-bounded text generation,
narrowing the focus to the $\alpha$ maximal cumulative probability on the
top-$k$ tokens at every step of the text generation process. Our verification
method considers an initial string and the subsequent top-$k$ tokens while
accommodating diverse text quantification methods, such as evaluating text
quality and biases. The threshold $\alpha$ further reduces the selected tokens,
only choosing those that exceed or meet it in cumulative probability.
LLMCHECKER then allows us to formally verify the PCTL properties of
$\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in
several LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our
knowledge, this is the first time PCTL-based model checking has been used to
check the consistency of the LLM text generation process.

</details>


### [34] [Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning](https://arxiv.org/abs/2509.18846)
*Hong-Jie Dai,Zheng-Hao Li,An-Tai Lu,Bo-Tsz Shain,Ming-Ta Li,Tatheer Hussain Mir,Kuang-Te Wang,Min-I Su,Pei-Kang Liu,Ming-Ju Tsai*

Main category: cs.AI

TL;DR: The paper introduces a framework using open-source large language models (LLMs) for accurate ICD-10-CM code prediction, addressing challenges like model selection, data redundancy, and context-aware input design.


<details>
  <summary>Details</summary>
Motivation: ICD coding is critical for healthcare operations but is labor-intensive and error-prone; existing automated solutions, like LLMs, face challenges in achieving effectiveness.

Method: The framework employs principled model selection with LLM-as-judge evaluation, redundancy-aware data sampling, and structured input design to enhance ICD-10-CM code prediction.

Result: Experiments on two datasets showed that the fine-tuned models significantly outperformed baseline LLMs, and including more clinical sections in the input improved prediction performance.

Conclusion: The framework offers a scalable and practical solution for real-world automated ICD-10-CM coding by combining optimized LLMs, efficient data use, and contextual insights.

Abstract: Accurate International Classification of Diseases (ICD) coding is critical
for clinical documentation, billing, and healthcare analytics, yet it remains a
labour-intensive and error-prone task. Although large language models (LLMs)
show promise in automating ICD coding, their challenges in base model
selection, input contextualization, and training data redundancy limit their
effectiveness. We propose a modular framework for ICD-10 Clinical Modification
(ICD-10-CM) code prediction that addresses these challenges through principled
model selection, redundancy-aware data sampling, and structured input design.
The framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce
aggregation to assess and rank open-source LLMs based on their intrinsic
comprehension of ICD-10-CM code definitions. We introduced embedding-based
similarity measures, a redundancy-aware sampling strategy to remove
semantically duplicated discharge summaries. We leverage structured discharge
summaries from Taiwanese hospitals to evaluate contextual effects and examine
section-wise content inclusion under universal and section-specific modelling
paradigms. Experiments across two institutional datasets demonstrate that the
selected base model after fine-tuning consistently outperforms baseline LLMs in
internal and external evaluations. Incorporating more clinical sections
consistently improves prediction performance. This study uses open-source LLMs
to establish a practical and principled approach to ICD-10-CM code prediction.
The proposed framework provides a scalable, institution-ready solution for
real-world deployment of automated medical coding systems by combining informed
model selection, efficient data refinement, and context-aware prompting.

</details>


### [35] [MAPO: Mixed Advantage Policy Optimization](https://arxiv.org/abs/2509.18849)
*Wenke Huang,Quan Zhang,Yiyang Fang,Jian Liang,Xuankun Rong,Huanjin Yao,Guancheng Wan,Ke Liang,Wenwen He,Mingjun Li,Leszek Rutkowski,Mang Ye,Bo Du,Dacheng Tao*

Main category: cs.AI

TL;DR: The paper introduces Mixed Advantage Policy Optimization (MAPO) to address issues in trajectory ranking during reinforcement learning for foundation models, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning methods for foundation models face challenges with advantage reversion and advantage mirror problems during trajectory ranking, which limits performance improvement.

Method: The paper proposes MAPO, which dynamically adjusts the advantage function by considering trajectory certainty and sample characteristics, using advantage percent deviation for high-certainty samples.

Result: MAPO demonstrates superior performance on reasoning tasks compared to state-of-the-art methods and is validated through ablation studies.

Conclusion: MAPO effectively resolves trajectory ranking issues, enhancing the advantage allocation mechanism and improving foundation model performance in reasoning tasks.

Abstract: Recent advances in reinforcement learning for foundation models, such as
Group Relative Policy Optimization (GRPO), have significantly improved the
performance of foundation models on reasoning tasks. Notably, the advantage
function serves as a central mechanism in GRPO for ranking the trajectory
importance. However, existing explorations encounter both advantage reversion
and advantage mirror problems, which hinder the reasonable advantage allocation
across different query samples. In this work, we propose an easy but effective
GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the
trajectory appears with different certainty and propose the advantage percent
deviation for samples with high-certainty trajectories. Furthermore, we
dynamically reweight the advantage function for samples with varying trajectory
certainty, thereby adaptively configuring the advantage function to account for
sample-specific characteristics. Comparison with related state-of-the-art
methods, along with ablation studies on different advantage variants, validates
the effectiveness of our approach.

</details>


### [36] [Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling](https://arxiv.org/abs/2509.18864)
*Yingxin Li,Jianbo Zhao,Xueyu Ren,Jie Tang,Wangjie You,Xu Chen,Kan Zhou,Chao Feng,Jiao Ran,Yuan Meng,Zhi Wang*

Main category: cs.AI

TL;DR: The paper introduces ProfileBench, a benchmark for user profiling using Large Language Models (LLMs) and proposes Conf-Profile, a two-stage confidence-driven reasoning framework to enhance profiling accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of user profiling with LLMs, which lacks comprehensive benchmarks and suffers from issues like noisy user data and the absence of large-scale ground-truth labels.

Method: The authors introduce ProfileBench as an industrial benchmark and develop Conf-Profile, a two-stage framework that synthesizes high-quality labels using LLMs with confidence indicators, followed by techniques like confidence-weighted voting, calibration, and unsupervised reinforcement learning.

Result: Conf-Profile demonstrates significant improvement by achieving a 13.97 F1-score boost on the Qwen3-8B model, showcasing its effectiveness for label-free and reliable user profiling.

Conclusion: The proposed method makes substantial advancements in user profiling, offering a robust and reliable framework for leveraging LLMs on heterogeneous and noisy user data without ground-truth labels.

Abstract: User profiling, as a core technique for user understanding, aims to infer
structural attributes from user information. Large Language Models (LLMs)
provide a promising avenue for user profiling, yet the progress is hindered by
the lack of comprehensive benchmarks. To bridge this gap, we propose
ProfileBench, an industrial benchmark derived from a real-world video platform,
encompassing heterogeneous user data and a well-structured profiling taxonomy.
However, the profiling task remains challenging due to the difficulty of
collecting large-scale ground-truth labels, and the heterogeneous and noisy
user information can compromise the reliability of LLMs. To approach label-free
and reliable user profiling, we propose a Confidence-driven Profile reasoning
framework Conf-Profile, featuring a two-stage paradigm. We first synthesize
high-quality labels by leveraging advanced LLMs with confidence hints, followed
by confidence-weighted voting for accuracy improvement and confidence
calibration for a balanced distribution. The multiple profile results,
rationales, and confidence scores are aggregated and distilled into a
lightweight LLM. We further enhance the reasoning ability via confidence-guided
unsupervised reinforcement learning, which exploits confidence for difficulty
filtering, quasi-ground truth voting, and reward weighting. Experimental
results demonstrate that Conf-Profile delivers substantial performance through
the two-stage training, improving F1 by 13.97 on Qwen3-8B.

</details>


### [37] [Memory in Large Language Models: Mechanisms, Evaluation and Evolution](https://arxiv.org/abs/2509.18868)
*Dianxing Zhang,Wendong Li,Kani Song,Jiaye Lu,Gang Li,Liuchun Yang,Sheng Li*

Main category: cs.AI

TL;DR: This paper defines LLM memory using a taxonomy and operational framework for evaluation, governance, and update processes to enhance reproducibility and comparability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to standardize how LLM memory is defined, evaluated, and governed to enable meaningful comparisons and improve deployment reliability.

Method: It introduces a taxonomy for LLM memory, an evaluation framework across different retrieval settings, and mechanisms for updating and forgetting memory, integrating governance and leakage auditing.

Result: The paper provides a unified definition and protocol for evaluating LLM memory, with layers for different memory types and testable propositions for future research.

Conclusion: This framework creates a structured, reproducible, and auditable system for investigating and deploying LLM memory, strengthening the field’s methodological foundation.

Abstract: Under a unified operational definition, we define LLM memory as a persistent
state written during pretraining, finetuning, or inference that can later be
addressed and that stably influences outputs. We propose a four-part taxonomy
(parametric, contextual, external, procedural/episodic) and a memory quadruple
(location, persistence, write/access path, controllability). We link mechanism,
evaluation, and governance via the chain write -> read -> inhibit/update. To
avoid distorted comparisons across heterogeneous setups, we adopt a
three-setting protocol (parametric only, offline retrieval, online retrieval)
that decouples capability from information availability on the same data and
timeline. On this basis we build a layered evaluation: parametric (closed-book
recall, edit differential, memorization/privacy), contextual (position curves
and the mid-sequence drop), external (answer correctness vs snippet
attribution/faithfulness), and procedural/episodic (cross-session consistency
and timeline replay, E MARS+). The framework integrates temporal governance and
leakage auditing (freshness hits, outdated answers, refusal slices) and
uncertainty reporting via inter-rater agreement plus paired tests with
multiple-comparison correction. For updating and forgetting, we present DMM
Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),
and RAG to form an auditable loop covering admission thresholds, rollout,
monitoring, rollback, and change audits, with specs for timeliness, conflict
handling, and long-horizon consistency. Finally, we give four testable
propositions: minimum identifiability; a minimal evaluation card; causally
constrained editing with verifiable forgetting; and when retrieval with
small-window replay outperforms ultra-long-context reading. This yields a
reproducible, comparable, and governable coordinate system for research and
deployment.

</details>


### [38] [LongCat-Flash-Thinking Technical Report](https://arxiv.org/abs/2509.18883)
*Meituan LongCat Team,Anchun Gui,Bei Li,Bingyang Tao,Bole Zhou,Borun Chen,Chao Zhang,Chao Zhang,Chengcheng Han,Chenhui Yang,Chi Zhang,Chong Peng,Chuyu Zhang,Cong Chen,Fengcun Li,Gang Xu,Guoyuan Lin,Hao Jiang,Hao Liang,Haomin Fu,Haoxiang Ma,Hong Liu,Hongyan Hao,Hongyin Tang,Hongyu Zang,Hongzhi Ni,Hui Su,Jiahao Liu,Jiahuan Li,Jialin Liu,Jianfei Zhang,Jianhao Xu,Jianing Wang,Jiaqi Sun,Jiaqi Zhang,Jiarong Shi,Jiawei Yang,Jingang Wang,Jinrui Ding,Jun Kuang,Jun Xu,Ke He,Kefeng Zhang,Keheng Wang,Keqing He,Li Wei,Liang Shi,Lin Qiu,Lingbin Kong,Lingchuan Liu,Linsen Guo,Longfei An,Mai Xia,Meng Zhou,Mengshen Zhu,Peng Pei,Pengcheng Jia,Qi Gu,Qi Guo,Qiong Huang,Quan Chen,Quanchi Weng,Rongxiang Weng,Ruichen Shao,Rumei Li,Shanglin Lei,Shuai Du,Shuaikang Liu,Shuang Zhou,Shuhao Hu,Siyu Xu,Songshan Gong,Tao Liang,Tianhao Hu,Wei He,Wei Shi,Wei Wang,Wei Wu,Wei Zhuo,Weifeng Tang,Wenjie Shi,Wenlong Zhu,Xi Su,Xiangcheng Liu,Xiangyu Xi,Xiangzhou Huang,Xiao Liu,Xiaochen Jiang,Xiaowei Shi,Xiaowen Shi,Xiaoyu Li,Xin Chen,Xinyue Zhao,Xuan Huang,Xuemiao Zhang,Xuezhi Cao,Xunliang Cai,Yajie Zhang,Yang Chen,Yang Liu,Yang Liu,Yang Zheng,Yaoming Wang,Yaqi Huo,Yerui Sun,Yifan Lu,Yiyang Li,Youshao Xiao,Yuanzhe Lei,Yuchen Xie,Yueqing Sun,Yufei Zhang,Yuhuai Wei,Yulei Qian,Yunke Zhao,Yuqing Ding,Yuwei Jiang,Zhaohua Yang,Zhengyu Chen,Zhijian Liu,Zhikang Xia,Zhongda Su,Ziran Li,Ziwen Wang,Ziyuan Zhuang,Zongyu Wang,Zunyuan Yang*

Main category: cs.AI

TL;DR: This paper introduces LongCat-Flash-Thinking, a 560-billion-parameter open-source reasoning model that employs a novel MoE architecture and advanced training techniques to excel in both efficiency and reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for efficient and high-performing reasoning models capable of handling complex tasks in various domains, such as STEM, code, and agentic reasoning.

Method: The model was created using a meticulously crafted training pipeline that starts with Chain-of-Thought (CoT) data cold-start, employs domain-parallel training to optimize distinct domain models individually, and integrates them using a Pareto-optimal fusion process powered by the DORA RL framework.

Result: LongCat-Flash-Thinking demonstrated state-of-the-art performance in complex reasoning tasks among open-source models, with improved efficiency in agentic reasoning where token consumption was reduced by 64.5% on AIME-25 tasks without compromising accuracy.

Conclusion: The work establishes that advanced training techniques and model architectures can significantly enhance both performance and efficiency in reasoning models, paving the way for further research in agentic AI and reasoning systems.

Abstract: We present LongCat-Flash-Thinking, an efficient 560-billion-parameter
open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities
are cultivated through a meticulously crafted training process, beginning with
long Chain-of-Thought (CoT) data cold-start and culminating in large-scale
Reinforcement Learning (RL). We first employ a well-designed cold-start
training strategy, which significantly enhances the reasoning potential and
equips the model with specialized skills in both formal and agentic reasoning.
Then, a core innovation is our domain-parallel training scheme, which decouples
optimization across distinct domains (e.g., STEM, Code, Agentic) and
subsequently fuses the resulting expert models into a single, nearly
Pareto-optimal model. This entire process is powered by our Dynamic
ORchestration for Asynchronous rollout (DORA) system, a large-scale RL
framework that delivers a greater than threefold training speedup over
synchronous methods on tens of thousands of accelerators. As a result,
LongCat-Flash-Thinking achieves state-of-the-art performance among open-source
models on a suite of complex reasoning tasks. The model exhibits exceptional
efficiency in agentic reasoning, reducing average token consumption by 64.5%
(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We
release LongCat-Flash-Thinking to promote further advances in reasoning systems
and agentic AI research.

</details>


### [39] [How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective](https://arxiv.org/abs/2509.18905)
*Songsong Yu,Yuxin Chen,Hao Ju,Lianjie Jia,Fuxi Zhang,Shaofei Huang,Yuhan Wu,Rundi Cui,Binghao Ran,Zaibin Zhang,Zhedong Zheng,Zhipeng Zhang,Yifan Wang,Lin Song,Lijun Wang,Yanwei Li,Ying Shan,Huchuan Lu*

Main category: cs.AI

TL;DR: The paper investigates VSR in Vision-Language Models (VLMs), categorizes spatial intelligence levels, and introduces SIBench, a benchmark for spatial tasks, showcasing the performance gap in planning and understanding.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Vision-Language Models (VLMs) in achieving human-level Visual Spatial Reasoning (VSR) and advance the field of spatial intelligence.

Method: Systematic investigation of VSR in VLMs involving a review of methodologies, categorization, and creation of the SIBench benchmark based on nearly 20 datasets encompassing 23 tasks.

Result: State-of-the-art VLMs excel in perceptual tasks but perform poorly in spatial reasoning and planning, revealing gaps in numerical estimation, multi-view reasoning, temporal dynamics, and spatial imagination.

Conclusion: The paper highlights challenges in achieving spatial intelligence, provides a roadmap for research, and introduces SIBench, setting the foundation for future advancements in the field.

Abstract: Visual Spatial Reasoning (VSR) is a core human cognitive ability and a
critical requirement for advancing embodied intelligence and autonomous
systems. Despite recent progress in Vision-Language Models (VLMs), achieving
human-level VSR remains highly challenging due to the complexity of
representing and reasoning over three-dimensional space. In this paper, we
present a systematic investigation of VSR in VLMs, encompassing a review of
existing methodologies across input modalities, model architectures, training
strategies, and reasoning mechanisms. Furthermore, we categorize spatial
intelligence into three levels of capability, ie, basic perception, spatial
understanding, spatial planning, and curate SIBench, a spatial intelligence
benchmark encompassing nearly 20 open-source datasets across 23 task settings.
Experiments with state-of-the-art VLMs reveal a pronounced gap between
perception and reasoning, as models show competence in basic perceptual tasks
but consistently underperform in understanding and planning tasks, particularly
in numerical estimation, multi-view reasoning, temporal dynamics, and spatial
imagination. These findings underscore the substantial challenges that remain
in achieving spatial intelligence, while providing both a systematic roadmap
and a comprehensive benchmark to drive future research in the field. The
related resources of this study are accessible at
https://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.

</details>


### [40] [Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning](https://arxiv.org/abs/2509.18942)
*Xiao Han,Zimo Zhao,Wanyu Wang,Maolin Wang,Zitao Liu,Yi Chang,Xiangyu Zhao*

Main category: cs.AI

TL;DR: The paper introduces DEAL, an improved fine-tuning framework for Large Language Models (LLMs), addressing challenges like catastrophic forgetting and data efficiency.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs for specific tasks is crucial for targeted applications, but existing methods have limitations like catastrophic forgetting and low data efficiency, which hinder their usability.

Method: The paper proposes DEAL, a framework combining Low-Rank Adaptation (LoRA) with continuous fine-tuning strategies, and includes modules for knowledge retention and adaptive parameter updates.

Result: Experiments on 15 datasets demonstrate that DEAL significantly outperforms baseline fine-tuning methods in task accuracy and resource efficiency.

Conclusion: DEAL shows substantial potential in improving continual adaptation of LLMs, offering better task performance and resource-efficiency in fine-tuning approaches.

Abstract: Recent advancements in Large Language Models (LLMs) have emphasized the
critical role of fine-tuning (FT) techniques in adapting LLMs to specific
tasks, especially when retraining from scratch is computationally infeasible.
Fine-tuning enables LLMs to leverage task- or domain-specific data, producing
models that more effectively meet the requirements of targeted applications.
However, con- ventional FT approaches often suffer from catastrophic forgetting
and suboptimal data efficiency, limiting their real-world applicability. To
address these challenges, this paper proposes DEAL, a novel framework that
integrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy.
By incorporating knowledge retention and adaptive parameter update modules, the
framework mitigates the lim- itations of existing FT methods while maintaining
efficiency in privacy-preserving settings. Experiments on 15 diverse datasets
show that DEAL consistently outper- forms baseline methods, yielding
substantial gains in task accuracy and resource efficiency. These findings
demonstrate the potential of our approach to advance continual adaptation in
LLMs by enhancing task performance while improving resource efficiency.

</details>


### [41] [LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions](https://arxiv.org/abs/2509.18970)
*Xixun Lin,Yucheng Ning,Jingwen Zhang,Yan Dong,Yilong Liu,Yongxuan Wu,Xiaohua Qi,Nan Sun,Yanmin Shang,Pengfei Cao,Lixin Zou,Xu Chen,Chuan Zhou,Jia Wu,Shirui Pan,Bin Wang,Yanan Cao,Kai Chen,Songlin Hu,Li Guo*

Main category: cs.AI

TL;DR: This paper provides a comprehensive survey on hallucinations in LLM-based agents, proposing a taxonomy, analyzing causes, and summarizing existing approaches for mitigation.


<details>
  <summary>Details</summary>
Motivation: To address vulnerabilities in LLM-based agents caused by hallucination issues that impact performance and reliability.

Method: The authors analyze the workflow of LLM agents, propose a taxonomy, examine 18 causes of hallucinations, and review mitigation strategies from prior studies.

Result: They identify distinct types of hallucinations, their causes, and compile existing approaches for detecting and reducing errors caused by hallucinations.

Conclusion: The study serves as a foundational resource to inspire future research aimed at creating robust and reliable LLM-based systems.

Abstract: Driven by the rapid advancements of Large Language Models (LLMs), LLM-based
agents have emerged as powerful intelligent systems capable of human-like
cognition, reasoning, and interaction. These agents are increasingly being
deployed across diverse real-world applications, including student education,
scientific research, and financial analysis. However, despite their remarkable
potential, LLM-based agents remain vulnerable to hallucination issues, which
can result in erroneous task execution and undermine the reliability of the
overall system design. Addressing this critical challenge requires a deep
understanding and a systematic consolidation of recent advances on LLM-based
agents. To this end, we present the first comprehensive survey of
hallucinations in LLM-based agents. By carefully analyzing the complete
workflow of agents, we propose a new taxonomy that identifies different types
of agent hallucinations occurring at different stages. Furthermore, we conduct
an in-depth examination of eighteen triggering causes underlying the emergence
of agent hallucinations. Through a detailed review of a large number of
existing studies, we summarize approaches for hallucination mitigation and
detection, and highlight promising directions for future research. We hope this
survey will inspire further efforts toward addressing hallucinations in
LLM-based agents, ultimately contributing to the development of more robust and
reliable agent systems.

</details>


### [42] [From latent factors to language: a user study on LLM-generated explanations for an inherently interpretable matrix-based recommender system](https://arxiv.org/abs/2509.18980)
*Maxime Manderlier,Fabian Lecron,Olivier Vu Thanh,Nicolas Gillis*

Main category: cs.AI

TL;DR: The paper explores whether large language models (LLMs) can provide effective user-facing explanations from an interpretable recommendation model, leveraging user feedback to evaluate explanation quality.


<details>
  <summary>Details</summary>
Motivation: To investigate the ability of LLMs to generate understandable and user-oriented explanations for a mathematically interpretable recommendation model, and address gaps in explainability evaluation through user-centered methods.

Method: The authors use a constrained matrix factorization-based recommendation model and employ LLM prompts to convert its interpretable structure into natural language explanations. They conduct a user study with 326 participants to evaluate explanations along five dimensions: transparency, effectiveness, persuasion, trust, and satisfaction.

Result: The study found that all types of explanations generated were generally well-received, with moderate differences among explanation strategies across evaluation dimensions.

Conclusion: The paper concludes that LLMs, paired with interpretable recommendation models, can produce effective explanations that users appreciate, and user feedback provides valuable insights beyond automatic metrics.

Abstract: We investigate whether large language models (LLMs) can generate effective,
user-facing explanations from a mathematically interpretable recommendation
model. The model is based on constrained matrix factorization, where user types
are explicitly represented and predicted item scores share the same scale as
observed ratings, making the model's internal representations and predicted
scores directly interpretable. This structure is translated into natural
language explanations using carefully designed LLM prompts. Many works in
explainable AI rely on automatic evaluation metrics, which often fail to
capture users' actual needs and perceptions. In contrast, we adopt a
user-centered approach: we conduct a study with 326 participants who assessed
the quality of the explanations across five key dimensions-transparency,
effectiveness, persuasion, trust, and satisfaction-as well as the
recommendations themselves.To evaluate how different explanation strategies are
perceived, we generate multiple explanation types from the same underlying
model, varying the input information provided to the LLM. Our analysis reveals
that all explanation types are generally well received, with moderate
statistical differences between strategies. User comments further underscore
how participants react to each type of explanation, offering complementary
insights beyond the quantitative results.

</details>


### [43] [Remaining Time Prediction in Outbound Warehouse Processes: A Case Study (Short Paper)](https://arxiv.org/abs/2509.18986)
*Erik Penther,Michael Grohs,Jana-Rebecca Rehse*

Main category: cs.AI

TL;DR: This study compares four approaches for predicting remaining time in a logistics process, concluding that deep learning is most accurate, but shallow models are resource-efficient and competitive.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of different predictive models on remaining time estimation in real-world process monitoring.

Method: The comparison involves four prediction approaches, tested on a novel, real-life event log containing 169,523 traces from a logistics company's outbound warehouse process.

Result: Deep learning models yield the highest prediction accuracy, but shallow models (e.g., boosting techniques) perform competitively with lower computational requirements.

Conclusion: While deep learning excels in accuracy, shallow techniques offer a viable alternative when computational efficiency is a priority.

Abstract: Predictive process monitoring is a sub-domain of process mining which aims to
forecast the future of ongoing process executions. One common prediction target
is the remaining time, meaning the time that will elapse until a process
execution is completed. In this paper, we compare four different remaining time
prediction approaches in a real-life outbound warehouse process of a logistics
company in the aviation business. For this process, the company provided us
with a novel and original event log with 169,523 traces, which we can make
publicly available. Unsurprisingly, we find that deep learning models achieve
the highest accuracy, but shallow methods like conventional boosting techniques
achieve competitive accuracy and require significantly fewer computational
resources.

</details>


### [44] [Landmarks, Monuments, and Beacons: Understanding Generative Calls to Action](https://arxiv.org/abs/2509.19030)
*Victoire Hervé,Henrik Warpefelt,Christoph Salge*

Main category: cs.AI

TL;DR: The paper introduces concepts (Landmarks, Monuments, Beacons) to improve algorithmic evaluation of procedurally generated game content, focusing on human-centric metrics and automated decomposition.


<details>
  <summary>Details</summary>
Motivation: Metrics for evaluating procedurally generated content often fail to align with human experiences, particularly for complex artefacts. A need exists for better frameworks to bridge this gap.

Method: The authors propose and define player-centric concepts (Landmarks, Monuments, Beacons) grounded in perceivability, evocativeness, and Calls to Action, aiming to make them viable for automated evaluation using existing tools.

Result: The proposed framework identifies generic game concepts for automatic decomposition and evaluation of procedurally generated content, applicable across genres and domains.

Conclusion: This work fosters integration between humanities and game AI research, advancing computational evaluation of procedurally generated content beyond mixed-initiative and compositional PCG domains.

Abstract: Algorithmic evaluation of procedurally generated content struggles to find
metrics that align with human experience, particularly for composite artefacts.
Automatic decomposition as a possible solution requires concepts that meet a
range of properties. To this end, drawing on Games Studies and Game AI
research, we introduce the nested concepts of \textit{Landmarks},
\textit{Monuments}, and \textit{Beacons}. These concepts are based on the
artefact's perceivability, evocativeness, and Call to Action, all from a
player-centric perspective. These terms are generic to games and usable across
genres. We argue that these entities can be found and evaluated with techniques
currently used in both research and industry, opening a path towards a fully
automated decomposition of PCG, and evaluation of the salient sub-components.
Although the work presented here emphasises mixed-initiative PCG and
compositional PCG, we believe it applies beyond those domains. With this
approach, we intend to create a connection between humanities and technical
game research and allow for better computational PCG evaluation

</details>


### [45] [Towards Causal Representation Learning with Observable Sources as Auxiliaries](https://arxiv.org/abs/2509.19058)
*Kwonho Kim,Heejeong Nam,Inwoo Hwang,Sanghack Lee*

Main category: cs.AI

TL;DR: This paper proposes using observable sources as auxiliary variables for causal representation learning to better identify latent variables.


<details>
  <summary>Details</summary>
Motivation: Previous causal representation learning methods rely on external auxiliary variables, limiting their applicability. This paper explores how system-driving latent factors, which may already be present in the data, can serve as auxiliaries.

Method: The method involves using observable sources as conditioning variables and employs volume-preserving encoders to identify latent variables. A variable-selection scheme is also proposed to optimize recoverability based on the latent causal graph.

Result: The proposed approach identifies latent variables up to subspace-wise transformations and permutations. Experiments on synthetic data show the framework's effectiveness in improving identification of latent factors.

Conclusion: The study extends traditional frameworks by introducing observable sources as auxiliaries, improving the identification of latent variables and expanding applicable scenarios for causal representation learning.

Abstract: Causal representation learning seeks to recover latent factors that generate
observational data through a mixing function. Needing assumptions on latent
structures or relationships to achieve identifiability in general, prior works
often build upon conditional independence given known auxiliary variables.
However, prior frameworks limit the scope of auxiliary variables to be external
to the mixing function. Yet, in some cases, system-driving latent factors can
be easily observed or extracted from data, possibly facilitating
identification. In this paper, we introduce a framework of observable sources
being auxiliaries, serving as effective conditioning variables. Our main
results show that one can identify entire latent variables up to subspace-wise
transformations and permutations using volume-preserving encoders. Moreover,
when multiple known auxiliary variables are available, we offer a
variable-selection scheme to choose those that maximize recoverability of the
latent factors given knowledge of the latent causal graph. Finally, we
demonstrate the effectiveness of our framework through experiments on synthetic
graph and image data, thereby extending the boundaries of current approaches.

</details>


### [46] [Code Driven Planning with Domain-Adaptive Critic](https://arxiv.org/abs/2509.19077)
*Zikang Tian,Shaohui Peng,Du Huang,Jiaming Guo,Ruizhi Chen,Rui Zhang,Xishan Zhang,Yuxuan Guo,Zidong Du,Qi Guo,Ling Li,Yewen Pu,Xing Hu,Yunji Chen*

Main category: cs.AI

TL;DR: The paper introduces CoPiC, aiming to address the drawbacks of frequent LLM query costs and short-term feedback issues in planning tasks, by using high-level planning programs and a trained critic.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance LLM-driven task planning by addressing the gap between general world knowledge of LLMs and environment-specific requirements, reducing query costs and enabling plans aligned with long-term rewards.

Method: CoPiC uses LLM-generated high-level planning programs to refine candidate plans iteratively, coupled with a trained domain-adaptive critic to evaluate and align plans with long-term rewards.

Result: CoPiC demonstrated superior performance over baselines with a 23.33% improvement in success rate and a 91.27% reduction in query costs across multiple benchmarks like ALFWorld, NetHack, and StarCraft II.

Conclusion: CoPiC effectively reduces query costs and improves success rates in sequential decision-making tasks, presenting an advancement over existing LLM-based approaches.

Abstract: Large Language Models (LLMs) have been widely adopted as task planners for AI
agents in sequential decision-making problems, leveraging their extensive world
knowledge. However, the gap between their general knowledge and
environment-specific requirements often leads to inaccurate plans. To address
this, existing approaches rely on frequent LLM queries to iteratively refine
plans based on immediate environmental feedback, which incurs substantial query
costs. However, this refinement is typically guided by short-term environmental
feedback, limiting LLMs from developing plans aligned with long-term rewards.
We propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of
relying on frequent queries, CoPiC employs LLMs to generate a diverse set of
high-level planning programs, which iteratively produce and refine candidate
plans. A trained domain-adaptive critic then evaluates these candidates and
selects the one most aligned with long-term rewards for execution. Using
high-level planning programs as planner and domain-adaptive critic as
estimator, CoPiC improves planning while significantly reducing query costs.
Results in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC
outperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving
an average (1) 23.33% improvement in success rate and (2) 91.27% reduction in
query costs.

</details>


### [47] [AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration](https://arxiv.org/abs/2509.19236)
*Chunhao Tian,Yutong Wang,Xuebo Liu,Zhexuan Wang,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: The paper presents AgentInit, a novel method for initializing multi-agent systems (MAS) that improves collaboration, efficiency, and performance by integrating team diversity and task relevance strategies.


<details>
  <summary>Details</summary>
Motivation: Existing MAS initialization methods inadequately address the collaborative needs of generated agents, limiting system efficiency and performance.

Method: AgentInit combines multi-round interactions, a Natural Language to Format mechanism for consistency, and balanced team selection strategies based on Pareto principles to optimize agent team composition.

Result: Experiments demonstrate that AgentInit outperforms current methods in frameworks and tasks, improving performance by up to 1.6 and reducing token consumption significantly.

Conclusion: AgentInit is an effective and adaptable MAS initialization method with proven transferability and strong performance in optimizing agent collaboration.

Abstract: Proper initialization is crucial for any system, particularly in multi-agent
systems (MAS), where it plays a pivotal role in determining both the system's
efficiency and effectiveness. However, existing MAS initialization methods do
not fully account for the collaborative needs of the generated agents in
subsequent stages. Inspired by the principles of effective team composition, we
propose AgentInit, which aims to optimize the structure of agent teams.
Specifically, in addition to multi-round interactions and reflections between
agents during agent generation, AgentInit incorporates a Natural Language to
Format mechanism to ensure consistency and standardization. Balanced team
selection strategies using Pareto principles are subsequently applied to
jointly consider agent team diversity and task relevance to promote effective
and efficient collaboration and enhance overall system performance. Experiments
show that AgentInit consistently outperforms state-of-the-art initialization
methods and pre-defined strategies across various frameworks and tasks,
achieving an overall performance improvement of up to 1.2 and 1.6,
respectively, while also significantly reducing token consumption. Further
analysis confirms its strong transferability to similar tasks and verifies the
effectiveness of its key components, demonstrating its capability and
adaptability as a reliable MAS initialization method. Source code and models
are available at https://github.com/1737423697/AgentInit.

</details>


### [48] [Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World](https://arxiv.org/abs/2509.19265)
*Saeed Almheiri,Rania Hossam,Mena Attia,Chenxi Wang,Preslav Nakov,Timothy Baldwin,Fajri Koto*

Main category: cs.AI

TL;DR: This paper explores cross-cultural transfer of commonsense reasoning using lightweight alignment methods and demonstrates that models can improve performance in different cultural contexts using examples from specific cultures.


<details>
  <summary>Details</summary>
Motivation: Large language models often exhibit Western-centric biases, limiting their adaptability across diverse cultures. The paper seeks to address this by investigating cross-cultural transfer in commonsense reasoning, particularly in the Arab world.

Method: The study employs lightweight alignment methods such as in-context learning and demonstration-based reinforcement (DITTO), along with baselines like supervised fine-tuning and direct preference optimization, using a culturally grounded dataset covering 13 Arab countries.

Result: The findings reveal that 12 culture-specific examples from one Arab country can enhance performance in other Arab countries by 10% on average. Furthermore, examples from Indonesia and the US can effectively transfer commonsense reasoning to Arab cultures.

Conclusion: Efficient cross-cultural alignment is feasible, providing an effective strategy for adapting large language models to culturally distinct and low-resource settings.

Abstract: Large language models (LLMs) often reflect Western-centric biases, limiting
their effectiveness in diverse cultural contexts. Although some work has
explored cultural alignment, the potential for cross-cultural transfer, using
alignment in one culture to improve performance in others, remains
underexplored. This paper investigates cross-cultural transfer of commonsense
reasoning in the Arab world, where linguistic and historical similarities
coexist with local cultural differences. Using a culturally grounded
commonsense reasoning dataset covering 13 Arab countries, we evaluate
lightweight alignment methods such as in-context learning and
demonstration-based reinforcement (DITTO), alongside baselines like supervised
fine-tuning and direct preference optimization. Our results show that merely 12
culture-specific examples from one country can improve performance in others by
10\% on average, within multilingual models. In addition, we demonstrate that
out-of-culture demonstrations from Indonesia and US contexts can match or
surpass in-culture alignment for MCQ reasoning, highlighting cultural
commonsense transferability beyond the Arab world. These findings demonstrate
that efficient cross-cultural alignment is possible and offer a promising
approach to adapt LLMs to low-resource cultural settings.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [49] [Lightweight Congruence Profiling for Early Design Exploration of Heterogeneous FPGAs](https://arxiv.org/abs/2509.18295)
*Allen Boston,Biruk Seyoum,Luca Carloni,Pierre-Emmanuel Gaillardon*

Main category: cs.AR

TL;DR: This paper proposes a profiling methodology inspired by the Roofline model to identify bottlenecks in heterogeneous FPGAs, enabling efficient architecture co-design for improved performance.


<details>
  <summary>Details</summary>
Motivation: With the evolution of FPGAs integrating diverse resources like DSPs and accelerators to support workloads such as machine learning, optimizing these complex systems becomes challenging due to intricate resource interactions.

Method: The authors introduce a lightweight profiling methodology based on the Roofline model, incorporating three congruence scores to effectively pinpoint bottlenecks related to resource interactions, fabric, and application logic.

Result: When tested on benchmark suites like Koios and VPR using a Stratix 10-like FPGA, the profiling approach successfully identifies bottlenecks and facilitates efficient architecture co-design.

Conclusion: This profiling method aids in streamlining FPGA optimization processes, enhancing the performance of heterogeneous FPGA architectures through effective bottleneck identification and resource management.

Abstract: Field-Programmable Gate Arrays (FPGAs) have evolved from uniform logic arrays
into heterogeneous fabrics integrating digital signal processors (DSPs),
memories, and specialized accelerators to support emerging workloads such as
machine learning. While these enhancements improve power, performance, and area
(PPA), they complicate design space exploration and application optimization
due to complex resource interactions.
  To address these challenges, we propose a lightweight profiling methodology
inspired by the Roofline model. It introduces three congruence scores that
quickly identify bottlenecks related to heterogeneous resources, fabric, and
application logic. Evaluated on the Koios and VPR benchmark suites using a
Stratix 10 like FPGA, this approach enables efficient FPGA architecture
co-design to improve heterogeneous FPGA performance.

</details>


### [50] [Chiplet-Based RISC-V SoC with Modular AI Acceleration](https://arxiv.org/abs/2509.18355)
*P. Ramkumar,S. S. Bharadwaj*

Main category: cs.AR

TL;DR: This paper introduces a modular chiplet-based RISC-V SoC for edge AI devices, achieving improved performance, power efficiency, and scalability compared to traditional monolithic designs.


<details>
  <summary>Details</summary>
Motivation: Overcoming performance, energy, and cost limitations in edge AI devices constrained by low manufacturing yields in monolithic SoCs.

Method: Proposes a modular chiplet architecture featuring 4 innovations: adaptive DVFS, AI-optimized UCIe protocol, distributed security, and sensor-driven workload shifts, leveraging advanced fabrication nodes.

Result: Experimental results show ~14.7% latency reduction, 17.3% throughput improvement, and 16.2% power savings, leading to ~40.1% efficiency gain in edge AI benchmarks.

Conclusion: The novel chiplet-based architecture achieves computational density near monolithic SoCs while enhancing flexibility, scalability, and performance for edge AI applications.

Abstract: Achieving high performance, energy efficiency, and cost-effectiveness while
maintaining architectural flexibility is a critical challenge in the
development and deployment of edge AI devices. Monolithic SoC designs struggle
with this complex balance mainly due to low manufacturing yields (below 16%) at
advanced 360 mm^2 process nodes. This paper presents a novel chiplet-based
RISC-V SoC architecture that addresses these limitations through modular AI
acceleration and intelligent system level optimization. Our proposed design
integrates 4 different key innovations in a 30mm x 30mm silicon interposer:
adaptive cross-chiplet Dynamic Voltage and Frequency Scaling (DVFS); AI-aware
Universal Chiplet Interconnect Express (UCIe) protocol extensions featuring
streaming flow control units and compression-aware transfers; distributed
cryptographic security across heterogeneous chiplets; and intelligent
sensor-driven load migration. The proposed architecture integrates a 7nm RISC-V
CPU chiplet with dual 5nm AI accelerators (15 TOPS INT8 each), 16GB HBM3 memory
stacks, and dedicated power management controllers. Experimental results across
industry standard benchmarks like MobileNetV2, ResNet-50 and real-time video
processing demonstrate significant performance improvements. The AI-optimized
configuration achieves ~14.7% latency reduction, 17.3% throughput improvement,
and 16.2% power reduction compared to previous basic chiplet implementations.
These improvements collectively translate to a 40.1% efficiency gain
corresponding to ~3.5 mJ per MobileNetV2 inference (860 mW/244 images/s), while
maintaining sub-5ms real-time capability across all experimented workloads.
These performance upgrades demonstrate that modular chiplet designs can achieve
near-monolithic computational density while enabling cost efficiency,
scalability and upgradeability, crucial for next-generation edge AI device
applications.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [51] [Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs](https://arxiv.org/abs/2509.18113)
*Xin Hu,Yue Kang,Guanzi Yao,Tianze Kang,Mengjie Wang,Heyao Liu*

Main category: cs.CL

TL;DR: The study proposes a dynamic prompt scheduling framework for multi-task learning in large language models to mitigate generalization limitations.


<details>
  <summary>Details</summary>
Motivation: To overcome generalization issues in large language models under multi-task and cross-domain conditions by improving task adaptation.

Method: A unified multi-task framework is introduced with prompt pool, task-aware scheduling, task embedding, and gating mechanisms. It features a dynamic alignment and fusion of prompts combined with joint multi-task optimization.

Result: Sensitivity experiments confirm improved model performance, stability, and transferability in language understanding and reasoning tasks.

Conclusion: The proposed dynamic prompt mechanism enhances multi-task learning, enabling robust cross-domain adaptation and unified modeling in large language models.

Abstract: This study addresses the generalization limitations commonly observed in
large language models under multi-task and cross-domain settings. Unlike prior
methods such as SPoT, which depends on fixed prompt templates, our study
introduces a unified multi-task learning framework with dynamic prompt
scheduling mechanism. By introducing a prompt pool and a task-aware scheduling
strategy, the method dynamically combines and aligns prompts for different
tasks. This enhances the model's ability to capture semantic differences across
tasks. During prompt fusion, the model uses task embeddings and a gating
mechanism to finely control the prompt signals. This ensures alignment between
prompt content and task-specific demands. At the same time, it builds flexible
sharing pathways across tasks. In addition, the proposed optimization objective
centers on joint multi-task learning. It incorporates an automatic learning
strategy for scheduling weights, which effectively mitigates task interference
and negative transfer. To evaluate the effectiveness of the method, a series of
sensitivity experiments were conducted. These experiments examined the impact
of prompt temperature parameters and task number variation. The results confirm
the advantages of the proposed mechanism in maintaining model stability and
enhancing transferability. Experimental findings show that the prompt
scheduling method significantly improves performance on a range of language
understanding and knowledge reasoning tasks. These results fully demonstrate
its applicability and effectiveness in unified multi-task modeling and
cross-domain adaptation.

</details>


### [52] [GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models](https://arxiv.org/abs/2509.18122)
*Yue Zhang,Jiaxin Zhang,Qiuyu Ren,Tahsin Saffat,Xiaoxuan Liu,Zitong Yang,Banghua Zhu,Yi Ma*

Main category: cs.CL

TL;DR: GAUSS is a benchmark for evaluating LLMs' mathematical skills across 12 dimensions, providing comprehensive and interpretable skill profiles.


<details>
  <summary>Details</summary>
Motivation: To create a systematic and multi-dimensional evaluation framework for assessing mathematical intelligence in large language models.

Method: GAUSS categorizes mathematical problems based on cognitive skills across three domains, isolating specific abilities for detailed analysis.

Result: The skill profile of GPT-5-thinking was derived, showing its strengths and weaknesses compared to another model, o4-mini-high.

Conclusion: The GAUSS benchmark enables detailed, skill-specific evaluations of mathematical capabilities in LLMs, aiding comparative and diagnostic assessments.

Abstract: We introduce \textbf{GAUSS} (\textbf{G}eneral \textbf{A}ssessment of
\textbf{U}nderlying \textbf{S}tructured \textbf{S}kills in Mathematics), a
benchmark that evaluates LLMs' mathematical abilities across twelve core skill
dimensions, grouped into three domains: knowledge and understanding, problem
solving and communication, and meta-skills and creativity. By categorizing
problems according to cognitive skills and designing tasks that isolate
specific abilities, GAUSS constructs comprehensive, fine-grained, and
interpretable profiles of models' mathematical abilities. These profiles
faithfully represent their underlying mathematical intelligence. To exemplify
how to use the \textsc{GAUSS} benchmark, we have derived the skill profile of
\textsc{GPT-5-thinking}, revealing its strengths and weaknesses as well as its
differences relative to \textsc{o4-mini-high}, thereby underscoring the value
of multidimensional, skill-based evaluation.

</details>


### [53] [Event Causality Identification with Synthetic Control](https://arxiv.org/abs/2509.18156)
*Haoyu Wang,Fengze Liu,Jiayao Zhang,Dan Roth,Kyle Richardson*

Main category: cs.CL

TL;DR: The paper proposes using the Rubin Causal Model and synthetic control methods to identify event causality by estimating changes in outcomes due to event interventions, achieving robust performance on causality benchmarks compared to prior methods.


<details>
  <summary>Details</summary>
Motivation: To improve Event Causality Identification (ECI) by addressing the limitations of traditional methods, such as false causality due to informal causation use and flawed graphical inference.

Method: Adopts the Rubin Causal Model, conceptualizing events as treatments and outcomes, and employs the synthetic control method to generate comparable twins using text embedding synthesis to analyze causal relations.

Result: The proposed method demonstrates robust performance on the COPES-hard causality benchmark, outperforming existing methods, including GPT-4.

Conclusion: The approach enhances the robustness of ECI by leveraging synthetic control and causal modeling, addressing prior limitations and advancing the field's ability to differentiate causation from correlation.

Abstract: Event causality identification (ECI), a process that extracts causal
relations between events from text, is crucial for distinguishing causation
from correlation. Traditional approaches to ECI have primarily utilized
linguistic patterns and multi-hop relational inference, risking false causality
identification due to informal usage of causality and specious graphical
inference. In this paper, we adopt the Rubin Causal Model to identify event
causality: given two temporally ordered events, we see the first event as the
treatment and the second one as the observed outcome. Determining their
causality involves manipulating the treatment and estimating the resultant
change in the likelihood of the outcome. Given that it is only possible to
implement manipulation conceptually in the text domain, as a work-around, we
try to find a twin for the protagonist from existing corpora. This twin should
have identical life experiences with the protagonist before the treatment but
undergoes an intervention of treatment. However, the practical difficulty of
locating such a match limits its feasibility. Addressing this issue, we use the
synthetic control method to generate such a twin' from relevant historical
data, leveraging text embedding synthesis and inversion techniques. This
approach allows us to identify causal relations more robustly than previous
methods, including GPT-4, which is demonstrated on a causality benchmark,
COPES-hard.

</details>


### [54] [ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization](https://arxiv.org/abs/2509.18158)
*Seungyoun Yi,Minsoo Khang,Sungrae Park*

Main category: cs.CL

TL;DR: The paper introduces ZERA, a framework for refining prompts to improve LLM performance more effectively than prior methods.


<details>
  <summary>Details</summary>
Motivation: Prior methods for optimizing LLM prompts are inefficient, requiring extensive feedback and high costs.

Method: ZERA employs structured criteria for scoring and refining prompts, optimizing both system and user prompts with minimal examples.

Result: ZERA consistently enhances performance across various datasets and LLMs, outperforming competitive baselines.

Conclusion: ZERA achieves fast and low-cost optimization of prompts, with demonstrated efficacy and public accessibility of its implementation.

Abstract: Automatic Prompt Optimization (APO) improves large language model (LLM)
performance by refining prompts for specific tasks. However, prior APO methods
typically focus only on user prompts, rely on unstructured feedback, and
require large sample sizes and long iteration cycles-making them costly and
brittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a
novel framework that jointly optimizes both system and user prompts through
principled, low-overhead refinement. ZERA scores prompts using eight
generalizable criteria with automatically inferred weights, and revises prompts
based on these structured critiques. This enables fast convergence to
high-quality prompts using minimal examples and short iteration cycles. We
evaluate ZERA across five LLMs and nine diverse datasets spanning reasoning,
summarization, and code generation tasks. Experimental results demonstrate
consistent improvements over strong baselines. Further ablation studies
highlight the contribution of each component to more effective prompt
construction. Our implementation including all prompts is publicly available at
https://github.com/younatics/zera-agent.

</details>


### [55] [Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning](https://arxiv.org/abs/2509.18163)
*Haodong Zhao,Chenyan Zhao,Yansi Li,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.CL

TL;DR: This paper explores the impact of external information on the reasoning abilities of Large Language Models (LLMs), revealing their vulnerability to misleading information, especially during step-by-step reasoning.


<details>
  <summary>Details</summary>
Motivation: Understanding how external information influences the reasoning abilities of LLMs is important for their reliable application in complex domains.

Method: The authors introduced 'SciAux,' a dataset derived from ScienceQA, to systematically evaluate the robustness of LLMs against helpful, irrelevant, and misleading external information.

Result: The study showed that while helpful context enhances LLM accuracy, misleading information disastrously undermines performance, particularly during deliberative reasoning.

Conclusion: Effective reasoning in LLMs requires not just the ability to deduce step-by-step but also critical evaluation of external information to mitigate errors caused by misinformation.

Abstract: The capacity of Large Language Models (LLMs) to reason is fundamental to
their application in complex, knowledge-intensive domains. In real-world
scenarios, LLMs are often augmented with external information that can be
helpful, irrelevant, or even misleading. This paper investigates the causal
impact of such auxiliary information on the reasoning process of LLMs with
explicit step-by-step thinking capabilities. We introduce SciAux, a new dataset
derived from ScienceQA, to systematically test the robustness of the model
against these types of information. Our findings reveal a critical
vulnerability: the model's deliberative "thinking mode" is a double-edged
sword. While helpful context improves accuracy, misleading information causes a
catastrophic drop in performance, which is amplified by the thinking process.
Instead of conferring robustness, thinking reinforces the degree of error when
provided with misinformation. This highlights that the challenge is not merely
to make models "think", but to endow them with the critical faculty to evaluate
the information upon which their reasoning is based. The SciAux dataset is
available at https://huggingface.co/datasets/billhdzhao/SciAux.

</details>


### [56] [SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework](https://arxiv.org/abs/2509.18167)
*Junlin Wang,Zehao Wu,Shaowei Lu,Yanlan Li,Xinghao Huang*

Main category: cs.CL

TL;DR: The paper introduces a multi-agent framework to improve the coordination between retrieval and generation in Retrieval-Augmented Generation (RAG) systems, leading to better accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the disconnect between retrievers and generators in Retrieval-Augmented Generation systems. This disconnect results in suboptimal retrieval and usage of external knowledge, undermining the quality of the generated answers.

Method: The method introduces two auxiliary agents—Decision Maker and Knowledge Selector—within a modular framework. It also employs an LLM-as-a-Judge for intermediate reward evaluation, uses tree-structured rollouts to find diverse reasoning paths, and trains the agents using Proximal Policy Optimization (PPO).

Result: The approach achieves improved accuracy, stable training convergence, and interpretable reasoning processes on both single-hop and multi-hop question answering datasets, outperforming standard RAG baselines.

Conclusion: The proposed process-supervised framework enhances the interaction between retriever and generator without requiring modifications to either, making it practical and effective for real-world applications of RAG.

Abstract: Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to
access external knowledge sources, but the effectiveness of RAG relies on the
coordination between the retriever and the generator. Since these components
are developed independently, their interaction is often suboptimal: the
retriever may return irrelevant or redundant documents, while the generator may
fail to fully leverage retrieved evidence. In this work, we propose a
process-supervised multi-agent framework to bridge the gap between retriever
and generator. The framework introduces two lightweight agents: a Decision
Maker, which determines when to continue retrieval or stop for answer
generation, and a Knowledge Selector, which filters retrieved documents to
retain only the most useful evidence. To provide fine-grained supervision, we
employ an LLM-as-a-Judge that evaluates each intermediate action with
process-level rewards, ensuring more accurate credit assignment than relying
solely on final answer correctness. We further adopt a tree-structured rollout
strategy to explore diverse reasoning paths, and train both agents with
Proximal Policy Optimization (PPO) in an end-to-end manner. Experiments on
single-hop and multi-hop question answering benchmarks show that our approach
achieves higher accuracy, more stable convergence, and produces more
interpretable reasoning trajectories compared with standard RAG baselines.
Importantly, the proposed framework is modular and plug-and-play, requiring no
modification to the retriever or generator, making it practical for real-world
RAG applications.

</details>


### [57] [ERFC: Happy Customers with Emotion Recognition and Forecasting in Conversation in Call Centers](https://arxiv.org/abs/2509.18175)
*Aditi Debsharma,Bhushan Jagyasi,Surajit Sen,Priyanka Pandey,Devicharith Dovari,Yuvaraj V. C,Rosalin Parida,Gopali Contractor*

Main category: cs.CL

TL;DR: The paper proposes a novel Emotion Recognition and Forecasting in Conversation (ERFC) architecture designed to predict emotions in future utterances, with potential applications such as enhancing customer experience in call centers.


<details>
  <summary>Details</summary>
Motivation: To improve customer experience in call centers by predicting and addressing customer emotions, which can influence conversational dynamics and satisfaction.

Method: ERFC architecture utilizes multi-modalities, emotional attributes, context, and speaker utterance interdependencies to predict future emotions.

Result: ERFC was tested on the IEMOCAP dataset and showed promising feasibility for emotional prediction in conversations.

Conclusion: The proposed ERFC system can enhance customer service experiences, particularly in call centers, by predicting and acting on customer emotions effectively.

Abstract: Emotion Recognition in Conversation has been seen to be widely applicable in
call center analytics, opinion mining, finance, retail, healthcare, and other
industries. In a call center scenario, the role of the call center agent is not
just confined to receiving calls but to also provide good customer experience
by pacifying the frustration or anger of the customers. This can be achieved by
maintaining neutral and positive emotion from the agent. As in any
conversation, the emotion of one speaker is usually dependent on the emotion of
other speaker. Hence the positive emotion of an agent, accompanied with the
right resolution will help in enhancing customer experience. This can change an
unhappy customer to a happy one. Imparting the right resolution at right time
becomes easier if the agent has the insight of the emotion of future
utterances. To predict the emotions of the future utterances we propose a novel
architecture, Emotion Recognition and Forecasting in Conversation. Our proposed
ERFC architecture considers multi modalities, different attributes of emotion,
context and the interdependencies of the utterances of the speakers in the
conversation. Our intensive experiments on the IEMOCAP dataset have shown the
feasibility of the proposed ERFC. This approach can provide a tremendous
business value for the applications like call center, where the happiness of
customer is utmost important.

</details>


### [58] [Evaluating Large Language Models for Detecting Antisemitism](https://arxiv.org/abs/2509.18293)
*Jay Patel,Hrudayangam Mehta,Jeremy Blackburn*

Main category: cs.CL

TL;DR: The paper evaluates eight open-source large language models (LLMs) on their ability to detect antisemitic content using in-context definitions as guidelines. A novel prompt design, Guided-CoT, improves overall model performance.


<details>
  <summary>Details</summary>
Motivation: The study addresses the need for better tools to detect hateful content, particularly antisemitism, as social media's dynamics frequently change.

Method: Eight LLMs were tested for detecting antisemitic content, employing novel prompting techniques, including the Guided-CoT design. Errors were analyzed and metrics for semantic divergence in rationales were proposed.

Result: Guided-CoT improved the performance of all evaluated models and demonstrated that Llama 3.1 70B surpassed fine-tuned GPT-3.5. It revealed significant differences and peculiarities in LLM explanations.

Conclusion: LLMs' utility, explainability, and reliability differ significantly. The Guided-CoT technique enhances performance universally, offering promising steps towards better detection in dynamic online environments.

Abstract: Detecting hateful content is a challenging and important problem. Automated
tools, like machine-learning models, can help, but they require continuous
training to adapt to the ever-changing landscape of social media. In this work,
we evaluate eight open-source LLMs' capability to detect antisemitic content,
specifically leveraging in-context definition as a policy guideline. We explore
various prompting techniques and design a new CoT-like prompt, Guided-CoT.
Guided-CoT handles the in-context policy well, increasing performance across
all evaluated models, regardless of decoding configuration, model sizes, or
reasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5.
Additionally, we examine LLM errors and introduce metrics to quantify semantic
divergence in model-generated rationales, revealing notable differences and
paradoxical behaviors among LLMs. Our experiments highlight the differences
observed across LLMs' utility, explainability, and reliability.

</details>


### [59] [Exploiting Tree Structure for Credit Assignment in RL Training of LLMs](https://arxiv.org/abs/2509.18314)
*Hieu Tran,Zonghai Yao,Hong Yu*

Main category: cs.CL

TL;DR: The paper introduces the Prefix-to-Tree (P2T) method and TEMPO algorithm to address token-level credit assignment challenges in reinforcement learning for LLMs using verifiable rewards. TEMPO improves precision at branching tokens without needing a critic model.


<details>
  <summary>Details</summary>
Motivation: Token-level credit assignment is challenging in reinforcement learning for long sequences with sparse, delayed rewards. The need for simple and effective reinforcement learning methods in reasoning tasks characterized by verifiable outcomes motivates this study.

Method: The Prefix-to-Tree (P2T) method generates a prefix tree from responses, aggregating descendant outcomes to compute nonparametric prefix values. Built on P2T, the TEMPO algorithm integrates branch-gated temporal-difference corrections and minimizes reliance on learned value networks or critic models.

Result: TEMPO achieves superior performance compared to PPO and GRPO on both in-distribution and out-of-distribution benchmarks, delivering higher validation accuracy within comparable training time frames.

Conclusion: TEMPO introduces a robust solution for token-level credit assignment, leveraging verifiable rewards and minimizing reliance on learned models. Its effectiveness across benchmarks positions TEMPO as a significant advancement in reinforcement learning for reasoning tasks.

Abstract: Reinforcement learning improves LLM reasoning, yet sparse delayed reward over
long sequences makes token-level credit assignment the key bottleneck. We study
the verifiable-reward setting, where the final answer is checkable and multiple
responses can be drawn per prompt. Reasoning tasks in math and medical QA align
with this setup, where only a few decision tokens significantly impact the
outcome. PPO offers token-level advantages with a learned value model, but it
is complex to train both the actor and critic models simultaneously, and it is
not easily generalizable, as the token-level values from the critic model can
make training prone to overfitting. GRPO is critic-free and supports verifiable
rewards, but spreads a single sequence-level return across tokens and ignores
branching. We introduce \textbf{Prefix-to-Tree (P2T)}, a simple procedure that
converts a group of responses into a prefix tree and computes
\emph{nonparametric} prefix values \(V(s)\) by aggregating descendant outcomes.
Built on P2T, we propose \textbf{TEMPO} (\emph{\textbf{T}ree-\textbf{E}stimated
\textbf{M}ean Prefix Value for \textbf{P}olicy \textbf{O}ptimization}), a
critic-free algorithm that augments the group-relative outcome signal of GRPO
with \emph{branch-gated} temporal-difference corrections derived from the tree.
At non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO
reduces to GRPO; at branching tokens, it supplies precise token-level credit
without a learned value network or extra judges/teachers. On Qwen3-1.7B/4B,
TEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and
out-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and
reaches higher validation accuracy with roughly the same wall-clock time.

</details>


### [60] [Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning](https://arxiv.org/abs/2509.18316)
*Saksham Khatwani,He Cheng,Majid Afshar,Dmitriy Dligach,Yanjun Gao*

Main category: cs.CL

TL;DR: The paper investigates using large language models (LLMs) to judge knowledge graph (KG) reasoning paths for improving diagnostic reasoning, revealing promise but limited generalization to other medical tasks.


<details>
  <summary>Details</summary>
Motivation: The research addresses the challenge of leveraging reliable, structured biomedical knowledge to enhance the reasoning capabilities of LLMs in diagnostic applications.

Method: The authors develop a novel approach where the LLM acts as a reward model to judge KG reasoning paths for correctness in diagnostics, systematically testing five task formulations and eight training paradigms.

Result: Experiments with instruct-tuned LLMs show strong performance in KG path judging using specific reward optimization, but limited generalization to downstream medical tasks like diagnosis summarization.

Conclusion: This work highlights the potential of reward-model-driven structured reasoning in LLMs for healthcare but underscores challenges in task transferability and suggests areas for further refinement.

Abstract: Large language models (LLMs) show promise for diagnostic reasoning but often
lack reliable, knowledge grounded inference. Knowledge graphs (KGs), such as
the Unified Medical Language System (UMLS), offer structured biomedical
knowledge that can support trustworthy reasoning. Prior approaches typically
integrate KGs via retrieval augmented generation or fine tuning, inserting KG
content into prompts rather than enabling structured reasoning. We explore an
alternative paradigm: treating the LLM as a reward model of KG reasoning paths,
where the model learns to judge whether a candidate path leads to correct
diagnosis for a given patient input. This approach is inspired by recent work
that leverages reward training to enhance model reasoning abilities, and
grounded in computational theory, which suggests that verifying a solution is
often easier than generating one from scratch. It also parallels physicians'
diagnostic assessment, where they judge which sequences of findings and
intermediate conditions most plausibly support a diagnosis. We first
systematically evaluate five task formulation for knowledge path judging and
eight training paradigm. Second, we test whether the path judging abilities
generalize to downstream diagnostic tasks, including diagnosis summarization
and medical question answering. Experiments with three open source
instruct-tuned LLMs reveal both promise and brittleness: while specific reward
optimization and distillation lead to strong path-judging performance, the
transferability to downstream tasks remain weak. Our finding provides the first
systematic assessment of "reward model style" reasoning over clinical KGs,
offering insights into how structured, reward-based supervision influences
diagnostic reasoning in GenAI systems for healthcare.

</details>


### [61] [Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding](https://arxiv.org/abs/2509.18344)
*Pei-Shuo Wang,Jian-Jia Chen,Chun-Che Yang,Chi-Chih Chang,Ning-Chi Huang,Mohamed S. Abdelfattah,Kai-Chiang Wu*

Main category: cs.CL

TL;DR: SubSpec addresses the challenge of deploying large language models (LLMs) on memory-limited consumer GPUs by using a plug-and-play, training-free speculative decoding technique that significantly improves inference speed without degrading quality.


<details>
  <summary>Details</summary>
Motivation: The paper tackles the challenges posed by the large memory requirements of LLMs, which hinder deployment on consumer GPUs. Existing compression and offloading strategies either impair model quality or lead to slow inference speeds, creating a need for a more efficient solution.

Method: The proposed method, SubSpec, constructs a highly aligned draft model by utilizing low-bit quantized substitute layers from offloaded LLM portions. It also shares GPU-resident layers and KV-Cache to enhance alignment and reduce memory overhead—all without requiring additional training.

Result: SubSpec achieves notable speedups, including a 9.1x improvement for Qwen2.5 7B on MT-Bench (8GB VRAM limit) and an average 12.5x speedup for Qwen2.5 32B across popular generation benchmarks (24GB VRAM limit).

Conclusion: The SubSpec method is a training-free and lossless speculative decoding strategy that offers significant speed enhancements in deploying LLMs on memory-limited hardware, making it a practical and efficient solution.

Abstract: The immense model sizes of large language models (LLMs) challenge deployment
on memory-limited consumer GPUs. Although model compression and parameter
offloading are common strategies to address memory limitations, compression can
degrade quality, and offloading maintains quality but suffers from slow
inference. Speculative decoding presents a promising avenue to accelerate
parameter offloading, utilizing a fast draft model to propose multiple draft
tokens, which are then verified by the target LLM in parallel with a single
forward pass. This method reduces the time-consuming data transfers in forward
passes that involve offloaded weight transfers. Existing methods often rely on
pretrained weights of the same family, but require additional training to align
with custom-trained models. Moreover, approaches that involve draft model
training usually yield only modest speedups. This limitation arises from
insufficient alignment with the target model, preventing higher token
acceptance lengths. To address these challenges and achieve greater speedups,
we propose SubSpec, a plug-and-play method to accelerate parameter offloading
that is lossless and training-free. SubSpec constructs a highly aligned draft
model by generating low-bit quantized substitute layers from offloaded target
LLM portions. Additionally, our method shares the remaining GPU-resident layers
and the KV-Cache, further reducing memory overhead and enhance alignment.
SubSpec achieves a high average acceptance length, delivering 9.1x speedup for
Qwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for
Qwen2.5 32B on popular generation benchmarks (24GB VRAM limit).

</details>


### [62] [Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents](https://arxiv.org/abs/2509.18360)
*Chutong Meng,Philipp Koehn*

Main category: cs.CL

TL;DR: Speech Vecalign is a new method for aligning parallel speech document embeddings without text transcriptions, improving alignment robustness and translation performance.


<details>
  <summary>Details</summary>
Motivation: To address the need for more effective methods in aligning speech segments for speech-to-speech translation tasks, particularly when text transcriptions are unavailable.

Method: Speech Vecalign aligns speech embeddings monotonically, avoiding reliance on text transcriptions. It generates longer and less noisy alignments compared to Global Mining and Local Mining methods.

Result: Applied to 3,000 hours of English-German speech data, the method achieved 1,000 hours of high-quality alignments, improving translation model performance by up to 0.37 ASR-BLEU over baselines.

Conclusion: Speech Vecalign offers robust alignment for speech-to-speech translation tasks and achieves competitive results using less raw data compared to alternatives like SpeechMatrix.

Abstract: We present Speech Vecalign, a parallel speech document alignment method that
monotonically aligns speech segment embeddings and does not depend on text
transcriptions. Compared to the baseline method Global Mining, a variant of
speech mining, Speech Vecalign produces longer speech-to-speech alignments. It
also demonstrates greater robustness than Local Mining, another speech mining
variant, as it produces less noise. We applied Speech Vecalign to 3,000 hours
of unlabeled parallel English-German (En-De) speech documents from VoxPopuli,
yielding about 1,000 hours of high-quality alignments. We then trained En-De
speech-to-speech translation models on the aligned data. Speech Vecalign
improves the En-to-De and De-to-En performance over Global Mining by 0.37 and
0.18 ASR-BLEU, respectively. Moreover, our models match or outperform
SpeechMatrix model performance, despite using 8 times fewer raw speech
documents.

</details>


### [63] [Interactive Real-Time Speaker Diarization Correction with Human Feedback](https://arxiv.org/abs/2509.18377)
*Xinlu He,Yiwen Guan,Badrivishal Paurana,Zilin Dai,Jacob Whitehill*

Main category: cs.CL

TL;DR: The paper proposes a system that combines ASR, speaker diarization, and LLM assistance to enable real-time corrections of speaker identification errors by users, significantly improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional speech processing systems lack real-time user interaction for correcting errors, limiting their accuracy. Incorporating user feedback can potentially enhance speaker diarization and ASR performance.

Method: The system processes streaming ASR and diarization data, uses an LLM to deliver summaries to users, and incorporates their verbal feedback. Techniques like split-when-merged detection and online speaker enrollments minimize future errors.

Result: Experiments on the AMI test set show significant improvements, with DER reduced by 9.92% and speaker confusion error minimized by 44.23%.

Conclusion: Integrating user feedback and LLM capabilities improves performance in speaker diarization. The approach suggests the promise of human-in-the-loop systems for real-time error correction.

Abstract: Most automatic speech processing systems operate in "open loop" mode without
user feedback about who said what; yet, human-in-the-loop workflows can
potentially enable higher accuracy. We propose an LLM-assisted speaker
diarization correction system that lets users fix speaker attribution errors in
real time. The pipeline performs streaming ASR and diarization, uses an LLM to
deliver concise summaries to the users, and accepts brief verbal feedback that
is immediately incorporated without disrupting interactions. Moreover, we
develop techniques to make the workflow more effective: First, a
split-when-merged (SWM) technique detects and splits multi-speaker segments
that the ASR erroneously attributes to just a single speaker. Second, online
speaker enrollments are collected based on users' diarization corrections, thus
helping to prevent speaker diarization errors from occurring in the future.
LLM-driven simulations on the AMI test set indicate that our system
substantially reduces DER by 9.92% and speaker confusion error by 44.23%. We
further analyze correction efficacy under different settings, including summary
vs full transcript display, the number of online enrollments limitation, and
correction frequency.

</details>


### [64] [NormGenesis: Multicultural Dialogue Generation via Exemplar-Guided Social Norm Modeling and Violation Recovery](https://arxiv.org/abs/2509.18395)
*Minki Hong,Jangho Choi,Jihie Kim*

Main category: cs.CL

TL;DR: NormGenesis introduces a multicultural framework aimed at generating and annotating dialogues that align with cultural social norms across English, Chinese, and Korean.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the ability of dialogue systems to generate culturally adaptive and socially acceptable responses in diverse languages.

Method: The paper proposes a Violation-to-Resolution (V2R) dialogue modeling approach and an exemplar-based iterative refinement process to generate socially grounded dialogues and annotations.

Result: A dataset of 10,800 multi-turn dialogues annotated for various parameters was created, showing improved refinement quality, dialogue naturalness, and model generalization performance.

Conclusion: NormGenesis sets a benchmark for culturally adaptive dialogue modeling, improving pragmatic competence in ethically sensitive contexts and enabling scalable norm-aware generation across languages.

Abstract: Social norms govern culturally appropriate behavior in communication,
enabling dialogue systems to produce responses that are not only coherent but
also socially acceptable. We present NormGenesis, a multicultural framework for
generating and annotating socially grounded dialogues across English, Chinese,
and Korean. To model the dynamics of social interaction beyond static norm
classification, we propose a novel dialogue type, Violation-to-Resolution
(V2R), which models the progression of conversations following norm violations
through recognition and socially appropriate repair. To improve pragmatic
consistency in underrepresented languages, we implement an exemplar-based
iterative refinement early in the dialogue synthesis process. This design
introduces alignment with linguistic, emotional, and sociocultural expectations
before full dialogue generation begins. Using this framework, we construct a
dataset of 10,800 multi-turn dialogues annotated at the turn level for norm
adherence, speaker intent, and emotional response. Human and LLM-based
evaluations demonstrate that NormGenesis significantly outperforms existing
datasets in refinement quality, dialogue naturalness, and generalization
performance. We show that models trained on our V2R-augmented data exhibit
improved pragmatic competence in ethically sensitive contexts. Our work
establishes a new benchmark for culturally adaptive dialogue modeling and
provides a scalable methodology for norm-aware generation across linguistically
and culturally diverse languages.

</details>


### [65] [Evaluating the Creativity of LLMs in Persian Literary Text Generation](https://arxiv.org/abs/2509.18401)
*Armin Tourajmehr,Mohammad Reza Modarres,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: This paper assesses the ability of large language models (LLMs) to generate culturally relevant Persian literary texts using the Torrance Tests of Creative Thinking and evaluates their use of key literary devices.


<details>
  <summary>Details</summary>
Motivation: To explore the creative ability of LLMs in non-English literary traditions, specifically Persian, and develop standardized methods for creativity assessment.

Method: The authors created a Persian literary dataset covering 20 topics and evaluated LLMs' outputs on originality, fluency, flexibility, and elaboration using an adapted Torrance test. Automated scoring with an LLM as a judge was validated against human judgment, and the use of literary devices like simile, metaphor, hyperbole, and antithesis was analyzed.

Result: Strong agreement was found between automated and human evaluation, showing the effectiveness of the scoring system. The study also revealed strengths and weaknesses in LLMs' ability to generate and utilize Persian literary text and devices.

Conclusion: LLMs show potential in generating Persian literary texts but require further refinements to address existing limitations in creativity and literary device usage.

Abstract: Large language models (LLMs) have demonstrated notable creative abilities in
generating literary texts, including poetry and short stories. However, prior
research has primarily centered on English, with limited exploration of
non-English literary traditions and without standardized methods for assessing
creativity. In this paper, we evaluate the capacity of LLMs to generate Persian
literary text enriched with culturally relevant expressions. We build a dataset
of user-generated Persian literary spanning 20 diverse topics and assess model
outputs along four creativity dimensions-originality, fluency, flexibility, and
elaboration-by adapting the Torrance Tests of Creative Thinking. To reduce
evaluation costs, we adopt an LLM as a judge for automated scoring and validate
its reliability against human judgments using intraclass correlation
coefficients, observing strong agreement. In addition, we analyze the models'
ability to understand and employ four core literary devices: simile, metaphor,
hyperbole, and antithesis. Our results highlight both the strengths and
limitations of LLMs in Persian literary text generation, underscoring the need
for further refinement.

</details>


### [66] [Developing an AI framework to automatically detect shared decision-making in patient-doctor conversations](https://arxiv.org/abs/2509.18439)
*Oscar J. Ponce-Ponte,David Toro-Tobon,Luis F. Figueroa,Michael Gionfriddo,Megan Branda,Victor M. Montori,Saturnino Luz,Juan P. Brito*

Main category: cs.CL

TL;DR: This study introduces an automated, scalable method to measure shared decision-making (SDM) in patient-doctor conversations using conversational alignment (CA) scores based on deep learning (DL) and BERT language models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of automated methods for measuring SDM at scale and to support patient-centered care.

Method: Deep learning and fine-tuned BERT models were trained using context-response pairs and the next sentence prediction task. CA scores were calculated and analyzed for association with SDM outcomes, using data from transcribed patient-doctor conversations.

Result: Fine-tuned BERT models achieved the highest recall, and CA scores were associated with SDM outcomes. Notably, the Max CA score from BERT models correlated negatively with Decisional Conflict Scale (DCS) scores, indicating better SDM.

Conclusion: The presented automated methodology using conversational alignment scores offers a scalable and explainable way to measure SDM in clinical conversations, enabling large-scale evaluation of SDM strategies.

Abstract: Shared decision-making (SDM) is necessary to achieve patient-centred care.
Currently no methodology exists to automatically measure SDM at scale. This
study aimed to develop an automated approach to measure SDM by using language
modelling and the conversational alignment (CA) score. A total of 157
video-recorded patient-doctor conversations from a randomized multi-centre
trial evaluating SDM decision aids for anticoagulation in atrial fibrillations
were transcribed and segmented into 42,559 sentences. Context-response pairs
and negative sampling were employed to train deep learning (DL) models and
fine-tuned BERT models via the next sentence prediction (NSP) task. Each
top-performing model was used to calculate four types of CA scores. A
random-effects analysis by clinician, adjusting for age, sex, race, and trial
arm, assessed the association between CA scores and SDM outcomes: the
Decisional Conflict Scale (DCS) and the Observing Patient Involvement in
Decision-Making 12 (OPTION12) scores. p-values were corrected for multiple
comparisons with the Benjamini-Hochberg method. Among 157 patients (34% female,
mean age 70 SD 10.8), clinicians on average spoke more words than patients
(1911 vs 773). The DL model without the stylebook strategy achieved a recall@1
of 0.227, while the fine-tuned BERTbase (110M) achieved the highest recall@1
with 0.640. The AbsMax (18.36 SE7.74 p=0.025) and Max CA (21.02 SE7.63 p=0.012)
scores generated with the DL without stylebook were associated with OPTION12.
The Max CA score generated with the fine-tuned BERTbase (110M) was associated
with the DCS score (-27.61 SE12.63 p=0.037). BERT model sizes did not have an
impact the association between CA scores and SDM. This study introduces an
automated, scalable methodology to measure SDM in patient-doctor conversations
through explainable CA scores, with potential to evaluate SDM strategies at
scale.

</details>


### [67] [CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density](https://arxiv.org/abs/2509.18458)
*Daniel Kaiser,Arnoldo Frigessi,Ali Ramezani-Kebrya,Benjamin Ricaud*

Main category: cs.CL

TL;DR: CogniLoad introduces a synthetic benchmark based on Cognitive Load Theory to analyze reasoning limitations in Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for evaluating long-context reasoning in LLMs lack precise analysis tools to differentiate factors like task complexity, distractors, and length.

Method: CogniLoad generates natural-language logic puzzles adjustable by parameters reflecting intrinsic difficulty, distractor-to-signal ratio, and task length.

Result: Testing on 22 state-of-the-art reasoning LLMs highlights performance sensitivities, showing task length as a key constraint and revealing varied tolerances to complexity and distractor interference.

Conclusion: CogniLoad provides valuable insights for understanding LLM limitations and offers a scalable framework for improving long-context reasoning in future AI models.

Abstract: Current benchmarks for long-context reasoning in Large Language Models (LLMs)
often blur critical factors like intrinsic task complexity, distractor
interference, and task length. To enable more precise failure analysis, we
introduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load
Theory (CLT). CogniLoad generates natural-language logic puzzles with
independently tunable parameters that reflect CLT's core dimensions: intrinsic
difficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($\rho$)
regulates extraneous load; and task length ($N$) serves as an operational proxy
for conditions demanding germane load. Evaluating 22 SotA reasoning LLMs,
CogniLoad reveals distinct performance sensitivities, identifying task length
as a dominant constraint and uncovering varied tolerances to intrinsic
complexity and U-shaped responses to distractor ratios. By offering systematic,
factorial control over these cognitive load dimensions, CogniLoad provides a
reproducible, scalable, and diagnostically rich tool for dissecting LLM
reasoning limitations and guiding future model development.

</details>


### [68] [LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling](https://arxiv.org/abs/2509.18467)
*Zeyu Liu,Souvik Kundu,Lianghao Jiang,Anni Li,Srikanth Ronanki,Sravan Bodapati,Gourav Datta,Peter A. Beerel*

Main category: cs.CL

TL;DR: LAWCAT, a novel framework, streamlines linear attention by transferring capabilities of pre-trained transformers to overcome training complexity while achieving efficient long-context processing.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the inefficiency of standard transformers dominated by quadratic computational complexity, making them unsuitable for latency-sensitive and long-context applications.

Method: The proposed LAWCAT framework integrates causal Conv1D layers for better local dependency modeling and uses normalized gated linear attention to improve generalization for varying context lengths. It efficiently distills capabilities from pre-trained transformer models into linear attention architectures.

Result: LAWCAT achieves significant improvements in passkey retrieval accuracy (over 90%) within extended context windows (up to 22K tokens). It performs well on benchmarks like S-NIAH and BABILong, while requiring less than 0.1% of pre-training tokens compared to standard models. It also offers faster processing for sequences longer than 8K tokens.

Conclusion: LAWCAT provides an efficient solution to the limitations of traditional and emerging linear transformers, enabling resource-efficient long-context modeling suitable for edge deployment while reducing dependency on extensive computational resources.

Abstract: Although transformer architectures have achieved state-of-the-art performance
across diverse domains, their quadratic computational complexity with respect
to sequence length remains a significant bottleneck, particularly for
latency-sensitive long-context applications. While recent linear-complexity
alternatives are increasingly powerful, effectively training them from scratch
is still resource-intensive. To overcome these limitations, we propose LAWCAT
(Linear Attention with Convolution Across Time), a novel linearization
framework designed to efficiently transfer the capabilities of pre-trained
transformers into a performant linear attention architecture. LAWCAT integrates
causal Conv1D layers to enhance local dependency modeling and employs
normalized gated linear attention to improve generalization across varying
context lengths. Our comprehensive evaluations demonstrate that, distilling
Mistral-7B with only 1K-length sequences yields over 90\% passkey retrieval
accuracy up to 22K tokens, significantly extending its effective context
window. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance
on S-NIAH 1\&2\&3 tasks (1K-8K context length) and BABILong benchmark
(QA2\&QA3, 0K-16K context length), requiring less than 0.1\% pre-training
tokens compared with pre-training models. Furthermore, LAWCAT exhibits faster
prefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT
thus provides an efficient pathway to high-performance, long-context linear
models suitable for edge deployment, reducing reliance on extensive
long-sequence training data and computational resources.

</details>


### [69] [Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference](https://arxiv.org/abs/2509.18487)
*Ben Finkelshtein,Silviu Cucerzan,Sujay Kumar Jauhar,Ryen White*

Main category: cs.CL

TL;DR: The paper evaluates how large language models (LLMs) interact with graph data for tasks like node classification, revealing insights into their strengths and weaknesses.


<details>
  <summary>Details</summary>
Motivation: To understand the capabilities of LLMs in graph machine learning tasks, given their growing popularity and diverse applications.

Method: The study performs controlled evaluations across multiple variables including interaction modes (prompting, tool-use, code generation), graph domain types, structural regimes, feature characteristics, and model configurations. Dependencies on various input elements are analyzed.

Result: LLMs as code generators perform best, especially for complex graph structures and long-text attributes. All interaction strategies work effectively on heterophilic graphs, which challenges assumptions about LLM performance. Code generation adapts flexibly to leverage the most informative input.

Conclusion: The findings offer actionable guidance on leveraging LLMs for graph tasks, emphasizing code generation and adaptability, while providing key design principles for future methods.

Abstract: Large language models (LLMs) are increasingly used for text-rich graph
machine learning tasks such as node classification in high-impact domains like
fraud detection and recommendation systems. Yet, despite a surge of interest,
the field lacks a principled understanding of the capabilities of LLMs in their
interaction with graph data. In this work, we conduct a large-scale, controlled
evaluation across several key axes of variability to systematically assess the
strengths and weaknesses of LLM-based graph reasoning methods in text-based
applications. The axes include the LLM-graph interaction mode, comparing
prompting, tool-use, and code generation; dataset domains, spanning citation,
web-link, e-commerce, and social networks; structural regimes contrasting
homophilic and heterophilic graphs; feature characteristics involving both
short- and long-text node attributes; and model configurations with varying LLM
sizes and reasoning capabilities. We further analyze dependencies by
methodically truncating features, deleting edges, and removing labels to
quantify reliance on input types. Our findings provide practical and actionable
guidance. (1) LLMs as code generators achieve the strongest overall performance
on graph data, with especially large gains on long-text or high-degree graphs
where prompting quickly exceeds the token budget. (2) All interaction
strategies remain effective on heterophilic graphs, challenging the assumption
that LLM-based methods collapse under low homophily. (3) Code generation is
able to flexibly adapt its reliance between structure, features, or labels to
leverage the most informative input type. Together, these findings provide a
comprehensive view of the strengths and limitations of current LLM-graph
interaction modes and highlight key design principles for future approaches.

</details>


### [70] [A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition](https://arxiv.org/abs/2509.18514)
*Mohamad Elzohbi,Richard Zhao*

Main category: cs.CL

TL;DR: The paper presents a system using ByT5 to insert phrases into Arabic poems to match rhythmic patterns while keeping their meaning intact.


<details>
  <summary>Details</summary>
Motivation: To develop a method that ensures classical Arabic poems maintain both their rhythm and semantic coherence during phrase insertion.

Method: The study fine-tuned ByT5 using a rule-based grapheme-to-beat transformation for rhythm extraction, a conditional denoising objective to reconstruct words matching rhythms, and a curriculum learning strategy with cross-lingual transfer from English.

Result: The model achieves high rhythmic alignment and semantic coherence during phrase insertion in Arabic poetry.

Conclusion: This methodology has promising potential in co-creative applications for generating and modifying classical Arabic poetry with rhythmic accuracy.

Abstract: This paper presents a methodology for inserting phrases in Arabic poems to
conform to a specific rhythm using ByT5, a byte-level multilingual
transformer-based model. Our work discusses a rule-based grapheme-to-beat
transformation tailored for extracting the rhythm from fully diacritized Arabic
script. Our approach employs a conditional denoising objective to fine-tune
ByT5, where the model reconstructs masked words to match a target rhythm. We
adopt a curriculum learning strategy, pre-training on a general Arabic dataset
before fine-tuning on poetic dataset, and explore cross-lingual transfer from
English to Arabic. Experimental results demonstrate that our models achieve
high rhythmic alignment while maintaining semantic coherence. The proposed
model has the potential to be used in co-creative applications in the process
of composing classical Arabic poems.

</details>


### [71] [Trace Is In Sentences: Unbiased Lightweight ChatGPT-Generated Text Detector](https://arxiv.org/abs/2509.18535)
*Mo Mu,Dianqiao Lei,Chang Li*

Main category: cs.CL

TL;DR: This paper proposes a lightweight framework to detect original and paraphrased AI-generated text by focusing on the internal structure of text rather than word patterns.


<details>
  <summary>Details</summary>
Motivation: The misuse of ChatGPT has raised concerns, necessitating detection methods for both original and paraphrased AI-generated texts that are robust against biases, word-level changes, and require minimal computational resources.

Method: The authors developed a detection framework by encoding sentence embeddings using pre-trained language models, leveraging attention to model relationships, applying contrastive learning to address embedding biases, and using causal graphs with counterfactual methods to identify structural features.

Result: Experiments on curated datasets confirmed the framework effectively detects AI-generated and paraphrased texts while mitigating biases and remaining computationally efficient.

Conclusion: The proposed method is a robust and lightweight solution for detecting both original and paraphrased AI-generated texts, demonstrating its potential as a practical alternative to traditional word-level methods.

Abstract: The widespread adoption of ChatGPT has raised concerns about its misuse,
highlighting the need for robust detection of AI-generated text. Current
word-level detectors are vulnerable to paraphrasing or simple prompts (PSP),
suffer from biases induced by ChatGPT's word-level patterns (CWP) and training
data content, degrade on modified text, and often require large models or
online LLM interaction. To tackle these issues, we introduce a novel task to
detect both original and PSP-modified AI-generated texts, and propose a
lightweight framework that classifies texts based on their internal structure,
which remains invariant under word-level changes. Our approach encodes sentence
embeddings from pre-trained language models and models their relationships via
attention. We employ contrastive learning to mitigate embedding biases from
autoregressive generation and incorporate a causal graph with counterfactual
methods to isolate structural features from topic-related biases. Experiments
on two curated datasets, including abstract comparisons and revised life FAQs,
validate the effectiveness of our method.

</details>


### [72] [CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs](https://arxiv.org/abs/2509.18536)
*Jin Young Kim,Ji Won Yoon*

Main category: cs.CL

TL;DR: This paper introduces CCQA, a method to improve inference reasoning in small language models (SLMs) using cycle-consistency and a Flan-T5 model for question generation.


<details>
  <summary>Details</summary>
Motivation: Investigate why current inference-time reasoning strategies benefit large models but are less effective for smaller language models.

Method: CCQA evaluates reasoning paths by generating corresponding questions and comparing their similarity to the original input, leveraging a specialized Flan-T5 model for question generation.

Result: CCQA outperforms previous methods on mathematical and commonsense reasoning tasks over eight SLMs.

Conclusion: CCQA represents an efficient and practical improvement over existing methods, providing a new baseline for reasoning in SLMs.

Abstract: Recently, inference-time reasoning strategies have further improved the
accuracy of large language models (LLMs), but their effectiveness on smaller
models remains unclear. Based on the observation that conventional approaches
often fail to improve performance in this context, we propose
\textbf{C}ycle-\textbf{C}onsistency in \textbf{Q}uestion \textbf{A}nswering
(CCQA), a novel reasoning method that can be effectively applied to SLMs.
Inspired by cycle consistency, CCQA generates a question from each reasoning
path and answer, evaluates each by its similarity to the original question, and
then selects the candidate solution with the highest similarity score as the
final response. Since conventional SLMs struggle to generate accurate questions
from their own reasoning paths and answers, we employ a lightweight Flan-T5
model specialized for question generation to support this process efficiently.
From the experimental results, it is verified that CCQA consistently
outperforms existing state-of-the-art (SOTA) methods across eight models on
mathematical and commonsense reasoning benchmarks. Furthermore, our method
establishes a new practical baseline for efficient reasoning in SLMs. Source
code can be found at https://github.com/scai-research/ccqa_official.

</details>


### [73] [Prior-based Noisy Text Data Filtering: Fast and Strong Alternative For Perplexity](https://arxiv.org/abs/2509.18577)
*Yeongbin Seo,Gayoung Kim,Jaehyung Kim,Jinyoung Yeo*

Main category: cs.CL

TL;DR: The paper introduces a fast, linguistics-inspired data filtering method for large language models that achieves superior performance and drastically reduces time costs compared to perplexity-based methods.


<details>
  <summary>Details</summary>
Motivation: Pretrained large language models require effective data filtering due to the noisy and diverse nature of massive web corpora.

Method: The authors propose a prior-based filtering approach that uses token priors derived from corpus-level term frequency statistics, evaluating documents based on mean and standard deviation without requiring model inference.

Result: The proposed method achieved the highest average performance across 20 benchmarks while being 1000x faster than perplexity-based filtering.

Conclusion: The prior-based filter is simple yet effective, adaptable for code, math, and multilingual datasets, and provides a competitive and efficient alternative to traditional filtering methods.

Abstract: As large language models (LLMs) are pretrained on massive web corpora,
careful selection of data becomes essential to ensure effective and efficient
learning. While perplexity (PPL)-based filtering has shown strong performance,
it suffers from drawbacks: substantial time costs and inherent unreliability of
the model when handling noisy or out-of-distribution samples. In this work, we
propose a simple yet powerful alternative: a prior-based data filtering method
that estimates token priors using corpus-level term frequency statistics,
inspired by linguistic insights on word roles and lexical density. Our approach
filters documents based on the mean and standard deviation of token priors,
serving as a fast proxy to PPL while requiring no model inference. Despite its
simplicity, the prior-based filter achieves the highest average performance
across 20 downstream benchmarks, while reducing time cost by over 1000x
compared to PPL-based filtering. We further demonstrate its applicability to
symbolic languages such as code and math, and its dynamic adaptability to
multilingual corpora without supervision

</details>


### [74] [TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning](https://arxiv.org/abs/2509.18585)
*Yu Chen,Yifei Han,Long Zhang,Yue Du,Bin Li*

Main category: cs.CL

TL;DR: The paper introduces TsqLoRA, a method that enhances efficiency and performance of fine-tuning NLP models by focusing on data quality and layer sensitivity.


<details>
  <summary>Details</summary>
Motivation: Current fine-tuning methods of large NLP models face computational and memory challenges, and they often neglect model layer sensitivity and data quality.

Method: The approach combines two elements: quality-aware data sampling to select informative training data and dynamic rank allocation that adapts each model layer's sensitivity to updates.

Result: TsqLoRA achieves improved fine-tuning efficiency and maintains or improves NLP task performance across various benchmarks.

Conclusion: TsqLoRA provides a viable, resource-efficient alternative for fine-tuning NLP models, optimizing performance while lowering computational demands.

Abstract: Fine-tuning large pre-trained models for downstream tasks has become a
fundamental approach in natural language processing. Fully fine-tuning all
model parameters is computationally expensive and memory-intensive, especially
in resource-constrained environments. Existing parameter-efficient fine-tuning
methods reduce the number of trainable parameters but typically overlook the
varying sensitivity of different model layers and the importance of training
data. In this work, we propose TsqLoRA, a novel method that integrates
data-quality-driven selection with sensitivity-aware low-rank adaptation,
consisted of two main components: a quality-aware sampling mechanism for
selecting the most informative training data, and a dynamic rank allocation
module that adjusts the rank of each layer based on its sensitivity to
parameter updates. The experimental results demonstrate that TsqLoRA improves
fine-tuning efficiency while maintaining or even improving performance on a
variety of NLP tasks. Our code will be available at
https://github.com/Benjamin-Ricky/TsqLoRA.

</details>


### [75] [UniECG: Understanding and Generating ECG in One Unified Model](https://arxiv.org/abs/2509.18588)
*Jiarui Jin,Haoyu Wang,Xiang Lan,Jun Li,Gaofeng Cheng,Hongyan Li,Shenda Hong*

Main category: cs.CL

TL;DR: The paper introduces UniECG, a unified model for ECG interpretation and generation, addressing limitations of existing vision-language models.


<details>
  <summary>Details</summary>
Motivation: Addressing the insufficiency of unified models like GPT-5 in supporting ECG signal understanding and generation.

Method: Proposed a two-stage training approach: 1) Evidence-based ECG interpretation (ECG-to-Text) and 2) Text-to-ECG generation via latent space alignment.

Result: UniECG enables both ECG interpretation and text-conditioned ECG generation, extending current ECG model capabilities.

Conclusion: UniECG represents a significant advancement in ECG modeling by bridging the gap between interpretation and generation tasks.

Abstract: Recent unified models such as GPT-5 have achieved encouraging progress on
vision-language tasks. However, these unified models typically fail to
correctly understand ECG signals and provide accurate medical diagnoses, nor
can they correctly generate ECG signals. To address these limitations, we
propose UniECG, the first unified model for ECG capable of concurrently
performing evidence-based ECG interpretation and text-conditioned ECG
generation tasks. Through a decoupled two-stage training approach, the model
first learns evidence-based interpretation skills (ECG-to-Text), and then
injects ECG generation capabilities (Text-to-ECG) via latent space alignment.
UniECG can autonomously choose to interpret or generate an ECG based on user
input, significantly extending the capability boundaries of current ECG models.
Our code and checkpoints will be made publicly available at
https://github.com/PKUDigitalHealth/UniECG upon acceptance.

</details>


### [76] [A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users](https://arxiv.org/abs/2509.18632)
*Nishant Balepur,Matthew Shu,Yoo Yeon Sung,Seraphina Goldfarb-Tarrant,Shi Feng,Fumeng Yang,Rachel Rudinger,Jordan Lee Boyd-Graber*

Main category: cs.CL

TL;DR: This paper investigates whether user preferences for LLM-generated plans align with their actual helpfulness in completing complex tasks. It finds misalignments and argues for alignment methods based on real user interactions rather than surface-level preferences.


<details>
  <summary>Details</summary>
Motivation: The authors aim to explore whether the alignment methods used for evaluating and training language models (e.g., based on user preferences) truly reflect their helpfulness in guiding users through complex, multi-step tasks.

Method: The study introduces Planorama, an interface where 126 participants executed 4388 LLM-generated plans and provided 5584 comparative evaluations. They analyzed helpfulness using success in question answering and recreated the setup in agents and reward models.

Result: The study found that user preferences and common alignment feedback often fail to accurately predict helpful plans. Preferences are influenced by surface-level characteristics like brevity, which don't correlate with actual helpfulness.

Conclusion: The authors argue that aligning LLMs to be helpful requires feedback directly derived from real-world user interactions rather than relying solely on user preferences or superficial plan features.

Abstract: To assist users in complex tasks, LLMs generate plans: step-by-step
instructions towards a goal. While alignment methods aim to ensure LLM plans
are helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer,
assuming this reflects what helps them. We test this with Planorama: an
interface where 126 users answer 300 multi-step questions with LLM plans. We
get 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA
success) and user preferences on plans, and recreate the setup in agents and
reward models to see if they simulate or prefer what helps users. We expose: 1)
user/model preferences and agent success do not accurately predict which plans
help users, so common alignment feedback can misalign with helpfulness; 2) this
gap is not due to user-specific preferences, as users are similarly successful
when using plans they prefer/disprefer; 3) surface-level cues like brevity and
question similarity strongly link to preferences, but such biases fail to
predict helpfulness. In all, we argue aligning helpful LLMs needs feedback from
real user interactions, not just preferences of what looks helpful, so we
discuss the plan NLP researchers can execute to solve this problem.

</details>


### [77] [Consistency-Aware Parameter-Preserving Knowledge Editing Framework for Multi-Hop Question Answering](https://arxiv.org/abs/2509.18655)
*Lingwen Deng,Yifei Han,Long Zhang,Yue Du,Bin Li*

Main category: cs.CL

TL;DR: CAPE-KG introduces a consistency-aware framework for Parameter-Preserving Knowledge Editing (PPKE) in multi-hop question answering (MHQA) tasks, addressing issues of knowledge contamination and inconsistent updates. It achieves improved performance on the MQuAKE benchmark.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to overcome the inconsistencies in existing PPKE approaches for multi-hop reasoning tasks, such as knowledge contamination and unstable updates, which undermine the reliability of models in MHQA.

Method: The paper introduces CAPE-KG, a framework that aligns KG construction, updates, and retrieval with the needs of MHQA to ensure consistency. It focuses on maintaining coherent reasoning across edited and unedited knowledge.

Result: Experiments on the MQuAKE benchmark show that CAPE-KG improves accuracy and enhances the overall PPKE performance for MHQA.

Conclusion: Addressing consistency challenges in PPKE through the CAPE-KG framework offers a reliable approach for knowledge editing in MHQA, leading to improved task performance.

Abstract: Parameter-Preserving Knowledge Editing (PPKE) enables updating models with
new or corrected information without retraining or parameter adjustment. Recent
PPKE approaches based on knowledge graphs (KG) to extend knowledge editing (KE)
capabilities to multi-hop question answering (MHQA). However, these methods
often lack consistency, leading to knowledge contamination, unstable updates,
and retrieval behaviors that fail to reflect the intended edits. Such
inconsistencies undermine the reliability of PPKE in multi- hop reasoning. We
present CAPE-KG, Consistency-Aware Parameter-Preserving Editing with Knowledge
Graphs, a novel consistency-aware framework for PPKE on MHQA. CAPE-KG ensures
KG construction, update, and retrieval are always aligned with the requirements
of the MHQA task, maintaining coherent reasoning over both unedited and edited
knowledge. Extensive experiments on the MQuAKE benchmark show accuracy
improvements in PPKE performance for MHQA, demonstrating the effectiveness of
addressing consistency in PPKE.

</details>


### [78] [Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal Prediction](https://arxiv.org/abs/2509.18658)
*Huanxin Sheng,Xinyi Liu,Hangfeng He,Jieyu Zhao,Jian Kang*

Main category: cs.CL

TL;DR: The paper presents a framework using conformal prediction to analyze uncertainty in LLM-based evaluations, offering prediction intervals with coverage guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the lack of reliability in using LLMs as evaluators for natural language generation, which limits their broader application.

Method: The authors introduce conformal prediction to create prediction intervals from LLM evaluations, propose an ordinal boundary adjustment for discrete tasks, and suggest a midpoint-based scoring method.

Result: Experiments demonstrate the validity of conformal prediction in offering reliable prediction intervals and evaluate the effectiveness of midpoint scoring and judge reprompting.

Conclusion: The framework enhances the reliability of LLM-based evaluations, making them more trustworthy through uncertainty quantification and improved judgment techniques.

Abstract: LLM-as-a-judge has become a promising paradigm for using large language
models (LLMs) to evaluate natural language generation (NLG), but the
uncertainty of its evaluation remains underexplored. This lack of reliability
may limit its deployment in many applications. This work presents the first
framework to analyze the uncertainty by offering a prediction interval of
LLM-based scoring via conformal prediction. Conformal prediction constructs
continuous prediction intervals from a single evaluation run, and we design an
ordinal boundary adjustment for discrete rating tasks. We also suggest a
midpoint-based score within the interval as a low-bias alternative to raw model
score and weighted average. We perform extensive experiments and analysis,
which show that conformal prediction can provide valid prediction interval with
coverage guarantees. We also explore the usefulness of interval midpoint and
judge reprompting for better judgment.

</details>


### [79] [MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service](https://arxiv.org/abs/2509.18713)
*Yizhe Huang,Yang Liu,Ruiyu Zhao,Xiaolong Zhong,Xingming Yue,Ling Jiang*

Main category: cs.CL

TL;DR: The paper proposes MemOrb, a lightweight memory layer for large language model (LLM) agents to improve task success rate and consistency across sessions.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents are prone to forgetting, repeating errors, and lacking continual self-improvement, making them unreliable in dynamic customer service environments.

Method: The authors introduce MemOrb, a verbal reinforcement memory layer that stores strategy reflections from multi-turn interactions in a shared memory bank. This memory aids decision-making without fine-tuning.

Result: The proposed method improved task success by up to 63 percentage points and achieved more consistent results across repeated trials.

Conclusion: Structured reflection using MemOrb enhances the long-term reliability and performance of frozen LLM agents in customer service scenarios.

Abstract: Large Language Model-based agents(LLM-based agents) are increasingly deployed
in customer service, yet they often forget across sessions, repeat errors, and
lack mechanisms for continual self-improvement. This makes them unreliable in
dynamic settings where stability and consistency are critical. To better
evaluate these properties, we emphasize two indicators: task success rate as a
measure of overall effectiveness, and consistency metrics such as Pass$^k$ to
capture reliability across multiple trials. To address the limitations of
existing approaches, we propose MemOrb, a lightweight and plug-and-play verbal
reinforcement memory layer that distills multi-turn interactions into compact
strategy reflections. These reflections are stored in a shared memory bank and
retrieved to guide decision-making, without requiring any fine-tuning.
Experiments show that MemOrb significantly improves both success rate and
stability, achieving up to a 63 percentage-point gain in multi-turn success
rate and delivering more consistent performance across repeated trials. Our
results demonstrate that structured reflection is a powerful mechanism for
enhancing long-term reliability of frozen LLM agents in customer service
scenarios.

</details>


### [80] [LOTUSDIS: A Thai far-field meeting corpus for robust conversational ASR](https://arxiv.org/abs/2509.18722)
*Pattara Tipaksorn,Sumonmas Thatphithakkul,Vataya Chunwijitra,Kwanchiva Thangthai*

Main category: cs.CL

TL;DR: LOTUSDIS is a Thai meeting dataset for far-field conversational ASR, featuring 114 hours of spontaneous speech captured with multiple microphone types at distances. The dataset improves ASR model robustness when fine-tuned.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve far-field Thai conversational ASR systems which struggle with robustness due to variations in distance and noise.

Method: The authors collected 114 hours of spontaneous dialogues recorded with nine independent single-channel devices across six microphone types at varied distances, and provided a reproducible training and evaluation baseline.

Result: Fine-tuning ASR models on LOTUSDIS significantly reduced WER, especially for far-field conditions, demonstrating the value of distance-diverse training data.

Conclusion: The work highlights the importance of distance-diverse datasets for ASR robustness and promotes reproducible research by releasing the dataset, scripts, and benchmarks.

Abstract: We present LOTUSDIS, a publicly available Thai meeting corpus designed to
advance far-field conversational ASR. The dataset comprises 114 hours of
spontaneous, unscripted dialogue collected in 15-20 minute sessions with three
participants, where overlapping speech is frequent and natural. Speech was
recorded simultaneously by nine independent single-channel devices spanning six
microphone types at distances from 0.12 m to 10 m, preserving the authentic
effects of reverberation, noise, and device coloration without relying on
microphone arrays. We provide standard train, dev, test splits and release a
reproducible baseline system. We benchmarked several Whisper variants under
zero-shot and fine-tuned conditions. Off-the-shelf models showed strong
degradation with distance, confirming a mismatch between pre-training data and
Thai far-field speech. Fine-tuning on LOTUSDIS dramatically improved
robustness: a Thai Whisper baseline reduced overall WER from 64.3 to 38.3 and
far-field WER from 81.6 to 49.5, with especially large gains on the most
distant microphones. These results underscore the importance of
distance-diverse training data for robust ASR. The corpus is available under
CC-BY-SA 4.0. We also release training and evaluation scripts as a baseline
system to promote reproducible research in this field.

</details>


### [81] [Global-Recent Semantic Reasoning on Dynamic Text-Attributed Graphs with Large Language Models](https://arxiv.org/abs/2509.18742)
*Yunan Wang,Jianxin Li,Ziwei Zhang*

Main category: cs.CL

TL;DR: This paper introduces DyGRASP, a new framework combining LLMs and temporal GNNs to efficiently handle Dynamic Text-Attribute Graphs (DyTAGs) for dynamic semantic reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing models for graph reasoning, such as GNNs and LLMs, struggle with dynamic graphs that evolve over time due to inefficiencies and lack of methods to handle recent-global temporal semantics.

Method: DyGRASP incorporates a node-centric reasoning approach with a sliding window mechanism for recent temporal semantics and uses tailored prompts with an RNN-like structure to capture global semantic dynamics.

Result: DyGRASP achieves significant improvements, including up to a 34% boost in Hit@10 for destination node retrieval tasks and strong cross-model generalization.

Conclusion: The proposed DyGRASP method effectively addresses semantic complexities in dynamic graphs, demonstrating its flexibility and performance across benchmarks and models.

Abstract: Dynamic Text-Attribute Graphs (DyTAGs), characterized by time-evolving graph
interactions and associated text attributes, are prevalent in real-world
applications. Existing methods, such as Graph Neural Networks (GNNs) and Large
Language Models (LLMs), mostly focus on static TAGs. Extending these existing
methods to DyTAGs is challenging as they largely neglect the recent-global
temporal semantics: the recent semantic dependencies among interaction texts
and the global semantic evolution of nodes over time. Furthermore, applying
LLMs to the abundant and evolving text in DyTAGs faces efficiency issues. To
tackle these challenges, we propose Dynamic Global-Recent Adaptive Semantic
Processing (DyGRASP), a novel method that leverages LLMs and temporal GNNs to
efficiently and effectively reason on DyTAGs. Specifically, we first design a
node-centric implicit reasoning method together with a sliding window mechanism
to efficiently capture recent temporal semantics. In addition, to capture
global semantic dynamics of nodes, we leverage explicit reasoning with tailored
prompts and an RNN-like chain structure to infer long-term semantics. Lastly,
we intricately integrate the recent and global temporal semantics as well as
the dynamic graph structural information using updating and merging layers.
Extensive experiments on DyTAG benchmarks demonstrate DyGRASP's superiority,
achieving up to 34% improvement in Hit@10 for destination node retrieval task.
Besides, DyGRASP exhibits strong generalization across different temporal GNNs
and LLMs.

</details>


### [82] [False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual Language Models](https://arxiv.org/abs/2509.18750)
*Julie Kallini,Dan Jurafsky,Christopher Potts,Martijn Bartelds*

Main category: cs.CL

TL;DR: The study explores the impact of token overlap in multilingual models, finding that overlap improves cross-lingual transfer and semantic representation.


<details>
  <summary>Details</summary>
Motivation: To determine whether overlapping tokens in multilingual tokenizers facilitate or hinder cross-lingual transfer.

Method: Conduct controlled experiments with bilingual autoregressive models across different vocabulary overlap settings, analyzing hidden representations and performance.

Result: Models with token overlap outperformed disjoint vocabulary models on XNLI and XQuAD, showing better transfer as overlap increases.

Conclusion: Token overlap is advantageous for multilingual models, enhancing semantic relationships and transfer performance in cross-lingual tasks.

Abstract: Subword tokenizers trained on multilingual corpora naturally produce
overlapping tokens across languages. Does token overlap facilitate
cross-lingual transfer or instead introduce interference between languages?
Prior work offers mixed evidence, partly due to varied setups and confounders,
such as token frequency or subword segmentation granularity. To address this
question, we devise a controlled experiment where we train bilingual
autoregressive models on multiple language pairs under systematically varied
vocabulary overlap settings. Crucially, we explore a new dimension to
understanding how overlap affects transfer: the semantic similarity of tokens
shared across languages. We first analyze our models' hidden representations
and find that overlap of any kind creates embedding spaces that capture
cross-lingual semantic relationships, while this effect is much weaker in
models with disjoint vocabularies. On XNLI and XQuAD, we find that models with
overlap outperform models with disjoint vocabularies, and that transfer
performance generally improves as overlap increases. Overall, our findings
highlight the advantages of token overlap in multilingual models and show that
substantial shared vocabulary remains a beneficial design choice for
multilingual tokenizers.

</details>


### [83] [When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models](https://arxiv.org/abs/2509.18762)
*Yingming Zheng,Hanqi Li,Kai Yu,Lu Chen*

Main category: cs.CL

TL;DR: The paper investigates how supervised fine-tuning (SFT) with long-context data affects language model behavior on short-context tasks, finding that long-context SFT surprisingly improves performance.


<details>
  <summary>Details</summary>
Motivation: To address unclear implications and explore contradictions in how long-context data affects performance during supervised fine-tuning of language models.

Method: Systematic investigations, including analysis of model components such as Multi-Head Attention and Feed-Forward Networks, alongside hybrid training methods to reduce biases.

Result: Long-context SFT enhances short-context performance, reveals interactions between components, and identifies a knowledge preference bias toward contextual knowledge.

Conclusion: Hybrid training offers a promising solution to mitigate biases, providing explainable fine-tuning strategies for language models.

Abstract: Large language models (LLMs) have achieved impressive performance across
natural language processing (NLP) tasks. As real-world applications
increasingly demand longer context windows, continued pretraining and
supervised fine-tuning (SFT) on long-context data has become a common approach.
While the effects of data length in continued pretraining have been extensively
studied, their implications for SFT remain unclear. In this work, we
systematically investigate how SFT data length influences LLM behavior on
short-context tasks. Counterintuitively, we find that long-context SFT improves
short-context performance, contrary to the commonly observed degradation from
long-context pretraining. To uncover the underlying mechanisms of this
phenomenon, we first decouple and analyze two key components, Multi-Head
Attention (MHA) and Feed-Forward Network (FFN), and show that both
independently benefit from long-context SFT. We further study their interaction
and reveal a knowledge preference bias: long-context SFT promotes contextual
knowledge, while short-context SFT favors parametric knowledge, making
exclusive reliance on long-context SFT suboptimal. Finally, we demonstrate that
hybrid training mitigates this bias, offering explainable guidance for
fine-tuning LLMs.

</details>


### [84] [Financial Risk Relation Identification through Dual-view Adaptation](https://arxiv.org/abs/2509.18775)
*Wei-Ning Chiu,Yu-Hsiang Wang,Andy Hsiao,Yu-Shiang Huang,Chuan-Ju Wang*

Main category: cs.CL

TL;DR: The paper proposes using Form 10-K financial filings and natural language processing to systematically identify inter-firm risk relations, outperforming traditional manual analysis methods.


<details>
  <summary>Details</summary>
Motivation: Inter-firm risk events can significantly influence portfolio management and investment strategies, but existing methods relying on expert judgment are labor-intensive and lack scalability.

Method: The paper introduces an unsupervised fine-tuning approach to extract risk relationships from Form 10-K filings using NLP techniques, developing a financial encoder and a quantitative risk relation scoring system.

Result: Experiments confirm the approach's effectiveness, showing it surpasses baseline methods under various evaluation settings.

Conclusion: The authors present a scalable and interpretable system for identifying risk relations, offering a superior alternative to manual expert-based risk assessment.

Abstract: A multitude of interconnected risk events -- ranging from regulatory changes
to geopolitical tensions -- can trigger ripple effects across firms.
Identifying inter-firm risk relations is thus crucial for applications like
portfolio management and investment strategy. Traditionally, such assessments
rely on expert judgment and manual analysis, which are, however, subjective,
labor-intensive, and difficult to scale. To address this, we propose a
systematic method for extracting inter-firm risk relations using Form 10-K
filings -- authoritative, standardized financial documents -- as our data
source. Leveraging recent advances in natural language processing, our approach
captures implicit and abstract risk connections through unsupervised
fine-tuning based on chronological and lexical patterns in the filings. This
enables the development of a domain-specific financial encoder with a deeper
contextual understanding and introduces a quantitative risk relation score for
transparency, interpretable analysis. Extensive experiments demonstrate that
our method outperforms strong baselines across multiple evaluation settings.

</details>


### [85] [AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field](https://arxiv.org/abs/2509.18776)
*Chen Liang,Zhaoqi Huang,Haofen Wang,Fu Chai,Chunying Yu,Huanhuan Wei,Zhengjie Liu,Yanpeng Li,Hongjun Wang,Ruifeng Luo,Xianzhong Zhao*

Main category: cs.CL

TL;DR: The paper introduces AECBench, a benchmark that evaluates the robustness and limitations of large language models (LLMs) in the Architecture, Engineering, and Construction (AEC) domain, revealing performance declines at higher cognitive levels.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of assessing LLMs in a safety-critical domain like Architecture, Engineering, and Construction, and ensure their reliable application in such contexts.

Method: The study created AECBench with 23 representative tasks across five cognitive levels, developed a 4,800-question dataset derived from actual AEC practice, and utilized an LLM-as-a-Judge approach for evaluating complex responses.

Result: Nine LLMs were tested, showing proficiency in basic tasks but significant performance deficits in higher cognitive tasks, such as reasoning, calculation, and specialized document generation.

Conclusion: This work provides a foundation for future advancements in integrating LLMs into critical engineering tasks, highlighting areas for improvement in their capabilities.

Abstract: Large language models (LLMs), as a novel information technology, are seeing
increasing adoption in the Architecture, Engineering, and Construction (AEC)
field. They have shown their potential to streamline processes throughout the
building lifecycle. However, the robustness and reliability of LLMs in such a
specialized and safety-critical domain remain to be evaluated. To address this
challenge, this paper establishes AECBench, a comprehensive benchmark designed
to quantify the strengths and limitations of current LLMs in the AEC domain.
The benchmark defines 23 representative tasks within a five-level
cognition-oriented evaluation framework encompassing Knowledge Memorization,
Understanding, Reasoning, Calculation, and Application. These tasks were
derived from authentic AEC practice, with scope ranging from codes retrieval to
specialized documents generation. Subsequently, a 4,800-question dataset
encompassing diverse formats, including open-ended questions, was crafted
primarily by engineers and validated through a two-round expert review.
Furthermore, an LLM-as-a-Judge approach was introduced to provide a scalable
and consistent methodology for evaluating complex, long-form responses
leveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear
performance decline across five cognitive levels was revealed. Despite
demonstrating proficiency in foundational tasks at the Knowledge Memorization
and Understanding levels, the models showed significant performance deficits,
particularly in interpreting knowledge from tables in building codes, executing
complex reasoning and calculation, and generating domain-specific documents.
Consequently, this study lays the groundwork for future research and
development aimed at the robust and reliable integration of LLMs into
safety-critical engineering practices.

</details>


### [86] [Beyond the Leaderboard: Understanding Performance Disparities in Large Language Models via Model Diffing](https://arxiv.org/abs/2509.18792)
*Sabri Boughorbel,Fahim Dalvi,Nadir Durrani,Majd Hawasly*

Main category: cs.CL

TL;DR: The paper uses a mechanistic interpretability approach called model diffing to identify detailed capability differences between two large language models, showing how fine-tuning changes latent representations and improves specific functions.


<details>
  <summary>Details</summary>
Motivation: To understand how fine-tuning alters large language models (LLMs) and provide a detailed explanation beyond traditional benchmarks.

Method: The study employs model diffing using crosscoders to identify and categorize latent representations differentiating Gemma-2-9b-it and a SimPO-enhanced version.

Result: SimPO improves safety (+32.8%), multilingual abilities (+43.8%), and instructions-following (+151.7%) while reducing self-reference (-44.1%) and hallucination management (-68.5%).

Conclusion: Model diffing allows fine-grained analysis of LLMs' performance differences, offering a more clear and targeted comparison framework beyond conventional metrics.

Abstract: As fine-tuning becomes the dominant paradigm for improving large language
models (LLMs), understanding what changes during this process is increasingly
important. Traditional benchmarking often fails to explain why one model
outperforms another. In this work, we use model diffing, a mechanistic
interpretability approach, to analyze the specific capability differences
between Gemma-2-9b-it and a SimPO-enhanced variant. Using crosscoders, we
identify and categorize latent representations that differentiate the two
models. We find that SimPO acquired latent concepts predominantly enhance
safety mechanisms (+32.8%), multilingual capabilities (+43.8%), and
instruction-following (+151.7%), while its additional training also reduces
emphasis on model self-reference (-44.1%) and hallucination management
(-68.5%). Our analysis shows that model diffing can yield fine-grained insights
beyond leaderboard metrics, attributing performance gaps to concrete
mechanistic capabilities. This approach offers a transparent and targeted
framework for comparing LLMs.

</details>


### [87] [MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction](https://arxiv.org/abs/2509.18813)
*Liting Zhang,Shiwan Zhao,Aobo Kong,Qicheng Li*

Main category: cs.CL

TL;DR: MAPEX introduces a multi-agent collaborative framework for keyphrase extraction, dynamically adapting to document length and achieving superior performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current unsupervised LLM-based keyphrase extraction methods use uniform prompting, which limits their ability to exploit LLMs across diverse scenarios and document complexities.

Method: MAPEX employs a multi-agent framework with modules for expert recruitment, candidate extraction, topic guidance, knowledge augmentation, and post-processing, along with a dual-path strategy tailored to document length.

Result: MAPEX outperformed state-of-the-art unsupervised methods by 2.44% and standard LLM baselines by 4.01% in F1@5 across six datasets and three different LLMs.

Conclusion: MAPEX enhances the generalization and universality of LLMs in keyphrase extraction, offering a more effective solution than previous unsupervised methods and demonstrating adaptability to diverse scenarios.

Abstract: Keyphrase extraction is a fundamental task in natural language processing.
However, existing unsupervised prompt-based methods for Large Language Models
(LLMs) often rely on single-stage inference pipelines with uniform prompting,
regardless of document length or LLM backbone. Such one-size-fits-all designs
hinder the full exploitation of LLMs' reasoning and generation capabilities,
especially given the complexity of keyphrase extraction across diverse
scenarios. To address these challenges, we propose MAPEX, the first framework
that introduces multi-agent collaboration into keyphrase extraction. MAPEX
coordinates LLM-based agents through modules for expert recruitment, candidate
extraction, topic guidance, knowledge augmentation, and post-processing. A
dual-path strategy dynamically adapts to document length: knowledge-driven
extraction for short texts and topic-guided extraction for long texts.
Extensive experiments on six benchmark datasets across three different LLMs
demonstrate its strong generalization and universality, outperforming the
state-of-the-art unsupervised method by 2.44\% and standard LLM baselines by
4.01\% in F1@5 on average. Code is available at
https://github.com/NKU-LITI/MAPEX.

</details>


### [88] [Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?](https://arxiv.org/abs/2509.18843)
*Damian Stachura,Joanna Konieczna,Artur Nowak*

Main category: cs.CL

TL;DR: This study assessed small open-weight large language models (LLMs) against proprietary LLMs in biomedical question-answering, finding that they can perform comparably and sometimes surpass closed-source models using ensemble strategies.


<details>
  <summary>Details</summary>
Motivation: Small open-weight LLMs are advancing rapidly, and the authors aim to investigate if they can match or exceed closed-source models in biomedical question-answering tasks.

Method: The authors compared open-weight LLMs to proprietary models using techniques like embedding-based snippet retrieval, in-context learning, structured outputs, and ensemble approaches.

Result: Open-weight LLMs performed comparably to proprietary models, and in some cases, surpassed them in biomedical question-answering using ensemble strategies.

Conclusion: Small open-weight LLMs can effectively rival and sometimes outperform proprietary models in specialized tasks, offering a feasible alternative for applications like biomedical question-answering.

Abstract: Open-weight versions of large language models (LLMs) are rapidly advancing,
with state-of-the-art models like DeepSeek-V3 now performing comparably to
proprietary LLMs. This progression raises the question of whether small
open-weight LLMs are capable of effectively replacing larger closed-source
models. We are particularly interested in the context of biomedical
question-answering, a domain we explored by participating in Task 13B Phase B
of the BioASQ challenge. In this work, we compare several open-weight models
against top-performing systems such as GPT-4o, GPT-4.1, Claude 3.5 Sonnet, and
Claude 3.7 Sonnet. To enhance question answering capabilities, we use various
techniques including retrieving the most relevant snippets based on embedding
distance, in-context learning, and structured outputs. For certain submissions,
we utilize ensemble approaches to leverage the diverse outputs generated by
different models for exact-answer questions. Our results demonstrate that
open-weight LLMs are comparable to proprietary ones. In some instances,
open-weight LLMs even surpassed their closed counterparts, particularly when
ensembling strategies were applied. All code is publicly available at
https://github.com/evidenceprime/BioASQ-13b.

</details>


### [89] [Multi-Hierarchical Feature Detection for Large Language Model Generated Text](https://arxiv.org/abs/2509.18862)
*Luyan Zhang,Xinyu Xie*

Main category: cs.CL

TL;DR: This paper investigates the effectiveness of integrating multiple features (semantic, syntactic, and statistical) for AI text detection but finds minimal performance gains relative to the computational overhead.


<details>
  <summary>Details</summary>
Motivation: To explore whether multi-feature approaches can significantly enhance performance in AI text detection, especially given the rapid advancements in large language models (LLMs).

Method: Implemented the MHFD (Multi-Hierarchical Feature Detection) framework, combining DeBERTa-based semantic analysis, syntactic parsing, and statistical features through adaptive fusion.

Result: Multi-feature integration resulted in only minor performance improvements (0.4-0.5%) while increasing computational costs by 4.2 times. The MHFD achieved 89.7% accuracy in in-domain detection and 84.2% in cross-domain detection across benchmark datasets.

Conclusion: The minimal improvements suggest that modern neural models already efficiently capture the necessary signals for AI text detection, making the additional computational cost of multi-feature integration largely unjustified.

Abstract: With the rapid advancement of large language model technology, there is
growing interest in whether multi-feature approaches can significantly improve
AI text detection beyond what single neural models achieve. While intuition
suggests that combining semantic, syntactic, and statistical features should
provide complementary signals, this assumption has not been rigorously tested
with modern LLM-generated text. This paper provides a systematic empirical
investigation of multi-hierarchical feature integration for AI text detection,
specifically testing whether the computational overhead of combining multiple
feature types is justified by performance gains. We implement MHFD
(Multi-Hierarchical Feature Detection), integrating DeBERTa-based semantic
analysis, syntactic parsing, and statistical probability features through
adaptive fusion. Our investigation reveals important negative results: despite
theoretical expectations, multi-feature integration provides minimal benefits
(0.4-0.5% improvement) while incurring substantial computational costs (4.2x
overhead), suggesting that modern neural language models may already capture
most relevant detection signals efficiently. Experimental results on multiple
benchmark datasets demonstrate that the MHFD method achieves 89.7% accuracy in
in-domain detection and maintains 84.2% stable performance in cross-domain
detection, showing modest improvements of 0.4-2.6% over existing methods.

</details>


### [90] [Diversity Boosts AI-Generated Text Detection](https://arxiv.org/abs/2509.18880)
*Advik Raj Basani,Pin-Yu Chen*

Main category: cs.CL

TL;DR: The paper introduces DivEye, a novel framework to detect AI-generated text by analyzing unpredictability fluctuations using surprisal-based features.


<details>
  <summary>Details</summary>
Motivation: To address the increasing need for effective detection of AI-generated text, which can mask misinformation or deception in critical areas like education, journalism, and social media.

Method: DivEye uses interpretable surprisal-based statistical features to capture variability in lexical and structural unpredictability, distinguishing AI-generated text from human-authored text.

Result: The method outperforms prior zero-shot detectors by up to 33.2%, achieves competitive results with fine-tuned baselines, and enhances existing detectors' performance by up to 18.7%.

Conclusion: DivEye provides both robust detection and interpretable insights, highlighting rhythmic unpredictability as a key signal while generalizing well across domains and resisting adversarial attacks.

Abstract: Detecting AI-generated text is an increasing necessity to combat misuse of
LLMs in education, business compliance, journalism, and social media, where
synthetic fluency can mask misinformation or deception. While prior detectors
often rely on token-level likelihoods or opaque black-box classifiers, these
approaches struggle against high-quality generations and offer little
interpretability. In this work, we propose DivEye, a novel detection framework
that captures how unpredictability fluctuates across a text using
surprisal-based features. Motivated by the observation that human-authored text
exhibits richer variability in lexical and structural unpredictability than LLM
outputs, DivEye captures this signal through a set of interpretable statistical
features. Our method outperforms existing zero-shot detectors by up to 33.2%
and achieves competitive performance with fine-tuned baselines across multiple
benchmarks. DivEye is robust to paraphrasing and adversarial attacks,
generalizes well across domains and models, and improves the performance of
existing detectors by up to 18.7% when used as an auxiliary signal. Beyond
detection, DivEye provides interpretable insights into why a text is flagged,
pointing to rhythmic unpredictability as a powerful and underexplored signal
for LLM detection.

</details>


### [91] [Extractive Fact Decomposition for Interpretable Natural Language Inference in one Forward Pass](https://arxiv.org/abs/2509.18901)
*Nicholas Popovič,Michael Färber*

Main category: cs.CL

TL;DR: JEDI is a model that avoids generative methods for interpreting Natural Language Inference, offering robust reasoning using synthetic rationales and an encoder-based approach.


<details>
  <summary>Details</summary>
Motivation: To improve interpretability and robustness in Natural Language Inference without relying on resource-intensive generative models.

Method: An encoder-only architecture that jointly handles extractive atomic fact decomposition and interpretable inference, trained on synthetic rationale datasets.

Result: JEDI achieves robust, competitive accuracy in distribution while improving resilience in adversarial and out-of-distribution tests.

Conclusion: Interpretability and strong generalization in NLI are achievable through encoder-only models and the use of synthetic rationales.

Abstract: Recent works in Natural Language Inference (NLI) and related tasks, such as
automated fact-checking, employ atomic fact decomposition to enhance
interpretability and robustness. For this, existing methods rely on
resource-intensive generative large language models (LLMs) to perform
decomposition. We propose JEDI, an encoder-only architecture that jointly
performs extractive atomic fact decomposition and interpretable inference
without requiring generative models during inference. To facilitate training,
we produce a large corpus of synthetic rationales covering multiple NLI
benchmarks. Experimental results demonstrate that JEDI achieves competitive
accuracy in distribution and significantly improves robustness out of
distribution and in adversarial settings over models based solely on extractive
rationale supervision. Our findings show that interpretability and robust
generalization in NLI can be realized using encoder-only architectures and
synthetic rationales. Code and data available at https://jedi.nicpopovic.com

</details>


### [92] [DTW-Align: Bridging the Modality Gap in End-to-End Speech Translation with Dynamic Time Warping Alignment](https://arxiv.org/abs/2509.18987)
*Abderrahmane Issam,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: This paper proposes a method improving End-to-End Speech Translation (E2E-ST) by using Dynamic Time Warping (DTW) to align speech and text embeddings effectively and efficiently.


<details>
  <summary>Details</summary>
Motivation: E2E-ST bypasses the intermediate transcription step but suffers from a modality gap between speech and text. Existing alignment methods are either not universally applicable for all languages or yield suboptimal results.

Method: The authors adapt Dynamic Time Warping (DTW) for aligning speech and text embeddings during training to mitigate the modality gap without relying on external alignment tools.

Result: Compared to previous methods, the proposed technique generates more accurate alignments, achieves comparable results in E2E-ST tasks with higher speed, and outperforms prior approaches in low-resource settings for 5 out of 6 language directions.

Conclusion: The paper demonstrates that DTW is a robust and efficient solution for speech-text modality alignment in E2E-ST, especially in resource-constrained scenarios.

Abstract: End-to-End Speech Translation (E2E-ST) is the task of translating source
speech directly into target text bypassing the intermediate transcription step.
The representation discrepancy between the speech and text modalities has
motivated research on what is known as bridging the modality gap.
State-of-the-art methods addressed this by aligning speech and text
representations on the word or token level. Unfortunately, this requires an
alignment tool that is not available for all languages. Although this issue has
been addressed by aligning speech and text embeddings using nearest-neighbor
similarity search, it does not lead to accurate alignments. In this work, we
adapt Dynamic Time Warping (DTW) for aligning speech and text embeddings during
training. Our experiments demonstrate the effectiveness of our method in
bridging the modality gap in E2E-ST. Compared to previous work, our method
produces more accurate alignments and achieves comparable E2E-ST results while
being significantly faster. Furthermore, our method outperforms previous work
in low resource settings on 5 out of 6 language directions.

</details>


### [93] [Investigating Test-Time Scaling with Reranking for Machine Translation](https://arxiv.org/abs/2509.19020)
*Shaomu Tan,Ryosuke Mitani,Ritvik Choudhary,Toshiyuki Sekiya*

Main category: cs.CL

TL;DR: The paper investigates Test-Time Scaling (TTS) for machine translation (MT) using a best-of-N approach, finding improvements for high-resource languages but mixed results for low-resource cases and smaller versus larger models under fixed compute budgets.


<details>
  <summary>Details</summary>
Motivation: Scaling NLP model parameters to improve performance is computationally costly, prompting the exploration of Test-Time Scaling as a more resource-efficient alternative.

Method: The study uses a best-of-N framework on WMT24 benchmarks, analyzing different model sizes (3B-72B), language pairs, and compute budgets with N up to 1024.

Result: High-resource language pairs benefit from TTS in translation quality, small models with large N can compete with larger models at N=1, but efficiency depends on compute budgets and low-resource cases may be negatively affected.

Conclusion: TTS can enhance machine translation quality effectively for high-resource languages but requires careful consideration of compute cost, model size, and resource specificity to avoid degraded performance in certain scenarios.

Abstract: Scaling model parameters has become the de facto strategy for improving NLP
systems, but it comes with substantial computational costs. Test-Time Scaling
(TTS) offers an alternative by allocating more computation at inference:
generating multiple candidates and selecting the best. While effective in tasks
such as mathematical reasoning, TTS has not been systematically explored for
machine translation (MT). In this paper, we present the first systematic study
of TTS for MT, investigating a simple but practical best-of-N framework on
WMT24 benchmarks. Our experiments cover six high-resource and one low-resource
language pairs, five model sizes (3B-72B), and various TTS compute budget (N up
to 1024). Our results show that a) For high-resource languages, TTS generally
improves translation quality according to multiple neural MT evaluation
metrics, and our human evaluation confirms these gains; b) Augmenting smaller
models with large $N$ can match or surpass larger models at $N{=}1$ with more
compute cost; c) Under fixed compute budgets, larger models are typically more
efficient, and TTS can degrade quality due to metric blind spots in
low-resource cases.

</details>


### [94] [Charting a Decade of Computational Linguistics in Italy: The CLiC-it Corpus](https://arxiv.org/abs/2509.19033)
*Chiara Alzetta,Serena Auriemma,Alessandro Bondielli,Luca Dini,Chiara Fazzone,Alessio Miaschi,Martina Miliani,Marta Sartor*

Main category: cs.CL

TL;DR: The paper analyzes the evolution of research topics in the Italian Computational Linguistics (CL) and Natural Language Processing (NLP) community by studying contributions to the CLiC-it conference from 2014 to 2024.


<details>
  <summary>Details</summary>
Motivation: To understand how research priorities in CL and NLP have shifted over time in the Italian research community, particularly with the rise of large language models (LLMs) and multimodality.

Method: The researchers compiled a corpus from the proceedings of 10 CLiC-it conferences. They analyzed both metadata (e.g., author information, affiliations) and paper content to identify trends and developments.

Result: The CLiC-it Corpus provides comprehensive insights into research contributions, highlighting how topics and author demographics have evolved over a decade.

Conclusion: This analysis serves as a resource for the Italian and global research community to understand past trends and guide future research directions in CL and NLP.

Abstract: Over the past decade, Computational Linguistics (CL) and Natural Language
Processing (NLP) have evolved rapidly, especially with the advent of
Transformer-based Large Language Models (LLMs). This shift has transformed
research goals and priorities, from Lexical and Semantic Resources to Language
Modelling and Multimodality. In this study, we track the research trends of the
Italian CL and NLP community through an analysis of the contributions to
CLiC-it, arguably the leading Italian conference in the field. We compile the
proceedings from the first 10 editions of the CLiC-it conference (from 2014 to
2024) into the CLiC-it Corpus, providing a comprehensive analysis of both its
metadata, including author provenance, gender, affiliations, and more, as well
as the content of the papers themselves, which address various topics. Our goal
is to provide the Italian and international research communities with valuable
insights into emerging trends and key developments over time, supporting
informed decisions and future directions in the field.

</details>


### [95] [Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering](https://arxiv.org/abs/2509.19094)
*Alireza Salemi,Cheng Li,Mingyang Zhang,Qiaozhu Mei,Zhuowan Li,Spurthi Amba Hombaiah,Weize Kong,Tao Chen,Hamed Zamani,Michael Bendersky*

Main category: cs.CL

TL;DR: The paper introduces Pathways of Thoughts (PoT), a method for improving personalized question answering (QA) with large language models by dynamically selecting cognitive operations and aggregating diverse reasoning paths to align with user preferences.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address personalization challenges in QA systems, including inferring preferences from noisy and implicit contexts and generating responses that are accurate, relevant, and user-aligned.

Method: The proposed PoT method uses an inference-stage process for large language models that models reasoning as an iterative decision process with dynamic cognitive operations, explores multiple reasoning paths, and aggregates responses based on inferred user preferences.

Result: The PoT approach achieves up to a 13.1% improvement on the LaMP-QA benchmark compared to baselines. Human evaluations show it is preferred in 66% of cases, highlighting its effectiveness.

Conclusion: PoT is a versatile and effective approach to personalized QA, enhancing response quality without requiring task-specific tuning, and demonstrates strong quantitative and qualitative advantages over baselines.

Abstract: Personalization is essential for adapting question answering (QA) systems to
user-specific information needs, thereby improving both accuracy and user
satisfaction. However, personalized QA remains relatively underexplored due to
challenges such as inferring preferences from long, noisy, and implicit
contexts, and generating responses that are simultaneously correct,
contextually appropriate, and aligned with user expectations and background
knowledge. To address these challenges, we propose Pathways of Thoughts (PoT),
an inference-stage method that applies to any large language model (LLM)
without requiring task-specific fine-tuning. The approach models the reasoning
of an LLM as an iterative decision process, where the model dynamically selects
among cognitive operations such as reasoning, revision, personalization, and
clarification. This enables exploration of multiple reasoning trajectories,
producing diverse candidate responses that capture different perspectives. PoT
then aggregates and reweights these candidates according to inferred user
preferences, yielding a final personalized response that benefits from the
complementary strengths of diverse reasoning paths. Experiments on the LaMP-QA
benchmark for personalized QA show that PoT consistently outperforms
competitive baselines, achieving up to a 13.1% relative improvement. Human
evaluation corroborates these results, with annotators preferring outputs from
PoT in 66% of cases and reporting ties in only 15% of cases.

</details>


### [96] [Are most sentences unique? An empirical examination of Chomskyan claims](https://arxiv.org/abs/2509.19108)
*Hiram Ring*

Main category: cs.CL

TL;DR: The paper investigates the uniqueness of linguistic sentences using empirical analysis of corpora.


<details>
  <summary>Details</summary>
Motivation: To empirically investigate the claim that most linguistic sentences are unique, as proposed by linguists like Chomsky.

Method: The authors use the NLTK Python library to analyze corpora from various genres and count occurrences of exact string matches.

Result: The findings reveal that completely unique sentences dominate in many genres, but duplicated sentences also constitute a notable portion of individual corpora.

Conclusion: The claim regarding the uniqueness of linguistic sentences holds, but genre constraints significantly influence uniqueness, and duplicate sentences cannot be overlooked.

Abstract: A repeated claim in linguistics is that the majority of linguistic utterances
are unique. For example, Pinker (1994: 10), summarizing an argument by Noam
Chomsky, states that "virtually every sentence that a person utters or
understands is a brand-new combination of words, appearing for the first time
in the history of the universe." With the increased availability of large
corpora, this is a claim that can be empirically investigated. The current
paper addresses the question by using the NLTK Python library to parse corpora
of different genres, providing counts of exact string matches in each. Results
show that while completely unique sentences are often the majority of corpora,
this is highly constrained by genre, and that duplicate sentences are not an
insignificant part of any individual corpus.

</details>


### [97] [Human-Annotated NER Dataset for the Kyrgyz Language](https://arxiv.org/abs/2509.19109)
*Timur Turatali,Anton Alekseev,Gulira Jumalieva,Gulnara Kabaeva,Sergey Nikolenko*

Main category: cs.CL

TL;DR: The study introduces KyrgyzNER, the first manually annotated named entity recognition dataset for the Kyrgyz language, evaluates various NER models on the dataset, and highlights challenges and opportunities in using multilingual pretrained models.


<details>
  <summary>Details</summary>
Motivation: To provide a foundational resource for named entity recognition in the under-resourced Kyrgyz language and advance its language processing capabilities.

Method: The researchers created a manually annotated dataset consisting of 1,499 news articles with 39,075 entity mentions across 27 named entity classes. They evaluated traditional sequence labeling approaches and transformer-based multilingual models like RoBERTa.

Result: The multilingual RoBERTa model achieved the best balance between precision and recall, though all models struggled with rare entity categories. Other multilingual models delivered comparable results.

Conclusion: Multilingual pretrained models are effective for low-resource languages like Kyrgyz, though further exploration of finer annotation schemes could lead to enhanced insights and system performance.

Abstract: We introduce KyrgyzNER, the first manually annotated named entity recognition
dataset for the Kyrgyz language. Comprising 1,499 news articles from the 24.KG
news portal, the dataset contains 10,900 sentences and 39,075 entity mentions
across 27 named entity classes. We show our annotation scheme, discuss the
challenges encountered in the annotation process, and present the descriptive
statistics. We also evaluate several named entity recognition models, including
traditional sequence labeling approaches based on conditional random fields and
state-of-the-art multilingual transformer-based models fine-tuned on our
dataset. While all models show difficulties with rare entity categories, models
such as the multilingual RoBERTa variant pretrained on a large corpus across
many languages achieve a promising balance between precision and recall. These
findings emphasize both the challenges and opportunities of using multilingual
pretrained models for processing languages with limited resources. Although the
multilingual RoBERTa model performed best, other multilingual models yielded
comparable results. This suggests that future work exploring more granular
annotation schemes may offer deeper insights for Kyrgyz language processing
pipelines evaluation.

</details>


### [98] [Context-Aware Hierarchical Taxonomy Generation for Scientific Papers via LLM-Guided Multi-Aspect Clustering](https://arxiv.org/abs/2509.19125)
*Kun Zhu,Lizi Liao,Yuxuan Gu,Lei Huang,Xiaocheng Feng,Bing Qin*

Main category: cs.CL

TL;DR: This paper introduces a new taxonomy construction method utilizing LLMs for multi-aspect encoding combined with dynamic clustering, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The growing volume of scientific literature necessitates effective tools to organize research findings, as existing methods often fail in coherence and detail.

Method: The authors developed a framework where LLMs identify key paper aspects, generate summaries, and cluster information to create a hierarchical taxonomy. They also introduced a benchmark dataset of 156 taxonomies covering 11.6k papers.

Result: Their proposed approach surpasses previous methods, excelling in coherence, granularity, and interpretability metrics.

Conclusion: The paper's framework advances taxonomy generation by integrating LLMs with dynamic clustering, supported by a novel evaluation dataset.

Abstract: The rapid growth of scientific literature demands efficient methods to
organize and synthesize research findings. Existing taxonomy construction
methods, leveraging unsupervised clustering or direct prompting of large
language models (LLMs), often lack coherence and granularity. We propose a
novel context-aware hierarchical taxonomy generation framework that integrates
LLM-guided multi-aspect encoding with dynamic clustering. Our method leverages
LLMs to identify key aspects of each paper (e.g., methodology, dataset,
evaluation) and generates aspect-specific paper summaries, which are then
encoded and clustered along each aspect to form a coherent hierarchy. In
addition, we introduce a new evaluation benchmark of 156 expert-crafted
taxonomies encompassing 11.6k papers, providing the first naturally annotated
dataset for this task. Experimental results demonstrate that our method
significantly outperforms prior approaches, achieving state-of-the-art
performance in taxonomy coherence, granularity, and interpretability.

</details>


### [99] [Anecdoctoring: Automated Red-Teaming Across Language and Place](https://arxiv.org/abs/2509.19143)
*Alejandro Cuevas,Saloni Dash,Bharat Kumar Nayak,Dan Vann,Madeleine I. G. Daepp*

Main category: cs.CL

TL;DR: The paper introduces "anecdoctoring," an approach to generate adversarial AI prompts across languages to improve global robustness against disinformation, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: To create scalable and culturally diverse defenses against disinformation generated by generative AI, addressing the limitations of current US- and English-centric red-teaming datasets.

Method: Proposed a novel red-teaming method called "anecdoctoring" that uses misinformation claims from multiple languages (English, Spanish, Hindi) and regions (US and India), clusters them into narratives, and augments attacker LLMs with knowledge graphs for systematic adversarial probing.

Result: The approach achieved higher attack success rates and improved interpretability compared to few-shot prompting, demonstrating effectiveness in uncovering global AI misuse vulnerabilities.

Conclusion: The study highlights the importance of globally scalable disinformation mitigations rooted in real-world adversarial scenarios to combat AI misuse effectively.

Abstract: Disinformation is among the top risks of generative artificial intelligence
(AI) misuse. Global adoption of generative AI necessitates red-teaming
evaluations (i.e., systematic adversarial probing) that are robust across
diverse languages and cultures, but red-teaming datasets are commonly US- and
English-centric. To address this gap, we propose "anecdoctoring", a novel
red-teaming approach that automatically generates adversarial prompts across
languages and cultures. We collect misinformation claims from fact-checking
websites in three languages (English, Spanish, and Hindi) and two geographies
(US and India). We then cluster individual claims into broader narratives and
characterize the resulting clusters with knowledge graphs, with which we
augment an attacker LLM. Our method produces higher attack success rates and
offers interpretability benefits relative to few-shot prompting. Results
underscore the need for disinformation mitigations that scale globally and are
grounded in real-world adversarial misuse.

</details>


### [100] [Measuring AI "Slop" in Text](https://arxiv.org/abs/2509.19163)
*Chantal Shaib,Tuhin Chakrabarty,Diego Garcia-Olano,Byron C. Wallace*

Main category: cs.CL

TL;DR: The paper explores the definition and assessment of AI-generated "slop" text, offering interpretable dimensions for its evaluation and quality measurement.


<details>
  <summary>Details</summary>
Motivation: There is no formal definition or framework to measure low-quality AI-generated text or "slop," which hinders understanding its occurrence and characteristics.

Method: A taxonomy of "slop" was developed through expert interviews, and interpretable dimensions were identified through span-level annotations to correlate binary judgments with underlying text properties.

Result: Binary assessments of slop are found to be somewhat subjective but correlate with latent text dimensions like coherence and relevance. The proposed framework provides a structured way to evaluate AI text quality.

Conclusion: The developed framework introduces a systematic way to assess AI-generated text quality, supporting both detection and binary preference evaluations while paving the way for deeper insights into text quality factors.

Abstract: AI "slop" is an increasingly popular term used to describe low-quality
AI-generated text, but there is currently no agreed upon definition of this
term nor a means to measure its occurrence. In this work, we develop a taxonomy
of "slop" through interviews with experts in NLP, writing, and philosophy, and
propose a set of interpretable dimensions for its assessment in text. Through
span-level annotation, we find that binary "slop" judgments are (somewhat)
subjective, but such determinations nonetheless correlate with latent
dimensions such as coherence and relevance. Our framework can be used to
evaluate AI-generated text in both detection and binary preference tasks,
potentially offering new insights into the linguistic and stylistic factors
that contribute to quality judgments.

</details>


### [101] [Soft Tokens, Hard Truths](https://arxiv.org/abs/2509.19170)
*Natasha Butt,Ariel Kwiatkowski,Ismail Labiad,Julia Kempe,Yann Ollivier*

Main category: cs.CL

TL;DR: This study introduces a method using reinforcement learning to train language models with continuous Chain-of-Thought (CoT) tokens, which enhances reasoning efficiency while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve reasoning efficiency in language models by leveraging the greater expressivity of continuous tokens, overcoming the training limitations faced in prior methods.

Method: The method involves training models with continuous CoTs using reinforcement learning, employing 'soft' tokens (mixtures with noise) for exploration without requiring ground-truth discrete CoTs for distillation.

Result: The approach achieves competitive or superior reasoning performance compared to discrete-token CoTs on benchmarks, enables computationally efficient training with hundreds of tokens, and enhances out-of-domain consistency.

Conclusion: Training with continuous CoTs via RL is scalable, improves model capabilities, supports standard deployment with discrete tokens, and maintains general prediction consistency.

Abstract: The use of continuous instead of discrete tokens during the Chain-of-Thought
(CoT) phase of reasoning LLMs has garnered attention recently, based on the
intuition that a continuous mixture of discrete tokens could simulate a
superposition of several reasoning paths simultaneously. Theoretical results
have formally proven that continuous tokens have much greater expressivity and
can solve specific problems more efficiently. However, practical use of
continuous tokens has been limited by strong training difficulties: previous
works either just use continuous tokens at inference time on a pre-trained
discrete-token model, or must distill the continuous CoT from ground-truth
discrete CoTs and face computational costs that limit the CoT to very few
tokens.
  This is the first work introducing a scalable method to learn continuous CoTs
via reinforcement learning (RL), without distilling from reference discrete
CoTs. We use "soft" tokens: mixtures of tokens together with noise on the input
embedding to provide RL exploration. Computational overhead is minimal,
enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning
benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs
match discrete-token CoTs for pass@1 and surpass them for pass@32, showing
greater CoT diversity. In systematic comparisons, the best-performing scenario
is to train with continuous CoT tokens then use discrete tokens for inference,
meaning the "soft" models can be deployed in a standard way. Finally, we show
continuous CoT RL training better preserves the predictions of the base model
on out-of-domain tasks, thus providing a softer touch to the base model.

</details>


### [102] [Online Process Reward Leanring for Agentic Reinforcement Learning](https://arxiv.org/abs/2509.19199)
*Xiaoqian Liu,Ke Wang,Yuchuan Wu,Fei Huang,Yongbin Li,Junge Zhang,Jianbin Jiao*

Main category: cs.CL

TL;DR: The paper introduces Online Process Reward Learning (OPRL), a new method to improve reinforcement learning (RL) for large language models (LLMs) by addressing credit assignment challenges and outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the challenges of sparse, unverifiable rewards and biased process supervision in RL for agentic LLMs, which hinder efficient temporal credit assignment.

Method: The proposed OPRL method optimizes a process reward model (PRM) to transform trajectory preferences into implicit step rewards. These step rewards are then combined with episode-level advantages for policy updates, creating a self-reinforcing training loop.

Result: OPRL achieves state-of-the-art results across multiple RL benchmarks such as WebShop, VisualSokoban, and SOTOPIA, showcasing higher sample efficiency, lower variance during training, and effective exploration.

Conclusion: The findings validate OPRL as a stable and efficient credit-assignment strategy for agentic RL, with potential to enhance real-world applications of LLMs in interactive environments.

Abstract: Large language models (LLMs) are increasingly trained with reinforcement
learning (RL) as autonomous agents that reason and act over long horizons in
interactive environments.
  However, sparse and sometimes unverifiable rewards make temporal credit
assignment extremely challenging.
  Recent work attempts to integrate process supervision into agent learning but
suffers from biased annotation, reward hacking, high-variance from overly
fine-grained signals or failtures when state overlap is rare.
  We therefore introduce Online Process Reward Learning (OPRL), a general
credit-assignment strategy for agentic RL that integrates seamlessly with
standard on-policy algorithms without relying on additional rollouts or
explicit step labels.
  In OPRL, we optimize an implicit process reward model (PRM) alternately with
the agent's policy to transform trajectory preferences into implicit step
rewards through a trajectory-based DPO objective.
  These step rewards are then used to compute step-level advantages, which are
combined with episode-level advantages from outcome rewards for policy update,
creating a self-reinforcing loop.
  Theoretical findings guarantee that the learned step rewards are consistent
with trajectory preferences and act as potential-based shaping rewards,
providing bounded gradients to stabilize training.
  Empirically, we evaluate OPRL on three distinct agent benmarks, including
WebShop and VisualSokoban, as well as open-ended social interactions with
unverfiable rewards in SOTOPIA.
  Crucially, OPRL shows superior performance over frontier LLMs and strong RL
baselines across domains, achieving state-of-the-art results with higher
sample-efficiency and lower variance during training.
  Further analysis also demonstrates the efficient exploration by OPRL using
fewer actions, underscoring its potential for agentic learning in real-world
scenarios.

</details>


### [103] [Steering Multimodal Large Language Models Decoding for Context-Aware Safety](https://arxiv.org/abs/2509.19212)
*Zheyuan Liu,Zhangchen Xu,Guangyao Dou,Xiangchi Yuan,Zhaoxuan Tan,Radha Poovendran,Meng Jiang*

Main category: cs.CL

TL;DR: Proposes SafeCoDe, a decoding framework addressing context-aware safety decision limitations in multimodal large language models (MLLMs).


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle to balance oversensitivity and undersensitivity in safety-critical decisions.

Method: SafeCoDe utilizes a two-stage approach: contrastive decoding and global-aware token modulation.

Result: SafeCoDe enhances safety decision-making across diverse frameworks while maintaining utility.

Conclusion: This framework improves safe, context-sensitive refusals in MLLMs without hindering their helpfulness.

Abstract: Multimodal Large Language Models (MLLMs) are increasingly deployed in
real-world applications, yet their ability to make context-aware safety
decisions remains limited. Existing methods often fail to balance
oversensitivity (unjustified refusals of benign queries) and undersensitivity
(missed detection of visually grounded risks), leaving a persistent gap in
safety alignment. To address this issue, we introduce Safety-aware Contrastive
Decoding (SafeCoDe), a lightweight and model-agnostic decoding framework that
dynamically adjusts token generation based on multimodal context. SafeCoDe
operates in two stages: (1) a contrastive decoding mechanism that highlights
tokens sensitive to visual context by contrasting real and Gaussian-noised
images, and (2) a global-aware token modulation strategy that integrates
scene-level reasoning with token-level adjustment to adapt refusals according
to the predicted safety verdict. Extensive experiments across diverse MLLM
architectures and safety benchmarks, covering undersensitivity,
oversensitivity, and general safety evaluations, show that SafeCoDe
consistently improves context-sensitive refusal behaviors while preserving
model helpfulness.

</details>


### [104] [Systematic Comparative Analysis of Large Pretrained Language Models on Contextualized Medication Event Extraction](https://arxiv.org/abs/2509.19224)
*Tariq Abdul-Quddoos,Xishuang Dong,Lijun Qian*

Main category: cs.CL

TL;DR: The paper conducts a comparative analysis of pre-trained attention-based NLP models for extracting medication information and events from electronic health records (EHRs), highlighting that clinical-specific models are effective for detection tasks, while general domain BERT achieves better context classification.


<details>
  <summary>Details</summary>
Motivation: Attention-based models have proven effective for medical NLP tasks, particularly for extracting relevant information from clinical notes in EHRs. The paper aims to evaluate different pre-trained attention-based models on EHR-specific tasks to improve medication-related data extraction.

Method: The research utilized pre-trained models (e.g., BERT Base, BioBERT, Clinical Longformer, etc.) fine-tuned on the Contextualized Medication Event Dataset (CMED). Tasks included medication extraction, event detection, and context classification, with performance evaluated using recall, precision, and F1-Score metrics.

Result: Models pre-trained on clinical data (e.g., BioBERT, Clinical Longformer) improved performance in detecting medication and medical events. However, the general-domain BERT Base model excelled in classifying the context of events.

Conclusion: Pre-trained models specific to clinical data are valuable for tasks like medication detection, while general models like BERT Base can perform better for broader context classification within EHRs, emphasizing the need for task-specific model selection.

Abstract: Attention-based models have become the leading approach in modeling medical
language for Natural Language Processing (NLP) in clinical notes. These models
outperform traditional techniques by effectively capturing contextual rep-
resentations of language. In this research a comparative analysis is done
amongst pre- trained attention based models namely Bert Base, BioBert, two
variations of Bio+Clinical Bert, RoBerta, and Clinical Long- former on task
related to Electronic Health Record (EHR) information extraction. The tasks
from Track 1 of Harvard Medical School's 2022 National Clinical NLP Challenges
(n2c2) are considered for this comparison, with the Contextualized Medication
Event Dataset (CMED) given for these task. CMED is a dataset of unstructured
EHRs and annotated notes that contain task relevant information about the EHRs.
The goal of the challenge is to develop effective solutions for extracting
contextual information related to patient medication events from EHRs using
data driven methods. Each pre-trained model is fine-tuned and applied on CMED
to perform medication extraction, medical event detection, and
multi-dimensional medication event context classification. Pro- cessing methods
are also detailed for breaking down EHRs for compatibility with the applied
models. Performance analysis has been carried out using a script based on
constructing medical terms from the evaluation portion of CMED with metrics
including recall, precision, and F1-Score. The results demonstrate that models
pre-trained on clinical data are more effective in detecting medication and
medication events, but Bert Base, pre- trained on general domain data showed to
be the most effective for classifying the context of events related to
medications.

</details>


### [105] [CompLLM: Compression for Long Context Q&A](https://arxiv.org/abs/2509.19228)
*Gabriele Berton,Jayakrishnan Unnikrishnan,Son Tran,Mubarak Shah*

Main category: cs.CL

TL;DR: CompLLM introduces a soft compression method for processing long contexts in Large Language Models efficiently, addressing the quadratic complexity challenges of self-attention.


<details>
  <summary>Details</summary>
Motivation: The work is motivated by the computational limitations of LLMs when dealing with long contexts due to the quadratic complexity of self-attention, and the inefficiencies of existing compression methods.

Method: CompLLM processes long contexts by dividing them into segments and compressing them independently. This leads to linear scaling, scalability for extremely long contexts, and reusable compressed segments.

Result: CompLLM demonstrates up to 4x speed-up in Time To First Token (TTFT), a 50% reduction in KV cache, and achieves comparable or better performance than uncompressed contexts, especially on long sequences.

Conclusion: CompLLM is a practical, efficient, and scalable method for enhancing LLMs' ability to handle long contexts without significant performance trade-offs.

Abstract: Large Language Models (LLMs) face significant computational challenges when
processing long contexts due to the quadratic complexity of self-attention.
While soft context compression methods, which map input text to smaller latent
representations, have shown promise, their real-world adoption is limited.
Existing techniques typically compress the context as a single unit, which
leads to quadratic compression complexity and an inability to reuse
computations across queries with overlapping contexts. In this work, we
introduce CompLLM, a soft compression technique designed for practical
deployment. Instead of processing the context holistically, CompLLM divides it
into segments and compresses each one independently. This simple design choice
yields three critical properties: efficiency, as the compression step scales
linearly with the context length; scalability, enabling models trained on short
sequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and
reusability, allowing compressed segments to be cached and reused across
different queries. Our experiments show that with a 2x compression rate, at
high context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x
and reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance
comparable to that obtained with the uncompressed context, and even surpasses
it on very long sequences, demonstrating its effectiveness and practical
utility.

</details>


### [106] [Reinforcement Learning on Pre-Training Data](https://arxiv.org/abs/2509.19249)
*Siheng Li,Kejiao Li,Zenan Xu,Guanhua Huang,Evander Yang,Kun Li,Haoyuan Wu,Jiajia Wu,Zihao Zheng,Chenchen Zhang,Kun Shi,Kyrierl Deng,Qi Yi,Ruibin Xiong,Tingqiang Xu,Yuhao Jiang,Jianfeng Yan,Yuyuan Zeng,Guanghui Xu,Jinbao Xue,Zhijiang Xu,Zheng Fang,Shuai Li,Qibin Liu,Xiaoxue Li,Zhuoyu Li,Yangyu Tao,Fei Gao,Cheng Jiang,Bo Chao Wang,Kai Liu,Jianchen Zhu,Wai Lam,Wayyt Wang,Bo Zhou,Di Wang*

Main category: cs.CL

TL;DR: The paper introduces Reinforcement Learning on Pre-Training data (RLPT), enabling large language models (LLMs) to scale training during pre-training using reinforcement learning rather than relying on supervised learning.


<details>
  <summary>Details</summary>
Motivation: The disparity between the exponential growth of computational resources and the slower growth of high-quality text data limits the conventional scaling of large language models (LLMs).

Method: The authors propose RLPT, a training-time scaling paradigm that uses reinforcement learning to improve LLMs' abilities by deriving reward signals directly from pre-training data, without human annotations. It employs a next-segment reasoning objective.

Result: Experiments show RLPT improves performance across benchmarks like MMLU and GPQA-Diamond on models such as Qwen3-4B-Base, with notable performance gains (e.g., +3.0 on MMLU, +8.1 on GPQA-Diamond). It also exhibits strong scaling potential.

Conclusion: RLPT enhances LLM reasoning ability, extends potential for future improvements with additional compute, and provides a robust foundation for tasks requiring reasoning skills.

Abstract: The growing disparity between the exponential scaling of computational
resources and the finite growth of high-quality text data now constrains
conventional scaling approaches for large language models (LLMs). To address
this challenge, we introduce Reinforcement Learning on Pre-Training data
(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast
to prior approaches that scale training primarily through supervised learning,
RLPT enables the policy to autonomously explore meaningful trajectories to
learn from pre-training data and improve its capability through reinforcement
learning (RL). While existing RL strategies such as reinforcement learning from
human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)
rely on human annotation for reward construction, RLPT eliminates this
dependency by deriving reward signals directly from pre-training data.
Specifically, it adopts a next-segment reasoning objective, rewarding the
policy for accurately predicting subsequent text segments conditioned on the
preceding context. This formulation allows RL to be scaled on pre-training
data, encouraging the exploration of richer trajectories across broader
contexts and thereby fostering more generalizable reasoning skills. Extensive
experiments on both general-domain and mathematical reasoning benchmarks across
multiple models validate the effectiveness of RLPT. For example, when applied
to Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,
$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and
AIME25, respectively. The results further demonstrate favorable scaling
behavior, suggesting strong potential for continued gains with more compute. In
addition, RLPT provides a solid foundation, extending the reasoning boundaries
of LLMs and enhancing RLVR performance.

</details>


### [107] [Extracting Conceptual Spaces from LLMs Using Prototype Embeddings](https://arxiv.org/abs/2509.19269)
*Nitesh Kumar,Usashi Chatterjee,Steven Schockaert*

Main category: cs.CL

TL;DR: The paper outlines a method to derive conceptual spaces from large language models (LLMs) by embedding prototype descriptions and fine-tuning LLMs for better alignment.


<details>
  <summary>Details</summary>
Motivation: Conceptual spaces have potential applications in explainable AI but are challenging to learn due to difficulties in encoding perceptual features within LLMs.

Method: Prototype-based descriptions are embedded, and LLMs are fine-tuned to align these embeddings with conceptual space dimensions.

Result: The proposed strategy was empirically found to be highly effective in extracting meaningful conceptual spaces.

Conclusion: This approach advances the practical extraction of conceptual spaces from LLMs, addressing a longstanding challenge in cognitive and AI fields.

Abstract: Conceptual spaces represent entities and concepts using cognitively
meaningful dimensions, typically referring to perceptual features. Such
representations are widely used in cognitive science and have the potential to
serve as a cornerstone for explainable AI. Unfortunately, they have proven
notoriously difficult to learn, although recent LLMs appear to capture the
required perceptual features to a remarkable extent. Nonetheless, practical
methods for extracting the corresponding conceptual spaces are currently still
lacking. While various methods exist for extracting embeddings from LLMs,
extracting conceptual spaces also requires us to encode the underlying
features. In this paper, we propose a strategy in which features (e.g.
sweetness) are encoded by embedding the description of a corresponding
prototype (e.g. a very sweet food). To improve this strategy, we fine-tune the
LLM to align the prototype embeddings with the corresponding conceptual space
dimensions. Our empirical analysis finds this approach to be highly effective.

</details>


### [108] [SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data](https://arxiv.org/abs/2509.19270)
*Erik Božík,Marek Šuppa*

Main category: cs.CL

TL;DR: The paper introduces SloPalSpeech, a 2,806-hour Slovak ASR dataset, and achieves significant Word Error Rate (WER) reductions by fine-tuning OpenAI Whisper models. It also publicly releases the dataset and models.


<details>
  <summary>Details</summary>
Motivation: The scarcity of training data hinders Automatic Speech Recognition (ASR) development for low-resource languages like Slovak.

Method: The researchers developed a processing pipeline to segment 2,806 hours of Slovak parliamentary recordings into clean 30-second audio-transcript pairs and fine-tuned OpenAI Whisper models using the dataset.

Result: Fine-tuned Whisper models showed up to 70% reduction in Word Error Rate (WER) on Slovak standard benchmarks like Common Voice and FLEURS.

Conclusion: The work significantly lowers WER for Slovak ASR tasks and provides resources for future research by releasing the robust SloPalSpeech dataset along with fine-tuned models.

Abstract: Automatic Speech Recognition (ASR) for low-resource languages like Slovak is
hindered by the scarcity of training data. To address this, we introduce
SloPalSpeech, a new, large-scale Slovak ASR dataset containing 2,806 hours of
speech from parliamentary proceedings. We developed a robust processing
pipeline to align and segment long-form recordings into clean, 30-second
audio-transcript pairs suitable for model training. We use this dataset to
fine-tune several OpenAI Whisper models (small, medium, large-v3, and
large-v3-turbo), achieving significant Word Error Rate (WER) reductions on
standard Slovak benchmarks like Common Voice and FLEURS. For instance, the
fine-tuned Whisper-small model's WER dropped by up to 70\%, approaching the
baseline performance of the much larger Whisper-large-v3 model. To foster
future research in low-resource speech recognition, we publicly release the
complete SloPalSpeech dataset, the fully segmented transcripts (60 million
words), and all our fine-tuned models.

</details>


### [109] [WolBanking77: Wolof Banking Speech Intent Classification Dataset](https://arxiv.org/abs/2509.19271)
*Abdou Karim Kandji,Frédéric Precioso,Cheikh Ba,Samba Ndiaye,Augustin Ndione*

Main category: cs.CL

TL;DR: This work introduces WolBanking77, a specialized dataset for intent classification in Wolof, consisting of text and spoken data, with promising baseline performance.


<details>
  <summary>Details</summary>
Motivation: Intent classification research largely focuses on high-resource languages, leaving gaps for low-resource languages like Wolof, which is widely spoken but associated with high illiteracy rates.

Method: The authors released the WolBanking77 dataset containing approximately 9,791 text sentences and 4 hours of spoken data, and conducted experiments using text and voice intent classification models.

Result: Baseline results for NLP and ASR models trained on WolBanking77 showcase promising performance, with f1-score and word error rate metrics detailed in comparisons.

Conclusion: WolBanking77 provides foundational data for advancing intent classification in low-resource spoken languages, and plans for dataset updates, maintenance, and open-source code release are outlined.

Abstract: Intent classification models have made a lot of progress in recent years.
However, previous studies primarily focus on high-resource languages datasets,
which results in a gap for low-resource languages and for regions with a high
rate of illiterate people where languages are more spoken than read or written.
This is the case in Senegal, for example, where Wolof is spoken by around 90\%
of the population, with an illiteracy rate of 42\% for the country. Wolof is
actually spoken by more than 10 million people in West African region. To
tackle such limitations, we release a Wolof Intent Classification Dataset
(WolBanking77), for academic research in intent classification. WolBanking77
currently contains 9,791 text sentences in the banking domain and more than 4
hours of spoken sentences. Experiments on various baselines are conducted in
this work, including text and voice state-of-the-art models. The results are
very promising on this current dataset. This paper also provides detailed
analyses of the contents of the data. We report baseline f1-score and word
error rate metrics respectively on NLP and ASR models trained on WolBanking77
dataset and also comparisons between models. We plan to share and conduct
dataset maintenance, updates and to release open-source code.

</details>


### [110] [DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models' Understanding on Indian Culture](https://arxiv.org/abs/2509.19274)
*Arijit Maji,Raghvendra Kumar,Akash Ghosh,Anushka,Nemil Shah,Abhilekh Borah,Vanshika Shah,Nishant Mishra,Sriparna Saha*

Main category: cs.CL

TL;DR: This paper introduces DRISHTIKON, a unique multimodal and multilingual benchmark aimed at evaluating generative AI systems' understanding of Indian culture.


<details>
  <summary>Details</summary>
Motivation: The paper aims to fill a gap in AI benchmarks by providing a culturally specific dataset to assess AI systems, focusing on India's diverse cultural heritage and low-resource scenarios.

Method: The DRISHTIKON dataset includes 64,000 aligned text-image pairs spanning 15 languages and various cultural themes. The study evaluates a range of vision-language models under different settings to identify their limitations.

Result: Findings reveal that current vision-language models struggle with culturally grounded reasoning, especially in low-resource languages and less-documented traditions.

Conclusion: DRISHTIKON provides a crucial resource for advancing culturally aware and multimodally competent AI technologies.

Abstract: We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual
benchmark centered exclusively on Indian culture, designed to evaluate the
cultural understanding of generative AI systems. Unlike existing benchmarks
with a generic or global scope, DRISHTIKON offers deep, fine-grained coverage
across India's diverse regions, spanning 15 languages, covering all states and
union territories, and incorporating over 64,000 aligned text-image pairs. The
dataset captures rich cultural themes including festivals, attire, cuisines,
art forms, and historical heritage amongst many more. We evaluate a wide range
of vision-language models (VLMs), including open-source small and large models,
proprietary systems, reasoning-specialized VLMs, and Indic-focused models,
across zero-shot and chain-of-thought settings. Our results expose key
limitations in current models' ability to reason over culturally grounded,
multimodal inputs, particularly for low-resource languages and less-documented
traditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a
robust testbed to advance culturally aware, multimodally competent language
technologies.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [111] [PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset](https://arxiv.org/abs/2509.18159)
*Akwasi Asare,Ulas Bagci*

Main category: cs.CV

TL;DR: PolypSeg-GradCAM combines the U-Net architecture with Grad-CAM to provide accurate and interpretable segmentation of GI polyps, achieving impressive metrics like a 0.9257 IoU on the Kvasir-SEG dataset.


<details>
  <summary>Details</summary>
Motivation: Colorectal cancer is a leading cause of mortality globally, and early detection of GI polyps during colonoscopy is pivotal in prevention. However, manual segmentation is challenging, and current deep-learning tools lack interpretability, deterring clinical adoption.

Method: The paper presents PolypSeg-GradCAM, an integration of U-Net for segmentation and Grad-CAM for visualization. The framework was trained and validated on the Kvasir-SEG dataset, consisting of 1000 annotated endoscopic images.

Result: The proposed model achieved a mean IoU of 0.9257 on the test set and F-scores above 0.96 on the training/validation sets. Grad-CAM visualizations indicated that the model's predictions focus on clinically relevant regions.

Conclusion: PolypSeg-GradCAM balances high segmentation accuracy with explainability, addressing both technical and clinical challenges. This makes it a promising tool in AI-assisted colonoscopy for early prevention of colorectal cancer.

Abstract: Colorectal cancer (CRC) remains one of the leading causes of cancer-related
morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as
critical precursors according to the World Health Organization (WHO). Early and
accurate segmentation of polyps during colonoscopy is essential for reducing
CRC progression, yet manual delineation is labor-intensive and prone to
observer variability. Deep learning methods have demonstrated strong potential
for automated polyp analysis, but their limited interpretability remains a
barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an
explainable deep learning framework that integrates the U-Net architecture with
Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp
segmentation. The model was trained and evaluated on the Kvasir-SEG dataset of
1000 annotated endoscopic images. Experimental results demonstrate robust
segmentation performance, achieving a mean Intersection over Union (IoU) of
0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)
on training and validation sets. Grad-CAM visualizations further confirmed that
predictions were guided by clinically relevant regions, enhancing transparency
and trust in the model's decisions. By coupling high segmentation accuracy with
interpretability, PolypSeg-GradCAM represents a step toward reliable,
trustworthy AI-assisted colonoscopy and improved early colorectal cancer
prevention.

</details>


### [112] [PerceptronCARE: A Deep Learning-Based Intelligent Teleopthalmology Application for Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2509.18160)
*Akwasi Asare,Isaac Baffour Senkyire,Emmanuel Freeman,Simon Hilary Ayinedenaba Aluze-Ele,Kelvin Kwao*

Main category: cs.CV

TL;DR: The paper introduces PerceptronCARE, a teleophthalmology system using deep learning for diabetic retinopathy detection with 85.4% accuracy, targeting underserved regions.


<details>
  <summary>Details</summary>
Motivation: Diabetic retinopathy is a leading global cause of adult vision loss, with limited screening access in underserved areas.

Method: Deep learning-based system utilizing convolutional neural networks like ResNet-18, EfficientNet-B0, and SqueezeNet for automated analysis of retinal images.

Result: Achieved disease severity classification with 85.4% accuracy, incorporating real-time screening capabilities and other cloud-based features.

Conclusion: AI-driven telemedicine solutions like PerceptronCARE can expand diabetic retinopathy screening access, especially in remote areas, improving healthcare outcomes and reducing costs.

Abstract: Diabetic retinopathy is a leading cause of vision loss among adults and a
major global health challenge, particularly in underserved regions. This study
presents PerceptronCARE, a deep learning-based teleophthalmology application
designed for automated diabetic retinopathy detection using retinal images. The
system was developed and evaluated using multiple convolutional neural
networks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine
the optimal balance between accuracy and computational efficiency. The final
model classifies disease severity with an accuracy of 85.4%, enabling real-time
screening in clinical and telemedicine settings. PerceptronCARE integrates
cloud-based scalability, secure patient data management, and a multi-user
framework, facilitating early diagnosis, improving doctor-patient interactions,
and reducing healthcare costs. This study highlights the potential of AI-driven
telemedicine solutions in expanding access to diabetic retinopathy screening,
particularly in remote and resource-constrained environments.

</details>


### [113] [Self Identity Mapping](https://arxiv.org/abs/2509.18165)
*Xiuding Cai,Yaoyao Zhu,Linjie Fu,Dong Miao,Yu Yao*

Main category: cs.CV

TL;DR: This paper introduces a novel regularization framework called Self Identity Mapping (SIM), which improves generalization and mitigates overfitting by leveraging inverse mapping mechanisms, offering consistent improvements across tasks and domains.


<details>
  <summary>Details</summary>
Motivation: Deep learning regularization methods often rely on heuristics, leading to inefficiencies and reduced applicability across varied settings. There is a need to improve representation learning reliability while maintaining computational efficiency.

Method: The paper proposes SIM, an inverse mapping-based framework, and its variant ρSIM, employing patch-level sampling and projection-based reconstruction. This makes the method model-agnostic, task-agnostic, and computationally efficient.

Result: ρSIM was evaluated on tasks like image classification, few-shot learning, and domain generalization, showing consistent performance boosts over baselines and synergy with existing regularization methods. It also performs well in dense-to-dense tasks and non-visual domains.

Conclusion: The SIM framework demonstrates its utility as a simple yet effective regularization method for enhancing representation learning across diverse models and tasks, with publicly available code for adoption.

Abstract: Regularization is essential in deep learning to enhance generalization and
mitigate overfitting. However, conventional techniques often rely on
heuristics, making them less reliable or effective across diverse settings. We
propose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic
regularization framework that leverages an inverse mapping mechanism to enhance
representation learning. By reconstructing the input from its transformed
output, SIM reduces information loss during forward propagation and facilitates
smoother gradient flow. To address computational inefficiencies, We instantiate
SIM as $ \rho\text{SIM} $ by incorporating patch-level feature sampling and
projection-based method to reconstruct latent features, effectively lowering
complexity. As a model-agnostic, task-agnostic regularizer, SIM can be
seamlessly integrated as a plug-and-play module, making it applicable to
different network architectures and tasks.
  We extensively evaluate $\rho\text{SIM}$ across three tasks: image
classification, few-shot prompt learning, and domain generalization.
Experimental results show consistent improvements over baseline methods,
highlighting $\rho\text{SIM}$'s ability to enhance representation learning
across various tasks. We also demonstrate that $\rho\text{SIM}$ is orthogonal
to existing regularization methods, boosting their effectiveness. Moreover, our
results confirm that $\rho\text{SIM}$ effectively preserves semantic
information and enhances performance in dense-to-dense tasks, such as semantic
segmentation and image translation, as well as in non-visual domains including
audio classification and time series anomaly detection. The code is publicly
available at https://github.com/XiudingCai/SIM-pytorch.

</details>


### [114] [MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients for Label-Inference-Free Gradient Inversion](https://arxiv.org/abs/2509.18170)
*Zhanting Zhou,Jinbo Wang,Zeqin Wu,Fengli Zhang*

Main category: cs.CV

TL;DR: The paper introduces MAGIA, an advanced method for reconstructing individual images from single-round averaged gradients.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in reconstructing individual data points in single-round averaged gradient regimes, which entangle signals from multiple samples.

Method: A new framework employing momentum-based adaptive correction, combinatorial rescaling, and mixed batch/subset losses for robust and accurate gradient inversion.

Result: MAGIA achieves superior image reconstruction fidelity in challenging scenarios with large batch sizes, outperforming previous methods without needing extra auxiliary data.

Conclusion: MAGIA sets a new benchmark for image reconstruction, balancing computational efficiency and high-quality results in gradient inversion tasks.

Abstract: We study gradient inversion in the challenging single round averaged gradient
SAG regime where per sample cues are entangled within a single batch mean
gradient. We introduce MAGIA a momentum based adaptive correction on gradient
inversion attack a novel label inference free framework that senses latent per
image signals by probing random data subsets. MAGIA objective integrates two
core innovations 1 a closed form combinatorial rescaling that creates a
provably tighter optimization bound and 2 a momentum based mixing of whole
batch and subset losses to ensure reconstruction robustness. Extensive
experiments demonstrate that MAGIA significantly outperforms advanced methods
achieving high fidelity multi image reconstruction in large batch scenarios
where prior works fail. This is all accomplished with a computational footprint
comparable to standard solvers and without requiring any auxiliary information.

</details>


### [115] [Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR](https://arxiv.org/abs/2509.18174)
*Khalil Hennara,Muhammad Hreden,Mohamed Motasim Hamed,Ahmad Bastati,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan*

Main category: cs.CV

TL;DR: Baseer is an Arabic-specific vision-language model for document OCR, achieving state-of-the-art performance with a WER of 0.25, using fine-tuned MLLMs and a benchmark dataset.


<details>
  <summary>Details</summary>
Motivation: Arabic OCR is challenging due to its unique linguistic and script features, and general MLLMs struggle with effective performance on Arabic documents.

Method: Baseer is fine-tuned using a decoder-only strategy on a large dataset combining synthetic and real-world Arabic documents. Additionally, Misraj-DocOCR benchmark aids in evaluation.

Result: Baseer outperforms existing solutions with remarkable accuracy (WER of 0.25) and sets a new standard in Arabic document OCR.

Conclusion: Domain-specific adaptation of MLLMs can achieve significant improvements, demonstrating effective OCR solutions for Arabic and similar morphologically rich languages.

Abstract: Arabic document OCR remains a challenging task due to the language's cursive
script, diverse fonts, diacritics, and right-to-left orientation. While modern
Multimodal Large Language Models (MLLMs) have advanced document understanding
for high-resource languages, their performance on Arabic remains limited. In
this work, we introduce Baseer, a vision-language model fine- tuned
specifically for Arabic document OCR. Leveraging a large-scale dataset
combining synthetic and real-world documents, Baseer is trained using a
decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving
general visual features. We also present Misraj-DocOCR, a high-quality,
expert-verified benchmark designed for rigorous evaluation of Arabic OCR
systems. Our experiments show that Baseer significantly outperforms existing
open-source and commercial solutions, achieving a WER of 0.25 and establishing
a new state-of-the-art in the domain of Arabic document OCR. Our results
highlight the benefits of domain-specific adaptation of general-purpose MLLMs
and establish a strong baseline for high-accuracy OCR on morphologically rich
languages like Arabic.

</details>


### [116] [A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland](https://arxiv.org/abs/2509.18176)
*Wendong Yao,Saeed Azadnejad,Binhua Huang,Shane Donohue,Soumyabrata Dev*

Main category: cs.CV

TL;DR: The paper presents a spatio-temporal deep learning framework utilizing CNN-LSTM models to forecast ground deformation from sparse InSAR data, outperforming traditional machine learning approaches.


<details>
  <summary>Details</summary>
Motivation: Forecasting ground deformation from sparse data remains a challenge critical for urban infrastructure stability and geological hazard mitigation.

Method: The study introduces a hybrid CNN-LSTM model that converts sparse InSAR point measurements into dense spatio-temporal tensors, leveraging advanced computer vision techniques for forecasting.

Result: Experimental results using Sentinel-1 data from eastern Ireland show the new approach outperforms baselines like LightGBM and LASSO, producing more accurate and coherent forecasts.

Conclusion: The paper establishes that spatio-temporal deep learning offers a superior method for resolving complex ground deformation dynamics, setting a new benchmark in the domain.

Abstract: Monitoring ground displacement is crucial for urban infrastructure stability
and mitigating geological hazards. However, forecasting future deformation from
sparse Interferometric Synthetic Aperture Radar (InSAR) time-series data
remains a significant challenge. This paper introduces a novel deep learning
framework that transforms these sparse point measurements into a dense
spatio-temporal tensor. This methodological shift allows, for the first time,
the direct application of advanced computer vision architectures to this
forecasting problem. We design and implement a hybrid Convolutional Neural
Network and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to
simultaneously learn spatial patterns and temporal dependencies from the
generated data tensor. The model's performance is benchmarked against powerful
machine learning baselines, Light Gradient Boosting Machine and LASSO
regression, using Sentinel-1 data from eastern Ireland. Results demonstrate
that the proposed architecture provides significantly more accurate and
spatially coherent forecasts, establishing a new performance benchmark for this
task. Furthermore, an interpretability analysis reveals that baseline models
often default to simplistic persistence patterns, highlighting the necessity of
our integrated spatio-temporal approach to capture the complex dynamics of
ground deformation. Our findings confirm the efficacy and potential of
spatio-temporal deep learning for high-resolution deformation forecasting.

</details>


### [117] [A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts](https://arxiv.org/abs/2509.18177)
*George Corrêa de Araújo,Helena de Almeida Maia,Helio Pedrini*

Main category: cs.CV

TL;DR: The Scrapbook framework introduces a methodology for creating extensive datasets to evaluate AI models' understanding of concepts like object recognition and positional information.


<details>
  <summary>Details</summary>
Motivation: AI models frequently tackle complex tasks, but their understanding of fundamental concepts is often not rigorously validated. The Scrapbook framework aims to address this gap by systematically probing foundational concepts before advancing to intricate tasks.

Method: The framework generates diverse datasets centered around basic concepts with numerous questions and varied linguistic patterns to assess AI models. Experiments evaluate models on object recognition, positions, and attribute identification.

Result: Models excel in object recognition but struggle with positional reasoning and complex inquiries. MobileVLM-V2 specifically shows answer inconsistencies, while others display biases and difficulties with geometric and positional tasks.

Conclusion: Scrapbook's dataset generation is vital for identifying AI model weaknesses in essential areas, serving as a tool to systematically assess and improve their performance.

Abstract: In this paper, we present the Scrapbook framework, a novel methodology
designed to generate extensive datasets for probing the learned concepts of
artificial intelligence (AI) models. The framework focuses on fundamental
concepts such as object recognition, absolute and relative positions, and
attribute identification. By generating datasets with a large number of
questions about individual concepts and a wide linguistic variation, the
Scrapbook framework aims to validate the model's understanding of these basic
elements before tackling more complex tasks. Our experimental findings reveal
that, while contemporary models demonstrate proficiency in recognizing and
enumerating objects, they encounter challenges in comprehending positional
information and addressing inquiries with additional constraints. Specifically,
the MobileVLM-V2 model showed significant answer disagreements and plausible
wrong answers, while other models exhibited a bias toward affirmative answers
and struggled with questions involving geometric shapes and positional
information, indicating areas for improvement in understanding and consistency.
The proposed framework offers a valuable instrument for generating diverse and
comprehensive datasets, which can be utilized to systematically assess and
enhance the performance of AI models.

</details>


### [118] [The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes](https://arxiv.org/abs/2509.18179)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CV

TL;DR: The paper investigates the information loss in vision-language-vision systems, finding significant perceptual and structural degradation when visual content is translated into text and regenerated into images.


<details>
  <summary>Details</summary>
Motivation: Understanding the limitations of multimodal AI systems is crucial as they are increasingly integrated into creative workflows, focusing on quantifying information loss in vision-language-vision pipelines.

Method: The authors generated 150 image pairs using a describe-then-generate pipeline and applied metrics like LPIPS, SSIM, and color distance to evaluate preservation of perceptual, structural, and chromatic information.

Result: The study discovered that 99.3% of samples showed substantial perceptual degradation and 91.5% exhibited significant structural information loss.

Conclusion: Natural language as an intermediate representation introduces measurable and consistent limitations for information retention in modern multimodal systems.

Abstract: With the increasing integration of multimodal AI systems in creative
workflows, understanding information loss in vision-language-vision pipelines
has become important for evaluating system limitations. However, the
degradation that occurs when visual content passes through textual
intermediation remains poorly quantified. In this work, we provide empirical
analysis of the describe-then-generate bottleneck, where natural language
serves as an intermediate representation for visual information. We generated
150 image pairs through the describe-then-generate pipeline and applied
existing metrics (LPIPS, SSIM, and color distance) to measure information
preservation across perceptual, structural, and chromatic dimensions. Our
evaluation reveals that 99.3% of samples exhibit substantial perceptual
degradation and 91.5% demonstrate significant structural information loss,
providing empirical evidence that the describe-then-generate bottleneck
represents a measurable and consistent limitation in contemporary multimodal
systems.

</details>


### [119] [AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines](https://arxiv.org/abs/2509.18182)
*Isabelle Tingzon,Yoji Toriumi,Caroline Gevaert*

Main category: cs.CV

TL;DR: The paper presents an AI-driven method for automatically classifying rooftop attributes from high-resolution satellite imagery, addressing the lack of structural building data in small island developing states (SIDS).


<details>
  <summary>Details</summary>
Motivation: Small island developing states (SIDS) often lack detailed building information necessary for urban resilience planning and disaster risk reduction, particularly in climate-vulnerable regions like the Caribbean.

Method: The study employs an AI workflow comparing geospatial foundation models with shallow classifiers against fine-tuned deep learning models for rooftop classification. Additionally, training datasets from neighboring SIDS are incorporated to enhance model performance.

Result: The best models achieved F1 scores of 0.88 for roof pitch and 0.83 for roof material classification. This demonstrates significant success in identifying rooftop attributes.

Conclusion: This AI workflow has strong potential to empower SIDS with advanced urban planning and disaster management tools by leveraging Earth Observation data and AI technologies.

Abstract: Detailed structural building information is used to estimate potential damage
from hazard events like cyclones, floods, and landslides, making them critical
for urban resilience planning and disaster risk reduction. However, such
information is often unavailable in many small island developing states (SIDS)
in climate-vulnerable regions like the Caribbean. To address this data gap, we
present an AI-driven workflow to automatically infer rooftop attributes from
high-resolution satellite imagery, with Saint Vincent and the Grenadines as our
case study. Here, we compare the utility of geospatial foundation models
combined with shallow classifiers against fine-tuned deep learning models for
rooftop classification. Furthermore, we assess the impact of incorporating
additional training data from neighboring SIDS to improve model performance.
Our best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof
material classification, respectively. Combined with local capacity building,
our work aims to provide SIDS with novel capabilities to harness AI and Earth
Observation (EO) data to enable more efficient, evidence-based urban
governance.

</details>


### [120] [VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation](https://arxiv.org/abs/2509.18183)
*Jinyue Bian,Zhaoxing Zhang,Zhengyu Liang,Shiwei Zheng,Shengtao Zhang,Rong Shen,Chen Yang,Anzhou Hou*

Main category: cs.CV

TL;DR: The paper introduces a module, VLA-LPAF, to enhance the perspective adaptivity of Visual-Language-Action (VLA) models, resulting in improved task success rates in various benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of perspective heterogeneity in VLA models caused by differences in number and angles of visual observations, which limit their generality across diverse environments.

Method: Proposing the lightweight VLA-LPAF module that is finetuned using single-view images and integrates multiview data in latent space, ensuring adaptability without requiring extensive 3D data.

Result: The VLA-LPAF integration into RoboFlamingo (creating RoboFlamingo-LPAF) significantly improved task success rates: around 8% on CALVIN, 15% on LIBERO, and 30% on a customized simulation benchmark.

Conclusion: VLA-LPAF effectively bridges the gap in visual feature inconsistency caused by perspective differences, enhancing VLA models’ adaptability and performance in real-world tasks.

Abstract: The Visual-Language-Action (VLA) models can follow text instructions
according to visual observations of the surrounding environment. This ability
to map multimodal inputs to actions is derived from the training of the VLA
model on extensive standard demonstrations. These visual observations captured
by third-personal global and in-wrist local cameras are inevitably varied in
number and perspective across different environments, resulting in significant
differences in the visual features. This perspective heterogeneity constrains
the generality of VLA models. In light of this, we first propose the
lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models
using only 2D data. VLA-LPAF is finetuned using images from a single view and
fuses other multiview observations in the latent space, which effectively and
efficiently bridge the gap caused by perspective inconsistency. We instantiate
our VLA-LPAF framework with the VLA model RoboFlamingo to construct
RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves
around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a
customized simulation benchmark. We also demonstrate the developed viewadaptive
characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.

</details>


### [121] [URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation](https://arxiv.org/abs/2509.18184)
*Yifeng Cheng,Alois Knoll,Hu Cao*

Main category: cs.CV

TL;DR: This paper introduces URNet, an uncertainty-aware network for event-based stereo depth estimation, achieving superior performance compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To leverage the advantages of event cameras—high temporal resolution, dynamic range, and low latency—for more reliable and precise stereo depth estimation.

Method: URNet integrates a local-global refinement module for capturing detailed local and global contexts, along with a KL divergence-based uncertainty modeling to improve prediction reliability.

Result: Experiments on the DSEC dataset confirmed that URNet outperforms existing state-of-the-art methods in both qualitative and quantitative evaluations.

Conclusion: URNet sets a new standard in event-based stereo depth estimation by combining advanced refinement techniques and uncertainty modeling.

Abstract: Event cameras provide high temporal resolution, high dynamic range, and low
latency, offering significant advantages over conventional frame-based cameras.
In this work, we introduce an uncertainty-aware refinement network called URNet
for event-based stereo depth estimation. Our approach features a local-global
refinement module that effectively captures fine-grained local details and
long-range global context. Additionally, we introduce a Kullback-Leibler (KL)
divergence-based uncertainty modeling method to enhance prediction reliability.
Extensive experiments on the DSEC dataset demonstrate that URNet consistently
outperforms state-of-the-art (SOTA) methods in both qualitative and
quantitative evaluations.

</details>


### [122] [Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases](https://arxiv.org/abs/2509.18185)
*Giammarco La Barbera,Enzo Bonnot,Thomas Isla,Juan Pablo de la Plata,Joy-Rose Dunoyer de Segonzac,Jennifer Attali,Cécile Lozach,Alexandre Bellucci,Louis Marcellin,Laure Fournier,Sabine Sarnacki,Pietro Gori,Isabelle Bloch*

Main category: cs.CV

TL;DR: The paper presents Visionerves, a hybrid AI framework for identifying peripheral nerves, improving accuracy in imaging nerve structures compared to conventional tractography methods.


<details>
  <summary>Details</summary>
Motivation: Endometriosis can cause chronic pelvic pain and nerve involvement, but conventional imaging methods struggle to capture detailed nerve data effectively.

Method: Visionerves combines deep learning for anatomical segmentation and symbolic spatial reasoning for tractography, removing manual ROI selection and using fuzzy spatial relationships.

Result: Visionerves was tested on the lumbosacral plexus in 10 women with endometriosis, achieving up to 25% Dice score improvements and reducing spatial errors to less than 5 mm.

Conclusion: The framework offers an automatic, non-invasive, and reproducible method for diagnosing endometriosis-related neuropathy and nerve-related conditions, enhancing precision in nerve imaging.

Abstract: Endometriosis often leads to chronic pelvic pain and possible nerve
involvement, yet imaging the peripheral nerves remains a challenge. We
introduce Visionerves, a novel hybrid AI framework for peripheral nervous
system recognition from multi-gradient DWI and morphological MRI data. Unlike
conventional tractography, Visionerves encodes anatomical knowledge through
fuzzy spatial relationships, removing the need for selection of manual ROIs.
The pipeline comprises two phases: (A) automatic segmentation of anatomical
structures using a deep learning model, and (B) tractography and nerve
recognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in
10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated
substantial improvements over standard tractography, with Dice score
improvements of up to 25% and spatial errors reduced to less than 5 mm. This
automatic and reproducible approach enables detailed nerve analysis and paves
the way for non-invasive diagnosis of endometriosis-related neuropathy, as well
as other conditions with nerve involvement.

</details>


### [123] [Investigating Traffic Accident Detection Using Multimodal Large Language Models](https://arxiv.org/abs/2509.19096)
*Ilhan Skender,Kailin Tong,Selim Solmaz,Daniel Watzenig*

Main category: cs.CV

TL;DR: This paper evaluates multimodal large language models (MLLMs) for detecting and describing traffic accidents from infrastructure camera images, minimizing the need for large labeled datasets by testing performance on the simulated DeepAccident dataset.


<details>
  <summary>Details</summary>
Motivation: To improve automated traffic monitoring by leveraging MLLMs for zero-shot traffic accident detection, reducing the dependence on extensive labeled datasets while enabling real-time hazard monitoring and emergency response.

Method: The research tested several MLLMs (Gemini 1.5 and 2.0, Gemma 3, and Pixtral) using the simulated DeepAccident dataset from CARLA. Advanced visual analytics like YOLO, Deep SORT, and SAM were integrated into prompts for enhanced model accuracy and explainability.

Result: Pixtral emerged as the best model with an F1-score of 0.71 and recall of 83%. Gemini models achieved high precision (e.g., 90% for Gemini 1.5 with enhanced prompts) but experienced drops in F1 and recall. Gemma 3 showed the most consistency with minimal performance variations.

Conclusion: Integrating MLLMs with advanced visual analytics can significantly improve their performance for real-world automated traffic monitoring applications, showing promise for scalable and efficient safety solutions.

Abstract: Traffic safety remains a critical global concern, with timely and accurate
accident detection essential for hazard reduction and rapid emergency response.
Infrastructure-based vision sensors offer scalable and efficient solutions for
continuous real-time monitoring, facilitating automated detection of acci-
dents directly from captured images. This research investigates the zero-shot
capabilities of multimodal large language models (MLLMs) for detecting and
describing traffic accidents using images from infrastructure cameras, thus
minimizing reliance on extensive labeled datasets. Main contributions include:
(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,
explicitly addressing the scarcity of diverse, realistic, infrastructure-based
accident data through controlled simulations; (2) Comparative performance
analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent
identification and descriptive capabilities without prior fine-tuning; and (3)
Integration of advanced visual analytics, specifically YOLO for object
detection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for
instance segmentation, into enhanced prompts to improve model accuracy and
explainability. Key numerical results show Pixtral as the top performer with an
F1-score of 0.71 and 83% recall, while Gemini models gained precision with
enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and
recall losses. Gemma 3 offered the most balanced performance with minimal
metric fluctuation. These findings demonstrate the substantial potential of
integrating MLLMs with advanced visual analytics techniques, enhancing their
applicability in real-world automated traffic monitoring systems.

</details>


### [124] [V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling](https://arxiv.org/abs/2509.18187)
*Muhammad Naveed,Nazia Perwaiz,Sidra Sultana,Mohaira Ahmad,Muhammad Moazam Fraz*

Main category: cs.CV

TL;DR: V-SenseDrive is a privacy-preserving, multimodal driver behavior dataset collected from Pakistan, designed for studying unsafe driving behaviors and supporting road safety initiatives.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for a dataset representing driving behavior in countries with diverse road and traffic conditions like Pakistan, which existing datasets fail to capture.

Method: A custom Android application was used to collect multimodal data (accelerometer, gyroscope, GPS, and road-facing video) for driver behavior analysis in Pakistani traffic conditions.

Result: V-SenseDrive dataset successfully recorded normal, aggressive, and risky driving behaviors on various types of roads using synchronized sensor and video data.

Conclusion: The dataset fills a void in existing research by providing a localized, privacy-preserving dataset for advancing road safety, ADAS technologies, and traffic analysis research.

Abstract: Road traffic accidents remain a major public health challenge, particularly
in countries with heterogeneous road conditions, mixed traffic flow, and
variable driving discipline, such as Pakistan. Reliable detection of unsafe
driving behaviours is a prerequisite for improving road safety, enabling
advanced driver assistance systems (ADAS), and supporting data driven decisions
in insurance and fleet management. Most of existing datasets originate from the
developed countries with limited representation of the behavioural diversity
observed in emerging economies and the driver's face recording voilates the
privacy preservation. We present V-SenseDrive, the first privacy-preserving
multimodal driver behaviour dataset collected entirely within the Pakistani
driving environment. V-SenseDrive combines smartphone based inertial and GPS
sensor data with synchronized road facing video to record three target driving
behaviours (normal, aggressive, and risky) on multiple types of roads,
including urban arterials, secondary roads, and motorways. Data was gathered
using a custom Android application designed to capture high frequency
accelerometer, gyroscope, and GPS streams alongside continuous video, with all
sources precisely time aligned to enable multimodal analysis. The focus of this
work is on the data acquisition process, covering participant selection,
driving scenarios, environmental considerations, and sensor video
synchronization techniques. The dataset is structured into raw, processed, and
semantic layers, ensuring adaptability for future research in driver behaviour
classification, traffic safety analysis, and ADAS development. By representing
real world driving in Pakistan, V-SenseDrive fills a critical gap in the global
landscape of driver behaviour datasets and lays the groundwork for context
aware intelligent transportation solutions.

</details>


### [125] [Qianfan-VL: Domain-Enhanced Universal Vision-Language Models](https://arxiv.org/abs/2509.18189)
*Daxiang Dong,Mingming Zheng,Dong Xu,Bairong Zhuang,Wenyu Zhang,Chunhua Luo,Haoran Wang,Zijian Zhao,Jie Li,Yuxuan Li,Hanjun Zhong,Mengyue Liu,Jieting Chen,Shupeng Li,Lun Tian,Yaping Feng,Xin Li,Donggang Jiang,Yong Chen,Yehua Xu,Duohao Qin,Chen Feng,Dan Wang,Henghua Zhang,Jingjing Ha,Jinhui He,Yanfeng Zhai,Chengxin Zheng,Jiayi Mao,Jiacheng Chen,Ruchang Yao,Ziye Yuan,Jianmin Wu,Guangjun Xie,Dou Shen*

Main category: cs.CV

TL;DR: Qianfan-VL is a set of multimodal large language models (3B to 70B parameters) achieving state-of-the-art (SOTA) performance through innovative training and data techniques.


<details>
  <summary>Details</summary>
Motivation: To develop domain-enhanced multimodal models that excel in both general benchmarks and specialized tasks, providing robust enterprise deployment options.

Method: Employ multi-stage progressive training, high-precision data synthesis pipelines, and leverage Baidu's Kunlun P800 chips for efficient large-scale training.

Result: Qianfan-VL models deliver SOTA performance on benchmarks like CCBench and ScienceQA; excel in OCR, document understanding, mathematical reasoning, and logical inference tasks.

Conclusion: The paper demonstrates an effective methodology for creating scalable and domain-specific multimodal models, validating the infrastructure for SOTA-level AI training.

Abstract: We present Qianfan-VL, a series of multimodal large language models ranging
from 3B to 70B parameters, achieving state-of-the-art performance through
innovative domain enhancement techniques. Our approach employs multi-stage
progressive training and high-precision data synthesis pipelines, which prove
to be critical technologies for enhancing domain-specific capabilities while
maintaining strong general performance. Qianfan-VL achieves comparable results
to leading open-source models on general benchmarks, with state-of-the-art
performance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and
MMStar. The domain enhancement strategy delivers significant advantages in OCR
and document understanding, validated on both public benchmarks (OCRBench 873,
DocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B
variants incorporate long chain-of-thought capabilities, demonstrating superior
performance on mathematical reasoning (MathVista 78.6%) and logical inference
tasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating
the capability of large-scale AI infrastructure to train SOTA-level multimodal
models with over 90% scaling efficiency on 5000 chips for a single task. This
work establishes an effective methodology for developing domain-enhanced
multimodal models suitable for diverse enterprise deployment scenarios.

</details>


### [126] [HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing](https://arxiv.org/abs/2509.18190)
*Junseong Shin,Seungwoo Chung,Yunjeong Yang,Tae Hyun Kim*

Main category: cs.CV

TL;DR: HazeFlow is an ODE-based dehazing framework that improves real-world image clarity while addressing data scarcity with a novel haze generation method.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in overcoming the limitations of existing deep learning and traditional ASM-based dehazing methods, which struggle with generalization to real-world scenarios due to complexities and diverse haze patterns.

Method: The authors reformulate the Atmospheric Scattering Model (ASM) as an ordinary differential equation (ODE) for optimal trajectory learning using HazeFlow, inspired by Rectified Flow. Additionally, they propose a non-homogeneous haze generation method via Markov Chain Brownian Motion (MCBM).

Result: HazeFlow demonstrates state-of-the-art performance across multiple real-world dehazing datasets, showcasing its effectiveness in handling diverse haze conditions.

Conclusion: The proposed HazeFlow framework advances real-world image dehazing by utilizing physics-grounded ODE reformulation and novel data generation techniques, demonstrating superior adaptability and performance.

Abstract: Dehazing involves removing haze or fog from images to restore clarity and
improve visibility by estimating atmospheric scattering effects. While deep
learning methods show promise, the lack of paired real-world training data and
the resulting domain gap hinder generalization to real-world scenarios. In this
context, physics-grounded learning becomes crucial; however, traditional
methods based on the Atmospheric Scattering Model (ASM) often fall short in
handling real-world complexities and diverse haze patterns. To solve this
problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM
as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),
HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,
enhancing real-world dehazing performance with only a single inference step.
Additionally, we introduce a non-homogeneous haze generation method using
Markov Chain Brownian Motion (MCBM) to address the scarcity of paired
real-world data. By simulating realistic haze patterns through MCBM, we enhance
the adaptability of HazeFlow to diverse real-world scenarios. Through extensive
experiments, we demonstrate that HazeFlow achieves state-of-the-art performance
across various real-world dehazing benchmark datasets.

</details>


### [127] [TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection](https://arxiv.org/abs/2509.18193)
*Omar H. Khater,Abdul Jabbar Siddiqui,Aiman El-Maleh,M. Shamim Hossain*

Main category: cs.CV

TL;DR: The paper optimizes EcoWeedNet for edge devices using pruning, QAT, and TensorRT. Model is 68.5% smaller, 28.7% faster, and outperforms YOLO models in precision agriculture.


<details>
  <summary>Details</summary>
Motivation: Deploying deep learning in agriculture faces resource limitations on edge devices, necessitating more efficient models.

Method: EcoWeedNet was compressed via structured channel pruning, QAT, and TensorRT acceleration to enhance its efficiency on NVIDIA Jetson Orin Nano.

Result: Achieved 68.5% model size reduction, 3.2 GFLOPs computation reduction, and 184 FPS inference speed at FP16, outperforming YOLO models on CottonWeedDet12 dataset.

Conclusion: The compressed EcoWeedNet model proves efficient and effective for precision agriculture, handling edge device constraints successfully.

Abstract: Deploying deep learning models in agriculture is difficult because edge
devices have limited resources, but this work presents a compressed version of
EcoWeedNet using structured channel pruning, quantization-aware training (QAT),
and acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the
challenges of pruning complex architectures with residual shortcuts, attention
mechanisms, concatenations, and CSP blocks, the model size was reduced by up to
68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at
FP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the
pruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n
(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%
mAP50, proving it to be both efficient and effective for precision agriculture.

</details>


### [128] [Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction](https://arxiv.org/abs/2509.18284)
*Yi Gu,Kuniaki Saito,Jiaxin Ma*

Main category: cs.CV

TL;DR: The paper introduces a multimodal learning framework to handle challenges like modality imbalance and missing data, enhancing robustness and adaptability in clinical disease prediction tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing multimodal learning in clinical settings, specifically modality imbalance and missing modalities, and to improve model robustness and scalability.

Method: The framework uses learnable modality tokens, enhanced modality dropout, and contrastive learning fused with multimodal representations for better handling of missing data and balance across modalities.

Result: The approach achieved state-of-the-art performance on large clinical datasets, excelling in scenarios with missing modalities and integrating well with a modern CT foundation model.

Conclusion: The proposed method is effective, efficient, and generalizable, offering a scalable solution for multimodal clinical applications with significant real-world potential.

Abstract: As medical diagnoses increasingly leverage multimodal data, machine learning
models are expected to effectively fuse heterogeneous information while
remaining robust to missing modalities. In this work, we propose a novel
multimodal learning framework that integrates enhanced modalities dropout and
contrastive learning to address real-world limitations such as modality
imbalance and missingness. Our approach introduces learnable modality tokens
for improving missingness-aware fusion of modalities and augments conventional
unimodal contrastive objectives with fused multimodal representations. We
validate our framework on large-scale clinical datasets for disease detection
and prediction tasks, encompassing both visual and tabular modalities.
Experimental results demonstrate that our method achieves state-of-the-art
performance, particularly in challenging and practical scenarios where only a
single modality is available. Furthermore, we show its adaptability through
successful integration with a recent CT foundation model. Our findings
highlight the effectiveness, efficiency, and generalizability of our approach
for multimodal learning, offering a scalable, low-cost solution with
significant potential for real-world clinical applications. The code is
available at https://github.com/omron-sinicx/medical-modality-dropout.

</details>


### [129] [Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model](https://arxiv.org/abs/2509.18308)
*Yixin Zhang,Ryan Chamberlain,Lawrance Ngo,Kevin Kramer,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: This study evaluates nine segmentation architectures on a curated PE-specific CT dataset and highlights that 3D U-Net remains highly effective, CNNs outperform Vision Transformers, and pretraining on PE classification may hinder segmentation performance.


<details>
  <summary>Details</summary>
Motivation: To assess and compare state-of-the-art segmentation architectures for pulmonary embolism (PE) detection and segmentation on a challenging CT dataset.

Method: The research uses a dense dataset of 490 annotated CTPA scans to systematically evaluate nine segmentation architectures (using CNNs and Vision Transformers) under a unified framework, considering factors like pretrained weights, model type, and domain-specific challenges.

Result: 3D U-Net with ResNet excels in PE segmentation. CNN models generally outperform Vision Transformers, but distal emboli are difficult to segment due to complexity and dataset limitations. The best model achieved a Dice score of 0.7131.

Conclusion: 3D architectures, particularly CNN-based models like 3D U-Net, are better suited to PE segmentation tasks. PE classification pretraining may negatively affect segmentation performance, and distal emboli detection needs improvement.

Abstract: In this study, we curated a densely annotated in-house dataset comprising 490
CTPA scans. Using this dataset, we systematically evaluated nine widely used
segmentation architectures from both the CNN and Vision Transformer (ViT)
families, initialized with either pretrained or random weights, under a unified
testing framework as a performance audit. Our study leads to several important
observations: (1) 3D U-Net with a ResNet encoder remains a highly effective
architecture for PE segmentation; (2) 3D models are particularly well-suited to
this task given the morphological characteristics of emboli; (3) CNN-based
models generally yield superior performance compared to their ViT-based
counterparts in PE segmentation; (4) classification-based pretraining, even on
large PE datasets, can adversely impact segmentation performance compared to
training from scratch, suggesting that PE classification and segmentation may
rely on different sets of discriminative features; (5) different model
architectures show a highly consistent pattern of segmentation performance when
trained on the same data; and (6) while central and large emboli can be
segmented with satisfactory accuracy, distal emboli remain challenging due to
both task complexity and the scarcity of high-quality datasets. Besides these
findings, our best-performing model achieves a mean Dice score of 0.7131 for
segmentation. It detects 181 emboli with 49 false positives and 28 false
negatives from 60 in-house testing scans. Its generalizability is further
validated on public datasets.

</details>


### [130] [Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach](https://arxiv.org/abs/2509.18309)
*Alessa Carbo,Eric Nalisnick*

Main category: cs.CV

TL;DR: This paper introduces a graph neural network for handshape recognition in American Sign Language, achieving a significant accuracy improvement over baseline methods.


<details>
  <summary>Details</summary>
Motivation: Handshapes in signed languages are critical for linguistic structure, but computational models often overlook handshape recognition, limiting accuracy and linguistic utility.

Method: The paper proposes a graph neural network that separates temporal dynamics and static handshape configurations, using anatomically-informed graph structures and contrastive learning.

Result: The model established the first benchmark in structured handshape recognition, achieving 46% accuracy across 37 handshape classes, compared to 25% with baseline methods.

Conclusion: The proposed method successfully enhances handshape recognition, underscoring the importance of modeling subtle interclass differences and temporal variations.

Abstract: Handshapes serve a fundamental phonological role in signed languages, with
American Sign Language employing approximately 50 distinct shapes.
However,computational approaches rarely model handshapes explicitly, limiting
both recognition accuracy and linguistic analysis.We introduce a novel graph
neural network that separates temporal dynamics from static handshape
configurations. Our approach combines anatomically-informed graph structures
with contrastive learning to address key challenges in handshape recognition,
including subtle interclass distinctions and temporal variations. We establish
the first benchmark for structured handshape recognition in signing sequences,
achieving 46% accuracy across 37 handshape classes (with baseline methods
achieving 25%).

</details>


### [131] [Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound](https://arxiv.org/abs/2509.18326)
*Chun Kit Wong,Anders N. Christensen,Cosmin I. Bercea,Julia A. Schnabel,Martin G. Tolsgaard,Aasa Feragen*

Main category: cs.CV

TL;DR: The paper explores how the choice of classification tasks affects out-of-distribution (OOD) detection in fetal ultrasound, showing variation depending on the type of OOD shift (image characteristics or anatomical features).


<details>
  <summary>Details</summary>
Motivation: To understand how the specific classification task impacts OOD detection in fetal ultrasound amidst varying image conditions, as existing research mainly focuses on uncertainty quantification methods rather than task selection.

Method: The analysis involves evaluating eight uncertainty quantification methods across four distinct classification tasks and observing their OOD detection performance under two different ID-OOD criteria.

Result: OOD detection performance significantly depends on the classification task and the type of OOD shift examined. The optimal task varies depending on whether the shift is due to image characteristics or anatomical features.

Conclusion: Task selection and uncertainty strategies should align with the downstream application in medical imaging to ensure both effective OOD detection and abstained prediction performance.

Abstract: Reliable out-of-distribution (OOD) detection is important for safe deployment
of deep learning models in fetal ultrasound amidst heterogeneous image
characteristics and clinical settings. OOD detection relies on estimating a
classification model's uncertainty, which should increase for OOD samples.
While existing research has largely focused on uncertainty quantification
methods, this work investigates the impact of the classification task itself.
Through experiments with eight uncertainty quantification methods across four
classification tasks, we demonstrate that OOD detection performance
significantly varies with the task, and that the best task depends on the
defined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an
image characteristic shift or ii) an anatomical feature shift. Furthermore, we
reveal that superior OOD detection does not guarantee optimal abstained
prediction, underscoring the necessity to align task selection and uncertainty
strategies with the specific downstream application in medical image analysis.

</details>


### [132] [Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2509.18504)
*Jiaxin Dai,Xiang Xiang*

Main category: cs.CV

TL;DR: This paper leverages hyperbolic space for the Coarse-To-Fine Few-Shot Class-Incremental Learning (C2FSCIL) task, utilizing a Poincaré ball model and introducing novel techniques to enhance classification and feature generation under few-shot conditions.


<details>
  <summary>Details</summary>
Motivation: Hyperbolic space has proven effective for hierarchical data representation, and this paper applies its capabilities to the challenging C2FSCIL task to overcome limitations of Euclidean representation in learning hierarchical class labels.

Method: The study integrates the Poincaré ball model of hyperbolic space into the feature extraction process, introduces hyperbolic contrastive loss, hyperbolic fully-connected layers, and employs maximum entropy distribution within hyperbolic space for feature augmentation under few-shot conditions.

Result: Experiments on benchmarks demonstrate improved accuracy for both coarse and fine classes compared to existing methods.

Conclusion: Embedding the feature extractor into hyperbolic space and adopting tailored methods significantly enhance performance in the C2FSCIL task by mitigating overfitting and improving hierarchical class learning.

Abstract: In the field of machine learning, hyperbolic space demonstrates superior
representation capabilities for hierarchical data compared to conventional
Euclidean space. This work focuses on the Coarse-To-Fine Few-Shot
Class-Incremental Learning (C2FSCIL) task. Our study follows the Knowe
approach, which contrastively learns coarse class labels and subsequently
normalizes and freezes the classifier weights of learned fine classes in the
embedding space. To better interpret the "coarse-to-fine" paradigm, we propose
embedding the feature extractor into hyperbolic space. Specifically, we employ
the Poincar\'e ball model of hyperbolic space, enabling the feature extractor
to transform input images into feature vectors within the Poincar\'e ball
instead of Euclidean space. We further introduce hyperbolic contrastive loss
and hyperbolic fully-connected layers to facilitate model optimization and
classification in hyperbolic space. Additionally, to enhance performance under
few-shot conditions, we implement maximum entropy distribution in hyperbolic
space to estimate the probability distribution of fine-class feature vectors.
This allows generation of augmented features from the distribution to mitigate
overfitting during training with limited samples. Experiments on C2FSCIL
benchmarks show that our method effectively improves both coarse and fine class
accuracies.

</details>


### [133] [OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata](https://arxiv.org/abs/2509.18350)
*Oussema Dhaouadi,Riccardo Marin,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: The study presents OrthoLoC, a dataset addressing UAV-to-geospatial data localization, and introduces AdHoP, a refinement method significantly enhancing localization accuracy.


<details>
  <summary>Details</summary>
Motivation: The need for precise UAV localization in resource-constrained environments without dependence on large databases, GPS, or internet; orthographic geodata remains under-explored despite being lightweight and accessible.

Method: Creation of the OrthoLoC dataset containing UAV imagery from two countries, addressing domain shifts and enabling benchmarking. The introduction of AdHoP, a refinement method for feature matching in localization systems.

Result: OrthoLoC facilitates detailed evaluation of localization methods, and AdHoP improves feature matching accuracy by up to 95% and decreases translation error by up to 63%.

Conclusion: OrthoLoC and AdHoP provide valuable advancements for UAV localization, leveraging available geodata and improving accuracy through a benchmarked dataset and a versatile refinement technique.

Abstract: Accurate visual localization from aerial views is a fundamental problem with
applications in mapping, large-area inspection, and search-and-rescue
operations. In many scenarios, these systems require high-precision
localization while operating with limited resources (e.g., no internet
connection or GNSS/GPS support), making large image databases or heavy 3D
models impractical. Surprisingly, little attention has been given to leveraging
orthographic geodata as an alternative paradigm, which is lightweight and
increasingly available through free releases by governmental authorities (e.g.,
the European Union). To fill this gap, we propose OrthoLoC, the first
large-scale dataset comprising 16,425 UAV images from Germany and the United
States with multiple modalities. The dataset addresses domain shifts between
UAV imagery and geospatial data. Its paired structure enables fair benchmarking
of existing solutions by decoupling image retrieval from feature matching,
allowing isolated evaluation of localization and calibration performance.
Through comprehensive evaluation, we examine the impact of domain shifts, data
resolutions, and covisibility on localization accuracy. Finally, we introduce a
refinement technique called AdHoP, which can be integrated with any feature
matcher, improving matching by up to 95% and reducing translation error by up
to 63%. The dataset and code are available at:
https://deepscenario.github.io/OrthoLoC.

</details>


### [134] [A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data](https://arxiv.org/abs/2509.18354)
*Mehrdad Moradi,Shengzhe Chen,Hao Yan,Kamran Paynabar*

Main category: cs.CV

TL;DR: This paper introduces SSDnet, a zero-shot anomaly localization method in images that does not require external training data or labels, achieving state-of-the-art performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection methods often rely on training data or references, which may not be available in some real-world scenarios. The authors aim to address the challenge of detecting anomalies in a single image without relying on external data.

Method: The authors introduced SSDnet, which uses the inductive bias of CNNs inspired by Deep Image Prior. They utilize patch-based training for self-reconstruction, applying masking, patch shuffling, Gaussian noise, and perceptual loss to learn from the image itself without external references.

Result: SSDnet demonstrated high performance with AUROC of 0.99 and AUPRC of 0.60 on MVTec-AD dataset, and AUROC of 0.98 and AUPRC of 0.67 on a fabric dataset, outperforming state-of-the-art methods.

Conclusion: SSDnet effectively detects and localizes image anomalies in a zero-shot manner and opens up possibilities for robust anomaly detection without training data or references, performing well in noisy or incomplete data conditions.

Abstract: Anomaly detection in images is typically addressed by learning from
collections of training data or relying on reference samples. In many
real-world scenarios, however, such training data may be unavailable, and only
the test image itself is provided. We address this zero-shot setting by
proposing a single-image anomaly localization method that leverages the
inductive bias of convolutional neural networks, inspired by Deep Image Prior
(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key
assumption is that natural images often exhibit unified textures and patterns,
and that anomalies manifest as localized deviations from these repetitive or
stochastic patterns. To learn the deep image prior, we design a patch-based
training framework where the input image is fed directly into the network for
self-reconstruction, rather than mapping random noise to the image as done in
DIP. To avoid the model simply learning an identity mapping, we apply masking,
patch shuffling, and small Gaussian noise. In addition, we use a perceptual
loss based on inner-product similarity to capture structure beyond pixel
fidelity. Our approach needs no external training data, labels, or references,
and remains robust in the presence of noise or missing pixels. SSDnet achieves
0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the
fabric dataset, outperforming state-of-the-art methods. The implementation code
will be released at https://github.com/mehrdadmoradi124/SSDnet

</details>


### [135] [Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning](https://arxiv.org/abs/2509.18369)
*Riad Ahmed Anonto,Sardar Md. Saffat Zabin,M. Saifur Rahman*

Main category: cs.CV

TL;DR: The paper proposes a new Bengali captioning pipeline using novel tri-loss training to address challenges in grounding vision-language models in low-resource languages like Bengali.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models struggle in low-resource languages due to limited paired data and problems with English-centric pretraining that ignore target-language semantics.

Method: The method includes a Bengali captioning pipeline leveraging a frozen MaxViT for visual processing, mBART-50 for Bengali text generation, and a tri-loss training strategy comprising Patch-Alignment Loss, InfoNCE, and Sinkhorn-based Optimal Transport.

Result: The approach improved grounding and reduced spurious matches, achieving significant performance gains on benchmarks like Flickr30k-1k and MSCOCO-1k, outperforming cross-entropy baselines.

Conclusion: The tri-loss synergy provides more accurate alignment and semantic grounding for Bengali, contributing to improved vision-language modeling in similar low-resource languages.

Abstract: Grounding vision--language models in low-resource languages remains
challenging, as they often produce fluent text about the wrong objects. This
stems from scarce paired data, translation pivots that break alignment, and
English-centric pretraining that ignores target-language semantics. We address
this with a compute-aware Bengali captioning pipeline trained on LaBSE-verified
EN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT
yields stable visual patches, a Bengali-native mBART-50 decodes, and a
lightweight bridge links the modalities. Our core novelty is a tri-loss
objective: Patch-Alignment Loss (PAL) aligns real and synthetic patch
descriptors using decoder cross-attention, InfoNCE enforces global
real--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained
patch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces
spurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR
27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,
BERTScore-F1 75.40), outperforming strong CE baselines and narrowing the
real--synthetic centroid gap by 41%.

</details>


### [136] [TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning](https://arxiv.org/abs/2509.18372)
*Reeshad Khan,John Gauch*

Main category: cs.CV

TL;DR: TinyBEV is a compact camera-only model designed for real-time full-stack autonomous driving, consisting of features like detection, HD-map segmentation, and planning, with significant parameter reduction and faster runtime.


<details>
  <summary>Details</summary>
Motivation: To create a resource-efficient, real-time BEV framework for autonomy while retaining high-capacity functionalities of large-scale models.

Method: The paper proposes a model-agnostic, multi-stage distillation strategy incorporating feature-level and adaptive region-aware supervision.

Result: TinyBEV achieves advanced metrics on nuScenes dataset including 39.0 mAP detection, 1.08 minADE for motion forecasting, and 0.32 collision rate, running 5x faster at 11 FPS.

Conclusion: The work demonstrates that full-scale driving capabilities are achievable in lightweight, real-time models using solely camera input, bridging deployment challenges in constrained environments.

Abstract: We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework
that distills the full-stack capabilities of a large planning-oriented teacher
(UniAD [19]) into a compact, real-time student model. Unlike prior efficient
camera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the
complete autonomy stack 3D detection, HD-map segmentation, motion forecasting,
occupancy prediction, and goal-directed planning within a streamlined
28M-parameter backbone, achieving a 78% reduction in parameters over UniAD
[19]. Our model-agnostic, multi-stage distillation strategy combines
feature-level, output-level, and adaptive region-aware supervision to
effectively transfer high-capacity multi-modal knowledge to a lightweight BEV
representation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08
minADE for motion forecasting, and a 0.32 collision rate, while running 5x
faster (11 FPS) and requiring only camera input. These results demonstrate that
full-stack driving intelligence can be retained in resource-constrained
settings, bridging the gap between large-scale, multi-modal perception-planning
models and deployment-ready real-time autonomy.

</details>


### [137] [BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking](https://arxiv.org/abs/2509.18387)
*Thomas Gossard,Filip Radovic,Andreas Ziegler,Andrea Zell*

Main category: cs.CV

TL;DR: The paper proposes a new labeling strategy for motion blur in sports ball detection that enhances performance and introduces the BlurBall model for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: To address challenges in detecting blurry, fast-moving sports balls, which traditional labeling approaches inadequately capture due to asymmetry and lost motion information.

Method: A new labeling system places the ball at the center of the blur streak and annotates blur attributes. Additionally, BlurBall, a detection model using multi-frame attention mechanisms, is introduced.

Result: The new labeling strategy improves detection performance across models. BlurBall achieves state-of-the-art results for ball detection and demonstrates enhanced trajectory prediction.

Conclusion: Leveraging motion blur improves detection performance and enables better trajectory predictions, which can significantly benefit real-time sports analytics.

Abstract: Motion blur reduces the clarity of fast-moving objects, posing challenges for
detection systems, especially in racket sports, where balls often appear as
streaks rather than distinct points. Existing labeling conventions mark the
ball at the leading edge of the blur, introducing asymmetry and ignoring
valuable motion cues correlated with velocity. This paper introduces a new
labeling strategy that places the ball at the center of the blur streak and
explicitly annotates blur attributes. Using this convention, we release a new
table tennis ball detection dataset. We demonstrate that this labeling approach
consistently enhances detection performance across various models. Furthermore,
we introduce BlurBall, a model that jointly estimates ball position and motion
blur attributes. By incorporating attention mechanisms such as
Squeeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art
results in ball detection. Leveraging blur not only improves detection accuracy
but also enables more reliable trajectory prediction, benefiting real-time
sports analytics.

</details>


### [138] [MVP: Motion Vector Propagation for Zero-Shot Video Object Detection](https://arxiv.org/abs/2509.18388)
*Binhua Huang,Ni Wang,Wendong Yao,Soumyabrata Dev*

Main category: cs.CV

TL;DR: The paper introduces a label-free, open-vocabulary method for video object detection that reduces computational costs using motion vectors for propagation instead of running detectors on every frame.


<details>
  <summary>Details</summary>
Motivation: Running an open-vocabulary detector for every frame in a video is computationally expensive. This motivates the need for a more efficient method to achieve similar accuracy while saving computational resources.

Method: The proposed method, MVP, uses OWLv2 only on keyframes and propagates detections to intermediate frames via a compressed-domain motion vector approach. It employs a 3x3 grid aggregation for translation and scale updates, plus optional modifications, without requiring labels or fine-tuning.

Result: On the ILSVRC2015-VID dataset, MVP achieves mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. It performs comparably to or better than detector-based propagation methods and supervised methods for coarse localization while remaining label-free.

Conclusion: MVP demonstrates that compressed-domain motion vector propagation is an effective and label-free strategy to reduce detector invocations, preserving strong zero-shot coverage in video analysis.

Abstract: Running a large open-vocabulary (Open-vocab) detector on every video frame is
accurate but expensive. We introduce a training-free pipeline that invokes
OWLv2 only on fixed-interval keyframes and propagates detections to
intermediate frames using compressed-domain motion vectors (MV). A simple 3x3
grid aggregation of motion vectors provides translation and uniform-scale
updates, augmented with an area-growth check and an optional single-class
switch. The method requires no labels, no fine-tuning, and uses the same prompt
list for all open-vocabulary methods. On ILSVRC2015-VID (validation dataset),
our approach (MVP) attains mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose
intersection-over-union (IoU) thresholds it remains close to framewise
OWLv2-Large (0.747/0.721 at 0.2/0.3 versus 0.784/0.780), reflecting that coarse
localization is largely preserved. Under the same keyframe schedule, MVP
outperforms tracker-based propagation (MOSSE, KCF, CSRT) at mAP@0.5. A
supervised reference (YOLOv12x) reaches 0.631 at mAP@0.5 but requires labeled
training, whereas our method remains label-free and open-vocabulary. These
results indicate that compressed-domain propagation is a practical way to
reduce detector invocations while keeping strong zero-shot coverage in videos.
Our code and models are available at https://github.com/microa/MVP.

</details>


### [139] [Improving the color accuracy of lighting estimation models](https://arxiv.org/abs/2509.18390)
*Zitian Zhang,Joshua Urban Davis,Jeanne Phuong Anh Vu,Jiangtao Kuang,Jean-François Lalonde*

Main category: cs.CV

TL;DR: This paper explores improving the color robustness of high dynamic range (HDR) lighting estimation methods for augmented reality by testing simple adaptation techniques, demonstrating that input preprocessing with a white balance network enhances color accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the often-overlooked issue of color robustness in HDR lighting estimation methods, which is crucial for achieving realistic rendering and compositing of virtual objects in AR applications.

Method: The study employs a novel HDR dataset featuring diverse lighting colors to test various adaptation strategies without altering existing lighting estimation algorithms. It evaluates preprocessing input images with a white balance network.

Result: Preprocessing input images with a pre-trained white balance network significantly enhances color robustness, outperforming other tested strategies and requiring no retraining of the models.

Conclusion: Simple preprocessing techniques like white balance adjustments can generalize across different HDR lighting estimation methods, leading to improved color accuracy and better visual realism in AR applications.

Abstract: Advances in high dynamic range (HDR) lighting estimation from a single image
have opened new possibilities for augmented reality (AR) applications.
Predicting complex lighting environments from a single input image allows for
the realistic rendering and compositing of virtual objects. In this work, we
investigate the color robustness of such methods -- an often overlooked yet
critical factor for achieving visual realism. While most evaluations conflate
color with other lighting attributes (e.g., intensity, direction), we isolate
color as the primary variable of interest. Rather than introducing a new
lighting estimation algorithm, we explore whether simple adaptation techniques
can enhance the color accuracy of existing models. Using a novel HDR dataset
featuring diverse lighting colors, we systematically evaluate several
adaptation strategies. Our results show that preprocessing the input image with
a pre-trained white balance network improves color robustness, outperforming
other strategies across all tested scenarios. Notably, this approach requires
no retraining of the lighting estimation model. We further validate the
generality of this finding by applying the technique to three state-of-the-art
lighting estimation methods from recent literature.

</details>


### [140] [Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models](https://arxiv.org/abs/2509.18405)
*Sourav Halder,Jinjun Tong,Xinyu Wu*

Main category: cs.CV

TL;DR: The paper proposes a novel framework for detecting fields on checks using training-free methods leveraging vision language models and multimodal large language models, achieving strong performance on varied check datasets.


<details>
  <summary>Details</summary>
Motivation: Checks are still widely used for transactions but are a significant target of fraud. Detecting key check fields accurately is crucial for verifying authenticity but current methods require large, labeled datasets which are scarce.

Method: Introduce a training-free framework for field-level check detection using a combination of vision language models (VLM) and multimodal large language models (MLLM), enabling zero-shot detection.

Result: The framework achieves good performance and generalization on a custom dataset of 110 checks with varied formats. It also assists in generating labeled datasets for specialized object detection models.

Conclusion: The proposed method lowers deployment barriers for check fraud detection systems and serves as a foundational tool for creating advanced models tailored to financial environments.

Abstract: Checks remain a foundational instrument in the financial ecosystem,
facilitating substantial transaction volumes across institutions. However,
their continued use also renders them a persistent target for fraud,
underscoring the importance of robust check fraud detection mechanisms. At the
core of such systems lies the accurate identification and localization of
critical fields, such as the signature, magnetic ink character recognition
(MICR) line, courtesy amount, legal amount, payee, and payer, which are
essential for subsequent verification against reference checks belonging to the
same customer. This field-level detection is traditionally dependent on object
detection models trained on large, diverse, and meticulously labeled datasets,
a resource that is scarce due to proprietary and privacy concerns. In this
paper, we introduce a novel, training-free framework for automated check field
detection, leveraging the power of a vision language model (VLM) in conjunction
with a multimodal large language model (MLLM). Our approach enables zero-shot
detection of check components, significantly lowering the barrier to deployment
in real-world financial settings. Quantitative evaluation of our model on a
hand-curated dataset of 110 checks spanning multiple formats and layouts
demonstrates strong performance and generalization capability. Furthermore,
this framework can serve as a bootstrap mechanism for generating high-quality
labeled datasets, enabling the development of specialized real-time object
detection models tailored to institutional needs.

</details>


### [141] [Losing the Plot: How VLM responses degrade on imperfect charts](https://arxiv.org/abs/2509.18425)
*Philip Wootaek Shin,Jack Sampson,Vijaykrishnan Narayanan,Andres Marquez,Mahantesh Halappanavar*

Main category: cs.CV

TL;DR: The paper evaluates the performance of leading vision-language models (VLMs) on distorted, occluded charts, finding significant reliability gaps like hallucination under imperfect conditions. They introduce the CHART NOISe dataset to rigorously test VLM performance and vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Real-world charts often contain distortions and require reasoning beyond simple matching, a scenario not addressed effectively by existing benchmarks.

Method: The researchers evaluated VLMs under corrupted chart conditions using a new dataset, CHART NOISe, and tested reverse inconsistency prompts along with proposing baseline defense strategies.

Result: Performance drops were evident when charts were corrupted or occluded, with hallucinations increasing and models exhibiting overconfidence. The CHART NOISe dataset highlighted systematic vulnerabilities in chart reasoning.

Conclusion: CHART NOISe sets a foundation for improving robustness and reliability in chart understanding tasks, exposing vulnerabilities in current VLMs and guiding future research directions.

Abstract: Vision language models (VLMs) show strong results on chart understanding, yet
existing benchmarks assume clean figures and fact based queries. Real world
charts often contain distortions and demand reasoning beyond simple matching.
We evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp
performance drops under corruption or occlusion, with hallucinations such as
value fabrication, trend misinterpretation, and entity confusion becoming more
frequent. Models remain overconfident in degraded settings, generating
plausible but unsupported explanations.
  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,
and Reasoning Testing on Noisy and Occluded Input Selections), a dataset
combining chart corruptions, occlusions, and exam style multiple choice
questions inspired by Korea's CSAT English section. A key innovation is prompt
reverse inconsistency, where models contradict themselves when asked to confirm
versus deny the same statement. Our contributions are threefold: (1)
benchmarking state of the art VLMs, exposing systematic vulnerabilities in
chart reasoning; (2) releasing CHART NOISe, the first dataset unifying
corruption, occlusion, and reverse inconsistency; and (3) proposing baseline
mitigation strategies such as quality filtering and occlusion detection.
Together, these efforts establish a rigorous testbed for advancing robustness
and reliability in chart understanding.

</details>


### [142] [CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction](https://arxiv.org/abs/2509.18427)
*Xinyang Wu,Muheng Li,Xia Li,Orso Pusterla,Sairos Safai,Philippe C. Cattin,Antony J. Lomax,Ye Zhang*

Main category: cs.CV

TL;DR: The paper presents a novel approach using neural networks to enhance 4D-MRI for radiation therapy, reducing processing time while improving motion representation accuracy.


<details>
  <summary>Details</summary>
Motivation: Conventional 4D-MRI methods encounter challenges such as temporal variability, complex workflows, and high computational demand.

Method: The authors propose a neural representation framework with two networks: the Spatial Anatomy Network for 3D anatomy representation and the Temporal Motion Network for deformation modeling using Transformer-derived signals.

Result: The new method reduces processing time from 5 hours to 15 minutes, reconstructs high-fidelity 3D images in under one second, and captures both regular and irregular respiratory patterns accurately.

Conclusion: The framework surpasses traditional 4D-MRI methods in efficiency, accuracy, and adaptability, highlighting its potential for use in radiation therapy planning and real-time treatment adaptation.

Abstract: Four-dimensional MRI (4D-MRI) is an promising technique for capturing
respiratory-induced motion in radiation therapy planning and delivery.
Conventional 4D reconstruction methods, which typically rely on phase binning
or separate template scans, struggle to capture temporal variability,
complicate workflows, and impose heavy computational loads. We introduce a
neural representation framework that considers respiratory motion as a smooth,
continuous deformation steered by a 1D surrogate signal, completely replacing
the conventional discrete sorting approach. The new method fuses motion
modeling with image reconstruction through two synergistic networks: the
Spatial Anatomy Network (SAN) encodes a continuous 3D anatomical
representation, while a Temporal Motion Network (TMN), guided by
Transformer-derived respiratory signals, produces temporally consistent
deformation fields. Evaluation using a free-breathing dataset of 19 volunteers
demonstrates that our template- and phase-free method accurately captures both
regular and irregular respiratory patterns, while preserving vessel and
bronchial continuity with high anatomical fidelity. The proposed method
significantly improves efficiency, reducing the total processing time from
approximately five hours required by conventional discrete sorting methods to
just 15 minutes of training. Furthermore, it enables inference of each 3D
volume in under one second. The framework accurately reconstructs 3D images at
any respiratory state, achieves superior performance compared to conventional
methods, and demonstrates strong potential for application in 4D radiation
therapy planning and real-time adaptive treatment.

</details>


### [143] [An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects](https://arxiv.org/abs/2509.18451)
*Prithvi Raj Singh,Raju Gottumukkala,Anthony Maida*

Main category: cs.CV

TL;DR: This paper evaluates the performance of five Kalman filter-based tracking methods for fast-moving, small objects using a custom racquetball dataset, identifying tracking accuracy and speed limitations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of tracking fast-moving tiny objects like a racquetball in computer vision, especially for sport robotics applications.

Method: Evaluation of five state-of-the-art Kalman filter-based tracking methods on 10,000 racquetball frames, analyzing inference speed and update frequency's impact on accuracy.

Result: DeepOCSORT achieved the best accuracy with 31.15 pixels ADE, while ByteTrack was fastest at 26.6ms inference time. All methods showed significant errors (3-11cm drift).

Conclusion: Existing trackers have fundamental limitations in handling such tasks, requiring significant improvements for reliable performance in fast-moving tiny object scenarios.

Abstract: Unpredictable movement patterns and small visual mark make precise tracking
of fast-moving tiny objects like a racquetball one of the challenging problems
in computer vision. This challenge is particularly relevant for sport robotics
applications, where lightweight and accurate tracking systems can improve robot
perception and planning capabilities. While Kalman filter-based tracking
methods have shown success in general object tracking scenarios, their
performance degrades substantially when dealing with rapidly moving objects
that exhibit irregular bouncing behavior. In this study, we evaluate the
performance of five state-of-the-art Kalman filter-based tracking
methods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom
dataset containing 10,000 annotated racquetball frames captured at 720p-1280p
resolution. We focus our analysis on two critical performance factors:
inference speed and update frequency per image, examining how these parameters
affect tracking accuracy and reliability for fast-moving tiny objects. Our
experimental evaluation across four distinct scenarios reveals that DeepOCSORT
achieves the lowest tracking error with an average ADE of 31.15 pixels compared
to ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest
processing at 26.6ms average inference time versus DeepOCSORT's 26.8ms.
However, our results show that all Kalman filter-based trackers exhibit
significant tracking drift with spatial errors ranging from 3-11cm (ADE values:
31-114 pixels), indicating fundamental limitations in handling the
unpredictable motion patterns of fast-moving tiny objects like racquetballs.
Our analysis demonstrates that current tracking approaches require substantial
improvements, with error rates 3-4x higher than standard object tracking
benchmarks, highlighting the need for specialized methodologies for fast-moving
tiny object tracking applications.

</details>


### [144] [MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition](https://arxiv.org/abs/2509.18473)
*Binhua Huang,Wendong Yao,Shaowu Chen,Guoxin Wang,Qingyuan Wang,Soumyabrata Dev*

Main category: cs.CV

TL;DR: MoCrop is a motion-aware cropping module for video action recognition that efficiently uses motion vectors in compressed video, improving accuracy and reducing computation requirements with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient video action recognition in the compressed domain by leveraging motion vectors to focus on motion-dense regions, optimizing accuracy, and computational resources.

Method: MoCrop uses a training-free adaptive cropping module that utilizes motion vectors, incorporating denoising, Monte Carlo sampling, and a motion-density submatrix search to generate a single clip-level crop for all I-frames at inference. This approach is computationally lightweight and easily applicable to various backbones.

Result: On UCF101, MoCrop improves ResNet-50 accuracy by +3.5% at equal FLOPs or +2.4% accuracy with 26.5% fewer FLOPs. It enhances CoViAR accuracy to 89.2% at the original cost and reduces compute from 11.6 to 8.5 GFLOPs at 88.5% accuracy. Additionally, it yields consistent improvements across MobileNet-V3, EfficientNet-B1, and Swin-B.

Conclusion: MoCrop effectively improves video action recognition accuracy while reducing computational overhead, demonstrating strong versatility across models and practical potential for real-time applications in compressed video processing.

Abstract: We introduce MoCrop, a motion-aware adaptive cropping module for efficient
video action recognition in the compressed domain. MoCrop uses motion vectors
that are available in H.264 video to locate motion-dense regions and produces a
single clip-level crop that is applied to all I-frames at inference. The module
is training free, adds no parameters, and can be plugged into diverse
backbones. A lightweight pipeline that includes denoising & merge (DM), Monte
Carlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix
search yields robust crops with negligible overhead. On UCF101, MoCrop improves
accuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy
at equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer
FLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy
at the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6
to 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B
indicate strong generality and make MoCrop practical for real-time deployment
in the compressed domain. Our code and models are available at
https://github.com/microa/MoCrop.

</details>


### [145] [Codebook-Based Adaptive Feature Compression With Semantic Enhancement for Edge-Cloud Systems](https://arxiv.org/abs/2509.18481)
*Xinyu Wang,Zikun Zhou,Yingjian Li,Xin An,Hongpeng Wang*

Main category: cs.CV

TL;DR: The paper introduces CAFC-SE, a framework for compressing visual features into discrete indices, aimed at improving tasks under low-bitrate conditions for edge-cloud systems.


<details>
  <summary>Details</summary>
Motivation: Improving coding efficiency and analysis performance in edge-cloud systems under low-bitrate conditions while minimizing redundant details.

Method: Proposes CAFC-SE framework based on Vector Quantization (VQ), which maps visual features to discrete codebook indices, transmitting only select data.

Result: Demonstrated superior performance of CAFC-SE in terms of both bitrate efficiency and analytical accuracy compared to previous methods.

Conclusion: CAFC-SE is an effective approach for compressing visual features under low-bitrate constraints, enhancing edge-cloud analysis frameworks.

Abstract: Coding images for machines with minimal bitrate and strong analysis
performance is key to effective edge-cloud systems. Several approaches deploy
an image codec and perform analysis on the reconstructed image. Other methods
compress intermediate features using entropy models and subsequently perform
analysis on the decoded features. Nevertheless, these methods both perform
poorly under low-bitrate conditions, as they retain many redundant details or
learn over-concentrated symbol distributions. In this paper, we propose a
Codebook-based Adaptive Feature Compression framework with Semantic
Enhancement, named CAFC-SE. It maps continuous visual features to discrete
indices with a codebook at the edge via Vector Quantization (VQ) and
selectively transmits them to the cloud. The VQ operation that projects feature
vectors onto the nearest visual primitives enables us to preserve more
informative visual patterns under low-bitrate conditions. Hence, CAFC-SE is
less vulnerable to low-bitrate conditions. Extensive experiments demonstrate
the superiority of our method in terms of rate and accuracy.

</details>


### [146] [MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation](https://arxiv.org/abs/2509.18493)
*Md Mostafijur Rahman,Radu Marculescu*

Main category: cs.CV

TL;DR: The paper introduces MK-UNet, a lightweight U-shaped convolutional neural network for medical image segmentation, with high efficiency and superior accuracy compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of developing a highly efficient and accurate medical image segmentation model suitable for real-time scenarios and resource-limited settings.

Method: The method revolves around the design of a multi-kernel depth-wise convolution block (MKDC) and advanced attention mechanisms, optimizing the network for lightweight performance and high accuracy in medical image segmentation.

Result: MK-UNet achieves superior performance, significantly outscoring benchmarks like TransUNet, UNeXt, and other lightweight networks in segmentation accuracy (DICE score) while requiring drastically fewer parameters and FLOPs.

Conclusion: MK-UNet represents a breakthrough in combining efficiency and accuracy for real-time medical image segmentation, making it highly suitable for deployment in point-of-care devices and low-resource environments.

Abstract: In this paper, we introduce MK-UNet, a paradigm shift towards
ultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image
segmentation. Central to MK-UNet is the multi-kernel depth-wise convolution
block (MKDC) we design to adeptly process images through multiple kernels,
while capturing complex multi-resolution spatial relationships. MK-UNet also
emphasizes the images salient features through sophisticated attention
mechanisms, including channel, spatial, and grouped gated attention. Our
MK-UNet network, with a modest computational footprint of only 0.316M
parameters and 0.314G FLOPs, represents not only a remarkably lightweight, but
also significantly improved segmentation solution that provides higher accuracy
over state-of-the-art (SOTA) methods across six binary medical imaging
benchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with
nearly 333$\times$ and 123$\times$ fewer parameters and FLOPs, respectively.
Similarly, when compared against UNeXt, MK-UNet exhibits superior segmentation
performance, improving the DICE score up to 6.7% margins while operating with
4.7$\times$ fewer #Params. Our MK-UNet also outperforms other recent
lightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with
much lower computational resources. This leap in performance, coupled with
drastic computational gains, positions MK-UNet as an unparalleled solution for
real-time, high-fidelity medical diagnostics in resource-limited settings, such
as point-of-care devices. Our implementation is available at
https://github.com/SLDGroup/MK-UNet.

</details>


### [147] [BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation](https://arxiv.org/abs/2509.18501)
*Maximilian Fehrentz,Alexander Winkler,Thomas Heiliger,Nazim Haouchine,Christian Heiliger,Nassir Navab*

Main category: cs.CV

TL;DR: The paper presents BridgeSplat, an approach that combines intraoperative 3D reconstruction with preoperative CT data for surgical navigation, ensuring alignment and deformability based on surgical video.


<details>
  <summary>Details</summary>
Motivation: To integrate surgical video with volumetric patient data, addressing the gap in deformable surgical navigation through better alignment and reconstruction.

Method: BridgeSplat employs rigging of 3D Gaussians to CT mesh, optimizing Gaussian parameters and mesh deformation using photometric supervision, and parametrizing Gaussians relative to the mesh triangles to maintain alignment.

Result: The method shows successful deformations of preoperative CT data on monocular RGB inputs in tests on visceral pig surgeries and synthetic human liver data.

Conclusion: BridgeSplat effectively bridges the gap between surgical video and CT data, enabling deformable surgical navigation. The code and resources are available for further research.

Abstract: We introduce BridgeSplat, a novel approach for deformable surgical navigation
that couples intraoperative 3D reconstruction with preoperative CT data to
bridge the gap between surgical video and volumetric patient data. Our method
rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian
parameters and mesh deformation through photometric supervision. By
parametrizing each Gaussian relative to its parent mesh triangle, we enforce
alignment between Gaussians and mesh and obtain deformations that can be
propagated back to update the CT. We demonstrate BridgeSplat's effectiveness on
visceral pig surgeries and synthetic data of a human liver under simulation,
showing sensible deformations of the preoperative CT on monocular RGB data.
Code, data, and additional resources can be found at
https://maxfehrentz.github.io/ct-informed-splatting/ .

</details>


### [148] [Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment](https://arxiv.org/abs/2509.18502)
*Wenjie Liu,Hongmin Liu,Lixin Zhang,Bin Fan*

Main category: cs.CV

TL;DR: The paper proposes DGLE, a framework leveraging diffusion models to propagate high-quality pseudo-labels in Source-Free Domain Adaptation for enhanced remote sensing image segmentation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges in domain adaptation for semantic segmentation in remote sensing images, specifically Source-Free Domain Adaptation (SFDA), where source domain data is inaccessible.

Method: The proposed method, Diffusion-Guided Label Enrichment (DGLE), combines confidence filtering and super-resolution enhancement to identify high-quality pseudo-labels, and uses a diffusion model to propagate these labels to create a complete, high-quality pseudo-label set.

Result: The DGLE method significantly improves the quality of pseudo-labels, making the optimization process more effective and enhancing the semantic segmentation model's performance in the target domain.

Conclusion: The framework avoids challenges associated with directly optimizing noisy pseudo-label sets, and demonstrates a practical solution to improve performance in source-free domain adaptation for remote sensing semantic segmentation.

Abstract: Research on unsupervised domain adaptation (UDA) for semantic segmentation of
remote sensing images has been extensively conducted. However, research on how
to achieve domain adaptation in practical scenarios where source domain data is
inaccessible namely, source-free domain adaptation (SFDA) remains limited.
Self-training has been widely used in SFDA, which requires obtaining as many
high-quality pseudo-labels as possible to train models on target domain data.
Most existing methods optimize the entire pseudo-label set to obtain more
supervisory information. However, as pseudo-label sets often contain
substantial noise, simultaneously optimizing all labels is challenging. This
limitation undermines the effectiveness of optimization approaches and thus
restricts the performance of self-training. To address this, we propose a novel
pseudo-label optimization framework called Diffusion-Guided Label Enrichment
(DGLE), which starts from a few easily obtained high-quality pseudo-labels and
propagates them to a complete set of pseudo-labels while ensuring the quality
of newly generated labels. Firstly, a pseudo-label fusion method based on
confidence filtering and super-resolution enhancement is proposed, which
utilizes cross-validation of details and contextual information to obtain a
small number of high-quality pseudo-labels as initial seeds. Then, we leverage
the diffusion model to propagate incomplete seed pseudo-labels with irregular
distributions due to its strong denoising capability for randomly distributed
noise and powerful modeling capacity for complex distributions, thereby
generating complete and high-quality pseudo-labels. This method effectively
avoids the difficulty of directly optimizing the complete set of pseudo-labels,
significantly improves the quality of pseudo-labels, and thus enhances the
model's performance in the target domain.

</details>


### [149] [GeoRemover: Removing Objects and Their Causal Visual Artifacts](https://arxiv.org/abs/2509.18538)
*Zixin Zhu,Haoxiang Li,Xuelu Feng,He Wu,Chunming Qiao,Junsong Yuan*

Main category: cs.CV

TL;DR: This paper presents a novel geometry-aware framework to perform object removal and eliminate associated visual artifacts like shadows and reflections.


<details>
  <summary>Details</summary>
Motivation: Current image editing methods either fail to remove causal effects not strictly masked or lack controllability, leading to unintended results. This occurs due to neglecting causal relationships between an object's presence and its visual impacts.

Method: A two-stage framework is proposed: (1) Geometry removal is done using mask-aligned supervision to edit the object at the structural level. (2) Appearance rendering updates the photorealistic RGB image based on the modified geometry, implicitly addressing causal artifacts.

Result: The framework achieves state-of-the-art performance on object and artifact removal tasks in two popular benchmarks.

Conclusion: By leveraging geometry-aware editing, this method successfully removes objects and associated visual effects while avoiding unintended consequences such as over-erasure of other elements. Code is publicly available.

Abstract: Towards intelligent image editing, object removal should eliminate both the
target object and its causal visual artifacts, such as shadows and reflections.
However, existing image appearance-based methods either follow strictly
mask-aligned training and fail to remove these causal effects which are not
explicitly masked, or adopt loosely mask-aligned strategies that lack
controllability and may unintentionally over-erase other objects. We identify
that these limitations stem from ignoring the causal relationship between an
object's geometry presence and its visual effects. To address this limitation,
we propose a geometry-aware two-stage framework that decouples object removal
into (1) geometry removal and (2) appearance rendering. In the first stage, we
remove the object directly from the geometry (e.g., depth) using strictly
mask-aligned supervision, enabling structure-aware editing with strong
geometric constraints. In the second stage, we render a photorealistic RGB
image conditioned on the updated geometry, where causal visual effects are
considered implicitly as a result of the modified 3D geometry. To guide
learning in the geometry removal stage, we introduce a preference-driven
objective based on positive and negative sample pairs, encouraging the model to
remove objects as well as their causal visual artifacts while avoiding new
structural insertions. Extensive experiments demonstrate that our method
achieves state-of-the-art performance in removing both objects and their
associated artifacts on two popular benchmarks. The code is available at
https://github.com/buxiangzhiren/GeoRemover.

</details>


### [150] [SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models](https://arxiv.org/abs/2509.18546)
*Yujia Liu,Dingquan Li,Tiejun Huang*

Main category: cs.CV

TL;DR: The paper introduces SEGA, a novel black-box adversarial attack method with enhanced transferability against No-Reference Image Quality Assessment models.


<details>
  <summary>Details</summary>
Motivation: Enhancing the transferability of adversarial attacks in black-box scenarios against NR-IQA models, addressing their vulnerabilities.

Method: SEGA employs Gaussian smoothing and ensembling smoothed gradients from source models to approximate target model gradients, combined with a perturbation filter mask to ensure imperceptibility.

Result: Testing on the CLIVE dataset demonstrates SEGA's superior transferability and effectiveness for black-box attacks against NR-IQA models.

Conclusion: SEGA achieves robust, transferable attacks in black-box settings, paving the way for improved NR-IQA system security.

Abstract: No-Reference Image Quality Assessment (NR-IQA) models play an important role
in various real-world applications. Recently, adversarial attacks against
NR-IQA models have attracted increasing attention, as they provide valuable
insights for revealing model vulnerabilities and guiding robust system design.
Some effective attacks have been proposed against NR-IQA models in white-box
settings, where the attacker has full access to the target model. However,
these attacks often suffer from poor transferability to unknown target models
in more realistic black-box scenarios, where the target model is inaccessible.
This work makes the first attempt to address the challenge of low
transferability in attacking NR-IQA models by proposing a transferable Signed
Ensemble Gaussian black-box Attack (SEGA). The main idea is to approximate the
gradient of the target model by applying Gaussian smoothing to source models
and ensembling their smoothed gradients. To ensure the imperceptibility of
adversarial perturbations, SEGA further removes inappropriate perturbations
using a specially designed perturbation filter mask. Experimental results on
the CLIVE dataset demonstrate the superior transferability of SEGA, validating
its effectiveness in enabling successful transfer-based black-box attacks
against NR-IQA models.

</details>


### [151] [HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles](https://arxiv.org/abs/2509.18550)
*Mohammad Junayed Hasan,Nabeel Mohammed,Shafin Rahman,Philipp Koehn*

Main category: cs.CV

TL;DR: The paper introduces HadaSmileNet, a novel feature fusion framework using Hadamard multiplicative interactions to efficiently detect genuine versus posed emotions via smile recognition, achieving state-of-the-art results and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of differentiating between genuine and posed emotions, specifically within smile recognition, for applications in social sciences, healthcare, and human-computer interaction using a computationally efficient approach.

Method: Employ a transformer-based feature representation fused with D-Markers through parameter-free Hadamard multiplicative interactions for optimal performance and reduced computational requirements.

Result: Achieves state-of-the-art performance on four benchmark datasets, with reduced parameters (26 percent) and simpler training compared to multi-task learning approaches.

Conclusion: HadaSmileNet successfully combines physiological domain knowledge with efficient deep learning-based feature fusion techniques, making it practical for real-time affective computing and multimedia data mining applications.

Abstract: The distinction between genuine and posed emotions represents a fundamental
pattern recognition challenge with significant implications for data mining
applications in social sciences, healthcare, and human-computer interaction.
While recent multi-task learning frameworks have shown promise in combining
deep learning architectures with handcrafted D-Marker features for smile facial
emotion recognition, these approaches exhibit computational inefficiencies due
to auxiliary task supervision and complex loss balancing requirements. This
paper introduces HadaSmileNet, a novel feature fusion framework that directly
integrates transformer-based representations with physiologically grounded
D-Markers through parameter-free multiplicative interactions. Through
systematic evaluation of 15 fusion strategies, we demonstrate that Hadamard
multiplicative fusion achieves optimal performance by enabling direct feature
interactions while maintaining computational efficiency. The proposed approach
establishes new state-of-the-art results for deep learning methods across four
benchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS
(98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational
analysis reveals 26 percent parameter reduction and simplified training
compared to multi-task alternatives, while feature visualization demonstrates
enhanced discriminative power through direct domain knowledge integration. The
framework's efficiency and effectiveness make it particularly suitable for
practical deployment in multimedia data mining applications that require
real-time affective computing capabilities.

</details>


### [152] [Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction](https://arxiv.org/abs/2509.18566)
*Xiaoting Yin,Hao Shi,Kailun Yang,Jiajun Zhai,Shangwei Guo,Lin Wang,Kaiwei Wang*

Main category: cs.CV

TL;DR: The paper proposes a novel method for reconstructing dynamic humans and static scenes using monocular event cameras combined with 3D Gaussian Splatting, offering state-of-the-art results especially for high-speed motion scenarios.


<details>
  <summary>Details</summary>
Motivation: There is a difficulty in reconstructing dynamic humans and scenes from monocular video under fast motion due to motion blur in RGB frames. Event cameras provide high temporal resolution, presenting an alternative for overcoming these challenges.

Method: A unified framework employs 3D Gaussian Splatting where Gaussians are semantically classified as human or scene. Human Gaussians undergo deformation for animation, while scene Gaussians remain static. An event-guided loss is introduced to match brightness change renderings to the event stream, enhancing fidelity in fast-moving areas.

Result: The proposed method achieves state-of-the-art performance on benchmark datasets, reducing motion artifacts and improving metrics like PSNR, SSIM, and LPIPS, with significant benefits in high-speed scenarios.

Conclusion: The framework effectively addresses challenges in reconstructing dynamic humans and scenes under motion blur using event cameras, improving reconstruction accuracy and simplifying processing pipelines.

Abstract: Reconstructing dynamic humans together with static scenes from monocular
videos remains difficult, especially under fast motion, where RGB frames suffer
from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond
temporal resolution, making them a superior sensing choice for dynamic human
reconstruction. Accordingly, we present a novel event-guided human-scene
reconstruction framework that jointly models human and scene from a single
monocular event camera via 3D Gaussian Splatting. Specifically, a unified set
of 3D Gaussians carries a learnable semantic attribute; only Gaussians
classified as human undergo deformation for animation, while scene Gaussians
stay static. To combat blur, we propose an event-guided loss that matches
simulated brightness changes between consecutive renderings with the event
stream, improving local fidelity in fast-moving regions. Our approach removes
the need for external human masks and simplifies managing separate Gaussian
sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers
state-of-the-art human-scene reconstruction, with notable gains over strong
baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.

</details>


### [153] [Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought](https://arxiv.org/abs/2509.18571)
*Yuhan Wang,Cheng Liu,Zihan Zhao,Weichao Wu*

Main category: cs.CV

TL;DR: The paper proposes Live-E2T, a framework for real-time threat monitoring using semantic tuples, event deduplication, and fine-tuned Large Language Models (LLMs) for explainable threat assessments.


<details>
  <summary>Details</summary>
Motivation: Prevailing methods for real-time threat monitoring struggle to balance performance with explainability, creating the need for a framework that excels in both aspects.

Method: The authors introduce three mechanisms: semantic tuple extraction from video frames, an efficient event deduplication process, and fine-tuning a Large Language Model with a Chain-of-Thought strategy for logical reasoning and coherent assessments.

Result: Experiments on datasets like XD-Violence and UCF-Crime show Live-E2T surpasses state-of-the-art methods in accuracy, efficiency, and explainability.

Conclusion: Live-E2T provides a significant advancement in real-time threat monitoring by combining efficiency and explainability, offering superior performance compared to existing methods.

Abstract: Real-time threat monitoring identifies threatening behaviors in video streams
and provides reasoning and assessment of threat events through explanatory
text. However, prevailing methodologies, whether based on supervised learning
or generative models, struggle to concurrently satisfy the demanding
requirements of real-time performance and decision explainability. To bridge
this gap, we introduce Live-E2T, a novel framework that unifies these two
objectives through three synergistic mechanisms. First, we deconstruct video
frames into structured Human-Object-Interaction-Place semantic tuples. This
approach creates a compact, semantically focused representation, circumventing
the information degradation common in conventional feature compression. Second,
an efficient online event deduplication and updating mechanism is proposed to
filter spatio-temporal redundancies, ensuring the system's real time
responsiveness. Finally, we fine-tune a Large Language Model using a
Chain-of-Thought strategy, endow it with the capability for transparent and
logical reasoning over event sequences to produce coherent threat assessment
reports. Extensive experiments on benchmark datasets, including XD-Violence and
UCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art
methods in terms of threat detection accuracy, real-time efficiency, and the
crucial dimension of explainability.

</details>


### [154] [The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers](https://arxiv.org/abs/2509.18582)
*Daiqing Qi,Handong Zhao,Jing Shi,Simon Jenni,Yifei Fan,Franck Dernoncourt,Scott Cohen,Sheng Li*

Main category: cs.CV

TL;DR: This paper addresses the challenge of aesthetic visual understanding in Multimodal Large Language Models (MLLMs) by introducing a new dataset, model, and benchmark.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle to differentiate between general and aesthetic visual understanding, especially in real-world applications requiring photographic expertise.

Method: Developed PhotoCritique dataset, proposed PhotoEye model with a language-guided multi-view vision fusion mechanism, and introduced PhotoBench benchmark.

Result: PhotoEye model showed superior performance compared to existing models on both traditional benchmarks and the proposed PhotoBench.

Conclusion: The paper successfully advances aesthetic visual understanding in MLLMs, providing tools and methods to better capture photographic expertise and aesthetics.

Abstract: While editing directly from life, photographers have found it too difficult
to see simultaneously both the blue and the sky. Photographer and curator,
Szarkowski insightfully revealed one of the notable gaps between general and
aesthetic visual understanding: while the former focuses on identifying the
factual element in an image (sky), the latter transcends such object
identification, viewing it instead as an aesthetic component--a pure color
block (blue). Such fundamental distinctions between general (detection,
localization, etc.) and aesthetic (color, lighting, composition, etc.) visual
understanding present a significant challenge for Multimodal Large Language
Models (MLLMs). Although some recent works have made initial explorations, they
are often limited to general and basic aesthetic commonsense. As a result, they
frequently fall short in real-world scenarios (Fig. 1), which require extensive
expertise--including photographic techniques, photo pre/post-processing
knowledge, and more, to provide a detailed analysis and description. To
fundamentally enhance the aesthetics understanding of MLLMs, we first introduce
a novel dataset, PhotoCritique, derived from extensive discussions among
professional photographers and enthusiasts, and characterized by the large
scale, expertise, and diversity. Then, to better learn visual aesthetics from
PhotoCritique, we furthur propose a novel model, PhotoEye, featuring a
languageguided multi-view vision fusion mechanism to understand image
aesthetics from multiple perspectives. Finally, we present a novel benchmark,
PhotoBench, a comprehensive and professional benchmark for aesthetic visual
understanding. On existing benchmarks and PhotoBench, our model demonstrates
clear advantages over existing models.

</details>


### [155] [Enhancing Video Object Segmentation in TrackRAD Using XMem Memory Network](https://arxiv.org/abs/2509.18591)
*Pengchao Deng,Shengqi Chen*

Main category: cs.CV

TL;DR: An advanced segmentation method using XMem architecture for tumor tracking in MRI-radiotherapy, tailored for the TrackRAD2025 challenge.


<details>
  <summary>Details</summary>
Motivation: Improve tumor tracking in MRI-guided radiotherapy to enhance accuracy and safety in cancer treatment.

Method: Utilized XMem, a memory-augmented framework, for efficient real-time segmentation across long cine-MRI sequences.

Result: Achieved reasonable real-time segmentation performance under challenging conditions, although no precise quantitative data is provided.

Conclusion: Framework highlights potential in clinical applications by meeting real-time requirements and addressing tumor tracking challenges effectively.

Abstract: This paper presents an advanced tumor segmentation framework for real-time
MRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method
leverages the XMem model, a memory-augmented architecture, to segment tumors
across long cine-MRI sequences. The proposed system efficiently integrates
memory mechanisms to track tumor motion in real-time, achieving high
segmentation accuracy even under challenging conditions with limited annotated
data. Unfortunately, the detailed experimental records have been lost,
preventing us from reporting precise quantitative results at this stage.
Nevertheless, From our preliminary impressions during development, the
XMem-based framework demonstrated reasonable segmentation performance and
satisfied the clinical real-time requirement. Our work contributes to improving
the precision of tumor tracking during MRI-guided radiotherapy, which is
crucial for enhancing the accuracy and safety of cancer treatments.

</details>


### [156] [SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution](https://arxiv.org/abs/2509.18593)
*Xiaoman Wu,Lubin Gan,Siying Wu,Jing Zhang,Yunwei Ou,Xiaoyan Sun*

Main category: cs.CV

TL;DR: This paper introduces the Spatial-Semantic Consistent Model (SSCM) to address the challenges of achieving accurate spatial-semantic consistency in multi-contrast MRI super-resolution.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of poor spatial-semantic consistency in multi-contrast MRI super-resolution, which is crucial for efficient imaging and accurate anatomical alignment.

Method: The proposed SSCM incorporates three key components—Dynamic Spatial Warping Module for spatial alignment, Semantic-Aware Token Aggregation Block for semantic consistency, and Spatial-Frequency Fusion Block for refining structures.

Result: Experiments demonstrate that SSCM achieves state-of-the-art performance in MRI super-resolution while using fewer parameters and maintaining spatial-semantic consistency across reconstructions.

Conclusion: SSCM effectively enhances multi-contrast MRI super-resolution by improving anatomical alignment and structural recovery, offering a new standard for efficient and high-quality imaging.

Abstract: Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims
to enhance low-resolution (LR) contrasts leveraging high-resolution (HR)
references, shortening acquisition time and improving imaging efficiency while
preserving anatomical details. The main challenge lies in maintaining
spatial-semantic consistency, ensuring anatomical structures remain
well-aligned and coherent despite structural discrepancies and motion between
the target and reference images. Conventional methods insufficiently model
spatial-semantic consistency and underuse frequency-domain information, which
leads to poor fine-grained alignment and inadequate recovery of high-frequency
details. In this paper, we propose the Spatial-Semantic Consistent Model
(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast
spatial alignment, a Semantic-Aware Token Aggregation Block for long-range
semantic consistency, and a Spatial-Frequency Fusion Block for fine structure
restoration. Experiments on public and private datasets show that SSCM achieves
state-of-the-art performance with fewer parameters while ensuring spatially and
semantically consistent reconstructions.

</details>


### [157] [OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation](https://arxiv.org/abs/2509.18600)
*Zhuoxiao Chen,Hongyang Yu,Ying Xu,Yadan Luo,Long Duong,Yuan-Fang Li*

Main category: cs.CV

TL;DR: The paper proposes OraPO, an RL-only framework with FactS rewards to generate radiology reports efficiently using minimal data and compute resources.


<details>
  <summary>Details</summary>
Motivation: To address the high data and compute demands of prevailing radiology report generation methods on chest X-ray images.

Method: The paper introduces a lightweight, single-stage reinforcement learning framework (OraPO) that incorporates oracle-driven preference supervision and FactScore-based rewards for better learning efficiency.

Result: Their approach achieves state-of-the-art performance on the CheXpert Plus dataset, attaining 0.341 F1 score with significantly reduced computational and data requirements.

Conclusion: OraPO and FactS prove to be an efficient and effective alternative for radiology report generation, offering accessibility through minimized resource usage while maintaining high accuracy and clinical relevance.

Abstract: Radiology report generation (RRG) aims to automatically produce clinically
faithful reports from chest X-ray images. Prevailing work typically follows a
scale-driven paradigm, by multi-stage training over large paired corpora and
oversized backbones, making pipelines highly data- and compute-intensive. In
this paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based
reward (FactS) to tackle the RRG task under constrained budgets. OraPO enables
single-stage, RL-only training by converting failed GRPO explorations on rare
or difficult studies into direct preference supervision via a lightweight
oracle step. FactS grounds learning in diagnostic evidence by extracting atomic
clinical facts and checking entailment against ground-truth labels, yielding
dense, interpretable sentence-level rewards. Together, OraPO and FactS create a
compact and powerful framework that significantly improves learning efficiency
on clinically challenging cases, setting the new SOTA performance on the
CheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training
data using a small base VLM on modest hardware.

</details>


### [158] [Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation](https://arxiv.org/abs/2509.18602)
*Xu Liu,Yibo Lu,Xinxian Wang,Xinyu Wu*

Main category: cs.CV

TL;DR: The paper introduces Adaptive Multi-Style Fusion (AMSF), a training-free framework for fusing multiple styles in diffusion models, allowing user-controlled and scalable multi-style blending with superior results.


<details>
  <summary>Details</summary>
Motivation: Existing reference-based fusion methods are limited to single style images and lack mechanisms to balance multiple stylistic inputs, restraining hybrid aesthetic capabilities.

Method: AMSF encodes the style images and textual hints using a semantic token decomposition module, injecting them into a frozen diffusion model. It utilizes a similarity-aware re-weighting module to balance stylistic influences dynamically during the denoising process, without requiring fine-tuning.

Result: AMSF achieves superior performance in multi-style fusion compared to state-of-the-art methods, demonstrating scalability to two or more styles and balanced results.

Conclusion: AMSF provides a practical framework for expressive and controllable multi-style generation, addressing the constraints of prior methods while enhancing flexibility and scalability.

Abstract: We propose Adaptive Multi-Style Fusion (AMSF), a reference-based
training-free framework that enables controllable fusion of multiple reference
styles in diffusion models. Most of the existing reference-based methods are
limited by (a) acceptance of only one style image, thus prohibiting hybrid
aesthetics and scalability to more styles, and (b) lack of a principled
mechanism to balance several stylistic influences. AMSF mitigates these
challenges by encoding all style images and textual hints with a semantic token
decomposition module that is adaptively injected into every cross-attention
layer of an frozen diffusion model. A similarity-aware re-weighting module then
recalibrates, at each denoising step, the attention allocated to every style
component, yielding balanced and user-controllable blends without any
fine-tuning or external adapters. Both qualitative and quantitative evaluations
show that AMSF produces multi-style fusion results that consistently outperform
the state-of-the-art approaches, while its fusion design scales seamlessly to
two or more styles. These capabilities position AMSF as a practical step toward
expressive multi-style generation in diffusion models.

</details>


### [159] [MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2509.18613)
*Yuzhi Wu,Li Xiao,Jun Liu,Guangfeng Jiang,XiangGen Xia*

Main category: cs.CV

TL;DR: MLF-4DRCNet is proposed for improved 3D object detection using multi-level radar-camera fusion, addressing shortcomings in prior methods like radar sparsity.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome limitations of 4D millimeter-wave radar sparsity and noise in 3D object detection by proposing a more comprehensive radar-camera fusion approach.

Method: MLF-4DRCNet utilizes a two-stage framework with three modules: Enhanced Radar Point Encoder, Hierarchical Scene Fusion Pooling, and Proposal-Level Fusion Enhancement, incorporating multi-level information for robust feature representation.

Result: MLF-4DRCNet achieves state-of-the-art performance on VoD and TJ4DRadSet datasets, and its results are comparable to LiDAR-based models on VoD.

Conclusion: The proposed multi-level fusion framework significantly enhances radar-camera perception, improving standalone radar applicability in autonomous driving scenes.

Abstract: The emerging 4D millimeter-wave radar, measuring the range, azimuth,
elevation, and Doppler velocity of objects, is recognized for its
cost-effectiveness and robustness in autonomous driving. Nevertheless, its
point clouds exhibit significant sparsity and noise, restricting its standalone
application in 3D object detection. Recent 4D radar-camera fusion methods have
provided effective perception. Most existing approaches, however, adopt
explicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera
fusion, neglecting radar's inherent drawbacks. Specifically, they overlook the
sparse and incomplete geometry of radar point clouds and restrict fusion to
coarse scene-level integration. To address these problems, we propose
MLF-4DRCNet, a novel two-stage framework for 3D object detection via
multi-level fusion of 4D radar and camera images. Our model incorporates the
point-, scene-, and proposal-level multi-modal information, enabling
comprehensive feature representation. It comprises three crucial components:
the Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion
Pooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.
Operating at the point-level, ERPE densities radar point clouds with 2D image
instances and encodes them into voxels via the proposed Triple-Attention Voxel
Feature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D
image features using deformable attention to capture scene context and adopts
pooling to the fused features. PLFE refines region proposals by fusing image
features, and further integrates with the pooled features from HSFP.
Experimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets
demonstrate that MLF-4DRCNet achieves the state-of-the-art performance.
Notably, it attains performance comparable to LiDAR-based models on the VoD
dataset.

</details>


### [160] [Prompt-Guided Dual Latent Steering for Inversion Problems](https://arxiv.org/abs/2509.18619)
*Yichen Wu,Xu Liu,Chenxuan Zhao,Xinyu Wu*

Main category: cs.CV

TL;DR: This paper introduces Prompt-Guided Dual Latent Steering (PDLS), a framework to improve image reconstruction from corrupted images using dual latent spaces for structural integrity and semantic accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with balancing structural fidelity and semantic accuracy while reconstructing corrupted images, leading to issues like semantic drift, blurred details, or incorrect attributes.

Method: The paper proposes PDLS, which uses Rectified Flow models, decomposes the process into structural and semantic paths, and formulates it as an optimal control problem solved via a Linear Quadratic Regulator (LQR) to dynamically steer the generative path.

Result: The method achieves better reconstruction fidelity and semantic alignment compared to single-latent baselines across various inversion tasks such as deblurring, super-resolution, and inpainting on FFHQ-1K and ImageNet-1K datasets.

Conclusion: PDLS effectively addresses semantic drift and detail preservation without costly optimization per image, offering an efficient solution for high-quality image inversion tasks.

Abstract: Inverting corrupted images into the latent space of diffusion models is
challenging. Current methods, which encode an image into a single latent
vector, struggle to balance structural fidelity with semantic accuracy, leading
to reconstructions with semantic drift, such as blurred details or incorrect
attributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering
(PDLS), a novel, training-free framework built upon Rectified Flow models for
their stable inversion paths. PDLS decomposes the inversion process into two
complementary streams: a structural path to preserve source integrity and a
semantic path guided by a prompt. We formulate this dual guidance as an optimal
control problem and derive a closed-form solution via a Linear Quadratic
Regulator (LQR). This controller dynamically steers the generative trajectory
at each step, preventing semantic drift while ensuring the preservation of fine
detail without costly, per-image optimization. Extensive experiments on FFHQ-1K
and ImageNet-1K under various inversion tasks, including Gaussian deblurring,
motion deblurring, super-resolution and freeform inpainting, demonstrate that
PDLS produces reconstructions that are both more faithful to the original image
and better aligned with the semantic information than single-latent baselines.

</details>


### [161] [Learning neuroimaging models from health system-scale data](https://arxiv.org/abs/2509.18638)
*Yiwei Lyu,Samir Harake,Asadur Chowdury,Soumyanil Banerjee,Rachel Gologorsky,Shixuan Liu,Anna-Katharina Meissner,Akshay Rao,Chenhui Zhao,Akhil Kondepudi,Cheng Jiang,Xinhai Hou,Rushikesh S. Joshi,Volker Neuschmelting,Ashok Srinivasan,Dawn Kleindorfer,Brian Athey,Vikas Gulani,Aditya Pandey,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: The paper introduces Prima, a vision language model (VLM) trained on over 220,000 MRI studies, which outperforms existing AI models in diagnosing neurological conditions.


<details>
  <summary>Details</summary>
Motivation: To address rising demand for MRI studies, prolonged turnaround times, physician burnout, and unequal access in low-resource areas by leveraging AI for neuroimaging analysis.

Method: Prima was developed using a hierarchical vision architecture and trained on over 220,000 MRI studies. It was tested over a 1-year period with 30,000 MRI studies, achieving high diagnostic performance across 52 neurological conditions.

Result: Prima achieved a diagnostic AUC of 92.0 across major neurologic disorders and outperformed other state-of-the-art AI models. It offers explainable diagnoses, prioritization features for radiologists, and fair healthcare outcomes.

Conclusion: Prima is a promising AI foundation for neuroimaging, improving healthcare efficiency and fairness, while addressing systemic issues such as resource disparity and diagnostic delays.

Abstract: Neuroimaging is a ubiquitous tool for evaluating patients with neurological
diseases. The global demand for magnetic resonance imaging (MRI) studies has
risen steadily, placing significant strain on health systems, prolonging
turnaround times, and intensifying physician burnout \cite{Chen2017-bt,
Rula2024-qp-1}. These challenges disproportionately impact patients in
low-resource and rural settings. Here, we utilized a large academic health
system as a data engine to develop Prima, the first vision language model (VLM)
serving as an AI foundation for neuroimaging that supports real-world, clinical
MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a
hierarchical vision architecture that provides general and transferable MRI
features. Prima was tested in a 1-year health system-wide study that included
30K MRI studies. Across 52 radiologic diagnoses from the major neurologic
disorders, including neoplastic, inflammatory, infectious, and developmental
lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,
outperforming other state-of-the-art general and medical AI models. Prima
offers explainable differential diagnoses, worklist priority for radiologists,
and clinical referral recommendations across diverse patient demographics and
MRI systems. Prima demonstrates algorithmic fairness across sensitive groups
and can help mitigate health system biases, such as prolonged turnaround times
for low-resource populations. These findings highlight the transformative
potential of health system-scale VLMs and Prima's role in advancing AI-driven
healthcare.

</details>


### [162] [Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation](https://arxiv.org/abs/2509.18639)
*Yuanhuiyi Lyu,Chi Kit Wong,Chenfei Liao,Lutao Jiang,Xu Zheng,Zexin Lu,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: The paper introduces Understanding-in-Generation (UiG), a new framework for text-to-image generation that improves performance via image editing and generative guidance.


<details>
  <summary>Details</summary>
Motivation: Current reasoning techniques in text-to-image models fail to combine understanding and generation effectively, reducing their ability to improve generative capabilities.

Method: The UiG framework integrates understanding into the generation process using image editing as a step-by-step bridge.

Result: UiG outperforms existing methods, achieving a 3.92% improvement in the long prompt setting of the TIIF benchmark.

Conclusion: The UiG framework enhances unified models' text-to-image generation by successfully blending understanding and generative reasoning.

Abstract: Recent works have made notable advancements in enhancing unified models for
text-to-image generation through the Chain-of-Thought (CoT). However, these
reasoning methods separate the processes of understanding and generation, which
limits their ability to guide the reasoning of unified models in addressing the
deficiencies of their generative capabilities. To this end, we propose a novel
reasoning framework for unified models, Understanding-in-Generation (UiG),
which harnesses the robust understanding capabilities of unified models to
reinforce their performance in image generation. The core insight of our UiG is
to integrate generative guidance by the strong understanding capabilities
during the reasoning process, thereby mitigating the limitations of generative
abilities. To achieve this, we introduce "Image Editing" as a bridge to infuse
understanding into the generation process. Initially, we verify the generated
image and incorporate the understanding of unified models into the editing
instructions. Subsequently, we enhance the generated image step by step,
gradually infusing the understanding into the generation process. Our UiG
framework demonstrates a significant performance improvement in text-to-image
generation over existing text-to-image reasoning methods, e.g., a 3.92% gain on
the long prompt setting of the TIIF benchmark. The project code:
https://github.com/QC-LY/UiG

</details>


### [163] [Zero-shot Monocular Metric Depth for Endoscopic Images](https://arxiv.org/abs/2509.18642)
*Nicolas Toussaint,Emanuele Colleoni,Ricardo Sanchez-Matilla,Joshua Sutcliffe,Vanessa Thompson,Muhammad Asad,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: This paper enhances monocular depth estimation for endoscopic images by benchmarking state-of-the-art models and introducing a synthetic dataset (EndoSynth).


<details>
  <summary>Details</summary>
Motivation: The lack of robust benchmarks and high-quality datasets for depth estimation in endoscopic images hampers clinical applications.

Method: The study benchmarks state-of-the-art depth estimation models using real endoscopic images and introduces a synthetic dataset (EndoSynth) with ground truth metric depth and segmentation masks.

Result: Fine-tuning depth foundation models using the EndoSynth dataset significantly improves accuracy on unseen real data.

Conclusion: The paper advances depth estimation research for endoscopic images and provides a pivotal resource with its benchmark and dataset.

Abstract: Monocular relative and metric depth estimation has seen a tremendous boost in
the last few years due to the sharp advancements in foundation models and in
particular transformer based networks. As we start to see applications to the
domain of endoscopic images, there is still a lack of robust benchmarks and
high-quality datasets in that area. This paper addresses these limitations by
presenting a comprehensive benchmark of state-of-the-art (metric and relative)
depth estimation models evaluated on real, unseen endoscopic images, providing
critical insights into their generalisation and performance in clinical
scenarios. Additionally, we introduce and publish a novel synthetic dataset
(EndoSynth) of endoscopic surgical instruments paired with ground truth metric
depth and segmentation masks, designed to bridge the gap between synthetic and
real-world data. We demonstrate that fine-tuning depth foundation models using
our synthetic dataset boosts accuracy on most unseen real data by a significant
margin. By providing both a benchmark and a synthetic dataset, this work
advances the field of depth estimation for endoscopic images and serves as an
important resource for future research. Project page, EndoSynth dataset and
trained weights are available at https://github.com/TouchSurgery/EndoSynth.

</details>


### [164] [LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection](https://arxiv.org/abs/2509.18683)
*Lanhu Wu,Zilin Gao,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: This paper introduces LEAF-Mamba, a method enhancing RGB-D salient object detection by leveraging state space models for both local dependencies and cross-modality integration, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations in existing RGB-D SOD methods, which struggle with balancing performance and computational efficiency due to the nature of CNNs and Vision Transformers.

Method: Proposes the LEAF-Mamba model, which includes two key components: a Local Emphatic State Space Module (LE-SSM) for better local dependency capture, and an Adaptive Fusion Module (AFM) for enhanced cross-modality integration.

Result: LEAF-Mamba outperforms 16 state-of-the-art RGB-D SOD methods in terms of both effectiveness and computational efficiency, while also generalizing well to RGB-T SOD tasks.

Conclusion: The LEAF-Mamba approach effectively balances performance and efficiency in RGB-D SOD tasks, showing significant potential for generalization across related tasks.

Abstract: RGB-D salient object detection (SOD) aims to identify the most conspicuous
objects in a scene with the incorporation of depth cues. Existing methods
mainly rely on CNNs, limited by the local receptive fields, or Vision
Transformers that suffer from the cost of quadratic complexity, posing a
challenge in balancing performance and computational efficiency. Recently,
state space models (SSM), Mamba, have shown great potential for modeling
long-range dependency with linear complexity. However, directly applying SSM to
RGB-D SOD may lead to deficient local semantics as well as the inadequate
cross-modality fusion. To address these issues, we propose a Local Emphatic and
Adaptive Fusion state space model (LEAF-Mamba) that contains two novel
components: 1) a local emphatic state space module (LE-SSM) to capture
multi-scale local dependencies for both modalities. 2) an SSM-based adaptive
fusion module (AFM) for complementary cross-modality interaction and reliable
cross-modality integration. Extensive experiments demonstrate that the
LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in
both efficacy and efficiency. Moreover, our method can achieve excellent
performance on the RGB-T SOD task, proving a powerful generalization ability.

</details>


### [165] [Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification](https://arxiv.org/abs/2509.18692)
*Xinle Gao,Linghui Ye,Zhiyong Xiao*

Main category: cs.CV

TL;DR: The paper proposes a lightweight food image classification model using Window Multi-Head Attention Mechanism (WMHAM) and Spatial Attention Mechanism (SAM) to reduce computational costs while improving accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of high computational complexity and large number of parameters in Vision Transformer models for food image classification, making the task efficient and deployable in resource-constrained environments.

Method: The model incorporates a Window Multi-Head Attention Mechanism (WMHAM) for efficient window partitioning and context capture, alongside a Spatial Attention Mechanism (SAM) for emphasizing key spatial regions.

Result: The proposed model achieves 95.24% accuracy on the Food-101 dataset and 94.33% on the Vireo Food-172 dataset, with reduced parameters and FLOPs compared to baseline methods.

Conclusion: This approach balances computational efficiency with high classification performance, making it ideal for use in resource-limited settings such as automated food quality control and intelligent agriculture.

Abstract: With the rapid development of society and continuous advances in science and
technology, the food industry increasingly demands higher production quality
and efficiency. Food image classification plays a vital role in enabling
automated quality control on production lines, supporting food safety
supervision, and promoting intelligent agricultural production. However, this
task faces challenges due to the large number of parameters and high
computational complexity of Vision Transformer models. To address these issues,
we propose a lightweight food image classification algorithm that integrates a
Window Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism
(SAM). The WMHAM reduces computational cost by capturing local and global
contextual features through efficient window partitioning, while the SAM
adaptively emphasizes key spatial regions to improve discriminative feature
representation. Experiments conducted on the Food-101 and Vireo Food-172
datasets demonstrate that our model achieves accuracies of 95.24% and 94.33%,
respectively, while significantly reducing parameters and FLOPs compared with
baseline methods. These results confirm that the proposed approach achieves an
effective balance between computational efficiency and classification
performance, making it well-suited for deployment in resource-constrained
environments.

</details>


### [166] [OSDA: A Framework for Open-Set Discovery and Automatic Interpretation of Land-cover in Remote Sensing Imagery](https://arxiv.org/abs/2509.18693)
*Siyi Chen,Kai Wang,Weicong Pang,Ruiming Yang,Ziru Chen,Renjun Gao,Alexis Kai Hon Lau,Dasa Gu,Chenchen Zhang,Cheng Li*

Main category: cs.CV

TL;DR: This paper introduces OSDA, an architecture-agnostic and annotation-free framework for open-set land-cover analysis, combining precise segmentation, semantic attribution, and evaluation in remote sensing data.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges of fine-grained spatial localization and semantically open categorization in open-set land-cover analysis without requiring manual annotations.

Method: The paper proposes OSDA, a three-stage pipeline that includes: (1) segmentation via a fine-tuned SAM model, (2) semantic reasoning with a fine-tuned multimodal LLM, and (3) evaluation through an LLM-as-judge mechanism and manual scoring.

Result: OSDA demonstrates precise segmentation and semantic understanding capabilities, showing promise for automated cartographic updates and efficient large-scale earth observation analysis.

Conclusion: The framework offers a scalable and interpretable approach to open-set land-cover analysis, enabling dynamic and automated monitoring across diverse satellite imagery.

Abstract: Open-set land-cover analysis in remote sensing requires the ability to
achieve fine-grained spatial localization and semantically open categorization.
This involves not only detecting and segmenting novel objects without
categorical supervision but also assigning them interpretable semantic labels
through multimodal reasoning. In this study, we introduce OSDA, an integrated
three-stage framework for annotation-free open-set land-cover discovery,
segmentation, and description. The pipeline consists of: (1) precise discovery
and mask extraction with a promptable fine-tuned segmentation model (SAM), (2)
semantic attribution and contextual description via a two-phase fine-tuned
multimodal large language model (MLLM), and (3) LLM-as-judge and manual scoring
of the MLLMs evaluation. By combining pixel-level accuracy with high-level
semantic understanding, OSDA addresses key challenges in open-world remote
sensing interpretation. Designed to be architecture-agnostic and label-free,
the framework supports robust evaluation across diverse satellite imagery
without requiring manual annotation. Our work provides a scalable and
interpretable solution for dynamic land-cover monitoring, showing strong
potential for automated cartographic updating and large-scale earth observation
analysis.

</details>


### [167] [Overview of PlantCLEF 2021: cross-domain plant identification](https://arxiv.org/abs/2509.18697)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: The paper evaluates the impact of leveraging herbarium collections for improving automated plant identification in biodiversity-rich but data-poor regions, using the LifeCLEF 2021 Plant Identification Challenge.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the imbalance in data availability for automated plant identification, where tropical, biodiversity-rich regions face a scarcity of field photos for machine learning, despite a wealth of historical herbarium records.

Method: The challenge used a dataset from the Guiana Shield, focusing on 1,000 species. Training data included herbarium sheets, photos, metadata, and morphological traits. The task involved cross-domain classification, training on herbarium and photo data while testing exclusively on field photos.

Result: The paper summarizes the evaluations, systems, and approaches used by participating research groups in the LifeCLEF 2021 challenge, analyzing how herbarium data can enhance plant identification in understudied regions.

Conclusion: Utilizing herbarium collections contributes significantly to automated plant identification in regions lacking field photo data, stressing the value of integrating diverse data sources for global biodiversity assessments.

Abstract: Automated plant identification has improved considerably thanks to recent
advances in deep learning and the availability of training data with more and
more field photos. However, this profusion of data concerns only a few tens of
thousands of species, mainly located in North America and Western Europe, much
less in the richest regions in terms of biodiversity such as tropical
countries. On the other hand, for several centuries, botanists have
systematically collected, catalogued and stored plant specimens in herbaria,
especially in tropical regions, and recent efforts by the biodiversity
informatics community have made it possible to put millions of digitised
records online. The LifeCLEF 2021 plant identification challenge (or "PlantCLEF
2021") was designed to assess the extent to which automated identification of
flora in data-poor regions can be improved by using herbarium collections. It
is based on a dataset of about 1,000 species mainly focused on the Guiana
Shield of South America, a region known to have one of the highest plant
diversities in the world. The challenge was evaluated as a cross-domain
classification task where the training set consisted of several hundred
thousand herbarium sheets and a few thousand photos to allow learning a
correspondence between the two domains. In addition to the usual metadata
(location, date, author, taxonomy), the training data also includes the values
of 5 morphological and functional traits for each species. The test set
consisted exclusively of photos taken in the field. This article presents the
resources and evaluations of the assessment carried out, summarises the
approaches and systems used by the participating research groups and provides
an analysis of the main results.

</details>


### [168] [AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping](https://arxiv.org/abs/2509.18699)
*Zedong Zhang,Ying Tai,Jianjun Qian,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: This paper introduces "Adaptive Group Swapping (AGSwap)" and a new dataset "Cross-category Object Fusion (COF)" to improve text-to-image generation for fused cross-category objects.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of generating visually coherent text-to-image results when fusing cross-category objects, a problem often plagued by bias, visual chaos, and semantic inconsistencies. Progress in this area has been hampered by the lack of a structured benchmark dataset.

Method: The proposed AGSwap involves two main components: 1) Group-wise Embedding Swapping for feature manipulation to merge attributes from different concepts, and 2) Adaptive Group Updating, which dynamically optimizes outputs according to a balance evaluation score for better coherence.

Result: AGSwap, along with the introduced COF dataset, significantly outperforms existing methods in generating coherent and semantically accurate images, including beating GPT-Image-1 with both simple and complex image generation prompts.

Conclusion: The study addresses the limitations of existing text-to-image generation methods and datasets, proposing AGSwap and COF as effective solutions to improve cross-category object fusion, paving the way for better practical applications.

Abstract: Fusing cross-category objects to a single coherent object has gained
increasing attention in text-to-image (T2I) generation due to its broad
applications in virtual reality, digital media, film, and gaming. However,
existing methods often produce biased, visually chaotic, or semantically
inconsistent results due to overlapping artifacts and poor integration.
Moreover, progress in this field has been limited by the absence of a
comprehensive benchmark dataset. To address these problems, we propose
\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective
approach comprising two key components: (1) Group-wise Embedding Swapping,
which fuses semantic attributes from different concepts through feature
manipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism
guided by a balance evaluation score to ensure coherent synthesis.
Additionally, we introduce \textbf{Cross-category Object Fusion (COF)}, a
large-scale, hierarchically structured dataset built upon ImageNet-1K and
WordNet. COF includes 95 superclasses, each with 10 subclasses, enabling
451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap
outperforms state-of-the-art compositional T2I methods, including GPT-Image-1
using simple and complex prompts.

</details>


### [169] [Overview of LifeCLEF Plant Identification task 2019: diving into data deficient tropical countries](https://arxiv.org/abs/2509.18705)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: The paper discusses the PlantCLEF 2019 challenge, which aims to improve automated plant identification in data-deficient regions using a dataset focused on the flora of the Guiana Shield and Northern Amazon rainforest.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of plant identification in areas with limited training data, particularly focusing on highly biodiverse yet data-deficient regions.

Method: The study uses a dataset of 10K species and evaluates various deep learning systems against tropical flora experts in the PlantCLEF 2019 challenge.

Result: Several systems were assessed, highlighting their performance and effectiveness as compared to tropical flora experts in identifying plant species from the provided dataset.

Conclusion: The challenge underscores the critical role of automated systems in identifying plants in biodiverse regions, with insights into the approaches, systems, and their relative success.

Abstract: Automated identification of plants has improved considerably thanks to the
recent progress in deep learning and the availability of training data.
However, this profusion of data only concerns a few tens of thousands of
species, while the planet has nearly 369K. The LifeCLEF 2019 Plant
Identification challenge (or "PlantCLEF 2019") was designed to evaluate
automated identification on the flora of data deficient regions. It is based on
a dataset of 10K species mainly focused on the Guiana shield and the Northern
Amazon rainforest, an area known to have one of the greatest diversity of
plants and animals in the world. As in the previous edition, a comparison of
the performance of the systems evaluated with the best tropical flora experts
was carried out. This paper presents the resources and assessments of the
challenge, summarizes the approaches and systems employed by the participating
research groups, and provides an analysis of the main outcomes.

</details>


### [170] [RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images](https://arxiv.org/abs/2509.18711)
*Ke Li,Di Wang,Ting Wang,Fuyu Dong,Yiming Zhang,Luyao Zhang,Xiangyu Wang,Shaofeng Li,Quan Wang*

Main category: cs.CV

TL;DR: This paper presents RSVG-ZeroOV, a training-free framework for zero-shot remote sensing visual grounding using frozen foundation models.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to remote sensing visual grounding rely on closed-set vocabularies or require expensive datasets and fine-tuning, limiting their scalability and adaptability.

Method: RSVG-ZeroOV leverages frozen generic foundation models and includes three stages: extracting cross-attention maps using a vision-language model (VLM), enriching object details via a diffusion model (DM), and refining segmentation masks with an attention evolution module.

Result: The proposed framework consistently surpasses weakly-supervised and zero-shot methods in performance, as evidenced by extensive experiments.

Conclusion: RSVG-ZeroOV is an efficient, scalable solution for open-vocabulary visual grounding, eliminating the need for costly training or fine-tuning.

Abstract: Remote sensing visual grounding (RSVG) aims to localize objects in remote
sensing images based on free-form natural language expressions. Existing
approaches are typically constrained to closed-set vocabularies, limiting their
applicability in open-world scenarios. While recent attempts to leverage
generic foundation models for open-vocabulary RSVG, they overly rely on
expensive high-quality datasets and time-consuming fine-tuning. To address
these limitations, we propose \textbf{RSVG-ZeroOV}, a training-free framework
that aims to explore the potential of frozen generic foundation models for
zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key
stages: (i) Overview: We utilize a vision-language model (VLM) to obtain
cross-attention\footnote[1]{In this paper, although decoder-only VLMs use
self-attention over all tokens, we refer to the image-text interaction part as
cross-attention to distinguish it from pure visual self-attention.}maps that
capture semantic correlations between text queries and visual regions. (ii)
Focus: By leveraging the fine-grained modeling priors of a diffusion model
(DM), we fill in gaps in structural and shape information of objects, which are
often overlooked by VLM. (iii) Evolve: A simple yet effective attention
evolution module is introduced to suppress irrelevant activations, yielding
purified segmentation masks over the referred objects. Without cumbersome
task-specific training, RSVG-ZeroOV offers an efficient and scalable solution.
Extensive experiments demonstrate that the proposed framework consistently
outperforms existing weakly-supervised and zero-shot methods.

</details>


### [171] [What Makes You Unique? Attribute Prompt Composition for Object Re-Identification](https://arxiv.org/abs/2509.18715)
*Yingquan Wang,Pingping Zhang,Chong Sun,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: The paper introduces a novel framework called Attribute Prompt Composition (APC) to enhance object Re-identification (ReID) across non-overlapping camera views. It addresses limitations of single-domain and cross-domain approaches using semantic attributes and a specialized training strategy.


<details>
  <summary>Details</summary>
Motivation: Current ReID models are either constrained to single-domain scenarios (prone to overfitting) or cross-domain setups (suppress identity cues), limiting their effectiveness in real-world applications.

Method: The APC framework utilizes an Attribute Prompt Generator (APG) with a Semantic Attribute Dictionary (SAD) and Prompt Composition Module (PCM) to adaptively generate discriminative features. Additionally, it employs a Fast-Slow Training Strategy (FSTS) using fast and slow update streams to balance discrimination and generalization.

Result: Experiments on diverse datasets show that APC outperforms state-of-the-art ReID methods in both discrimination and generalization capacities.

Conclusion: The integration of textual semantics and dual-stream training in APC effectively addresses key limitations in ReID, making it more adaptable and accurate for real-world scenarios.

Abstract: Object Re-IDentification (ReID) aims to recognize individuals across
non-overlapping camera views. While recent advances have achieved remarkable
progress, most existing models are constrained to either single-domain or
cross-domain scenarios, limiting their real-world applicability. Single-domain
models tend to overfit to domain-specific features, whereas cross-domain models
often rely on diverse normalization strategies that may inadvertently suppress
identity-specific discriminative cues. To address these limitations, we propose
an Attribute Prompt Composition (APC) framework, which exploits textual
semantics to jointly enhance discrimination and generalization. Specifically,
we design an Attribute Prompt Generator (APG) consisting of a Semantic
Attribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an
over-complete attribute dictionary to provide rich semantic descriptions, while
PCM adaptively composes relevant attributes from SAD to generate discriminative
attribute-aware features. In addition, motivated by the strong generalization
ability of Vision-Language Models (VLM), we propose a Fast-Slow Training
Strategy (FSTS) to balance ReID-specific discrimination and generalizable
representation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)
to rapidly acquire ReID-specific discriminative knowledge and a Slow Update
Stream (SUS) to retain the generalizable knowledge inherited from the
pre-trained VLM. Through a mutual interaction, the framework effectively
focuses on ReID-relevant features while mitigating overfitting. Extensive
experiments on both conventional and Domain Generalized (DG) ReID datasets
demonstrate that our framework surpasses state-of-the-art methods, exhibiting
superior performances in terms of both discrimination and generalization. The
source code is available at https://github.com/AWangYQ/APC.

</details>


### [172] [Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment](https://arxiv.org/abs/2509.18717)
*Tong Zhang,Kuofeng Gao,Jiawang Bai,Leo Yu Zhang,Xin Yin,Zonghui Wang,Shouling Ji,Wenzhi Chen*

Main category: cs.CV

TL;DR: The paper addresses limitations of prior defenses against data poisoning in CLIP models by proposing OTCCLIP, an approach based on optimal transport for better alignment of image-caption pairs, improving robustness and model performance.


<details>
  <summary>Details</summary>
Motivation: Ensure robustness of CLIP models against targeted data poisoning and backdoor attacks caused by the use of vast, unverified datasets from the Internet.

Method: Propose OTCCLIP, a framework using optimal transport-based distance measures and objective functions to better align fine-grained features of images and captions.

Result: OTCCLIP effectively reduces attack success rates and significantly enhances the performance of CLIP's zero-shot and linear probing tasks on poisoned datasets.

Conclusion: Optimal transport-based techniques can robustly mitigate data poisoning issues in CLIP models, offering improved model dependability and performance.

Abstract: Recent studies have shown that Contrastive Language-Image Pre-training (CLIP)
models are threatened by targeted data poisoning and backdoor attacks due to
massive training image-caption pairs crawled from the Internet. Previous
defense methods correct poisoned image-caption pairs by matching a new caption
for each image. However, the matching process relies solely on the global
representations of images and captions, overlooking fine-grained features of
visual and textual features. It may introduce incorrect image-caption pairs and
harm the CLIP pre-training. To address their limitations, we propose an Optimal
Transport-based framework to reconstruct image-caption pairs, named OTCCLIP. We
propose a new optimal transport-based distance measure between fine-grained
visual and textual feature sets and re-assign new captions based on the
proposed optimal transport distance. Additionally, to further reduce the
negative impact of mismatched pairs, we encourage the inter- and intra-modality
fine-grained alignment by employing optimal transport-based objective
functions. Our experiments demonstrate that OTCCLIP can successfully decrease
the attack success rates of poisoning attacks. Also, compared to previous
methods, OTCCLIP significantly improves CLIP's zero-shot and linear probing
performance trained on poisoned datasets.

</details>


### [173] [Knowledge Transfer from Interaction Learning](https://arxiv.org/abs/2509.18733)
*Yilin Gao,Kangyi Chen,Zhongxing Peng,Hengjie Lu,Shugong Xu*

Main category: cs.CV

TL;DR: Current visual foundation models (VFMs) face limitations in knowledge transfer from vision-language models (VLMs). This paper introduces a cognitive-inspired framework, Learning from Interactions (LFI), to address this gap, achieving notable improvements in diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Existing VFMs struggle to effectively transfer knowledge from VLMs due to a discrepancy in representational paradigms. The aim is to enable better generalization and knowledge transfer while preserving the effectiveness of VLMs' interaction processes.

Method: The LFI framework introduces two innovations: Interaction Queries that retain relational structures across layers and interaction-based supervision extracted from VLMs' cross-modal attention mechanisms.

Result: The approach delivers significant performance gains, including improvements in TinyImageNet classification (3.3mAP) and COCO detection/segmentation (1.6mAP/2.4AP). It also excels in cross-domain tasks with zero-shot improvements on PACS (2.4) and VLCS (9.3). Human evaluations validate its cognitive efficacy.

Conclusion: LFI demonstrates successful knowledge transfer from VLMs to VFMs, enhancing task performance, cognitive alignment, and semantic consistency, paving the way for more generalizable and efficient VFMs.

Abstract: Current visual foundation models (VFMs) face a fundamental limitation in
transferring knowledge from vision language models (VLMs), while VLMs excel at
modeling cross-modal interactions through unified representation spaces,
existing VFMs predominantly adopt result-oriented paradigms that neglect the
underlying interaction processes. This representational discrepancy hinders
effective knowledge transfer and limits generalization across diverse vision
tasks. We propose Learning from Interactions (LFI), a cognitive-inspired
framework that addresses this gap by explicitly modeling visual understanding
as an interactive process. Our key insight is that capturing the dynamic
interaction patterns encoded in pre-trained VLMs enables more faithful and
efficient knowledge transfer to VFMs. The approach centers on two technical
innovations, Interaction Queries, which maintain persistent relational
structures across network layers, and interaction-based supervision, derived
from the cross-modal attention mechanisms of VLMs. Comprehensive experiments
demonstrate consistent improvements across multiple benchmarks, achieving 3.3
and 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO
detection/segmentation respectively, with minimal parameter overhead and faster
convergence. The framework particularly excels in cross-domain settings,
delivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human
evaluations further confirm its cognitive alignment, outperforming
result-oriented methods by 2.7 times in semantic consistency metrics.

</details>


### [174] [HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection](https://arxiv.org/abs/2509.18738)
*Ruichao Hou,Xingyuan Li,Tongwei Ren,Dongming Zhou,Gangshan Wu,Jinde Cao*

Main category: cs.CV

TL;DR: This paper introduces a hybrid prompt-driven approach using the Segment Anything Model (SAM) for improving RGB-Thermal salient object detection (RGB-T SOD), addressing feature fusion challenges and data scarcity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in RGB-Thermal salient object detection, specifically the limitations in precise boundary learning and object completion caused by insufficient feature fusion and data scarcity.

Method: The paper presents HyPSAM, combining a dynamic fusion network (DFNet) that enhances cross-modality feature representation, and a plug-and-play refinement network (P2RNet) leveraging hybrid prompts (text, mask, box) to refine object localization by integrating SAM's zero-shot generalization capabilities.

Result: HyPSAM achieves state-of-the-art performance on three public datasets and enhances existing RGB-T SOD methods through significant performance gains.

Conclusion: The proposed method demonstrates the potential of hybrid prompting and prompt engineering in overcoming RGB-T SOD challenges, showing versatility and scalability in integrating with various methods.

Abstract: RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent
objects by integrating complementary information from RGB and thermal
modalities. However, learning the precise boundaries and complete objects
remains challenging due to the intrinsic insufficient feature fusion and the
extrinsic limitations of data scarcity. In this paper, we propose a novel
hybrid prompt-driven segment anything model (HyPSAM), which leverages the
zero-shot generalization capabilities of the segment anything model (SAM) for
RGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that
generates high-quality initial saliency maps as visual prompts. DFNet employs
dynamic convolution and multi-branch decoding to facilitate adaptive
cross-modality interaction, overcoming the limitations of fixed-parameter
kernels and enhancing multi-modal feature representation. Moreover, we propose
a plug-and-play refinement network (P2RNet), which serves as a general
optimization strategy to guide SAM in refining saliency maps by using hybrid
prompts. The text prompt ensures reliable modality input, while the mask and
box prompts enable precise salient object localization. Extensive experiments
on three public datasets demonstrate that our method achieves state-of-the-art
performance. Notably, HyPSAM has remarkable versatility, seamlessly integrating
with different RGB-T SOD methods to achieve significant performance gains,
thereby highlighting the potential of prompt engineering in this field. The
code and results of our method are available at:
https://github.com/milotic233/HyPSAM.

</details>


### [175] [TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing](https://arxiv.org/abs/2509.18743)
*Susmit Neogi*

Main category: cs.CV

TL;DR: The paper introduces TriFusion-AE, a robust multimodal cross-attention autoencoder, that fuses text, depth maps, and LiDAR point clouds to improve LiDAR-based perception under noise and adversarial conditions.


<details>
  <summary>Details</summary>
Motivation: To address vulnerabilities of raw LiDAR point clouds to noise, occlusion, and adversarial perturbations, and enhance robustness in real-world perception tasks.

Method: TriFusion-AE leverages multimodal cross-attention between text, depth maps, and LiDAR data, aligning semantic, geometric, and spatial cues for resilient autoencoder reconstruction.

Result: The model outperformed CNN-based autoencoders in reconstruction under strong adversarial attacks and heavy noise; tested on realistic low-data scenarios using the nuScenes-mini dataset.

Conclusion: TriFusion-AE is robust in challenging conditions and is model-agnostic, enabling integration with existing CNN-based point cloud autoencoders for improved joint representation learning.

Abstract: LiDAR-based perception is central to autonomous driving and robotics, yet raw
point clouds remain highly vulnerable to noise, occlusion, and adversarial
corruptions. Autoencoders offer a natural framework for denoising and
reconstruction, but their performance degrades under challenging real-world
conditions. In this work, we propose TriFusion-AE, a multimodal cross-attention
autoencoder that integrates textual priors, monocular depth maps from
multi-view images, and LiDAR point clouds to improve robustness. By aligning
semantic cues from text, geometric (depth) features from images, and spatial
structure from LiDAR, TriFusion-AE learns representations that are resilient to
stochastic noise and adversarial perturbations. Interestingly, while showing
limited gains under mild perturbations, our model achieves significantly more
robust reconstruction under strong adversarial attacks and heavy noise, where
CNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to
reflect realistic low-data deployment scenarios. Our multimodal fusion
framework is designed to be model-agnostic, enabling seamless integration with
any CNN-based point cloud autoencoder for joint representation learning.

</details>


### [176] [COLT: Enhancing Video Large Language Models with Continual Tool Usage](https://arxiv.org/abs/2509.18754)
*Yuyang Liu,Xinyuan Shi,Bang Yang,Peilin Zhou,Jiahua Dong,Long Chen,Ian Reid,Xiaondan Liang*

Main category: cs.CV

TL;DR: The paper introduces COLT, a method to enable open-source video LLMs to handle streaming and evolving tool data without forgetting previously learned tools.


<details>
  <summary>Details</summary>
Motivation: Existing methods for video understanding struggle to adapt to environments with continuously evolving tool data, limiting their real-world applicability.

Method: COLT uses a learnable tool codebook as a memory system to retain past tools. It dynamically selects relevant tools based on user instruction and a similarity search. The proposed VideoToolBench dataset is utilized for instruction tuning.

Result: COLT achieves state-of-the-art performance on video LLM benchmarks and the new VideoToolBench dataset.

Conclusion: COLT effectively enhances video LLMs, allowing them to adapt to continuously evolving tool streams while maintaining the knowledge of previously learned tools.

Abstract: The success of Large Language Models (LLMs) has significantly propelled the
research of video understanding. To harvest the benefits of well-trained expert
models (i.e., tools), video LLMs prioritize the exploration of tool usage
capabilities. Existing methods either prompt closed-source LLMs or employ the
instruction tuning paradigm for tool-use fine-tuning. These methods, however,
assume an established repository of fixed tools and struggle to generalize to
real-world environments where tool data is perpetually evolving and streaming
in. To this end, we propose to enhance open-source video LLMs with COntinuaL
Tool usage (termed COLT), which automatically acquires tool-use ability in a
successive tool stream without suffering 'catastrophic forgetting' of the past
learned tools. Specifically, our COLT incorporates a learnable tool codebook as
a tool-specific memory system. Then relevant tools are dynamically selected
based on the similarity between user instruction and tool features within the
codebook. To unleash the tool usage potential of video LLMs, we collect a
video-centric tool-use instruction tuning dataset VideoToolBench. Extensive
experiments on both previous video LLM benchmarks and the tool-use-specific
VideoToolBench dataset demonstrate the state-of-the-art performance of our
proposed COLT.

</details>


### [177] [FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation](https://arxiv.org/abs/2509.18759)
*Zhaorui Wang,Yi Gu,Deming Zhou,Renjing Xu*

Main category: cs.CV

TL;DR: FixingGS proposes a training-free method that leverages an existing diffusion model to enhance 3D Gaussian Splatting reconstructions from sparse viewpoints, overcoming visual artifacts and improving multi-view consistency.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the challenge of reconstructing 3D scenes from sparse viewpoints, which often leads to artifacts and poor visual information in the generated 3D representations.

Method: FixingGS introduces a training-free approach utilizing a distillation technique to enhance cross-view coherence in diffusion priors, supplemented by an adaptive progressive enhancement scheme for refining under-constrained areas.

Result: The method significantly improves visual quality and reconstruction performance in sparse-view scenarios, outperforming state-of-the-art methods.

Conclusion: FixingGS provides an effective and coherent solution for artifact removal and inpainting in sparse-view 3DGS reconstructions, demonstrating robust improvements without requiring additional training.

Abstract: Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in
3D reconstruction and novel view synthesis. However, reconstructing 3D scenes
from sparse viewpoints remains highly challenging due to insufficient visual
information, which results in noticeable artifacts persisting across the 3D
representation. To address this limitation, recent methods have resorted to
generative priors to remove artifacts and complete missing content in
under-constrained areas. Despite their effectiveness, these approaches struggle
to ensure multi-view consistency, resulting in blurred structures and
implausible details. In this work, we propose FixingGS, a training-free method
that fully exploits the capabilities of the existing diffusion model for
sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our
distillation approach, which delivers more accurate and cross-view coherent
diffusion priors, thereby enabling effective artifact removal and inpainting.
In addition, we propose an adaptive progressive enhancement scheme that further
refines reconstructions in under-constrained regions. Extensive experiments
demonstrate that FixingGS surpasses existing state-of-the-art methods with
superior visual quality and reconstruction performance. Our code will be
released publicly.

</details>


### [178] [Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models](https://arxiv.org/abs/2509.18763)
*Xijun Wang,Junyun Huang,Rayyan Abdalla,Chengyuan Zhang,Ruiqi Xian,Dinesh Manocha*

Main category: cs.CV

TL;DR: The paper introduces Bi-VLM, a novel quantization method to tackle computational inefficiencies in vision-language models (VLMs), achieving substantial improvements in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: The computational demands and memory requirements of vision-language models restrict their usability in hardware-constrained environments.

Method: Bi-VLM separates model weights non-uniformly based on Gaussian quantiles, using a saliency-aware hybrid quantization algorithm to optimize quantization by distinguishing between salient and unsalient weight subsets.

Result: Bi-VLM achieves up to 45% performance improvement over state-of-the-art benchmarks in visual question answering tasks, and identifies 90%-99% redundancy in image tokens, enabling further efficiency gains.

Conclusion: Bi-VLM enhances both computational efficiency and task performance in VLMs, providing a practical solution for deployment in constrained environments.

Abstract: We address the critical gap between the computational demands of
vision-language models and the possible ultra-low-bit weight precision
(bitwidth $\leq2$ bits) we can use for higher efficiency. Our work is motivated
by the substantial computational cost and memory requirements of VLMs, which
restrict their applicability in hardware-constrained environments. We propose
Bi-VLM, which separates model weights non-uniformly based on the Gaussian
quantiles. Our formulation groups the model weights into outlier (salient) and
multiple inlier (unsalient) subsets, ensuring that each subset contains a
proportion of weights corresponding to its quantile in the distribution. We
propose a saliency-aware hybrid quantization algorithm and use it to quantize
weights by imposing different constraints on the scaler and binary matrices
based on the saliency metric and compression objective. We have evaluated our
approach on different VLMs. For the language model part of the VLM, our Bi-VLM
outperforms the SOTA by 3%-47% on the visual question answering task in terms
of four different benchmarks and three different models. For the overall VLM,
our Bi-VLM outperforms the SOTA by 4%-45%. We also perform token pruning on the
quantized models and observe that there is redundancy of image tokens 90% - 99%
in the quantized models. This helps us to further prune the visual tokens to
improve efficiency.

</details>


### [179] [DiSSECT: Structuring Transfer-Ready Medical Image Representations through Discrete Self-Supervision](https://arxiv.org/abs/2509.18765)
*Azad Singh,Deepak Mishra*

Main category: cs.CV

TL;DR: This paper introduces DiSSECT, an SSL framework for medical imaging that enhances scalability, generalizability, and robustness of learned representations using multi-scale vector quantization.


<details>
  <summary>Details</summary>
Motivation: Current SSL methods for medical imaging struggle with scalability, reliance on domain-specific priors, tuning, and susceptibility to shortcut learning in certain modalities, calling for improved representation learning techniques.

Method: The authors employ multi-scale vector quantization to create a discrete representational bottleneck within the SSL pipeline, enabling the model to learn robust, structure-aware features while mitigating shortcut learning.

Result: DiSSECT demonstrates strong classification and segmentation performance across tasks and datasets with minimal fine-tuning, particularly excelling in low-label settings.

Conclusion: DiSSECT enhances the medical image representation learning pipeline, offering improved efficiency, label scarcity performance, and superior transfer across domains compared to state-of-the-art methods.

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for medical
image representation learning, particularly in settings with limited labeled
data. However, existing SSL methods often rely on complex architectures,
anatomy-specific priors, or heavily tuned augmentations, which limit their
scalability and generalizability. More critically, these models are prone to
shortcut learning, especially in modalities like chest X-rays, where anatomical
similarity is high and pathology is subtle. In this work, we introduce DiSSECT
-- Discrete Self-Supervision for Efficient Clinical Transferable
Representations, a framework that integrates multi-scale vector quantization
into the SSL pipeline to impose a discrete representational bottleneck. This
constrains the model to learn repeatable, structure-aware features while
suppressing view-specific or low-utility patterns, improving representation
transfer across tasks and domains. DiSSECT achieves strong performance on both
classification and segmentation tasks, requiring minimal or no fine-tuning, and
shows particularly high label efficiency in low-label regimes. We validate
DiSSECT across multiple public medical imaging datasets, demonstrating its
robustness and generalizability compared to existing state-of-the-art
approaches.

</details>


### [180] [Real-time Deer Detection and Warning in Connected Vehicles via Thermal Sensing and Deep Learning](https://arxiv.org/abs/2509.18779)
*Hemanth Puppala,Wayne Sarasua,Srinivas Biyaguda,Farhad Farzinpour,Mashrur Chowdhury*

Main category: cs.CV

TL;DR: The paper introduces a technological solution to mitigate deer-vehicle collisions using thermal imaging, deep learning, and CV2X communication, achieving high detection accuracy and fast response times.


<details>
  <summary>Details</summary>
Motivation: Deer-vehicle collisions cause significant safety, economic, and ecological challenges, necessitating advanced solutions for mitigating these incidents.

Method: The system integrates thermal imaging, deep learning algorithms trained on 12,000 thermal deer images, and CV2X communication for real-time detection and warnings.

Result: The system demonstrated 98.84% mean average precision, 95.44% precision, and 95.96% recall in experiments, achieving 88-92% accuracy under harsh conditions in field tests, far surpassing conventional cameras.

Conclusion: The research provides strong evidence that thermal imaging combined with connected vehicle technologies can reduce deer-vehicle collisions effectively, offering a reliable pathway for safety and ecological improvements.

Abstract: Deer-vehicle collisions represent a critical safety challenge in the United
States, causing nearly 2.1 million incidents annually and resulting in
approximately 440 fatalities, 59,000 injuries, and 10 billion USD in economic
damages. These collisions also contribute significantly to declining deer
populations. This paper presents a real-time detection and driver warning
system that integrates thermal imaging, deep learning, and
vehicle-to-everything communication to help mitigate deer-vehicle collisions.
Our system was trained and validated on a custom dataset of over 12,000 thermal
deer images collected in Mars Hill, North Carolina. Experimental evaluation
demonstrates exceptional performance with 98.84 percent mean average precision,
95.44 percent precision, and 95.96 percent recall. The system was field tested
during a follow-up visit to Mars Hill and readily sensed deer providing the
driver with advanced warning. Field testing validates robust operation across
diverse weather conditions, with thermal imaging maintaining between 88 and 92
percent detection accuracy in challenging scenarios where conventional visible
light based cameras achieve less than 60 percent effectiveness. When a high
probability threshold is reached sensor data sharing messages are broadcast to
surrounding vehicles and roadside units via cellular vehicle to everything
(CV2X) communication devices. Overall, our system achieves end to end latency
consistently under 100 milliseconds from detection to driver alert. This
research establishes a viable technological pathway for reducing deer-vehicle
collisions through thermal imaging and connected vehicles.

</details>


### [181] [Towards Application Aligned Synthetic Surgical Image Synthesis](https://arxiv.org/abs/2509.18796)
*Danush Kumar Venkatesh,Stefanie Speidel*

Main category: cs.CV

TL;DR: SAADi is a framework that uses diffusion models to improve annotations for surgical data, focusing on downstream tasks and addressing issues of data memorization and inconsistency.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of limited annotated surgical data, which hinders the progress of deep learning in medical imaging, and the issue of data memorization and sample inconsistency in diffusion models.

Method: SAADi generates pairs of preferred and non-preferred synthetic images, then fine-tunes diffusion models to explicitly align image generation with downstream tasks, improving data usability.

Result: Experiments on three surgical datasets show consistent performance improvements of 7-9% in classification and 2-10% in segmentation tasks, with significant gains for underrepresented classes and iterative refinement boosting results further.

Conclusion: By aligning diffusion models with downstream objectives, SAADi mitigates annotated data scarcity and enhances performance for surgical vision applications, making it a reliable framework for this domain.

Abstract: The scarcity of annotated surgical data poses a significant challenge for
developing deep learning systems in computer-assisted interventions. While
diffusion models can synthesize realistic images, they often suffer from data
memorization, resulting in inconsistent or non-diverse samples that may fail to
improve, or even harm, downstream performance. We introduce \emph{Surgical
Application-Aligned Diffusion} (SAADi), a new framework that aligns diffusion
models with samples preferred by downstream models. Our method constructs pairs
of \emph{preferred} and \emph{non-preferred} synthetic images and employs
lightweight fine-tuning of diffusion models to align the image generation
process with downstream objectives explicitly. Experiments on three surgical
datasets demonstrate consistent gains of $7$--$9\%$ in classification and
$2$--$10\%$ in segmentation tasks, with the considerable improvements observed
for underrepresented classes. Iterative refinement of synthetic samples further
boosts performance by $4$--$10\%$. Unlike baseline approaches, our method
overcomes sample degradation and establishes task-aware alignment as a key
principle for mitigating data scarcity and advancing surgical vision
applications.

</details>


### [182] [A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising](https://arxiv.org/abs/2509.18801)
*Kuang Xiaodong,Li Bingxuan,Li Yuan,Rao Fan,Ma Gege,Xie Qingguo,Mok Greta S P,Liu Huafeng,Zhu Wentao*

Main category: cs.CV

TL;DR: The paper introduces a deep learning-based method, Neural-KMDS-Net, for denoising dynamic PET images by utilizing inter-frame and intra-frame consistencies, improving image quality significantly over existing methods.


<details>
  <summary>Details</summary>
Motivation: High-quality temporal imaging in dynamic PET is limited by low statistics, particularly for short frames, making effective denoising methods necessary.

Method: A KMDS model leveraging inter-frame spatial correlations and intra-frame structural consistencies serves as a basis, replacing traditional parameter optimization with end-to-end neural networks to create Neural-KMDS-Net.

Result: Extensive experiments on both simulated and real datasets reveal that Neural-KMDS-Net outperforms baseline approaches in dynamic PET image denoising.

Conclusion: Neural-KMDS-Net provides an effective method for achieving high temporal and spatial resolution in dynamic PET imaging, with significant improvements over prior techniques.

Abstract: Achieving high image quality for temporal frames in dynamic positron emission
tomography (PET) is challenging due to the limited statistic especially for the
short frames. Recent studies have shown that deep learning (DL) is useful in a
wide range of medical image denoising tasks. In this paper, we propose a
model-based neural network for dynamic PET image denoising. The inter-frame
spatial correlation and intra-frame structural consistency in dynamic PET are
used to establish the kernel space-based multidimensional sparse (KMDS) model.
We then substitute the inherent forms of the parameter estimation with neural
networks to enable adaptive parameters optimization, forming the end-to-end
neural KMDS-Net. Extensive experimental results from simulated and real data
demonstrate that the neural KMDS-Net exhibits strong denoising performance for
dynamic PET, outperforming previous baseline methods. The proposed method may
be used to effectively achieve high temporal and spatial resolution for dynamic
PET. Our source code is available at
https://github.com/Kuangxd/Neural-KMDS-Net/tree/main.

</details>


### [183] [Surgical Video Understanding with Label Interpolation](https://arxiv.org/abs/2509.18802)
*Garam Kim,Tae Kyeong Jeong,Juyoun Park*

Main category: cs.CV

TL;DR: The paper proposes an optical flow-based framework to enhance surgical scene understanding by addressing the lack of sufficient pixel-level segmentation data in robot-assisted surgery. It combines interpolation with multi-task learning to enrich sparse annotations.


<details>
  <summary>Details</summary>
Motivation: The need to fully leverage visual data generated in robot-assisted surgery (RAS) to improve accuracy and efficiency, amidst challenges such as temporospatial annotation imbalance in current methodologies.

Method: Proposed an optical flow-based label interpolation method for propagating annotations from key frames to adjacent unlabeled frames, integrated with multi-task learning for balanced temporal-spatial information.

Result: Improved accuracy and efficiency in understanding complex surgical scenes, enabling better training outcomes for RAS systems.

Conclusion: The integration of optical flow-based label propagation with multi-task learning effectively enhances surgical scene understanding and promotes the utility of robot-assisted surgery.

Abstract: Robot-assisted surgery (RAS) has become a critical paradigm in modern
surgery, promoting patient recovery and reducing the burden on surgeons through
minimally invasive approaches. To fully realize its potential, however, a
precise understanding of the visual data generated during surgical procedures
is essential. Previous studies have predominantly focused on single-task
approaches, but real surgical scenes involve complex temporal dynamics and
diverse instrument interactions that limit comprehensive understanding.
Moreover, the effective application of multi-task learning (MTL) requires
sufficient pixel-level segmentation data, which are difficult to obtain due to
the high cost and expertise required for annotation. In particular, long-term
annotations such as phases and steps are available for every frame, whereas
short-term annotations such as surgical instrument segmentation and action
detection are provided only for key frames, resulting in a significant
temporal-spatial imbalance. To address these challenges, we propose a novel
framework that combines optical flow-based segmentation label interpolation
with multi-task learning. optical flow estimated from annotated key frames is
used to propagate labels to adjacent unlabeled frames, thereby enriching sparse
spatial supervision and balancing temporal and spatial information for
training. This integration improves both the accuracy and efficiency of
surgical scene understanding and, in turn, enhances the utility of RAS.

</details>


### [184] [Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation](https://arxiv.org/abs/2509.18824)
*Yanzuo Lu,Xin Xia,Manlin Zhang,Huafeng Kuang,Jianbin Zheng,Yuxi Ren,Xuefeng Xiao*

Main category: cs.CV

TL;DR: The paper proposes Hyper-Bagel, a framework that accelerates multimodal understanding and generation tasks significantly, achieving up to 22x speedup in computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational overhead in iterative processes like diffusion denoising and autoregressive decoding as multimodal token contexts become increasingly complex.

Method: Hyper-Bagel employs speculative decoding for next-token prediction and a multi-stage distillation process for diffusion denoising, combining advanced adversarial distillation with human feedback learning.

Result: Hyper-Bagel shows over a 2x speedup in multimodal understanding, 16.67x speedup in text-to-image generation, and 22x speedup in image editing while maintaining high-quality results.

Conclusion: The framework offers an ultimate balance of cost-effectiveness, responsiveness, and computational efficiency, making multimodal interactions seamless and enabling near real-time editing and generation.

Abstract: Unified multimodal models have recently attracted considerable attention for
their remarkable abilities in jointly understanding and generating diverse
content. However, as contexts integrate increasingly numerous interleaved
multimodal tokens, the iterative processes of diffusion denoising and
autoregressive decoding impose significant computational overhead. To address
this, we propose Hyper-Bagel, a unified acceleration framework designed to
simultaneously speed up both multimodal understanding and generation tasks. Our
approach uses a divide-and-conquer strategy, employing speculative decoding for
next-token prediction and a multi-stage distillation process for diffusion
denoising. The framework delivers substantial performance gains, achieving over
a 2x speedup in multimodal understanding. For generative tasks, our resulting
lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a
22x speedup in image editing, all while preserving the high-quality output of
the original model. We further develop a highly efficient 1-NFE model that
enables near real-time interactive editing and generation. By combining
advanced adversarial distillation with human feedback learning, this model
achieves ultimate cost-effectiveness and responsiveness, making complex
multimodal interactions seamless and instantaneous.

</details>


### [185] [Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions](https://arxiv.org/abs/2509.18847)
*Junhao Su,Yuanliang Wan,Junwei Yang,Hengyu Shi,Tianyang Han,Junfeng Luo,Yurui Qiu*

Main category: cs.CV

TL;DR: The paper introduces structured reflection to improve tool-augmented large language models' error diagnosis and repair, using optimized training and a benchmark to test reliability.


<details>
  <summary>Details</summary>
Motivation: Tool-augmented LLMs struggle with error diagnosis and repair in multi-turn interactions. Existing methods prompt models to 'think more' without targeted strategies, leading to repeated mistakes.

Method: Proposes structured reflection where the model explicitly diagnoses and corrects failures. Training combines DAPO and GSPO objectives with a tailored reward system. A new benchmark, Tool-Reflection-Bench, evaluates performance across tasks.

Result: Experiments on BFCL v3 and Tool-Reflection-Bench show increased multi-turn tool-call success, better error recovery, and reduced redundant calls.

Conclusion: Explicit reflection and targeted optimization significantly enhance tool interaction reliability and teach agents to learn from mistakes effectively.

Abstract: Tool-augmented large language models (LLMs) are usually trained with
supervised imitation or coarse-grained reinforcement learning that optimizes
single tool calls. Current self-reflection practices rely on heuristic prompts
or one-way reasoning: the model is urged to 'think more' instead of learning
error diagnosis and repair. This is fragile in multi-turn interactions; after a
failure the model often repeats the same mistake. We propose structured
reflection, which turns the path from error to repair into an explicit,
controllable, and trainable action. The agent produces a short yet precise
reflection: it diagnoses the failure using evidence from the previous step and
then proposes a correct, executable follow-up call. For training we combine
DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing
the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce
Tool-Reflection-Bench, a lightweight benchmark that programmatically checks
structural validity, executability, parameter correctness, and result
consistency. Tasks are built as mini trajectories of erroneous call,
reflection, and corrected call, with disjoint train and test splits.
Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn
tool-call success and error recovery, and a reduction of redundant calls. These
results indicate that making reflection explicit and optimizing it directly
improves the reliability of tool interaction and offers a reproducible path for
agents to learn from failure.

</details>


### [186] [Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography](https://arxiv.org/abs/2509.18839)
*Gianmarco Spinaci,Lukas Klic,Giovanni Colavizza*

Main category: cs.CV

TL;DR: This study evaluates general-purpose Multimodal LLMs and VLMs for classifying Christian Iconography against datasets and ResNet50 baselines.


<details>
  <summary>Details</summary>
Motivation: The study explores whether general-purpose models like GPT-4o and Gemini-2.5 can achieve comparable or better performance than traditional models for this domain.

Method: The authors benchmarked model performance on three datasets (ArtDL, ICONCLASS, Wikidata) using three settings: label classification, description-aided classification, and few-shot learning.

Result: Gemini-2.5 Pro and GPT-4o outperformed fine-tuned ResNet50 on most tasks, though performance dropped significantly on the Wikidata dataset. Class description enrichment helped, whereas few-shot learning showed limited improvement.

Conclusion: General-purpose multimodal LLMs can be effective for visually complex cultural heritage tasks, highlighting potential for future refinement in prompt optimization and expanded studies.

Abstract: This study evaluates the capabilities of Multimodal Large Language Models
(LLMs) and Vision Language Models (VLMs) in the task of single-label
classification of Christian Iconography. The goal was to assess whether
general-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5,
can interpret the Iconography, typically addressed by supervised classifiers,
and evaluate their performance. Two research questions guided the analysis:
(RQ1) How do multimodal LLMs perform on image classification of Christian
saints? And (RQ2), how does performance vary when enriching input with
contextual information or few-shot exemplars? We conducted a benchmarking study
using three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and
Wikidata, filtered to include the top 10 most frequent classes. Models were
tested under three conditions: (1) classification using class labels, (2)
classification with Iconclass descriptions, and (3) few-shot learning with five
exemplars. Results were compared against ResNet50 baselines fine-tuned on the
same datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed
the ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset,
where Siglip reached the highest accuracy score, suggesting model sensitivity
to image size and metadata alignment. Enriching prompts with class descriptions
generally improved zero-shot performance, while few-shot learning produced
lower results, with only occasional and minimal increments in accuracy. We
conclude that general-purpose multimodal LLMs are capable of classification in
visually complex cultural heritage domains. These results support the
application of LLMs as metadata curation tools in digital humanities workflows,
suggesting future research on prompt optimization and the expansion of the
study to other classification strategies and models.

</details>


### [187] [VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction](https://arxiv.org/abs/2509.19002)
*Hao Wang,Eiki Murata,Lingfang Zhang,Ayako Sato,So Fukuda,Ziqi Yin,Wentao Hu,Keisuke Nakao,Yusuke Nakamura,Sebastian Zwirner,Yi-Chia Chen,Hiroyuki Otomo,Hiroki Ouchi,Daisuke Kawahara*

Main category: cs.CV

TL;DR: VIR-Bench introduces a benchmark for evaluating multimodal large language models (MLLMs) on long-distance travel videos, highlighting their challenges in geospatial-temporal understanding.


<details>
  <summary>Details</summary>
Motivation: Current video benchmarks lack emphasis on extended geospatial-temporal trajectories, which are essential for real-world tasks like AI planning and navigation.

Method: The study involves creating VIR-Bench with 200 travel videos to assess MLLMs' abilities in itinerary reconstruction, along with a case study implementing a travel-planning agent based on this benchmark.

Result: Findings show that even state-of-the-art MLLMs struggle with these extended geospatial-temporal tasks, while the prototype agent demonstrates improved planning capabilities using VIR-Bench insights.

Conclusion: VIR-Bench effectively identifies weaknesses in current MLLMs and provides actionable insights to enhance user-facing tools, driving advancements in geospatial-temporal intelligence.

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly enhanced video understanding capabilities, opening new
possibilities for practical applications. Yet current video benchmarks focus
largely on indoor scenes or short-range outdoor activities, leaving the
challenges associated with long-distance travel largely unexplored. Mastering
extended geospatial-temporal trajectories is critical for next-generation
MLLMs, underpinning real-world tasks such as embodied-AI planning and
navigation. To bridge this gap, we present VIR-Bench, a novel benchmark
consisting of 200 travel videos that frames itinerary reconstruction as a
challenging task designed to evaluate and push forward MLLMs'
geospatial-temporal intelligence. Experimental results reveal that
state-of-the-art MLLMs, including proprietary ones, struggle to achieve high
scores, underscoring the difficulty of handling videos that span extended
spatial and temporal scales. Moreover, we conduct an in-depth case study in
which we develop a prototype travel-planning agent that leverages the insights
gained from VIR-Bench. The agent's markedly improved itinerary recommendations
verify that our evaluation protocol not only benchmarks models effectively but
also translates into concrete performance gains in user-facing applications.

</details>


### [188] [ViG-LRGC: Vision Graph Neural Networks with Learnable Reparameterized Graph Construction](https://arxiv.org/abs/2509.18840)
*Ismael Elsharkawi,Hossam Sharara,Ahmed Rafea*

Main category: cs.CV

TL;DR: The paper introduces a learnable reparameterized graph construction (LRGC) method for Vision Graph Neural Networks (ViG), addressing limitations in traditional non-learnable graph construction methods.


<details>
  <summary>Details</summary>
Motivation: Existing Vision Graph Neural Networks rely on non-parameterized, non-learnable statistical methods for node graph construction, which fail to optimally represent node relationships.

Method: The proposed LRGC employs a key-query attention mechanism between nodes with a soft-threshold reparameterization to enable differentiable, learnable edge selection without hyper-parameters.

Result: The LRGC-based ViG model outperforms state-of-the-art ViG methods of comparable sizes on the ImageNet-1k dataset.

Conclusion: The results demonstrate that LRGC effectively enables learnable, hyper-parameter-free graph construction, enhancing performance in Vision Graph Neural Networks.

Abstract: Image Representation Learning is an important problem in Computer Vision.
Traditionally, images were processed as grids, using Convolutional Neural
Networks or as a sequence of visual tokens, using Vision Transformers.
Recently, Vision Graph Neural Networks (ViG) have proposed the treatment of
images as a graph of nodes; which provides a more intuitive image
representation. The challenge is to construct a graph of nodes in each layer
that best represents the relations between nodes and does not need a
hyper-parameter search. ViG models in the literature depend on
non-parameterized and non-learnable statistical methods that operate on the
latent features of nodes to create a graph. This might not select the best
neighborhood for each node. Starting from k-NN graph construction to HyperGraph
Construction and Similarity-Thresholded graph construction, these methods lack
the ability to provide a learnable hyper-parameter-free graph construction
method. To overcome those challenges, we present the Learnable Reparameterized
Graph Construction (LRGC) for Vision Graph Neural Networks. LRGC applies
key-query attention between every pair of nodes; then uses soft-threshold
reparameterization for edge selection, which allows the use of a differentiable
mathematical model for training. Using learnable parameters to select the
neighborhood removes the bias that is induced by any clustering or thresholding
methods previously introduced in the literature. In addition, LRGC allows
tuning the threshold in each layer to the training data since the thresholds
are learnable through training and are not provided as hyper-parameters to the
model. We demonstrate that the proposed ViG-LRGC approach outperforms
state-of-the-art ViG models of similar sizes on the ImageNet-1k benchmark
dataset.

</details>


### [189] [ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?](https://arxiv.org/abs/2509.19070)
*Zijian Ling,Han Zhang,Yazhuo Zhou,Jiahao Cui*

Main category: cs.CV

TL;DR: The paper introduces ColorBlindnessEval, a benchmark for evaluating Vision-Language Models (VLMs) in challenging visual scenarios inspired by color blindness tests.


<details>
  <summary>Details</summary>
Motivation: To address the critical need for evaluating and improving the robustness of VLMs in visually adversarial and complex scenarios inspired by real-world applications.

Method: The paper develops a dataset containing 500 Ishihara-like images and evaluates 9 VLMs' ability to recognize embedded numerical information. Both Yes/No and open-ended prompt tests are conducted.

Result: Findings show that VLMs struggle to correctly interpret numbers in such adversarial contexts, demonstrating prevalent hallucinations and limitations.

Conclusion: This benchmark highlights the necessity of enhancing VLM robustness and serves as a tool for improving their accuracy in real-world, visually demanding tasks.

Abstract: This paper presents ColorBlindnessEval, a novel benchmark designed to
evaluate the robustness of Vision-Language Models (VLMs) in visually
adversarial scenarios inspired by the Ishihara color blindness test. Our
dataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with
varying color combinations, challenging VLMs to accurately recognize numerical
information embedded in complex visual patterns. We assess 9 VLMs using Yes/No
and open-ended prompts and compare their performance with human participants.
Our experiments reveal limitations in the models' ability to interpret numbers
in adversarial contexts, highlighting prevalent hallucination issues. These
findings underscore the need to improve the robustness of VLMs in complex
visual environments. ColorBlindnessEval serves as a valuable tool for
benchmarking and improving the reliability of VLMs in real-world applications
where accuracy is critical.

</details>


### [190] [Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning](https://arxiv.org/abs/2509.19090)
*Guoxin Wang,Jun Zhao,Xinyi Liu,Yanbo Liu,Xuyang Cao,Chao Li,Zhuoyun Liu,Qintian Sun,Fangru Zhou,Haoqiang Xing,Zhenhong Yang*

Main category: cs.CV

TL;DR: The paper presents Citrus-V, a unified multimodal model for medical imaging tasks, integrating image analysis and textual reasoning.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing medical imaging models that are narrowly focused and require multiple specialized networks, hindering generalization.

Method: It introduces Citrus-V, a model combining detection, segmentation, and multimodal chain-of-thought reasoning, trained with a novel multimodal approach and tested using a new curated data suite.

Result: Evaluations show that Citrus-V outperforms current medical imaging models and expert systems, excelling in lesion localization, report generation, and diagnostic inference.

Conclusion: Citrus-V offers a unified, reliable approach for medical imaging tasks, enhancing clinical decision-making with features like lesion quantification, automated reporting, and second opinion support.

Abstract: Medical imaging provides critical evidence for clinical diagnosis, treatment
planning, and surgical decisions, yet most existing imaging models are narrowly
focused and require multiple specialized networks, limiting their
generalization. Although large-scale language and multimodal models exhibit
strong reasoning and multi-task capabilities, real-world clinical applications
demand precise visual grounding, multimodal integration, and chain-of-thought
reasoning. We introduce Citrus-V, a multimodal medical foundation model that
combines image analysis with textual reasoning. The model integrates detection,
segmentation, and multimodal chain-of-thought reasoning, enabling pixel-level
lesion localization, structured report generation, and physician-like
diagnostic inference in a single framework. We propose a novel multimodal
training approach and release a curated open-source data suite covering
reasoning, detection, segmentation, and document understanding tasks.
Evaluations demonstrate that Citrus-V outperforms existing open-source medical
models and expert-level imaging systems across multiple benchmarks, delivering
a unified pipeline from visual grounding to clinical reasoning and supporting
precise lesion quantification, automated reporting, and reliable second
opinions.

</details>


### [191] [Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model](https://arxiv.org/abs/2509.18891)
*Xueyu Liu,Xiaoyi Zhang,Guangze Shi,Meilin Liu,Yexin Lai,Yongfei Wu,Mingqiang Wei*

Main category: cs.CV

TL;DR: This study introduces "Point Prompt Defender," an adversarial reinforcement learning method to optimize prompts for the Segment Anything Model (SAM), enhancing its segmentation outcomes.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for SAM rely on predefined or heuristic prompts, which limit scalability and generalization, necessitating a more adaptive solution.

Method: The framework uses adversarial reinforcement learning, where an attacker disrupts segmentation through poor prompts and a defender rectifies them. Both agents are trained with Deep Q-Networks within a graph-based prompt environment.

Result: The framework significantly improves SAM's segmentation robustness and generalization capabilities without requiring model retraining.

Conclusion: Point Prompt Defender provides a flexible and interpretable method to refine prompts, boosting SAM's performance across various tasks.

Abstract: Prompt quality plays a critical role in the performance of the Segment
Anything Model (SAM), yet existing approaches often rely on heuristic or
manually crafted prompts, limiting scalability and generalization. In this
paper, we propose Point Prompt Defender, an adversarial reinforcement learning
framework that adopts an attack-for-defense paradigm to automatically optimize
point prompts. We construct a task-agnostic point prompt environment by
representing image patches as nodes in a dual-space graph, where edges encode
both physical and semantic distances. Within this environment, an attacker
agent learns to activate a subset of prompts that maximally degrade SAM's
segmentation performance, while a defender agent learns to suppress these
disruptive prompts and restore accuracy. Both agents are trained using Deep
Q-Networks with a reward signal based on segmentation quality variation. During
inference, only the defender is deployed to refine arbitrary coarse prompt
sets, enabling enhanced SAM segmentation performance across diverse tasks
without retraining. Extensive experiments show that Point Prompt Defender
effectively improves SAM's robustness and generalization, establishing a
flexible, interpretable, and plug-and-play framework for prompt-based
segmentation.

</details>


### [192] [SmartWilds: Multimodal Wildlife Monitoring Dataset](https://arxiv.org/abs/2509.18894)
*Jenna Kline,Anirudh Potlapally,Bharath Pillai,Tanishka Wani,Rugved Katole,Vedant Patil,Penelope Covey,Hari Subramoni,Tanya Berger-Wolf,Christopher Stewart*

Main category: cs.CV

TL;DR: SmartWilds is a multimodal dataset for synchronized wildlife monitoring that combines drone imagery, camera trap media, and bioacoustic recordings for comprehensive study of species and habitat.


<details>
  <summary>Details</summary>
Motivation: There is a need for improved methodologies and datasets for comprehensive environmental monitoring, which are critical for endangered species research, conservation ecology, and effective habitat management.

Method: The SmartWilds dataset includes synchronized recordings from drones, camera traps, and bioacoustics collected over four days in a 220-acre habitat, featuring diverse species. A comparative analysis of the performance of these modalities was also performed.

Result: The dataset revealed complementary strengths of different sensor modalities for land use, species detection, and behavioral monitoring, paving the way for more integrative approaches in wildlife monitoring.

Conclusion: This work creates standardized, reproducible protocols for multimodal wildlife monitoring and offers an open dataset to advance AI research in conservation. Future updates will expand the dataset with GPS tracking, citizen science data, and seasonal coverage.

Abstract: We present the first release of SmartWilds, a multimodal wildlife monitoring
dataset. SmartWilds is a synchronized collection of drone imagery, camera trap
photographs and videos, and bioacoustic recordings collected during summer 2025
at The Wilds safari park in Ohio. This dataset supports multimodal AI research
for comprehensive environmental monitoring, addressing critical needs in
endangered species research, conservation ecology, and habitat management. Our
pilot deployment captured four days of synchronized monitoring across three
modalities in a 220-acre pasture containing Pere David's deer, Sichuan takin,
Przewalski's horses, as well as species native to Ohio, including bald eagles,
white-tailed deer, and coyotes. We provide a comparative analysis of sensor
modality performance, demonstrating complementary strengths for landuse
patterns, species detection, behavioral analysis, and habitat monitoring. This
work establishes reproducible protocols for multimodal wildlife monitoring
while contributing open datasets to advance conservation computer vision
research. Future releases will include synchronized GPS tracking data from
tagged individuals, citizen science data, and expanded temporal coverage across
multiple seasons.

</details>


### [193] [RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing](https://arxiv.org/abs/2509.18897)
*Jiayu Wang,Ruizhi Wang,Jie Song,Haofei Zhang,Mingli Song,Zunlei Feng,Li Sun*

Main category: cs.CV

TL;DR: The paper introduces RS3DBench, a benchmark dataset with paired remote sensing images and aligned depth maps for advancing 3D vision models in remote sensing.


<details>
  <summary>Details</summary>
Motivation: Existing remote sensing datasets lack comprehensive depth alignment with images, hindering the development of accurate 3D spatial understanding.

Method: The authors created RS3DBench, a dataset with 54,951 aligned image-depth pairs and textual descriptions, and developed a depth estimation model based on stable diffusion.

Result: The proposed model achieved state-of-the-art performance on the RS3DBench dataset, showcasing its effectiveness in 3D visual perception tasks.

Conclusion: RS3DBench and the introduced model aim to significantly advance geographic AI and 3D vision in remote sensing, with resources made publicly available.

Abstract: In this paper, we introduce a novel benchmark designed to propel the
advancement of general-purpose, large-scale 3D vision models for remote sensing
imagery. While several datasets have been proposed within the realm of remote
sensing, many existing collections either lack comprehensive depth information
or fail to establish precise alignment between depth data and remote sensing
images. To address this deficiency, we present a visual Benchmark for 3D
understanding of Remotely Sensed images, dubbed RS3DBench. This dataset
encompasses 54,951 pairs of remote sensing images and pixel-level aligned depth
maps, accompanied by corresponding textual descriptions, spanning a broad array
of geographical contexts. It serves as a tool for training and assessing 3D
visual perception models within remote sensing image spatial understanding
tasks. Furthermore, we introduce a remotely sensed depth estimation model
derived from stable diffusion, harnessing its multimodal fusion capabilities,
thereby delivering state-of-the-art performance on our dataset. Our endeavor
seeks to make a profound contribution to the evolution of 3D visual perception
models and the advancement of geographic artificial intelligence within the
remote sensing domain. The dataset, models and code will be accessed on the
https://rs3dbench.github.io.

</details>


### [194] [DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring](https://arxiv.org/abs/2509.18898)
*Pengteng Li,Yunfan Lu,Pinhao Song,Weiyu Guo,Huizai Yao,F. Richard Yu,Hui Xiong*

Main category: cs.CV

TL;DR: The paper introduces DeblurSplat, a Structure-from-Motion-free deblurring technique leveraging a dense stereo module and event cameras for efficient 3D Gaussian Splatting.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenges of motion deblurring in 3D scene reconstruction, minimizing cumulative errors and improving detail recovery in dynamic setups.

Method: DeblurSplat utilizes a dense stereo module for accurate initial point cloud generation and incorporates event stream data for enhanced supervision in decoding sharp images.

Result: Extensive tests show DeblurSplat delivers high-quality novel views and significantly improved rendering efficiency compared to state-of-the-art methods.

Conclusion: DeblurSplat effectively achieves motion deblurring and high-efficiency rendering without relying on camera pose calculations, marking a step forward in 3D-GS techniques.

Abstract: In this paper, we propose the first Structure-from-Motion (SfM)-free
deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.
We address the motion-deblurring problem in two ways. First, we leverage the
pretrained capability of the dense stereo module (DUSt3R) to directly obtain
accurate initial point clouds from blurred images. Without calculating camera
poses as an intermediate result, we avoid the cumulative errors transfer from
inaccurate camera poses to the initial point clouds' positions. Second, we
introduce the event stream into the deblur pipeline for its high sensitivity to
dynamic change. By decoding the latent sharp images from the event stream and
blurred images, we can provide a fine-grained supervision signal for scene
reconstruction optimization. Extensive experiments across a range of scenes
demonstrate that DeblurSplat not only excels in generating high-fidelity novel
views but also achieves significant rendering efficiency compared to the SOTAs
in deblur 3D-GS.

</details>


### [195] [MoiréNet: A Compact Dual-Domain Network for Image Demoiréing](https://arxiv.org/abs/2509.18910)
*Shuwei Guo,Simin Luan,Yan Ke,Zeyd Boukhers,John See,Cong Yang*

Main category: cs.CV

TL;DR: The paper introduces MoiréNet, a U-Net-based neural network to address moiré artifact removal by incorporating both frequency and spatial domain features.


<details>
  <summary>Details</summary>
Motivation: Moiré patterns are challenging to remove from images due to their anisotropic and multi-scale nature, requiring more effective solutions in imaging applications.

Method: MoiréNet combines a Directional Frequency-Spatial Encoder (DFSE) and a Frequency-Spatial Adaptive Selector (FSAS) to analyze and suppress moiré patterns in images using both frequency and spatial domains.

Result: MoiréNet achieves state-of-the-art demoiréing performance on multiple datasets with only 5.513M parameters, showing a 48% parameter reduction compared to ESDNet-L.

Conclusion: MoiréNet is computationally efficient and capable of providing high-quality image restoration, making it suitable for resource-constrained applications such as smartphone imaging and augmented reality.

Abstract: Moir\'e patterns arise from spectral aliasing between display pixel lattices
and camera sensor grids, manifesting as anisotropic, multi-scale artifacts that
pose significant challenges for digital image demoir\'eing. We propose
Moir\'eNet, a convolutional neural U-Net-based framework that synergistically
integrates frequency and spatial domain features for effective artifact
removal. Moir\'eNet introduces two key components: a Directional
Frequency-Spatial Encoder (DFSE) that discerns moir\'e orientation via
directional difference convolution, and a Frequency-Spatial Adaptive Selector
(FSAS) that enables precise, feature-adaptive suppression. Extensive
experiments demonstrate that Moir\'eNet achieves state-of-the-art performance
on public and actively used datasets while being highly parameter-efficient.
With only 5.513M parameters, representing a 48% reduction compared to ESDNet-L,
Moir\'eNet combines superior restoration quality with parameter efficiency,
making it well-suited for resource-constrained applications including
smartphone photography, industrial imaging, and augmented reality.

</details>


### [196] [Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual Segmentation](https://arxiv.org/abs/2509.18912)
*Yunzhe Shen,Kai Peng,Leiye Liu,Wei Ji,Jingjing Li,Miao Zhang,Yongri Piao,Huchuan Lu*

Main category: cs.CV

TL;DR: The paper introduces a Frequency-Aware Audio-Visual Segmentation (FAVS) framework to address audio-visual segmentation challenges by considering differences in frequency-domain characteristics between audio and visual signals.


<details>
  <summary>Details</summary>
Motivation: To improve audio-visual segmentation methods by tackling the overlooked frequency-domain contradictions between audio and visual modalities, which limit segmentation performance.

Method: The approach includes the FAVS framework with two modules: the Frequency-Domain Enhanced Decomposer (FDED) for iterative frequency-based discrimination and the Synergistic Cross-Modal Consistency (SCMC) module for dynamic expert routing to maintain feature consistency.

Result: The proposed FAVS framework achieves state-of-the-art performance on three benchmark datasets, with extensive qualitative visualizations evidencing its effectiveness.

Conclusion: Frequency-domain considerations significantly enhance audio-visual segmentation. The FAVS framework effectively accounts for modality-specific semantics and structural features, leading to superior performance.

Abstract: Audio-visual segmentation (AVS) plays a critical role in multimodal machine
learning by effectively integrating audio and visual cues to precisely segment
objects or regions within visual scenes. Recent AVS methods have demonstrated
significant improvements. However, they overlook the inherent frequency-domain
contradictions between audio and visual modalities--the pervasively interfering
noise in audio high-frequency signals vs. the structurally rich details in
visual high-frequency signals. Ignoring these differences can result in
suboptimal performance. In this paper, we rethink the AVS task from a deeper
perspective by reformulating AVS task as a frequency-domain decomposition and
recomposition problem. To this end, we introduce a novel Frequency-Aware
Audio-Visual Segmentation (FAVS) framework consisting of two key modules:
Frequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal
Consistency (SCMC) module. FDED module employs a residual-based iterative
frequency decomposition to discriminate modality-specific semantics and
structural features, and SCMC module leverages a mixture-of-experts
architecture to reinforce semantic consistency and modality-specific feature
preservation through dynamic expert routing. Extensive experiments demonstrate
that our FAVS framework achieves state-of-the-art performance on three
benchmark datasets, and abundant qualitative visualizations further verify the
effectiveness of the proposed FDED and SCMC modules. The code will be released
as open source upon acceptance of the paper.

</details>


### [197] [xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision](https://arxiv.org/abs/2509.18913)
*Nguyen Van Tu,Pham Nguyen Hai Long,Vo Hoai Viet*

Main category: cs.CV

TL;DR: This paper surveys four explainable AI methods for visual perception to improve interpretability in deep learning models.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of understanding deep learning's 'black-box' models to enhance reliability in critical applications.

Method: It surveys four approaches in xAI: Saliency Maps, Concept Bottleneck Models, Prototype-based methods, and Hybrid approaches, analyzing mechanisms and evaluation metrics.

Result: The analysis provides strengths, limitations, and evaluation metrics of each approach.

Conclusion: The paper offers insights and an overview that can guide future research and practical applications in explainable AI for image analysis.

Abstract: Deep learning has become the de facto standard and dominant paradigm in image
analysis tasks, achieving state-of-the-art performance. However, this approach
often results in "black-box" models, whose decision-making processes are
difficult to interpret, raising concerns about reliability in critical
applications. To address this challenge and provide human a method to
understand how AI model process and make decision, the field of xAI has
emerged. This paper surveys four representative approaches in xAI for visual
perception tasks: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM),
(iii) Prototype-based methods, and (iv) Hybrid approaches. We analyze their
underlying mechanisms, strengths and limitations, as well as evaluation
metrics, thereby providing a comprehensive overview to guide future research
and applications.

</details>


### [198] [LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2509.18917)
*Amirhesam Aghanouri,Cristina Olaverri-Monreal*

Main category: cs.CV

TL;DR: This paper develops an enhanced denoising diffusion probabilistic model (DDPM) to generate high-quality synthetic LiDAR data to improve object detection and navigation in autonomous vehicles.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges caused by noisy, sparse, and time-consuming real-world LiDAR data collection that affects autonomous vehicles' perception and functionality.

Method: The authors introduce an improved DDPM with novel noise scheduling and time-step embedding techniques to generate realistic point clouds for data augmentation, assessed on IAMCV and KITTI-360 datasets.

Result: The enhanced DDPM achieved superior performance compared to state-of-the-art methods, producing diverse and detailed synthetic point clouds while mitigating noise and sparsity in LiDAR data.

Conclusion: The study demonstrated the effectiveness of the proposed approach in improving the performance of computer vision tasks in autonomous vehicles, highlighting its promise for enhancing 3D vision systems.

Abstract: Autonomous vehicles (AVs) are expected to revolutionize transportation by
improving efficiency and safety. Their success relies on 3D vision systems that
effectively sense the environment and detect traffic agents. Among sensors AVs
use to create a comprehensive view of surroundings, LiDAR provides
high-resolution depth data enabling accurate object detection, safe navigation,
and collision avoidance. However, collecting real-world LiDAR data is
time-consuming and often affected by noise and sparsity due to adverse weather
or sensor limitations. This work applies a denoising diffusion probabilistic
model (DDPM), enhanced with novel noise scheduling and time-step embedding
techniques to generate high-quality synthetic data for augmentation, thereby
improving performance across a range of computer vision tasks, particularly in
AV perception. These modifications impact the denoising process and the model's
temporal awareness, allowing it to produce more realistic point clouds based on
the projection. The proposed method was extensively evaluated under various
configurations using the IAMCV and KITTI-360 datasets, with four performance
metrics compared against state-of-the-art (SOTA) methods. The results
demonstrate the model's superior performance over most existing baselines and
its effectiveness in mitigating the effects of noisy and sparse LiDAR data,
producing diverse point clouds with rich spatial relationships and structural
detail.

</details>


### [199] [Advancing Metallic Surface Defect Detection via Anomaly-Guided Pretraining on a Large Industrial Dataset](https://arxiv.org/abs/2509.18919)
*Chuni Liu,Hongjie Li,Jiaqi Du,Yangyang Hou,Qian Sun,Lei Jin,Ke Xu*

Main category: cs.CV

TL;DR: The paper introduces Anomaly-Guided Self-Supervised Pretraining (AGSSP), a new approach for metallic surface defect detection that leverages anomaly priors to overcome domain gaps and ineffective pretraining. It achieves notable improvements in detection accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the data scarcity issue in metallic surface defect detection and overcome limitations of pretraining on natural image datasets (e.g., domain gaps) and existing self-supervised learning objectives that fail to capture subtle defect patterns.

Method: The proposed AGSSP consists of a two-stage framework: (1) Pretraining the model's backbone using anomaly maps to extract defect-focused representations, and (2) Pretraining the detector using pseudo-defect boxes from anomaly maps for better localization. Additionally, it introduces a knowledge-enhanced method for anomaly map generation, a large-scale industrial dataset (120,000 images), and two pixel-level labeled defect datasets.

Result: AGSSP demonstrates improved performance, with up to a 10% increase in mAP@0.5 and an 11.4% increase in mAP@0.5:0.95 compared to standard ImageNet pretrained models across multiple experimental settings.

Conclusion: AGSSP effectively addresses the limitations of traditional pretraining strategies by leveraging anomaly-guided priors, improving representation learning and localization in metallic surface defect detection. Its availability facilitates further research and practical applications.

Abstract: The pretraining-finetuning paradigm is a crucial strategy in metallic surface
defect detection for mitigating the challenges posed by data scarcity. However,
its implementation presents a critical dilemma. Pretraining on natural image
datasets such as ImageNet, faces a significant domain gap. Meanwhile, naive
self-supervised pretraining on in-domain industrial data is often ineffective
due to the inability of existing learning objectives to distinguish subtle
defect patterns from complex background noise and textures. To resolve this, we
introduce Anomaly-Guided Self-Supervised Pretraining (AGSSP), a novel paradigm
that explicitly guides representation learning through anomaly priors. AGSSP
employs a two-stage framework: (1) it first pretrains the model's backbone by
distilling knowledge from anomaly maps, encouraging the network to capture
defect-salient features; (2) it then pretrains the detector using pseudo-defect
boxes derived from these maps, aligning it with localization tasks. To enable
this, we develop a knowledge-enhanced method to generate high-quality anomaly
maps and collect a large-scale industrial dataset of 120,000 images.
Additionally, we present two small-scale, pixel-level labeled metallic surface
defect datasets for validation. Extensive experiments demonstrate that AGSSP
consistently enhances performance across various settings, achieving up to a
10\% improvement in mAP@0.5 and 11.4\% in mAP@0.5:0.95 compared to
ImageNet-based models. All code, pretrained models, and datasets are publicly
available at https://clovermini.github.io/AGSSP-Dev/.

</details>


### [200] [Audio-Driven Universal Gaussian Head Avatars](https://arxiv.org/abs/2509.18924)
*Kartik Teotia,Helge Rhodin,Mohit Mendiratta,Hyeongwoo Kim,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: The paper proposes an audio-driven universal photorealistic avatar synthesis method using their novel Universal Head Avatar Prior (UHAP) to achieve high-fidelity results.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of creating universal, photorealistic avatars that combine audio-driven geometric and appearance variations.

Method: A universal speech model maps raw audio directly into a UHAP latent expression space. UHAP is trained on cross-identity multi-view videos, supervised by neutral scan data, and fine-tuned using a monocular encoder for efficient personalization.

Result: The proposed method generates realistic avatars with precise lip-sync, nuanced expressions, and dynamic appearance details. It outperforms competing methods in lip-sync accuracy, image quality, and perceptual realism.

Conclusion: This method is the first generalizable audio-driven avatar model capable of detailed appearance modeling and superior performance across various metrics.

Abstract: We introduce the first method for audio-driven universal photorealistic
avatar synthesis, combining a person-agnostic speech model with our novel
Universal Head Avatar Prior (UHAP). UHAP is trained on cross-identity
multi-view videos. In particular, our UHAP is supervised with neutral scan
data, enabling it to capture the identity-specific details at high fidelity. In
contrast to previous approaches, which predominantly map audio features to
geometric deformations only while ignoring audio-dependent appearance
variations, our universal speech model directly maps raw audio inputs into the
UHAP latent expression space. This expression space inherently encodes, both,
geometric and appearance variations. For efficient personalization to new
subjects, we employ a monocular encoder, which enables lightweight regression
of dynamic expression variations across video frames. By accounting for these
expression-dependent changes, it enables the subsequent model fine-tuning stage
to focus exclusively on capturing the subject's global appearance and geometry.
Decoding these audio-driven expression codes via UHAP generates highly
realistic avatars with precise lip synchronization and nuanced expressive
details, such as eyebrow movement, gaze shifts, and realistic mouth interior
appearance as well as motion. Extensive evaluations demonstrate that our method
is not only the first generalizable audio-driven avatar model that can account
for detailed appearance modeling and rendering, but it also outperforms
competing (geometry-only) methods across metrics measuring lip-sync accuracy,
quantitative image quality, and perceptual realism.

</details>


### [201] [SynapFlow: A Modular Framework Towards Large-Scale Analysis of Dendritic Spines](https://arxiv.org/abs/2509.18926)
*Pamela Osuna-Vargas,Altug Kamacioglu,Dominik F. Aschauer,Petros E. Vlachos,Sercan Alipek,Jochen Triesch,Simon Rumpel,Matthias Kaschube*

Main category: cs.CV

TL;DR: The paper introduces a machine-learning pipeline for efficient analysis of dendritic spine dynamics in microscopy data, targeting their detection, tracking, and feature extraction across time.


<details>
  <summary>Details</summary>
Motivation: Analyzing dendritic spine dynamics is pivotal for understanding synaptic efficacy related to learning and memory; however, existing methods require labor-intensive and inefficient processes for large-scale data analysis.

Method: The approach includes a modular pipeline with a transformer-based detection module, a depth-tracking system integrating spatial features, a time-tracking module leveraging spatial consistency, and a feature extraction unit for quantifying spine properties.

Result: The pipeline was validated using open-source labeled data and newly-published datasets for spine detection, depth-tracking, and time-tracking (the latter being the first of its kind).

Conclusion: This work contributes a scalable solution for dendritic spine analysis in neuroscience, encouraging future research through the release of open data, code, and pre-trained models.

Abstract: Dendritic spines are key structural components of excitatory synapses in the
brain. Given the size of dendritic spines provides a proxy for synaptic
efficacy, their detection and tracking across time is important for studies of
the neural basis of learning and memory. Despite their relevance, large-scale
analyses of the structural dynamics of dendritic spines in 3D+time microscopy
data remain challenging and labor-intense. Here, we present a modular machine
learning-based pipeline designed to automate the detection, time-tracking, and
feature extraction of dendritic spines in volumes chronically recorded with
two-photon microscopy. Our approach tackles the challenges posed by biological
data by combining a transformer-based detection module, a depth-tracking
component that integrates spatial features, a time-tracking module to associate
3D spines across time by leveraging spatial consistency, and a feature
extraction unit that quantifies biologically relevant spine properties. We
validate our method on open-source labeled spine data, and on two complementary
annotated datasets that we publish alongside this work: one for detection and
depth-tracking, and one for time-tracking, which, to the best of our knowledge,
is the first data of this kind. To encourage future research, we release our
data, code, and pre-trained weights at
https://github.com/pamelaosuna/SynapFlow, establishing a baseline for scalable,
end-to-end analysis of dendritic spine dynamics.

</details>


### [202] [No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning](https://arxiv.org/abs/2509.18938)
*Matheus Vinícius Todescato,Joel Luís Carbonera*

Main category: cs.CV

TL;DR: The paper introduces a novel zero-shot image classification framework leveraging a vision-language model (VLM) with a pre-trained visual model in a self-learning cycle, eliminating the need for labeled data.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current deep learning models that require extensive annotated datasets, which pose challenges in scenarios with limited data.

Method: The authors propose a confidence-based pseudo-labeling strategy combined with a self-learning cycle. A VLM identifies high-confidence samples, while a pre-trained visual model enhances visual features to iteratively train a lightweight classifier without supervision.

Result: The method was experimentally validated on ten diverse datasets, showing superior performance compared to the baseline zero-shot method.

Conclusion: The proposed framework enables dynamic adaptation in zero-shot classification without fine-tuning VLMs or using large language models, effectively leveraging semantic and visual information.

Abstract: While deep learning, including Convolutional Neural Networks (CNNs) and
Vision Transformers (ViTs), has significantly advanced classification
performance, its typical reliance on extensive annotated datasets presents a
major obstacle in many practical scenarios where such data is scarce.
Vision-language models (VLMs) and transfer learning with pre-trained visual
models appear as promising techniques to deal with this problem. This paper
proposes a novel zero-shot image classification framework that combines a VLM
and a pre-trained visual model within a self-learning cycle. Requiring only the
set of class names and no labeled training data, our method utilizes a
confidence-based pseudo-labeling strategy to train a lightweight classifier
directly on the test data, enabling dynamic adaptation. The VLM identifies
high-confidence samples, and the pre-trained visual model enhances their visual
representations. These enhanced features then iteratively train the classifier,
allowing the system to capture complementary semantic and visual cues without
supervision. Notably, our approach avoids VLM fine-tuning and the use of large
language models, relying on the visual-only model to reduce the dependence on
semantic representation. Experimental evaluations on ten diverse datasets
demonstrate that our approach outperforms the baseline zero-shot method.

</details>


### [203] [Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting](https://arxiv.org/abs/2509.18956)
*Zijing Guo,Yunyang Zhao,Lin Wang*

Main category: cs.CV

TL;DR: The paper introduces MirrorScene3D, a dataset for 3D reconstruction in mirror-rich environments, and ReflectiveGS, a method leveraging mirror reflections, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Standard 3D reconstruction techniques struggle in mirror-containing environments due to distortions and inconsistencies introduced by mirror reflections.

Method: The authors developed MirrorScene3D, a dataset including diverse indoor scenes with annotated mirror masks, and ReflectiveGS, an extension of 3D Gaussian Splatting that uses mirror reflections as complementary viewpoints.

Result: ReflectiveGS outperformed existing methods in SSIM, PSNR, LPIPS metrics, and showed faster training speed when evaluated on the MirrorScene3D dataset.

Conclusion: Utilizing mirror reflections as complementary perspectives improves 3D reconstruction quality, offering a new direction for handling reflective environments.

Abstract: Mirror-containing environments pose unique challenges for 3D reconstruction
and novel view synthesis (NVS), as reflective surfaces introduce view-dependent
distortions and inconsistencies. While cutting-edge methods such as Neural
Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical
scenes, their performance deteriorates in the presence of mirrors. Existing
solutions mainly focus on handling mirror surfaces through symmetry mapping but
often overlook the rich information carried by mirror reflections. These
reflections offer complementary perspectives that can fill in absent details
and significantly enhance reconstruction quality. To advance 3D reconstruction
in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset
featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror
masks, providing a benchmark for evaluating reconstruction methods in
reflective settings. Building on this, we propose ReflectiveGS, an extension of
3D Gaussian Splatting that utilizes mirror reflections as complementary
viewpoints rather than simple symmetry artifacts, enhancing scene geometry and
recovering absent details. Experiments on MirrorScene3D show that
ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and
training speed, setting a new benchmark for 3D reconstruction in mirror-rich
environments.

</details>


### [204] [Generative data augmentation for biliary tract detection on intraoperative images](https://arxiv.org/abs/2509.18958)
*Cristina Iacono,Mariarosaria Meola,Federica Conte,Laura Mecozzi,Umberto Bracale,Pietro Falco,Fanny Ficuciello*

Main category: cs.CV

TL;DR: The paper focuses on using deep-learning techniques, specifically the Yolo detection algorithm and GANs, to enhance bile duct visualization during cholecystectomy, reducing the risk of bile duct injury.


<details>
  <summary>Details</summary>
Motivation: Bile duct injuries during laparoscopic cholecystectomy significantly affect quality of life and survival, emphasizing the need for improved intraoperative bile duct visualization.

Method: The paper utilizes a Yolo detection algorithm trained on an annotated image database. It also employs Generative Adversarial Networks (GANs) to augment the training dataset with synthetic images, alongside classical data augmentation.

Result: Experimental validation demonstrates the potential of using the proposed approach in localizing the biliary tract from white-light surgical images.

Conclusion: The proposed strategy combining Yolo, GANs, and data augmentation is promising for minimizing surgical risks by enhancing intraoperative visualization of critical structures like the bile duct.

Abstract: Cholecystectomy is one of the most frequently performed procedures in
gastrointestinal surgery, and the laparoscopic approach is the gold standard
for symptomatic cholecystolithiasis and acute cholecystitis. In addition to the
advantages of a significantly faster recovery and better cosmetic results, the
laparoscopic approach bears a higher risk of bile duct injury, which has a
significant impact on quality of life and survival. To avoid bile duct injury,
it is essential to improve the intraoperative visualization of the bile duct.
This work aims to address this problem by leveraging a deep-learning approach
for the localization of the biliary tract from white-light images acquired
during the surgical procedures. To this end, the construction and annotation of
an image database to train the Yolo detection algorithm has been employed.
Besides classical data augmentation techniques, the paper proposes Generative
Adversarial Network (GAN) for the generation of a synthetic portion of the
training dataset. Experimental results have been discussed along with ethical
considerations.

</details>


### [205] [Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images](https://arxiv.org/abs/2509.18973)
*Jiabao Chen,Shan Xiong,Jialin Peng*

Main category: cs.CV

TL;DR: Prompt-DAS is a multitask framework enabling flexible domain adaptive segmentation of organelles in EM images using varying levels of prompt data.


<details>
  <summary>Details</summary>
Motivation: Efficient annotation for large-scale electron microscopy organelle segmentation requires adaptable methods capable of working with varying levels of supervisory cues.

Method: The proposed framework, Prompt-DAS, uses a prompt-based multitask approach, incorporating auxiliary center-point detection and innovative prompt-guided contrastive learning for better feature discrimination.

Result: Prompt-DAS outperforms existing unsupervised domain adaptation (UDA), weakly supervised domain adaptation (WDA), and SAM-based approaches in comprehensive benchmarks.

Conclusion: Prompt-DAS is a flexible, annotation-efficient framework that supports a wide range of segmentation scenarios, improving domain adaptation and segmentation capabilities over traditional methods.

Abstract: Domain adaptive segmentation (DAS) of numerous organelle instances from
large-scale electron microscopy (EM) is a promising way to enable
annotation-efficient learning. Inspired by SAM, we propose a promptable
multitask framework, namely Prompt-DAS, which is flexible enough to utilize any
number of point prompts during the adaptation training stage and testing stage.
Thus, with varying prompt configurations, Prompt-DAS can perform unsupervised
domain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well
as interactive segmentation during testing. Unlike the foundation model SAM,
which necessitates a prompt for each individual object instance, Prompt-DAS is
only trained on a small dataset and can utilize full points on all instances,
sparse points on partial instances, or even no points at all, facilitated by
the incorporation of an auxiliary center-point detection task. Moreover, a
novel prompt-guided contrastive learning is proposed to enhance discriminative
feature learning. Comprehensive experiments conducted on challenging benchmarks
demonstrate the effectiveness of the proposed approach over existing UDA, WDA,
and SAM-based approaches.

</details>


### [206] [Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards](https://arxiv.org/abs/2509.19003)
*Honghao Chen,Xingzhou Lou,Xiaokun Feng,Kaiqi Huang,Xinlong Wang*

Main category: cs.CV

TL;DR: The paper proposes a novel framework to enable fine-grained chain of thought reasoning in vision-language models by focusing on step-level reasoning and employing reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of adapting chain of thought reasoning to vision-language tasks, especially in achieving fine-grained structured reasoning and evaluating intermediate reasoning steps.

Method: The researchers introduce a framework that includes step-level reasoning data, a process reward model (PRM), and reinforcement learning training, enabling quality assessment of each reasoning step.

Result: The proposed methods lead to consistent improvements on challenging vision-language benchmarks and establish strong baselines.

Conclusion: The paper provides a foundation for incorporating fine-grained reasoning in vision-language models and introduces tools to advance multimodal reasoning research.

Abstract: Chain of thought reasoning has demonstrated remarkable success in large
language models, yet its adaptation to vision-language reasoning remains an
open challenge with unclear best practices. Existing attempts typically employ
reasoning chains at a coarse-grained level, which struggles to perform
fine-grained structured reasoning and, more importantly, are difficult to
evaluate the reward and quality of intermediate reasoning. In this work, we
delve into chain of step reasoning for vision-language models, enabling
assessing reasoning step quality accurately and leading to effective
reinforcement learning and inference-time scaling with fine-grained rewards. We
present a simple, effective, and fully transparent framework, including the
step-level reasoning data, process reward model (PRM), and reinforcement
learning training. With the proposed approaches, our models set strong
baselines with consistent improvements on challenging vision-language
benchmarks. More importantly, we conduct a thorough empirical analysis and
ablation study, unveiling the impact of each component and several intriguing
properties of inference-time scaling. We believe this paper serves as a
baseline for vision-language models and offers insights into more complex
multimodal reasoning. Our dataset, PRM, and code will be available at
https://github.com/baaivision/CoS.

</details>


### [207] [Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model](https://arxiv.org/abs/2509.19028)
*Ioannis Sarafis,Alexandros Papadopoulos,Anastasios Delopoulos*

Main category: cs.CV

TL;DR: The paper proposes a weakly supervised food image segmentation method using SAM's zero-shot and prompt capabilities with CAMs from a ViT model.


<details>
  <summary>Details</summary>
Motivation: To develop a food image segmentation approach that avoids reliance on pixel-level annotations, making annotation tasks easier in food-related applications.

Method: Combines class activation maps from Swin Transformers to generate prompts for Segment Anything Model. Evaluation was conducted using FoodSeg103 dataset.

Result: Achieved an average of 2.4 masks per image and an mIoU of 0.54 in a multi-mask generation scenario.

Conclusion: The method demonstrates potential in speeding up food image annotations and aiding in nutrition tracking applications.

Abstract: In this paper, we propose a weakly supervised semantic segmentation approach
for food images which takes advantage of the zero-shot capabilities and
promptability of the Segment Anything Model (SAM) along with the attention
mechanisms of Vision Transformers (ViTs). Specifically, we use class activation
maps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable
for food image segmentation. The ViT model, a Swin Transformer, is trained
exclusively using image-level annotations, eliminating the need for pixel-level
annotations during training. Additionally, to enhance the quality of the
SAM-generated masks, we examine the use of image preprocessing techniques in
combination with single-mask and multi-mask SAM generation strategies. The
methodology is evaluated on the FoodSeg103 dataset, generating an average of
2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for
the multi-mask scenario. We envision the proposed approach as a tool to
accelerate food image annotation tasks or as an integrated component in food
and nutrition tracking applications.

</details>


### [208] [A DyL-Unet framework based on dynamic learning for Temporally Consistent Echocardiographic Segmentation](https://arxiv.org/abs/2509.19052)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: The paper introduces DyL-UNet, a segmentation system that enhances temporal stability and accuracy for echocardiographic videos, addressing issues like deformation and speckle noise.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reduce frame-to-frame segmentation jitter and enhance temporal stability, which are critical for reliable cardiovascular diagnosis and treatment using echocardiography.

Method: The authors propose DyL-UNet, which uses a dynamic Echo-Dynamics Graph (EDG) for extracting video dynamics and integrates Swin-Transformer-based encoder-decoder networks with a novel Cardiac Phase-Dynamics Attention (CPDA) mechanism for temporal consistency enforcement.

Result: Experiments on datasets like CAMUS and EchoNet-Dynamic show that DyL-UNet matches existing methods in accuracy but outperforms them in maintaining temporal consistency.

Conclusion: DyL-UNet offers a robust solution for temporally stable echocardiographic segmentation, facilitating more reliable automated clinical analysis.

Abstract: Accurate segmentation of cardiac anatomy in echocardiography is essential for
cardiovascular diagnosis and treatment. Yet echocardiography is prone to
deformation and speckle noise, causing frame-to-frame segmentation jitter. Even
with high accuracy in single-frame segmentation, temporal instability can
weaken functional estimates and impair clinical interpretability. To address
these issues, we propose DyL-UNet, a dynamic learning-based temporal
consistency U-Net segmentation architecture designed to achieve temporally
stable and precise echocardiographic segmentation. The framework constructs an
Echo-Dynamics Graph (EDG) through dynamic learning to extract dynamic
information from videos. DyL-UNet incorporates multiple Swin-Transformer-based
encoder-decoder branches for processing single-frame images. It further
introduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections,
which uses EDG-encoded dynamic features and cardiac-phase cues to enforce
temporal consistency during segmentation. Extensive experiments on the CAMUS
and EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation
accuracy comparable to existing methods while achieving superior temporal
consistency, providing a reliable solution for automated clinical
echocardiography.

</details>


### [209] [WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction](https://arxiv.org/abs/2509.19073)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: The paper presents WaveletGaussian, a framework for efficient sparse-view 3D Gaussian object reconstruction using wavelet domain diffusion and lightweight refinement.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inefficiency of existing approaches in sparse-view 3D Gaussian object reconstruction, which suffer from heavy computational costs due to the diffusion fine-tuning and repair steps.

Method: The method involves applying diffusion only to the low-resolution LL subband in the wavelet domain and refining high-frequency subbands with a lightweight network. Additionally, an efficient online random masking strategy is introduced to improve training efficiency.

Result: The WaveletGaussian framework achieves competitive rendering quality compared to existing methods while significantly reducing training time, as confirmed through experiments on Mip-NeRF 360 and OmniObject3D datasets.

Conclusion: WaveletGaussian is a computationally efficient framework that enhances the practicality of 3D Gaussian splatting for sparse-view settings, maintaining high rendering quality with reduced training requirements.

Abstract: 3D Gaussian Splatting (3DGS) has become a powerful representation for
image-based object reconstruction, yet its performance drops sharply in
sparse-view settings. Prior works address this limitation by employing
diffusion models to repair corrupted renders, subsequently using them as pseudo
ground truths for later optimization. While effective, such approaches incur
heavy computation from the diffusion fine-tuning and repair steps. We present
WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object
reconstruction. Our key idea is to shift diffusion into the wavelet domain:
diffusion is applied only to the low-resolution LL subband, while
high-frequency subbands are refined with a lightweight network. We further
propose an efficient online random masking strategy to curate training pairs
for diffusion fine-tuning, replacing the commonly used, but inefficient,
leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360
and OmniObject3D, show WaveletGaussian achieves competitive rendering quality
while substantially reducing training time.

</details>


### [210] [3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference](https://arxiv.org/abs/2509.19082)
*Alexey Nekrasov,Ali Athar,Daan de Geus,Alexander Hermans,Bastian Leibe*

Main category: cs.CV

TL;DR: This paper identifies and resolves inconsistencies in the Sa2VA model for referring video object segmentation, resulting in significant performance improvements across benchmarks.


<details>
  <summary>Details</summary>
Motivation: The study aims to address limitations in the Sa2VA model's performance on referring video object segmentation tasks, which are attributed to inconsistencies between training and inference procedures.

Method: The authors propose an improved version called Sa2VA-i, which revises the training and inference procedure inconsistencies of Sa2VA without altering the core model checkpoints.

Result: Sa2VA-i achieves significant performance improvements on multiple video benchmarks, including +11.6 J&F on MeViS and other notable increases on Ref-YT-VOS, Ref-DAVIS, and ReVOS. It performs better than the original Sa2VA and sometimes rivals larger models.

Conclusion: This work demonstrates how resolving minor implementation details can yield notable performance gains in video object segmentation and encourages future research to pay attention to these aspects.

Abstract: Sa2VA is a recent model for language-guided dense grounding in images and
video that achieves state-of-the-art results on multiple segmentation
benchmarks and that has become widely popular. However, we found that Sa2VA
does not perform according to its full potential for referring video object
segmentation tasks. We identify inconsistencies between training and inference
procedures as the key factor holding it back. To mitigate this issue, we
propose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and
improves the results. In fact, Sa2VA-i sets a new state of the art for multiple
video benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on
Ref-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA
checkpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the
original Sa2VA-26B model on the MeViS benchmark. We hope that this work will
show the importance of seemingly trivial implementation details and that it
will provide valuable insights for the referring video segmentation field. We
provide the code and updated models at https://github.com/kumuji/sa2va-i

</details>


### [211] [Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications](https://arxiv.org/abs/2509.19087)
*Ganesh Mallya,Yotam Gigi,Dahun Kim,Maxim Neumann,Genady Beryozkin,Tomer Shekel,Anelia Angelova*

Main category: cs.CV

TL;DR: The paper presents a training-free method to use multi-spectral imagery with RGB-only generalist multimodal models, like Gemini2.5, achieving strong Zero-Shot performance for land cover and land use classification.


<details>
  <summary>Details</summary>
Motivation: Multi-spectral imagery is highly valuable in Remote Sensing due to its correlation with physical materials, but training specialized machine learning models is costly and incompatible with generalist multimodal models, limiting their accessibility.

Method: The authors introduce a Zero-Shot approach where multi-spectral data is adapted and injected as instructions into RGB-trained multimodal models, allowing them to process multi-spectral inputs without retraining.

Result: The proposed approach enhances the Zero-Shot performance of Gemini2.5 on Remote Sensing benchmarks, improving land cover and land use classification while simplifying adaptability to new inputs.

Conclusion: The method enables geospatial professionals to leverage powerful generalist models for specialized tasks, combining their advanced reasoning capabilities with the utility of multi-spectral sensor data, without requiring extensive retraining.

Abstract: Multi-spectral imagery plays a crucial role in diverse Remote Sensing
applications including land-use classification, environmental monitoring and
urban planning. These images are widely adopted because their additional
spectral bands correlate strongly with physical materials on the ground, such
as ice, water, and vegetation. This allows for more accurate identification,
and their public availability from missions, such as Sentinel-2 and Landsat,
only adds to their value. Currently, the automatic analysis of such data is
predominantly managed through machine learning models specifically trained for
multi-spectral input, which are costly to train and support. Furthermore,
although providing a lot of utility for Remote Sensing, such additional inputs
cannot be used with powerful generalist large multimodal models, which are
capable of solving many visual problems, but are not able to understand
specialized multi-spectral signals.
  To address this, we propose a training-free approach which introduces new
multi-spectral data in a Zero-Shot-only mode, as inputs to generalist
multimodal models, trained on RGB-only inputs. Our approach leverages the
multimodal models' understanding of the visual space, and proposes to adapt to
inputs to that space, and to inject domain-specific information as instructions
into the model. We exemplify this idea with the Gemini2.5 model and observe
strong Zero-Shot performance gains of the approach on popular Remote Sensing
benchmarks for land cover and land use classification and demonstrate the easy
adaptability of Gemini2.5 to new inputs. These results highlight the potential
for geospatial professionals, working with non-standard specialized inputs, to
easily leverage powerful multimodal models, such as Gemini2.5, to accelerate
their work, benefiting from their rich reasoning and contextual capabilities,
grounded in the specialized sensor data.

</details>


### [212] [Track-On2: Enhancing Online Point Tracking with Memory](https://arxiv.org/abs/2509.19115)
*Görkay Aydemir,Weidi Xie,Fatma Güney*

Main category: cs.CV

TL;DR: This paper presents Track-On2, an improved transformer-based model for online long-term point tracking, which surpasses previous methods in both efficiency and performance on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of consistently tracking points across video frames under significant appearance changes, motion, and occlusion, particularly suitable for real-time applications.

Method: The authors developed Track-On2, a causal transformer-based model with architectural refinements and memory mechanisms, along with optimized synthetic training strategies.

Result: Track-On2 achieves state-of-the-art results across five benchmarks, outperforming both online and offline trackers, including those using bidirectional context.

Conclusion: Causal, memory-based architectures trained entirely on synthetic data provide scalable and robust solutions to real-world point tracking problems.

Abstract: In this paper, we consider the problem of long-term point tracking, which
requires consistent identification of points across video frames under
significant appearance changes, motion, and occlusion. We target the online
setting, i.e. tracking points frame-by-frame, making it suitable for real-time
and streaming applications. We extend our prior model Track-On into Track-On2,
a simple and efficient transformer-based model for online long-term tracking.
Track-On2 improves both performance and efficiency through architectural
refinements, more effective use of memory, and improved synthetic training
strategies. Unlike prior approaches that rely on full-sequence access or
iterative updates, our model processes frames causally and maintains temporal
coherence via a memory mechanism, which is key to handling drift and occlusions
without requiring future frames. At inference, we perform coarse patch-level
classification followed by refinement. Beyond architecture, we systematically
study synthetic training setups and their impact on memory behavior, showing
how they shape temporal robustness over long sequences. Through comprehensive
experiments, Track-On2 achieves state-of-the-art results across five synthetic
and real-world benchmarks, surpassing prior online trackers and even strong
offline methods that exploit bidirectional context. These results highlight the
effectiveness of causal, memory-based architectures trained purely on synthetic
data as scalable solutions for real-world point tracking. Project page:
https://kuis-ai.github.io/track_on2

</details>


### [213] [KAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic Environments](https://arxiv.org/abs/2509.19129)
*Adam Romlein,Benjamin X. Hou,Yuval Boss,Cynthia L. Christman,Stacie Koslovsky,Erin E. Moreland,Jason Parham,Anthony Hoogs*

Main category: cs.CV

TL;DR: KAMERA offers a system for detecting seals and polar bears in real-time using multi-camera and multi-spectral synchronization, achieving an 80% reduction in dataset processing time.


<details>
  <summary>Details</summary>
Motivation: The work aims to improve efficiency and accuracy in animal detection during aerial surveys in Arctic seas, addressing the need for better tools in wildlife monitoring.

Method: The system uses calibrated, synchronized hardware for multi-spectral imaging, maps detections onto a plane for area estimates, and annotates data with metadata.

Result: A significant reduction in dataset processing time (up to 80%) compared to previous methods was achieved.

Conclusion: KAMERA contributes to wildlife monitoring by enhancing detection accuracy and efficiency. The system and its components are open-sourced to inspire further study.

Abstract: We introduce KAMERA: a comprehensive system for multi-camera, multi-spectral
synchronization and real-time detection of seals and polar bears. Utilized in
aerial surveys for ice-associated seals in the Bering, Chukchi, and Beaufort
seas around Alaska, KAMERA provides up to an 80% reduction in dataset
processing time over previous methods. Our rigorous calibration and hardware
synchronization enable using multiple spectra for object detection. All
collected data are annotated with metadata so they can be easily referenced
later. All imagery and animal detections from a survey are mapped onto a world
plane for accurate surveyed area estimates and quick assessment of survey
results. We hope KAMERA will inspire other mapping and detection efforts in the
scientific community, with all software, models, and schematics fully
open-sourced.

</details>


### [214] [NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and Dynamic Early-Exit](https://arxiv.org/abs/2509.19156)
*Maurf Hassan,Steven Davy,Muhammad Zawish,Owais Bin Zuber,Nouman Ashraf*

Main category: cs.CV

TL;DR: The paper presents NeuCODEX, an architecture optimizing SNN edge-cloud co-inference by reducing redundancy, energy consumption, and latency.


<details>
  <summary>Details</summary>
Motivation: Challenges in deploying Spiking Neural Networks at the edge due to high latency and energy constraints, coupled with inefficiencies in edge-cloud co-inference systems.

Method: NeuCODEX uses spike-driven compression to reduce data transmission, and early-exit mechanisms to dynamically end inference based on confidence levels.

Result: NeuCODEX reduces data transfer by up to 2048x, edge energy usage by over 90%, and latency by up to 3x, with an accuracy drop of less than 2%.

Conclusion: NeuCODEX significantly enhances practical deployment of SNNs in resource-limited environments, making edge-cloud co-inference systems viable.

Abstract: Spiking Neural Networks (SNNs) offer significant potential for enabling
energy-efficient intelligence at the edge. However, performing full SNN
inference at the edge can be challenging due to the latency and energy
constraints arising from fixed and high timestep overheads. Edge-cloud
co-inference systems present a promising solution, but their deployment is
often hindered by high latency and feature transmission costs. To address these
issues, we introduce NeuCODEX, a neuromorphic co-inference architecture that
jointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a
learned spike-driven compression module to reduce data transmission and employs
a dynamic early-exit mechanism to adaptively terminate inference based on
output confidence. We evaluated NeuCODEX on both static images (CIFAR10 and
Caltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To
demonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16
backbones in a real edge-to-cloud testbed. Our proposed system reduces data
transfer by up to 2048x and edge energy consumption by over 90%, while reducing
end-to-end latency by up to 3x compared to edge-only inference, all with a
negligible accuracy drop of less than 2%. In doing so, NeuCODEX enables
practical, high-performance SNN deployment in resource-constrained
environments.

</details>


### [215] [RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions](https://arxiv.org/abs/2509.19165)
*Yun Wang,Junjie Hu,Junhui Hou,Chenghao Zhang,Renwei Yang,Dapeng Oliver Wu*

Main category: cs.CV

TL;DR: This paper addresses the limitations of self-supervised stereo matching under adverse weather by introducing robust priors and a novel training paradigm, achieving better performance than state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to tackle the performance degradation of self-supervised stereo matching methods under adverse weather (e.g., night, rain, fog), which introduce noise, reduce visibility, and disrupt pixel correspondences.

Method: The authors propose injecting robust priors from a visual foundation model into feature extractors and constructing synthetic stereo datasets with weather degradations. They also introduce a robust self-supervised training paradigm with two steps: scene correspondence learning and adverse weather distillation.

Result: Extensive experiments show that the proposed approach outperforms existing state-of-the-art self-supervised stereo matching methods, demonstrating effectiveness under adverse weather conditions.

Conclusion: By introducing robust priors and enhancing training with weather-degraded synthetic datasets, the proposed method significantly improves stereo matching performance in challenging weather conditions, setting a new standard in the field.

Abstract: Recent self-supervised stereo matching methods have made significant
progress, but their performance significantly degrades under adverse weather
conditions such as night, rain, and fog. We identify two primary weaknesses
contributing to this performance degradation. First, adverse weather introduces
noise and reduces visibility, making CNN-based feature extractors struggle with
degraded regions like reflective and textureless areas. Second, these degraded
regions can disrupt accurate pixel correspondences, leading to ineffective
supervision based on the photometric consistency assumption. To address these
challenges, we propose injecting robust priors derived from the visual
foundation model into the CNN-based feature extractor to improve feature
representation under adverse weather conditions. We then introduce scene
correspondence priors to construct robust supervisory signals rather than
relying solely on the photometric consistency assumption. Specifically, we
create synthetic stereo datasets with realistic weather degradations. These
datasets feature clear and adverse image pairs that maintain the same semantic
context and disparity, preserving the scene correspondence property. With this
knowledge, we propose a robust self-supervised training paradigm, consisting of
two key steps: robust self-supervised scene correspondence learning and adverse
weather distillation. Both steps aim to align underlying scene results from
clean and adverse image pairs, thus improving model disparity estimation under
adverse weather effects. Extensive experiments demonstrate the effectiveness
and versatility of our proposed solution, which outperforms existing
state-of-the-art self-supervised methods. Codes are available at
\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.

</details>


### [216] [YOLO-LAN: Precise Polyp Detection via Optimized Loss, Augmentations and Negatives](https://arxiv.org/abs/2509.19166)
*Siddharth Gupta,Jitin Singla*

Main category: cs.CV

TL;DR: This paper proposes YOLO-LAN, a YOLO-based deep learning model for accurate polyp detection during colonoscopy, achieving high mAP scores on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Manual detection of polyps during colonoscopy can be inconsistent and prone to errors, motivating the need for an accurate, real-time automated detection approach using deep learning.

Method: The authors propose a deep learning framework called YOLO-LAN, incorporating M2IoU loss, versatile data augmentations, and negative data to mimic clinical conditions, and evaluate it on benchmark polyp detection datasets.

Result: YOLO-LAN achieved state-of-the-art performance on Kvasir-seg and BKAI-IGH NeoPolyp datasets, with mAP$_{50}$ of 0.9619 and mAP$_{50:95}$ of 0.8599 using YOLOv12, and slightly lower but still competitive results with YOLOv8.

Conclusion: The proposed YOLO-LAN demonstrates robustness in detecting polyps across sizes and locations, making it a clinically relevant tool for AI-assisted colorectal cancer screening.

Abstract: Colorectal cancer (CRC), a lethal disease, begins with the growth of abnormal
mucosal cell proliferation called polyps in the inner wall of the colon. When
left undetected, polyps can become malignant tumors. Colonoscopy is the
standard procedure for detecting polyps, as it enables direct visualization and
removal of suspicious lesions. Manual detection by colonoscopy can be
inconsistent and is subject to oversight. Therefore, object detection based on
deep learning offers a better solution for a more accurate and real-time
diagnosis during colonoscopy. In this work, we propose YOLO-LAN, a YOLO-based
polyp detection pipeline, trained using M2IoU loss, versatile data
augmentations and negative data to replicate real clinical situations. Our
pipeline outperformed existing methods for the Kvasir-seg and BKAI-IGH NeoPolyp
datasets, achieving mAP$_{50}$ of 0.9619, mAP$_{50:95}$ of 0.8599 with YOLOv12
and mAP$_{50}$ of 0.9540, mAP$_{50:95}$ of 0.8487 with YOLOv8 on the Kvasir-seg
dataset. The significant increase is achieved in mAP$_{50:95}$ score, showing
the precision of polyp detection. We show robustness based on polyp size and
precise location detection, making it clinically relevant in AI-assisted
colorectal screening.

</details>


### [217] [The 1st Solution for MOSEv2 Challenge 2025: Long-term and Concept-aware Video Segmentation via SeC](https://arxiv.org/abs/2509.19183)
*Mingqi Gao,Jingkun Chen,Yunqi Miao,Gengshen Wu,Zhijin Qin,Jungong Han*

Main category: cs.CV

TL;DR: The paper examines the MOSEv2 track in the LSVOS Challenge, focusing on semi-supervised video object segmentation using an adapted SeC framework.


<details>
  <summary>Details</summary>
Motivation: To address challenges in semi-supervised video object segmentation, particularly preserving temporal continuity and reducing distractors.

Method: The study analyzes and adapts SeC framework, focusing on its long-term memory and concept-aware memory components.

Result: The proposed method ranks 1st in the MOSEv2 track with a JF score of 39.89%.

Conclusion: Enhancements in memory design benefit core challenges in video object segmentation, leading to state-of-the-art performance.

Abstract: This technical report explores the MOSEv2 track of the LSVOS Challenge, which
targets complex semi-supervised video object segmentation. By analysing and
adapting SeC, an enhanced SAM-2 framework, we conduct a detailed study of its
long-term memory and concept-aware memory, showing that long-term memory
preserves temporal continuity under occlusion and reappearance, while
concept-aware memory supplies semantic priors that suppress distractors;
together, these traits directly benefit several MOSEv2's core challenges. Our
solution achieves a JF score of 39.89% on the test set, ranking 1st in the
MOSEv2 track of the LSVOS Challenge.

</details>


### [218] [Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models](https://arxiv.org/abs/2509.19191)
*Yueyan Li,Chenggong Zhao,Zeyuan Zang,Caixia Yuan,Xiaojie Wang*

Main category: cs.CV

TL;DR: The paper studies Vision-Language Models (VLMs), proposing improvements by decomposing visual processing into object and spatial analyses, introducing token compression and spatial reasoning enhancements.


<details>
  <summary>Details</summary>
Motivation: Current VLMs process visual data sequentially, unlike human parallel vision, and their inner workings are not well understood.

Method: The authors examined the visual processing of VLMs, using a dual-stream human vision hypothesis to analyze object recognition and spatial perception separately, and introduced a token compression algorithm and spatial reasoning improvement.

Result: Findings include a two-stage process for image content perception in VLMs and verification of positional representation geometry, leading to enhanced decoding efficiency and spatial reasoning.

Conclusion: The work provides insights into VLM operations, laying the foundation for improved and more efficient design of future vision-language models.

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable performance across
a variety of real-world tasks. However, existing VLMs typically process visual
information by serializing images, a method that diverges significantly from
the parallel nature of human vision. Moreover, their opaque internal mechanisms
hinder both deeper understanding and architectural innovation. Inspired by the
dual-stream hypothesis of human vision, which distinguishes the "what" and
"where" pathways, we deconstruct the visual processing in VLMs into object
recognition and spatial perception for separate study. For object recognition,
we convert images into text token maps and find that the model's perception of
image content unfolds as a two-stage process from shallow to deep layers,
beginning with attribute recognition and culminating in semantic
disambiguation. For spatial perception, we theoretically derive and empirically
verify the geometric structure underlying the positional representation in
VLMs. Based on these findings, we introduce an instruction-agnostic token
compression algorithm based on a plug-and-play visual decoder to improve
decoding efficiency, and a RoPE scaling technique to enhance spatial reasoning.
Through rigorous experiments, our work validates these analyses, offering a
deeper understanding of VLM internals and providing clear principles for
designing more capable future architectures.

</details>


### [219] [Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions](https://arxiv.org/abs/2509.19203)
*Ioanna Ntinou,Alexandros Xenos,Yassine Ouali,Adrian Bulat,Georgios Tzimiropoulos*

Main category: cs.CV

TL;DR: This paper proposes replacing vision-encoders in retrieval tasks with textual descriptions generated by vision-language models. The resulting text-to-text paradigm reduces computational costs, modality gaps, and privacy concerns while achieving state-of-the-art retrieval performance.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models rely heavily on vision encoders and large-scale data, leading to modality gaps and privacy concerns. This paper aims to address these issues by removing the necessity of vision encoders for retrieval tasks.

Method: The study introduces a text-to-text retrieval paradigm where structured textual descriptions generated by Vision-Language Models (VLLMs) replace raw image data. New benchmarks (subFlickr and subCOCO) are proposed to evaluate performance.

Result: The vision-free retrieval approach reduces the modality gap, improves compositional understanding, performs well on diverse queries, and matches or surpasses multimodal models. It achieves state-of-the-art zero-shot performance with relatively small models.

Conclusion: Transitioning from a text-to-image to a text-to-text retrieval paradigm successfully addresses computational, privacy, and modality issues while maintaining high retrieval performance. This new method shows strong generalization capabilities for diverse retrieval tasks.

Abstract: Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have
become the standard approach for learning discriminative vision-language
representations. However, these models often exhibit shallow language
understanding, manifesting bag-of-words behaviour. These limitations are
reinforced by their dual-encoder design, which induces a modality gap.
Additionally, the reliance on vast web-collected data corpora for training
makes the process computationally expensive and introduces significant privacy
concerns. To address these limitations, in this work, we challenge the
necessity of vision encoders for retrieval tasks by introducing a vision-free,
single-encoder retrieval pipeline. Departing from the traditional text-to-image
retrieval paradigm, we migrate to a text-to-text paradigm with the assistance
of VLLM-generated structured image descriptions. We demonstrate that this
paradigm shift has significant advantages, including a substantial reduction of
the modality gap, improved compositionality, and better performance on short
and long caption queries, all attainable with only a few hours of calibration
on two GPUs. Additionally, substituting raw images with textual descriptions
introduces a more privacy-friendly alternative for retrieval. To further assess
generalisation and address some of the shortcomings of prior compositionality
benchmarks, we release two benchmarks derived from Flickr30k and COCO,
containing diverse compositional queries made of short captions, which we coin
subFlickr and subCOCO. Our vision-free retriever matches and often surpasses
traditional multimodal models. Importantly, our approach achieves
state-of-the-art zero-shot performance on multiple retrieval and
compositionality benchmarks, with models as small as 0.3B parameters. Code is
available at: https://github.com/IoannaNti/LexiCLIP

</details>


### [220] [Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs](https://arxiv.org/abs/2509.19207)
*Israfel Salazar,Desmond Elliott,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: This research investigates the role of compositionality in improving vision-language model performance on long-caption understanding and vice versa, identifying key factors like data quality and design strategies.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address challenges in understanding long, dense captions in contrastive vision-language models, exploring whether compositional reasoning contributes to better performance.

Method: Training various models targeted at compositionality and long-caption tasks, testing bidirectional relationships, and analyzing the impact of data structure, quality, and model design.

Result: Training for compositionality improves long-caption retrieval, and vice versa, under the condition of high-quality, well-structured data and thoughtful model approaches.

Conclusion: Long-caption understanding and compositionality are interrelated abilities that benefit from joint training, especially when supported by dense, high-quality descriptions.

Abstract: Contrastive vision-language models (VLMs) have made significant progress in
binding visual and textual information, but understanding long, dense captions
remains an open challenge. We hypothesize that compositionality, the capacity
to reason about object-attribute bindings and inter-object relationships, is
key to understanding longer captions. In this paper, we investigate the
interaction between compositionality and long-caption understanding, asking
whether training for one property enhances the other. We train and evaluate a
range of models that target each of these capabilities. Our results reveal a
bidirectional relationship: compositional training improves performance on
long-caption retrieval, and training on long captions promotes
compositionality. However, these gains are sensitive to data quality and model
design. We find that training on poorly structured captions, or with limited
parameter updates, fails to support generalization. Likewise, strategies that
aim at retaining general alignment, such as freezing positional embeddings, do
not improve compositional understanding. Overall, we find that compositional
understanding and long-caption understanding are intertwined capabilities that
can be jointly learned through training on dense, grounded descriptions.
Despite these challenges, we show that models trained on high-quality,
long-caption data can achieve strong performance in both tasks, offering
practical guidance for improving VLM generalization.

</details>


### [221] [Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data](https://arxiv.org/abs/2509.19208)
*Earl Ranario,Ismael Mayanja,Heesup Yun,Brian N. Bailey,J. Mason Earles*

Main category: cs.CV

TL;DR: This study proposes a framework using synthetic RGB imagery, minimal real annotations, and GANs for better plant segmentation in thermal images, improving performance in challenging field conditions.


<details>
  <summary>Details</summary>
Motivation: Plant segmentation in thermal images is difficult in field conditions due to low contrast and occlusions, hindering effective phenotyping.

Method: The approach combines synthetic RGB training data, real annotations, and CycleGAN-turbo to align RGB and thermal modalities, enabling better semantic segmentation.

Result: The proposed method enhanced segmentation results by up to 22% for weeds and 17% for plants compared to real-data baselines.

Conclusion: Integrating synthetic data, limited real annotations, and GAN-based translations can effectively improve plant segmentation in thermal imagery for field phenotyping.

Abstract: Accurate plant segmentation in thermal imagery remains a significant
challenge for high throughput field phenotyping, particularly in outdoor
environments where low contrast between plants and weeds and frequent
occlusions hinder performance. To address this, we present a framework that
leverages synthetic RGB imagery, a limited set of real annotations, and
GAN-based cross-modality alignment to enhance semantic segmentation in thermal
images. We trained models on 1,128 synthetic images containing complex mixtures
of crop and weed plants in order to generate image segmentation masks for crop
and weed plants. We additionally evaluated the benefit of integrating as few as
five real, manually segmented field images within the training process using
various sampling strategies. When combining all the synthetic images with a few
labeled real images, we observed a maximum relative improvement of 22% for the
weed class and 17% for the plant class compared to the full real-data baseline.
Cross-modal alignment was enabled by translating RGB to thermal using
CycleGAN-turbo, allowing robust template matching without calibration. Results
demonstrated that combining synthetic data with limited manual annotations and
cross-domain translation via generative models can significantly boost
segmentation performance in complex field environments for multi-model imagery.

</details>


### [222] [HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and Choroid Plexus in Pediatric Hydrocephalus](https://arxiv.org/abs/2509.19218)
*Yunzhi Xu,Yushuang Ding,Hu Sun,Hongxi Zhang,Li Zhao*

Main category: cs.CV

TL;DR: This paper introduces HyKid, an open-source pediatric hydrocephalus dataset, emphasizing high-resolution MRI and precise segmentations to aid in neuroimaging algorithm development.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the lack of publicly available, expert-annotated datasets that include segmentation for the choroid plexus, a crucial challenge in evaluating pediatric hydrocephalus.

Method: The researchers reconstructed 3D MRIs at 1mm isotropic resolution using slice-to-volume algorithms and provided manually corrected brain tissue segmentations by an experienced neurologist. They also extracted structured data from clinical radiology reports using a Retrieval-Augmented Generation framework.

Result: The dataset revealed a strong correlation between choroid plexus volume and total CSF volume, serving as a potential biomarker for hydrocephalus evaluation. A predictive model achieved an AUC of 0.87.

Conclusion: HyKid serves as a high-quality benchmark for neuroimaging research, providing insights into choroid plexus-related features in hydrocephalus assessment and aiding algorithm development.

Abstract: Evaluation of hydrocephalus in children is challenging, and the related
research is limited by a lack of publicly available, expert-annotated datasets,
particularly those with segmentation of the choroid plexus. To address this, we
present HyKid, an open-source dataset from 48 pediatric patients with
hydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was
reconstructed from routine low-resolution images using a slice-to-volume
algorithm. Manually corrected segmentations of brain tissues, including white
matter, grey matter, lateral ventricle, external CSF, and the choroid plexus,
were provided by an experienced neurologist. Additionally, structured data was
extracted from clinical radiology reports using a Retrieval-Augmented
Generation framework. The strong correlation between choroid plexus volume and
total CSF volume provided a potential biomarker for hydrocephalus evaluation,
achieving excellent performance in a predictive model (AUC = 0.87). The
proposed HyKid dataset provided a high-quality benchmark for neuroimaging
algorithms development, and it revealed the choroid plexus-related features in
hydrocephalus assessments. Our datasets are publicly available at
https://www.synapse.org/Synapse:syn68544889.

</details>


### [223] [MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation](https://arxiv.org/abs/2509.19227)
*Tongshuai Wu,Chao Lu,Ze Song,Yunlong Lin,Sizhe Fan,Xuemei Chen*

Main category: cs.CV

TL;DR: This paper introduces MsFIN, a network for anticipating accidents from dashcam videos by addressing feature-level interactions and complex asynchronous temporal cues. It achieves significant improvement over existing models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve accident anticipation from dashcam videos, addressing challenges in modeling interactions among traffic participants and capturing multi-temporal behavioral cues preceding accidents.

Method: A Multi-scale Feature Interaction Network (MsFIN) is proposed using transformer architecture, multi-scale feature aggregation, temporal processing, and feature fusion to generate risk representations.

Result: Experiments show MsFIN outperforms state-of-the-art models on DAD and DADA datasets with better prediction correctness and earliness. Ablation studies confirm the efficacy of its modules.

Conclusion: MsFIN effectively handles multi-scale feature fusion and contextual interactions to enhance accident prediction, providing better benchmarks for proactive safety measures.

Abstract: With the widespread deployment of dashcams and advancements in computer
vision, developing accident prediction models from the dashcam perspective has
become critical for proactive safety interventions. However, two key challenges
persist: modeling feature-level interactions among traffic participants (often
occluded in dashcam views) and capturing complex, asynchronous multi-temporal
behavioral cues preceding accidents. To deal with these two challenges, a
Multi-scale Feature Interaction Network (MsFIN) is proposed for early-stage
accident anticipation from dashcam videos. MsFIN has three layers for
multi-scale feature aggregation, temporal feature processing and multi-scale
feature post fusion, respectively. For multi-scale feature aggregation, a
Multi-scale Module is designed to extract scene representations at short-term,
mid-term and long-term temporal scales. Meanwhile, the Transformer architecture
is leveraged to facilitate comprehensive feature interactions. Temporal feature
processing captures the sequential evolution of scene and object features under
causal constraints. In the multi-scale feature post fusion stage, the network
fuses scene and object features across multiple temporal scales to generate a
comprehensive risk representation. Experiments on DAD and DADA datasets show
that MsFIN significantly outperforms state-of-the-art models with single-scale
feature extraction in both prediction correctness and earliness. Ablation
studies validate the effectiveness of each module in MsFIN, highlighting how
the network achieves superior performance through multi-scale feature fusion
and contextual interaction modeling.

</details>


### [224] [DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces](https://arxiv.org/abs/2509.19230)
*Tianshuo Zhang,Li Gao,Siran Peng,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: The paper introduces a continual learning framework for face forgery detection using a Developmental Mixture of Experts (MoE) architecture.


<details>
  <summary>Details</summary>
Motivation: Realistic digital face manipulation techniques are advancing rapidly, outpacing detection methods and posing social risks. A model is needed that adapts to evolving forgery techniques without data or computational constraints.

Method: The paper employs a Developmental Mixture of Experts (MoE) architecture with LoRA models, divided into Real-LoRA for genuine faces and Fake-LoRAs for incremental forgery types. It uses orthogonal gradients and loss to prevent forgetting while learning.

Result: The proposed method is experimentally validated to be effective on datasets and manipulation types under incremental protocols.

Conclusion: The approach successfully enables continual learning, addressing the challenge of adapting to evolving face forgery types while avoiding catastrophic forgetting.

Abstract: The rise of realistic digital face generation and manipulation poses
significant social risks. The primary challenge lies in the rapid and diverse
evolution of generation techniques, which often outstrip the detection
capabilities of existing models. To defend against the ever-evolving new types
of forgery, we need to enable our model to quickly adapt to new domains with
limited computation and data while avoiding forgetting previously learned
forgery types. In this work, we posit that genuine facial samples are abundant
and relatively stable in acquisition methods, while forgery faces continuously
evolve with the iteration of manipulation techniques. Given the practical
infeasibility of exhaustively collecting all forgery variants, we frame face
forgery detection as a continual learning problem and allow the model to
develop as new forgery types emerge. Specifically, we employ a Developmental
Mixture of Experts (MoE) architecture that uses LoRA models as its individual
experts. These experts are organized into two groups: a Real-LoRA to learn and
refine knowledge of real faces, and multiple Fake-LoRAs to capture incremental
information from different forgery types. To prevent catastrophic forgetting,
we ensure that the learning direction of Fake-LoRAs is orthogonal to the
established subspace. Moreover, we integrate orthogonal gradients into the
orthogonal loss of Fake-LoRAs, preventing gradient interference throughout the
training process of each task. Experimental results under both the datasets and
manipulation types incremental protocols demonstrate the effectiveness of our
method.

</details>


### [225] [Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2509.19244)
*Shufan Li,Jiuxiang Gu,Kangning Liu,Zhe Lin,Zijun Wei,Aditya Grover,Jason Kuen*

Main category: cs.CV

TL;DR: Lavida-O is a unified multi-modal Masked Diffusion Model (MDM) designed for both image understanding and generation tasks, supporting high-resolution output and innovative capabilities like object grounding and image editing.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models struggle with unified multi-modal task support, especially achieving high-quality image understanding and generation.

Method: Lavida-O utilizes novel techniques like an Elastic Mixture-of-Transformer architecture, universal text conditioning, and stratified sampling for enhanced training and image synthesis capabilities.

Result: Lavida-O achieves state-of-the-art performance in object grounding, text-to-image generation, and image editing benchmarks, outperforming existing models like Qwen2.5-VL, with faster inference speeds.

Conclusion: Lavida-O demonstrates innovative capabilities in merging image understanding and generation, bridging gaps in multi-modal diffusion models.

Abstract: We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)
capable of image understanding and generation tasks. Unlike existing multimodal
diffsion language models such as MMaDa and Muddit which only support simple
image-level understanding tasks and low-resolution image generation, Lavida-O
exhibits many new capabilities such as object grounding, image-editing, and
high-resolution (1024px) image synthesis. It is also the first unified MDM that
uses its understanding capabilities to improve image generation and editing
results through planning and iterative self-reflection. To allow effective and
efficient training and sampling, Lavida-O ntroduces many novel techniques such
as Elastic Mixture-of-Transformer architecture, universal text conditioning,
and stratified sampling. \ours~achieves state-of-the-art performance on a wide
range of benchmarks such as RefCOCO object grounding, GenEval text-to-image
generation, and ImgEdit image editing, outperforming existing autoregressive
and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while
offering considerable speedup at inference.

</details>


### [226] [ConViS-Bench: Estimating Video Similarity Through Semantic Concepts](https://arxiv.org/abs/2509.19245)
*Benedetta Liberatori,Alessandro Conti,Lorenzo Vaquero,Yiming Wang,Elisa Ricci,Paolo Rota*

Main category: cs.CV

TL;DR: The paper introduces ConViS, a task that estimates video similarity through multiple semantic concepts and proposes the ConViS-Bench benchmark for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing video models rely on global similarity scores, which don’t capture human-like nuanced reasoning when comparing videos based on different aspects like actions or locations.

Method: ConViS computes interpretable similarity scores across key predefined semantic concepts, supplemented by a new benchmark (ConViS-Bench) featuring annotated video pairs with concept-level scores and textual descriptions.

Result: State-of-the-art models show varying alignment with human judgments in benchmarks; certain semantic concepts are more challenging for video similarity estimation.

Conclusion: ConViS-Bench is expected to drive progress in language-driven video understanding by addressing challenges in concept-based video similarity estimation.

Abstract: What does it mean for two videos to be similar? Videos may appear similar
when judged by the actions they depict, yet entirely different if evaluated
based on the locations where they were filmed. While humans naturally compare
videos by taking different aspects into account, this ability has not been
thoroughly studied and presents a challenge for models that often depend on
broad global similarity scores. Large Multimodal Models (LMMs) with video
understanding capabilities open new opportunities for leveraging natural
language in comparative video tasks. We introduce Concept-based Video
Similarity estimation (ConViS), a novel task that compares pairs of videos by
computing interpretable similarity scores across a predefined set of key
semantic concepts. ConViS allows for human-like reasoning about video
similarity and enables new applications such as concept-conditioned video
retrieval. To support this task, we also introduce ConViS-Bench, a new
benchmark comprising carefully annotated video pairs spanning multiple domains.
Each pair comes with concept-level similarity scores and textual descriptions
of both differences and similarities. Additionally, we benchmark several
state-of-the-art models on ConViS, providing insights into their alignment with
human judgments. Our results reveal significant performance differences on
ConViS, indicating that some concepts present greater challenges for estimating
video similarity. We believe that ConViS-Bench will serve as a valuable
resource for advancing research in language-driven video understanding.

</details>


### [227] [Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps](https://arxiv.org/abs/2509.19252)
*Gabriel Maldonado,Narges Rashvand,Armin Danesh Pazho,Ghazal Alinezhad Noghre,Vinit Katariya,Hamed Tabkhi*

Main category: cs.CV

TL;DR: The paper introduces an adversarially-refined VQ-GAN with dense motion tokenization for efficient compression and representation of spatio-temporal human motion heatmaps, reducing artifacts and improving performance.


<details>
  <summary>Details</summary>
Motivation: Understanding continuous human motion is challenging due to its high dimensionality and redundancy, making efficient compression and representation methods critical.

Method: The authors propose an adversarially-refined VQ-GAN framework that combines dense motion tokenization with adversarial refinement to eliminate motion smearing and temporal misalignment artifacts.

Result: Experiments on the CMU Panoptic dataset show a 9.31% SSIM improvement and a 37.1% reduction in temporal instability compared to the dVAE baseline. Dense tokenization revealed optimal token vocabularies for 2D and 3D motion.

Conclusion: The proposed method showcases practical deployment feasibility for various motion analysis applications by achieving superior performance in motion representation and compression.

Abstract: Continuous human motion understanding remains a core challenge in computer
vision due to its high dimensionality and inherent redundancy. Efficient
compression and representation are crucial for analyzing complex motion
dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework
with dense motion tokenization for compressing spatio-temporal heatmaps while
preserving the fine-grained traces of human motion. Our approach combines dense
motion tokenization with adversarial refinement, which eliminates
reconstruction artifacts like motion smearing and temporal misalignment
observed in non-adversarial baselines. Our experiments on the CMU Panoptic
dataset provide conclusive evidence of our method's superiority, outperforming
the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.
Furthermore, our dense tokenization strategy enables a novel analysis of motion
complexity, revealing that 2D motion can be optimally represented with a
compact 128-token vocabulary, while 3D motion's complexity demands a much
larger 1024-token codebook for faithful reconstruction. These results establish
practical deployment feasibility across diverse motion analysis applications.
The code base for this work is available at
https://github.com/TeCSAR-UNCC/Pose-Quantization.

</details>


### [228] [Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging Heterogeneity in Confounding Tumor Pathologies](https://arxiv.org/abs/2509.19258)
*Dheerendranath Battalapalli,Apoorva Safai,Maria Jaramillo,Hyemin Um,Gustavo Adalfo Pineda Ortiz,Ulas Bagci,Manmeet Singh Ahluwalia,Marwa Ismail,Pallavi Tiwari*

Main category: cs.CV

TL;DR: The paper introduces GrRAiL, a novel graph-radiomic learning framework for reliably capturing intralesional heterogeneity in MRI scans to differentiate between malignancies and confounding pathologies.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of reliably distinguishing between malignant neoplasms and confounding pathologies in solid tumors using imaging techniques that fail to capture complex spatial relationships.

Method: GrRAiL characterizes intralesional heterogeneity by clustering sub-regions of MRI voxels based on radiomic measurements and utilizing graph-theoretic metrics to encode spatial relationships within the region of interest.

Result: GrRAiL was tested across 947 subjects in three scenarios: glioblastoma recurrence vs pseudo-progression, brain metastasis recurrence vs radiation necrosis, and risk stratification in pancreatic IPMNs. It consistently outperformed existing methods with up to 13% improvement in test accuracy.

Conclusion: GrRAiL shows potential for improving clinical decision-making by reliably identifying malignancies and stratifying risks, outperforming state-of-the-art techniques in multiple scenarios and institutions.

Abstract: A significant challenge in solid tumors is reliably distinguishing
confounding pathologies from malignant neoplasms on routine imaging. While
radiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI,
many aggregate features across the region of interest (ROI) and miss complex
spatial relationships among varying intensity compositions. We present a new
Graph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesional
heterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters of
sub-regions using per-voxel radiomic measurements, then (2) computes
graph-theoretic metrics to quantify spatial associations among clusters. The
resulting weighted graphs encode higher-order spatial relationships within the
ROI, aiming to reliably capture ILH and disambiguate confounding pathologies
from malignancy. To assess efficacy and clinical feasibility, GrRAiL was
evaluated in n=947 subjects spanning three use cases: differentiating tumor
recurrence from radiation effects in glioblastoma (GBM; n=106) and brain
metastasis (n=233), and stratifying pancreatic intraductal papillary mucinous
neoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutional
setting, GrRAiL consistently outperformed state-of-the-art baselines - Graph
Neural Networks (GNNs), textural radiomics, and intensity-graph analysis. In
GBM, cross-validation (CV) and test accuracies for recurrence vs
pseudo-progression were 89% and 78% with >10% test-accuracy gains over
comparators. In brain metastasis, CV and test accuracies for recurrence vs
radiation necrosis were 84% and 74% (>13% improvement). For IPMN risk
stratification, CV and test accuracies were 84% and 75%, showing >10%
improvement.

</details>


### [229] [Moving by Looking: Towards Vision-Driven Avatar Motion Generation](https://arxiv.org/abs/2509.19259)
*Markos Diomataris,Berat Mert Albaba,Giorgio Becherini,Partha Ghosh,Omid Taheri,Michael J. Black*

Main category: cs.CV

TL;DR: The paper introduces CLOPS, the first human avatar that uses egocentric vision to navigate and behave, addressing the challenge of training with limited contextual human motion data.


<details>
  <summary>Details</summary>
Motivation: Current motion-generation models ignore the correlation between perception and movement, and often use non-human-like perception systems, limiting the development of avatars that demonstrate realistic human behavior.

Method: The authors propose CLOPS, where the learning is split into low-level motion skill training on motion capture data and high-level control training using Q-learning to map egocentric vision to motion.

Result: Empirical experiments show that CLOPS avatars exhibit human-like motion, effectively avoiding obstacles visible through egocentric visual inputs.

Conclusion: Egocentric vision-based avatars can achieve human-like behavior, emphasizing the potential of human-sensor-like inputs for realistic motion generation in avatars.

Abstract: The way we perceive the world fundamentally shapes how we move, whether it is
how we navigate in a room or how we interact with other humans. Current human
motion generation methods, neglect this interdependency and use task-specific
``perception'' that differs radically from that of humans. We argue that the
generation of human-like avatar behavior requires human-like perception.
Consequently, in this work we present CLOPS, the first human avatar that solely
uses egocentric vision to perceive its surroundings and navigate. Using vision
as the primary driver of motion however, gives rise to a significant challenge
for training avatars: existing datasets have either isolated human motion,
without the context of a scene, or lack scale. We overcome this challenge by
decoupling the learning of low-level motion skills from learning of high-level
control that maps visual input to motion. First, we train a motion prior model
on a large motion capture dataset. Then, a policy is trained using Q-learning
to map egocentric visual inputs to high-level control commands for the motion
prior. Our experiments empirically demonstrate that egocentric vision can give
rise to human-like motion characteristics in our avatars. For example, the
avatars walk such that they avoid obstacles present in their visual field.
These findings suggest that equipping avatars with human-like sensors,
particularly egocentric vision, holds promise for training avatars that behave
like humans.

</details>


### [230] [OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps](https://arxiv.org/abs/2509.19282)
*Bingnan Li,Chen-Yu Wang,Haiyang Xu,Xiang Zhang,Ethan Armand,Divyansh Srivastava,Xiaojun Shan,Zeyuan Chen,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: This paper addresses the challenges in layout-to-image generation when handling overlapping bounding boxes and introduces tools and a model to tackle these issues.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of current layout-to-image generation methods, which perform poorly with significant overlaps in bounding boxes, and to highlight the shortcomings of existing benchmarks.

Method: The authors propose a novel metric called OverLayScore to quantify bounding box overlap complexity, introduce a benchmark (OverLayBench) with balanced overlap complexity, and fine-tune a model, CreatiLayout-AM, for better performance under these conditions.

Result: The analysis shows that current benchmarks favor simpler cases, while OverLayBench provides a fairer testbed for model evaluation. CreatiLayout-AM improves performance on layouts with significant overlaps.

Conclusion: This research establishes tools and models to address layout overlap challenges, setting the stage for further advances in layout-to-image generation.

Abstract: Despite steady progress in layout-to-image generation, current methods still
struggle with layouts containing significant overlap between bounding boxes. We
identify two primary challenges: (1) large overlapping regions and (2)
overlapping instances with minimal semantic distinction. Through both
qualitative examples and quantitative analysis, we demonstrate how these
factors degrade generation quality. To systematically assess this issue, we
introduce OverLayScore, a novel metric that quantifies the complexity of
overlapping bounding boxes. Our analysis reveals that existing benchmarks are
biased toward simpler cases with low OverLayScore values, limiting their
effectiveness in evaluating model performance under more challenging
conditions. To bridge this gap, we present OverLayBench, a new benchmark
featuring high-quality annotations and a balanced distribution across different
levels of OverLayScore. As an initial step toward improving performance on
complex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a
curated amodal mask dataset. Together, our contributions lay the groundwork for
more robust layout-to-image generation under realistic and challenging
scenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.

</details>


### [231] [Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation](https://arxiv.org/abs/2509.19296)
*Sherwin Bahmani,Tianchang Shen,Jiawei Ren,Jiahui Huang,Yifeng Jiang,Haithem Turki,Andrea Tagliasacchi,David B. Lindell,Zan Gojcic,Sanja Fidler,Huan Ling,Jun Gao,Xuanchi Ren*

Main category: cs.CV

TL;DR: This paper presents a self-distillation framework integrating video diffusion models with 3D Gaussian Splatting (3DGS) representations for real-time 3D scene generation, eliminating the need for multi-view training data.


<details>
  <summary>Details</summary>
Motivation: The motivation is the need for generating virtual environments for broad applications like robotics and autonomous driving, overcoming limitations of current methods requiring multi-view real-world data.

Method: The method involves augmenting an RGB decoder with a 3DGS decoder, trained using synthetic data generated by video diffusion models without requiring multi-view training data.

Result: The framework achieves state-of-the-art results in static and dynamic 3D scene generation, leveraging monocular input video capabilities.

Conclusion: This approach introduces a practical way to synthesize and render 3D scenes, advancing 3D scene generation technologies without reliance on multi-view training datasets.

Abstract: The ability to generate virtual environments is crucial for applications
ranging from gaming to physical AI domains such as robotics, autonomous
driving, and industrial AI. Current learning-based 3D reconstruction methods
rely on the availability of captured real-world multi-view data, which is not
always readily available. Recent advancements in video diffusion models have
shown remarkable imagination capabilities, yet their 2D nature limits the
applications to simulation where a robot needs to navigate and interact with
the environment. In this paper, we propose a self-distillation framework that
aims to distill the implicit 3D knowledge in the video diffusion models into an
explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for
multi-view training data. Specifically, we augment the typical RGB decoder with
a 3DGS decoder, which is supervised by the output of the RGB decoder. In this
approach, the 3DGS decoder can be purely trained with synthetic data generated
by video diffusion models. At inference time, our model can synthesize 3D
scenes from either a text prompt or a single image for real-time rendering. Our
framework further extends to dynamic 3D scene generation from a monocular input
video. Experimental results show that our framework achieves state-of-the-art
performance in static and dynamic 3D scene generation.

</details>


### [232] [VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction](https://arxiv.org/abs/2509.19297)
*Weijie Wang,Yeqing Chen,Zeyu Zhang,Hengyu Liu,Haoxiao Wang,Zhiyuan Feng,Wenkang Qin,Zheng Zhu,Donny Y. Chen,Bohan Zhuang*

Main category: cs.CV

TL;DR: VolSplat presents an enhancement for novel view synthesis by using voxel-aligned Gaussians instead of pixel-aligned ones, addressing issues of dependency on input views, view bias, and alignment errors, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The main motivation is to address limitations in existing feed-forward 3D Gaussian Splatting methods, primarily their dependency on the number of input views, bias in density distributions, and alignment errors caused by occlusions or low texture.

Method: The method replaces pixel alignment with voxel alignment for Gaussian predictions. It predicts 3D Gaussians directly from a 3D voxel grid, avoiding reliance on error-prone 2D feature matching and offering adaptive density control based on scene complexity.

Result: VolSplat demonstrates state-of-the-art performance with improved geometric consistency and rendering quality on benchmarks like RealEstate10K and ScanNet, producing denser and more plausible Gaussian reconstructions.

Conclusion: The proposed VolSplat framework addresses fundamental limitations of prior approaches, offering a robust and scalable feed-forward 3D reconstruction method that enhances multi-view consistency and paves the way for further advancements in the field.

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective
solution for novel view synthesis. Existing methods predominantly rely on a
pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a
3D Gaussian. We rethink this widely adopted formulation and identify several
inherent limitations: it renders the reconstructed 3D models heavily dependent
on the number of input views, leads to view-biased density distributions, and
introduces alignment errors, particularly when source views contain occlusions
or low texture. To address these challenges, we introduce VolSplat, a new
multi-view feed-forward paradigm that replaces pixel alignment with
voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D
voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature
matching, ensuring robust multi-view consistency. Furthermore, it enables
adaptive control over Gaussian density based on 3D scene complexity, yielding
more faithful Gaussian point clouds, improved geometric consistency, and
enhanced novel-view rendering quality. Experiments on widely used benchmarks
including RealEstate10K and ScanNet demonstrate that VolSplat achieves
state-of-the-art performance while producing more plausible and view-consistent
Gaussian reconstructions. In addition to superior results, our approach
establishes a more scalable framework for feed-forward 3D reconstruction with
denser and more robust representations, paving the way for further research in
wider communities. The video results, code and trained models are available on
our project page: https://lhmd.top/volsplat.

</details>


### [233] [CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching](https://arxiv.org/abs/2509.19300)
*Chen Chen,Pengsheng Guo,Liangchen Song,Jiasen Lu,Rui Qian,Xinze Wang,Tsu-Jui Fu,Wei Liu,Yinfei Yang,Alex Schwing*

Main category: cs.CV

TL;DR: The paper introduces CAR-Flow, a conditional-aware reparameterization method to improve conditional generative modeling efficiency and accuracy by relocating probability distributions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge faced by generative models in learning mass transport and conditional injection simultaneously, which adds complexity and slows training.

Method: The proposed CAR-Flow uses a lightweight, learned shift to adjust the source, target, or both distributions, effectively shortening the probability path the model needs to learn.

Result: CAR-Flow enhances performance on ImageNet-256, reducing FID from 2.07 to 1.68 with minimal parameter overhead (<0.6%), and demonstrates benefits on synthetic and high-dimensional image data.

Conclusion: CAR-Flow simplifies training for conditional generative models by optimizing distribution paths, improving both training speed and model performance.

Abstract: Conditional generative modeling aims to learn a conditional data distribution
from samples containing data-condition pairs. For this, diffusion and
flow-based methods have attained compelling results. These methods use a
learned (flow) model to transport an initial standard Gaussian noise that
ignores the condition to the conditional data distribution. The model is hence
required to learn both mass transport and conditional injection. To ease the
demand on the model, we propose Condition-Aware Reparameterization for Flow
Matching (CAR-Flow) -- a lightweight, learned shift that conditions the source,
the target, or both distributions. By relocating these distributions, CAR-Flow
shortens the probability path the model must learn, leading to faster training
in practice. On low-dimensional synthetic data, we visualize and quantify the
effects of CAR. On higher-dimensional natural image data (ImageNet-256),
equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while
introducing less than 0.6% additional parameters.

</details>


### [234] [Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models](https://arxiv.org/abs/2509.15156)
*Haobo Yang,Minghao Guo,Dequan Yang,Wenyu Wang*

Main category: cs.CV

TL;DR: The paper explores the integration of perceptual psychology principles, specifically geometric visual illusions, into deep learning image classification models. This approach improves generalization in complex visual tasks.


<details>
  <summary>Details</summary>
Motivation: Contemporary deep learning predominantly relies on statistical patterns in data. However, structured insights from perceptual psychology, such as visual illusions, remain underutilized despite their potential to enhance model generalization and structural understanding.

Method: The authors developed a synthetic dataset based on geometric visual illusions and tested three multi-source learning strategies that incorporated illusion recognition alongside ImageNet classification objectives.

Result: Adding geometric illusion-based auxiliary supervision improved model generalization, especially for challenging visual contexts. Both CNNs and transformers showed enhanced sensitivity to structural details when perceptual biases were embedded.

Conclusion: Integrating perceptual science insights like geometric illusions into machine learning models leads to improved structural sensitivity and generalization, paving the way for innovative methods of embedding perceptual priors into vision model design.

Abstract: Contemporary deep learning models have achieved impressive performance in
image classification by primarily leveraging statistical regularities within
large datasets, but they rarely incorporate structured insights drawn directly
from perceptual psychology. To explore the potential of perceptually motivated
inductive biases, we propose integrating classic geometric visual illusions
well-studied phenomena from human perception into standard image-classification
training pipelines. Specifically, we introduce a synthetic, parametric
geometric-illusion dataset and evaluate three multi-source learning strategies
that combine illusion recognition tasks with ImageNet classification
objectives. Our experiments reveal two key conceptual insights: (i)
incorporating geometric illusions as auxiliary supervision systematically
improves generalization, especially in visually challenging cases involving
intricate contours and fine textures; and (ii) perceptually driven inductive
biases, even when derived from synthetic stimuli traditionally considered
unrelated to natural image recognition, can enhance the structural sensitivity
of both CNN and transformer-based architectures. These results demonstrate a
novel integration of perceptual science and machine learning and suggest new
directions for embedding perceptual priors into vision model design.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [235] [Bridging Simulation and Silicon: A Study of RISC-V Hardware and FireSim Simulation](https://arxiv.org/abs/2509.18472)
*Atanu Barai,Kamalavasan Kamalakkannan,Patrick Diehl,Maxim Moraru,Jered Dominguez-Trujillo,Howard Pritchard,Nandakishore Santhi,Farzad Fatollahi-Fard,Galen Shipman*

Main category: cs.DC

TL;DR: This paper evaluates FireSim—a tool for simulating RISC-V system configurations—and compares its simulated results with actual hardware to examine discrepancies and performance prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: RISC-V processors are gaining traction in HPC, but tools like FireSim lack systematic evaluations of their accuracy in performance predictions compared to real hardware.

Method: The authors modeled a single-board computer and a desktop-grade RISC-V CPU using FireSim. Benchmarks were measured and compared for single-core and four-core setups, followed by evaluating a mini-application and LAMMPS molecular dynamics code in both simulated and real environments.

Result: FireSim provides useful insights into performance trends but shows discrepancies in runtimes when compared to physical hardware due to simulation environment limitations and incomplete CPU performance specifications.

Conclusion: FireSim is valuable for architectural exploration, though prediction accuracy needs refinement due to limitations in simulation fidelity and configuration matching challenges.

Abstract: RISC-V ISA-based processors have recently emerged as both powerful and
energy-efficient computing platforms. The release of the MILK-V Pioneer marked
a significant milestone as the first desktop-grade RISC-V system. With
increasing engagement from both academia and industry, such platforms exhibit
strong potential for adoption in high-performance computing (HPC) environments.
  The open-source, FPGA-accelerated FireSim framework has emerged as a flexible
and scalable tool for architectural exploration, enabling simulation of various
system configurations using RISC-V cores. Despite its capabilities, there
remains a lack of systematic evaluation regarding the feasibility and
performance prediction accuracy of FireSim when compared to physical hardware.
  In this study, we address this gap by modeling a commercially available
single-board computer and a desktop-grade RISC-V CPU within FireSim. To ensure
fidelity between simulation and real hardware, we first measure the performance
of a series of benchmarks to compare runtime behavior under single-core and
four-core configurations. Based on the closest matching simulation parameters,
we subsequently evaluate performance using a representative mini-application
and the LAMMPS molecular dynamics code.
  Our findings indicate that while FireSim provides valuable insights into
architectural performance trends, discrepancies remain between simulated and
measured runtimes. These deviations stem from both inherent limitations of the
simulation environment and the restricted availability of detailed performance
specifications from CPU manufacturers, which hinder precise configuration
matching.

</details>


### [236] [6G Twin: Hybrid Gaussian Radio Fields for Channel Estimation and Non-Linear Precoder Design for Radio Access Networks](https://arxiv.org/abs/2509.18735)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: The paper introduces 6G Twin, an AI-native radio access network design focused on efficient CSI acquisition, mobility management, and energy-optimal precoding.


<details>
  <summary>Details</summary>
Motivation: To address challenges in modern wireless networks such as pilot overhead reduction, accurate prediction during mobility and transitions, and energy-efficient communication.

Method: The paper proposes three components: (1) neural Gaussian Radio Fields (GRF) for sparse and compressed CSI acquisition, (2) a replay-driven continual learning mechanism for channel prediction during handovers, and (3) an energy-optimal nonlinear precoder called minPMAC.

Result: The approach reduces pilot overhead by ~100x, enhances channel prediction accuracy by 10+ dB under mobility and handovers, and achieves up to 5x higher data rates at comparable power or significantly lower power consumption.

Conclusion: This AI-native framework enables real-time CSI acquisition, robust mobility tracking, and energy-efficient communication while achieving significant performance improvements in dynamic wireless networks.

Abstract: This work introduces 6G Twin, the first end-to-end artificial intelligence
(AI)-native radio access network (RAN) design that unifies (i) neural Gaussian
Radio Fields (GRF) for compressed channel state information (CSI) acquisition,
(ii) continual channel prediction with handover persistence, and (iii) an
energy-optimal nonlinear precoder (minPMAC). GRF replaces dense pilots with a
sparse Gaussian field, cutting pilot overhead by about 100x while delivering
1.1 ms inference and less than 2 minutes on-site training, thus enabling
millisecond-scale closed-loop operation. A replay-driven continual learner
sustains accuracy under mobility and cell transitions, improving channel
normalized mean square error (NMSE) by more than 10 dB over frozen predictors
and an additional 2-5 dB over uniform replay, thereby stabilizing performance
across UMi/UMa handovers. Finally, minPMAC solves a convex, order-free MAC
precoder design that recovers the globally optimal order from Broadcast Channel
(BC) duals and minimizes transmit energy subject to minimum-rate guarantees,
achieving 4-10 times lower energy (scenario dependent) with monotonically
increasing bits per joule as SNR grows. This translates to up to 5 times higher
data rate at comparable power or the same rates at substantially lower power.
Together, these components form a practical, GPU-ready framework that attains
real-time CSI, robust tracking in dynamic networks with efficient handovers,
and state-of-the-art throughput-energy tradeoffs under 3GPP-style settings.

</details>


### [237] [On The Reproducibility Limitations of RAG Systems](https://arxiv.org/abs/2509.18869)
*Baiqiang Wang,Dongfang Zhao,Nathan R Tallent,Luanzheng Guo*

Main category: cs.DC

TL;DR: ReproRAG is a framework that benchmarks reproducibility in retrieval-augmented generation systems, accounting for various uncertainties in the pipeline.


<details>
  <summary>Details</summary>
Motivation: To address the issue of non-determinism and unreliable reproducibility in retrieval-augmented generation systems used in scientific workflows.

Method: A benchmarking framework named ReproRAG is introduced, which uses metrics (e.g., Exact Match Rate, Jaccard Similarity, Kendall's Tau) to analyze the reproducibility of RAG systems across factors like embedding models, precision, and hardware.

Result: An extensive empirical study showed that embedding model variety significantly impacts RAG reproducibility, highlighting critical trade-offs between reproducibility and performance.

Conclusion: ReproRAG equips researchers with tools to validate, benchmark, and design reproducible scientific AI systems, promoting trustworthiness in AI-driven science.

Abstract: Retrieval-Augmented Generation (RAG) is increasingly employed in generative
AI-driven scientific workflows to integrate rapidly evolving scientific
knowledge bases, yet its reliability is frequently compromised by
non-determinism in their retrieval components. This paper introduces ReproRAG,
a comprehensive benchmarking framework designed to systematically measure and
quantify the reproducibility of vector-based retrieval systems. ReproRAG
investigates sources of uncertainty across the entire pipeline, including
different embedding models, precision, retrieval algorithms, hardware
configurations, and distributed execution environments. Utilizing a suite of
metrics, such as Exact Match Rate, Jaccard Similarity, and Kendall's Tau, the
proposed framework effectively characterizes the trade-offs between
reproducibility and performance. Our large-scale empirical study reveals
critical insights; for instance, we observe that different embedding models
have remarkable impact on RAG reproducibility. The open-sourced ReproRAG
framework provides researchers and engineers productive tools to validate
deployments, benchmark reproducibility, and make informed design decisions,
thereby fostering more trustworthy AI for science.

</details>


### [238] [TD3-Sched: Learning to Orchestrate Container-based Cloud-Edge Resources via Distributed Reinforcement Learning](https://arxiv.org/abs/2509.18957)
*Shengye Song,Minxian Xu,Kan Hu,Wenxia Guo,Kejiang Ye*

Main category: cs.DC

TL;DR: The paper introduces TD3-Sched, a distributed reinforcement learning scheduler using TD3 for optimizing resource allocation in cloud-edge environments, reducing latency and improving stability.


<details>
  <summary>Details</summary>
Motivation: Current resource scheduling in cloud-edge systems faces challenges due to tight resource constraints and limitations of centralized approaches. Improved distributed solutions are needed.

Method: The researchers developed TD3-Sched, a distributed reinforcement learning scheduler based on the TD3 method, for fine-tuned CPU and memory allocation under dynamic workloads.

Result: In tests using realistic setups (e.g., SockShop application and Alibaba traces), TD3-Sched reduced latency by 17.9%-38.6% under normal loads and 16%-31.6% under high loads, with only 0.47% SLO violations.

Conclusion: TD3-Sched offers faster convergence, decreased latency, stable performance, and enhanced service quality, making it a significant improvement over existing baselines in cloud-edge resource scheduling.

Abstract: Resource scheduling in cloud-edge systems is challenging as edge nodes run
latency-sensitive workloads under tight resource constraints, while existing
centralized schedulers can suffer from performance bottlenecks and user
experience degradation. To address the issues of distributed decisions in
cloud-edge environments, we present TD3-Sched, a distributed reinforcement
learning (DRL) scheduler based on Twin Delayed Deep Deterministic Policy
Gradient (TD3) for continuous control of CPU and memory allocation, which can
achieve optimized decisions for resource provisioning under dynamic workloads.
On a realistic cloud-edge testbed with SockShop application and Alibaba traces,
TD3-Sched achieves reductions of 17.9% to 38.6% in latency under same loads
compared with other reinforcement-learning and rule-based baselines, and 16% to
31.6% under high loads. TD3-Sched also shows superior Service Level Objective
(SLO) compliance with only 0.47% violations. These results indicate faster
convergence, lower latency, and more stable performance while preserving
service quality in container-based cloud-edge environment compared with the
baselines.

</details>


### [239] [Scheduler-Driven Job Atomization](https://arxiv.org/abs/2509.19086)
*Michal Konopa,Jan Fesl,Ladislav Beránek*

Main category: cs.DC

TL;DR: The paper introduces Scheduler-Driven Job Atomization (SJA), a paradigm that enhances GPU cluster efficiency by enabling jobs to create subjobs to fit execution gaps advertised by the scheduler.


<details>
  <summary>Details</summary>
Motivation: GPU clusters face inefficiencies due to rigid job structures and reliance on static peak memory estimates, leading to low utilization and high fragmentation.

Method: SJA establishes a scheduler-job interaction where the scheduler advertises available execution gaps. Jobs generate subjobs tailored to fit these slots, avoiding preemption and migration overhead.

Result: No experimental evaluation is provided; the paper focuses on conceptualizing a model and defining its components.

Conclusion: SJA aims to align jobs with real-time opportunities to optimize GPU utilization and reduce inefficiencies, serving as a promising framework for future research.

Abstract: Modern GPU clusters, particularly those built on NVIDIA's Multi-Instance GPU
(MIG) architecture, often suffer from inefficiencies because jobs are treated
as rigid, indivisible blocks that occupy a fixed slice until completion. The
reliance on static peak memory estimates exacerbates fragmentation,
underutilization, and job rejections. We propose Scheduler-Driven Job
Atomization (SJA), a new paradigm that establishes a bidirectional interaction
between scheduler and jobs. In SJA, the scheduler advertises available
execution gaps, and jobs respond by signaling interest if they can potentially
generate a subjob that fits the offered time-capacity window. The scheduler may
collect multiple signals for the same slot and, based on its allocation policy
(e.g., fairness, efficiency, or SLA priorities), selects which job is granted
the slot. Only then does the chosen job materialize a safe, self-contained
subjob tailored to that opportunity. Unlike migration or preemption, SJA
proactively shapes workloads before execution, thereby avoiding costly state
transfers and unpredictable interruptions. It aims to increase GPU utilization,
reduce wait times, and minimize migration overhead by aligning jobs with
opportunities in real time, ensuring that each admitted subjob is correct by
construction. This paper is presented as a concept paper: it introduces the
paradigm, defines its building blocks, and outlines future research directions,
rather than offering a full experimental evaluation.

</details>


### [240] [In-Transit Data Transport Strategies for Coupled AI-Simulation Workflow Patterns](https://arxiv.org/abs/2509.19150)
*Harikrishna Tummalapalli,Riccardo Balin,Christine M. Simpson,Andrew Park,Aymen Alsaadi,Andrew E. Shao,Wesley Brewer,Shantenu Jha*

Main category: cs.DC

TL;DR: SimAI-Bench was developed as a tool for prototyping and evaluating coupled AI-simulation workflows. It was used to benchmark data transport strategies on the Aurora supercomputer, revealing strengths and bottlenecks in different approaches.


<details>
  <summary>Details</summary>
Motivation: The complexity of coupled AI-simulation workflows used in HPC facilities necessitates tools for performance analysis and prototyping to improve efficiency.

Method: The authors implemented SimAI-Bench to benchmark data transport performance of two workflow patterns on the Aurora supercomputer: one-to-one (co-located simulation and AI training) and many-to-one (ensemble of simulations feeding a single AI model).

Result: For the one-to-one pattern, node-local and DragonHPC strategies outperformed Redis and Lustre file systems. For the many-to-one pattern, data transport became a bottleneck with larger ensemble sizes, showing file systems to be optimal among tested strategies.

Conclusion: SimAI-Bench effectively prototypes and benchmarks coupled workflows, demonstrating the necessity of appropriate data transport strategies for different workflow patterns in HPC environments.

Abstract: Coupled AI-Simulation workflows are becoming the major workloads for HPC
facilities, and their increasing complexity necessitates new tools for
performance analysis and prototyping of new in-situ workflows. We present
SimAI-Bench, a tool designed to both prototype and evaluate these coupled
workflows. In this paper, we use SimAI-Bench to benchmark the data transport
performance of two common patterns on the Aurora supercomputer: a one-to-one
workflow with co-located simulation and AI training instances, and a
many-to-one workflow where a single AI model is trained from an ensemble of
simulations. For the one-to-one pattern, our analysis shows that node-local and
DragonHPC data staging strategies provide excellent performance compared Redis
and Lustre file system. For the many-to-one pattern, we find that data
transport becomes a dominant bottleneck as the ensemble size grows. Our
evaluation reveals that file system is the optimal solution among the tested
strategies for the many-to-one pattern.

</details>


### [241] [Non-Uniform Content-Oblivious Leader Election on Oriented Asynchronous Rings](https://arxiv.org/abs/2509.19187)
*Jérémie Chalopin,Yi-Jun Chang,Lyuting Chen,Giuseppe A. Di Luna,Haoran Zhou*

Main category: cs.DC

TL;DR: This paper investigates leader election in ring networks under content-oblivious asynchronous systems, proposing algorithms with reduced message complexity and analyzing uniform and non-uniform settings.


<details>
  <summary>Details</summary>
Motivation: Understanding message complexity in leader election on ring networks, especially under content corruption by adversaries, is crucial for developing resilient and efficient algorithms.

Method: The authors analyze limitations of uniform algorithms and propose non-uniform and randomized approaches. They assess message complexity with identifiers and ring size bounds, optimizing algorithm efficiency.

Result: The study shows that uniform algorithms can't work with fixed message counts, while non-uniform algorithms achieve lower message complexity. A randomized algorithm for anonymous settings performs with high success probability.

Conclusion: The paper advances the field by providing efficient leader election algorithms for different settings, addressing identifier dependence, uniform limitations, and enhancing message efficiency in both anonymous and identifier-based systems.

Abstract: We study the leader election problem in oriented ring networks under
content-oblivious asynchronous message-passing systems, where an adversary may
arbitrarily corrupt message contents.
  Frei et al. (DISC 2024) presented a uniform terminating leader election
algorithm for oriented rings in this setting, with message complexity $O(n
\cdot \mathsf{ID}_{\max})$ on a ring of size $n$, where $\mathsf{ID}_{\max}$ is
the largest identifier in the system, this result has been recently extended by
Chalopin et al. (DISC 2025) to unoriented rings.
  In this paper, we investigate the message complexity of leader election on
ring networks in the content-oblivious model, showing that no uniform algorithm
can solve the problem if each process is limited to sending a constant number
of messages in one direction.
  Interestingly, this limitation hinges on the uniformity assumption. In the
non-uniform setting, where processes know an upper bound $U \geq n$ on the ring
size, we present an algorithm with message complexity $O(n \cdot U \cdot
\mathsf{ID}_{\min})$, in which each process sends $O(U \cdot
\mathsf{ID}_{\min})$ messages clockwise and only three messages
counter-clockwise. Here, $\mathsf{ID}_{\min}$ is the smallest identifier in the
system. This dependence on the identifiers compares favorably with the
dependence on $\mathsf{ID}_{\max}$ of Frei et al.
  We also show a non-uniform algorithm where each process sends $O(U \cdot
\log\mathsf{ID}_{\min})$ messages in one direction and
$O(\log\mathsf{ID}_{\min})$ in the other. The factor $\log \mathsf{ID}_{\min}$
is optimal, matching the lower bound of Frei et al.
  Finally, in the anonymous setting, where processes do not have identifiers,
we propose a randomized algorithm where each process sends only $O(\log^2 U)$
messages, with a success probability of $1 - U^{-c}$.

</details>


### [242] [Accelerating Gravitational $N$-Body Simulations Using the RISC-V-Based Tenstorrent Wormhole](https://arxiv.org/abs/2509.19294)
*Jenny Lynn Almerol,Elisabetta Boella,Mario Spera,Daniele Gregori*

Main category: cs.DC

TL;DR: This paper explores the use of a RISC-V-based accelerator for accelerating astrophysical $N$-body simulations, showing significant speedup and energy savings.


<details>
  <summary>Details</summary>
Motivation: To investigate the potential of RISC-V-based accelerators for high-performance scientific computing, particularly astrophysical simulations with $N$-body codes.

Method: The authors accelerated an astrophysical $N$-body code on the RISC-V-based Wormhole n300 card and evaluated its performance against a highly optimized CPU implementation.

Result: The RISC-V platform delivered over $2 \times$ speedup and approximately $2 \times$ energy savings compared to the CPU implementation.

Conclusion: RISC-V-based accelerators like the Wormhole n300 card demonstrate strong potential as competitive platforms for scientific workloads such as astrophysical $N$-body simulations.

Abstract: Although originally developed primarily for artificial intelligence
workloads, RISC-V-based accelerators are also emerging as attractive platforms
for high-performance scientific computing. In this work, we present our
approach to accelerating an astrophysical $N$-body code on the RISC-V-based
Wormhole n300 card developed by Tenstorrent. Our results show that this
platform can be highly competitive for astrophysical simulations employing this
class of algorithms, delivering more than a $2 \times$ speedup and
approximately $2 \times$ energy savings compared to a highly optimized CPU
implementation of the same code.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [243] [Machine Learnability as a Measure of Order in Aperiodic Sequences](https://arxiv.org/abs/2509.18103)
*Jennifer Dodgson,Michael Joedhitya,Adith Ramdas,Surender Suresh Kumar,Adarsh Singh Chauhan,Akira Rafhael,Wang Mingshu,Nordine Lotfi*

Main category: cs.LG

TL;DR: This study explores the application of machine learning to examine the comparative regularity of prime number distributions on an Ulam spiral. It shows that regions with higher numbers exhibit more learnable order than lower-number regions, suggesting a connection between machine learning performance and mathematical conjectures about prime distributions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate how machine learning models can detect and analyze structured patterns within the seemingly irregular behavior of prime number distributions and understand these patterns' implications for number theory and cryptography.

Method: The authors used image-focused machine learning models trained on blocks of Ulam spiral regions. They compared model performance based on regions centered around higher-order (500 million) versus lower-order (25 million) integers.

Result: The models demonstrated higher accuracy when trained on regions with higher numbers. The analysis highlighted different focus areas for the models, with lower-number regions emphasizing prime identification and higher-number regions emphasizing composite elimination.

Conclusion: The findings suggest that the regularity of prime distributions increases at higher magnitudes and align with number theory conjectures about diminishing randomness. The study highlights machine learning as a promising tool to investigate prime number patterns and their cryptographic significance.

Abstract: Research on the distribution of prime numbers has revealed a dual character:
deterministic in definition yet exhibiting statistical behavior reminiscent of
random processes. In this paper we show that it is possible to use an
image-focused machine learning model to measure the comparative regularity of
prime number fields at specific regions of an Ulam spiral. Specifically, we
demonstrate that in pure accuracy terms, models trained on blocks extracted
from regions of the spiral in the vicinity of 500m outperform models trained on
blocks extracted from the region representing integers lower than 25m. This
implies existence of more easily learnable order in the former region than in
the latter. Moreover, a detailed breakdown of precision and recall scores seem
to imply that the model is favouring a different approach to classification in
different regions of the spiral, focusing more on identifying prime patterns at
lower numbers and more on eliminating composites at higher numbers. This aligns
with number theory conjectures suggesting that at higher orders of magnitude we
should see diminishing noise in prime number distributions, with averages
(density, AP equidistribution) coming to dominate, while local randomness
regularises after scaling by log x. Taken together, these findings point toward
an interesting possibility: that machine learning can serve as a new
experimental instrument for number theory. Notably, the method shows potential
1 for investigating the patterns in strong and weak primes for cryptographic
purposes.

</details>


### [244] [Data Valuation and Selection in a Federated Model Marketplace](https://arxiv.org/abs/2509.18104)
*Wenqian Li,Youjia Yang,Ruoxi Jia,Yan Pang*

Main category: cs.LG

TL;DR: This paper proposes a framework using a Wasserstein-based estimator to address data valuation and selection challenges in Federated Learning (FL) marketplaces, ensuring privacy and enabling efficient data usage.


<details>
  <summary>Details</summary>
Motivation: To address challenges in establishing trustworthy Federated Learning (FL) data marketplaces, particularly data valuation and selection from heterogeneous data sources, while ensuring data privacy.

Method: The paper designs a framework that employs a Wasserstein-based estimator for model performance prediction and incorporates a distributed method to calculate Wasserstein distance without accessing raw data. It also leverages the neural scaling law for reliable performance extrapolation.

Result: Extensive experiments on various scenarios demonstrate that the proposed approach effectively identifies high-performing data combinations while addressing data heterogeneity challenges.

Conclusion: The study provides a robust methodology for enabling functional and reliable FL-based data marketplaces, ensuring both data privacy and enhanced model performance through effective data selection.

Abstract: In the era of Artificial Intelligence (AI), marketplaces have become
essential platforms for facilitating the exchange of data products to foster
data sharing. Model transactions provide economic solutions in data
marketplaces that enhance data reusability and ensure the traceability of data
ownership. To establish trustworthy data marketplaces, Federated Learning (FL)
has emerged as a promising paradigm to enable collaborative learning across
siloed datasets while safeguarding data privacy. However, effective data
valuation and selection from heterogeneous sources in the FL setup remain key
challenges. This paper introduces a comprehensive framework centered on a
Wasserstein-based estimator tailored for FL. The estimator not only predicts
model performance across unseen data combinations but also reveals the
compatibility between data heterogeneity and FL aggregation algorithms. To
ensure privacy, we propose a distributed method to approximate Wasserstein
distance without requiring access to raw data. Furthermore, we demonstrate that
model performance can be reliably extrapolated under the neural scaling law,
enabling effective data selection without full-scale training. Extensive
experiments across diverse scenarios, such as label skew, mislabeled, and
unlabeled sources, show that our approach consistently identifies
high-performing data combinations, paving the way for more reliable FL-based
model marketplaces.

</details>


### [245] [Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices](https://arxiv.org/abs/2509.18118)
*Marcelo Ribeiro,Diogo Costa,Gonçalo Moreira,Sandro Pinto,Tiago Gomes*

Main category: cs.LG

TL;DR: This paper extends the lightweight L-SGD optimization algorithm for machine learning training on resource-constrained RISC-V-based microcontrollers and proposes an 8-bit quantized version to improve performance.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of decentralized machine learning training directly on IoT devices, particularly on RISC-V microcontrollers, without relying on cloud services.

Method: Extend L-SGD optimization algorithm to RISC-V microcontrollers and introduce an 8-bit quantized version to reduce memory usage and training time.

Result: The 8-bit quantized L-SGD led to nearly 4x reduction in memory usage and 2.2x faster training time on RISC-V, with minimal accuracy losses.

Conclusion: Quantized optimization algorithms like 8-bit L-SGD can enable efficient on-device training for IoT devices, supporting the transition to decentralized machine learning on RISC-V platforms.

Abstract: Modern IoT devices increasingly rely on machine learning solutions to process
data locally. However, the lack of graphics processing units (GPUs) or
dedicated accelerators on most platforms makes on-device training largely
infeasible, often requiring cloud-based services to perform this task. This
procedure often raises privacy-related concerns, and creates dependency on
reliable and always-on connectivity. Federated Learning (FL) is a new trend
that addresses these issues by enabling decentralized and collaborative
training directly on devices, but it requires highly efficient optimization
algorithms. L-SGD, a lightweight variant of stochastic gradient descent, has
enabled neural network training on Arm Cortex-M Microcontroller Units (MCUs).
This work extends L-SGD to RISC-V-based MCUs, an open and emerging architecture
that still lacks robust support for on-device training. L-SGD was evaluated on
both Arm and RISC-V platforms using 32-bit floating-point arithmetic,
highlighting the performance impact of the absence of Floating-Point Units
(FPUs) in RISC-V MCUs. To mitigate these limitations, we introduce an 8-bit
quantized version of L-SGD for RISC-V, which achieves nearly 4x reduction in
memory usage and a 2.2x speedup in training time, with negligible accuracy
degradation.

</details>


### [246] [BULL-ODE: Bullwhip Learning with Neural ODEs and Universal Differential Equations under Stochastic Demand](https://arxiv.org/abs/2509.18105)
*Nachiket N. Naik,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: The study investigates the interplay between structured and fully learned neural models for forecasting inventory dynamics under various demand regimes. It finds structured models (UDE) to outperform unstructured ones (NODE) under stable demands, while NODE excels in extreme demand scenarios.


<details>
  <summary>Details</summary>
Motivation: To understand whether enforcing structural bias aids or hinders forecasts in supply chain inventory systems under different stochastic demand regimes.

Method: Comparison between two modeling approaches: Neural ODE (NODE) and Universal Differential Equation (UDE). The NODE is fully learned, whereas the UDE incorporates domain constraints with residual learning. Evaluation is based on multi-step forecasts using three demand regimes.

Result: UDE performs better under light-tailed, correlated demands (AR(1) and Gaussian regimes), with significant drops in RMSE compared to NODE. In heavy-tailed lognormal demand, NODE demonstrates superior flexibility.

Conclusion: Structural bias improves forecasting when demand is stable or light-tailed but is disadvantageous under extreme, heavy-tailed events. This hybrid modeling guidance applies beyond supply chains to broader scientific systems.

Abstract: We study learning of continuous-time inventory dynamics under stochastic
demand and quantify when structure helps or hurts forecasting of the bullwhip
effect. BULL-ODE compares a fully learned Neural ODE (NODE) that models the
entire right-hand side against a physics-informed Universal Differential
Equation (UDE) that preserves conservation and order-up-to structure while
learning a small residual policy term. Classical supply chain models explain
the bullwhip through control/forecasting choices and information sharing, while
recent physics-informed and neural differential equation methods blend domain
constraints with learned components. It is unclear whether structural bias
helps or hinders forecasting under different demand regimes. We address this by
using a single-echelon testbed with three demand regimes - AR(1)
(autocorrelated), i.i.d. Gaussian, and heavy-tailed lognormal. Training is done
on varying fractions of each trajectory, followed by evaluation of multi-step
forecasts for inventory I, order rate O, and demand D. Across the structured
regimes, UDE consistently generalizes better: with 90% of the training horizon,
inventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96
to 0.95 under Gaussian demand. Under heavy-tailed lognormal shocks, the
flexibility of NODE is better. These trends persist as train18 ing data
shrinks, with NODE exhibiting phase drift in extrapolation while UDE remains
stable but underreacts to rare spikes. Our results provide concrete guidance:
enforce structure when noise is light-tailed or temporally correlated; relax
structure when extreme events dominate. Beyond inventory control, the results
offer guidance for hybrid modeling in scientific and engineering systems:
enforce known structure when conservation laws and modest noise dominate, and
relax structure to capture extremes in settings where rare events drive
dynamics.

</details>


### [247] [Model-Based Transfer Learning for Real-Time Damage Assessment of Bridge Networks](https://arxiv.org/abs/2509.18106)
*Elisa Tomassini,Enrique García-Macías,Filippo Ubertini*

Main category: cs.LG

TL;DR: The paper proposes a transfer learning method for bridge monitoring using model-based neural networks, validated with real data to improve scalability and precision.


<details>
  <summary>Details</summary>
Motivation: To address scalability challenges in monitoring large bridge networks by enabling knowledge transfer between structures with similar characteristics.

Method: A model-based transfer learning method using neural network surrogate models, validated through real-world data and integrated with Bayesian inference.

Result: The approach demonstrated high sensitivity to damage characteristics, improving real-time monitoring and enabling cross-structure knowledge sharing.

Conclusion: The proposed framework supports scalable and resilient monitoring strategies for large bridge networks, offering improved efficiency and accuracy.

Abstract: The growing use of permanent monitoring systems has increased data
availability, offering new opportunities for structural assessment but also
posing scalability challenges, especially across large bridge networks.
Managing multiple structures requires tracking and comparing long-term
behaviour efficiently. To address this, knowledge transfer between similar
structures becomes essential. This study proposes a model-based transfer
learning approach using neural network surrogate models, enabling a model
trained on one bridge to be adapted to another with similar characteristics.
These models capture shared damage mechanisms, supporting a scalable and
generalizable monitoring framework. The method was validated using real data
from two bridges. The transferred model was integrated into a Bayesian
inference framework for continuous damage assessment based on modal features
from monitoring data. Results showed high sensitivity to damage location,
severity, and extent. This approach enhances real-time monitoring and enables
cross-structure knowledge transfer, promoting smart monitoring strategies and
improved resilience at the network level.

</details>


### [248] [AdaMixT: Adaptive Weighted Mixture of Multi-Scale Expert Transformers for Time Series Forecasting](https://arxiv.org/abs/2509.18107)
*Huanyao Zhang,Jiaye Lin,Wentao Zhang,Haitao Yuan,Guoliang Li*

Main category: cs.LG

TL;DR: The paper proposes AdaMixT, a transformative architecture for multivariate time series forecasting, focusing on multi-scale feature fusion and adaptive weight allocation among expert models.


<details>
  <summary>Details</summary>
Motivation: Current methods for multivariate time series forecasting face limitations in capturing complex patterns due to reliance on predefined single-scale patches and insufficient feature fusion mechanisms.

Method: AdaMixT employs multi-scale feature extraction using both general and domain-specific models. A gating network dynamically allocates weights among expert models to achieve adaptive fusion.

Result: Experiments across eight benchmark datasets, including Weather, Traffic, and Electricity, show AdaMixT consistently achieving high performance in real-world applications.

Conclusion: AdaMixT improves forecasting accuracy by addressing feature heterogeneity and enhancing multi-scale pattern recognition, reinforcing its practicality for diverse applications.

Abstract: Multivariate time series forecasting involves predicting future values based
on historical observations. However, existing approaches primarily rely on
predefined single-scale patches or lack effective mechanisms for multi-scale
feature fusion. These limitations hinder them from fully capturing the complex
patterns inherent in time series, leading to constrained performance and
insufficient generalizability. To address these challenges, we propose a novel
architecture named Adaptive Weighted Mixture of Multi-Scale Expert Transformers
(AdaMixT). Specifically, AdaMixT introduces various patches and leverages both
General Pre-trained Models (GPM) and Domain-specific Models (DSM) for
multi-scale feature extraction. To accommodate the heterogeneity of temporal
features, AdaMixT incorporates a gating network that dynamically allocates
weights among different experts, enabling more accurate predictions through
adaptive multi-scale fusion. Comprehensive experiments on eight widely used
benchmarks, including Weather, Traffic, Electricity, ILI, and four ETT
datasets, consistently demonstrate the effectiveness of AdaMixT in real-world
scenarios.

</details>


### [249] [Solve it with EASE](https://arxiv.org/abs/2509.18108)
*Adam Viktorin,Tomas Kadavy,Jozef Kovac,Michal Pluhacek,Roman Senkerik*

Main category: cs.LG

TL;DR: EASE is a modular framework that uses large language models for creating algorithms and solutions in a controlled iterative environment.


<details>
  <summary>Details</summary>
Motivation: The paper aims to simplify the process of generating, testing, and evaluating algorithms using LLMs, providing a platform for collaborative and controlled iterative development.

Method: EASE incorporates functionalities for generation, testing, analysis, and evaluation of solutions, utilizing an iterative feedback loop with multiple roles assigned to LLMs.

Result: The framework delivers a transparent, extensible, and reproducible system for algorithm co-design, supporting diverse applications.

Conclusion: EASE streamlines collaborative generative solution design while abstracting the complexities of LLM management, benefiting researchers and practitioners.

Abstract: This paper presents EASE (Effortless Algorithmic Solution Evolution), an
open-source and fully modular framework for iterative algorithmic solution
generation leveraging large language models (LLMs). EASE integrates generation,
testing, analysis, and evaluation into a reproducible feedback loop, giving
users full control over error handling, analysis, and quality assessment. Its
architecture supports the orchestration of multiple LLMs in complementary
roles-such as generator, analyst, and evaluator. By abstracting the complexity
of prompt design and model management, EASE provides a transparent and
extensible platform for researchers and practitioners to co-design algorithms
and other generative solutions across diverse domains.

</details>


### [250] [Machine Learning-Based Classification of Vessel Types in Straits Using AIS Tracks](https://arxiv.org/abs/2509.18109)
*Jonatan Katz Nielsen*

Main category: cs.LG

TL;DR: This paper introduces a machine-learning pipeline to classify vessel types from AIS data in the Bornholm Strait, achieving high accuracy with tree-based models.


<details>
  <summary>Details</summary>
Motivation: Improving vessel type recognition for safety oversight and detection of illegal activities through AIS data analysis.

Method: Historical AIS data was processed using techniques like feature extraction, outlier removal, train/test splits by MMSI, and classification using Random Forest models with SMOTE.

Result: The pipeline achieved high accuracy (92.15%) and macro-F1 (93.27%) in classifying vessel types, with bridge-position ratio and maximum SOG being the most influential features.

Conclusion: The study demonstrates the feasibility of real-time vessel classification, suggesting further improvements like DBSCAN segmentation and gradient-boosted ensembles.

Abstract: Accurate recognition of vessel types from Automatic Identification System
(AIS) tracks is essential for safety oversight and combating illegal,
unreported, and unregulated (IUU) activity. This paper presents a strait-scale,
machine-learning pipeline that classifies moving vessels using only AIS data.
We analyze eight days of historical AIS from the Danish Maritime Authority
covering the Bornholm Strait in the Baltic Sea (January 22-30, 2025). After
forward/backward filling voyage records, removing kinematic and geospatial
outliers, and segmenting per-MMSI tracks while excluding stationary periods
($\ge 1$ h), we derive 31 trajectory-level features spanning kinematics (e.g.,
SOG statistics), temporal, geospatial (Haversine distances, spans), and
ship-shape attributes computed from AIS A/B/C/D reference points (length,
width, aspect ratio, bridge-position ratio). To avoid leakage, we perform
grouped train/test splits by MMSI and use stratified 5-fold cross-validation.
Across five classes (cargo, tanker, passenger, high-speed craft, fishing;
N=1{,}910 trajectories; test=382), tree-based models dominate: a Random Forest
with SMOTE attains 92.15% accuracy (macro-precision 94.11%, macro-recall
92.51%, macro-F1 93.27%) on the held-out test set, while a tuned RF reaches
one-vs-rest ROC-AUC up to 0.9897. Feature-importance analysis highlights the
bridge-position ratio and maximum SOG as the most discriminative signals;
principal errors occur between cargo and tanker, reflecting similar transit
behavior. We demonstrate operational value by backfilling missing ship types on
unseen data and discuss improvements such as DBSCAN based trip segmentation and
gradient-boosted ensembles to handle frequent-stop ferries and further lift
performance. The results show that lightweight features over AIS trajectories
enable real-time vessel type classification in straits.

</details>


### [251] [Probabilistic Geometric Principal Component Analysis with application to neural data](https://arxiv.org/abs/2509.18469)
*Han-Lin Hsieh,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: This paper introduces Probabilistic Geometric Principal Component Analysis (PGPCA), a novel dimensionality reduction method that extends PPCA to datasets distributed around nonlinear manifolds.


<details>
  <summary>Details</summary>
Motivation: Traditional linear methods like PPCA struggle with datasets distributed on nonlinear geometries, which are common in neuroscience and similar fields.

Method: The paper develops PGPCA, incorporating nonlinear manifold geometry explicitly. It also introduces a data-driven EM algorithm for parameter learning.

Result: Simulations and brain data analyses show PGPCA effectively models datasets around various manifolds, outperforming PPCA.

Conclusion: PGPCA enhances dimensionality reduction by capturing both manifold structure and noise, making it crucial for analyzing high-dimensional, noisy nonlinear data.

Abstract: Dimensionality reduction is critical across various domains of science
including neuroscience. Probabilistic Principal Component Analysis (PPCA) is a
prominent dimensionality reduction method that provides a probabilistic
approach unlike the deterministic approach of PCA and serves as a connection
between PCA and Factor Analysis (FA). Despite their power, PPCA and its
extensions are mainly based on linear models and can only describe the data in
a Euclidean coordinate system. However, in many neuroscience applications, data
may be distributed around a nonlinear geometry (i.e., manifold) rather than
lying in the Euclidean space. We develop Probabilistic Geometric Principal
Component Analysis (PGPCA) for such datasets as a new dimensionality reduction
algorithm that can explicitly incorporate knowledge about a given nonlinear
manifold that is first fitted from these data. Further, we show how in addition
to the Euclidean coordinate system, a geometric coordinate system can be
derived for the manifold to capture the deviations of data from the manifold
and noise. We also derive a data-driven EM algorithm for learning the PGPCA
model parameters. As such, PGPCA generalizes PPCA to better describe data
distributions by incorporating a nonlinear manifold geometry. In simulations
and brain data analyses, we show that PGPCA can effectively model the data
distribution around various given manifolds and outperforms PPCA for such data.
Moreover, PGPCA provides the capability to test whether the new geometric
coordinate system better describes the data than the Euclidean one. Finally,
PGPCA can perform dimensionality reduction and learn the data distribution both
around and on the manifold. These capabilities make PGPCA valuable for
enhancing the efficacy of dimensionality reduction for analysis of
high-dimensional data that exhibit noise and are distributed around a nonlinear
manifold.

</details>


### [252] [Localized PCA-Net Neural Operators for Scalable Solution Reconstruction of Elliptic PDEs](https://arxiv.org/abs/2509.18110)
*Mrigank Dhingra,Romit Maulik,Adil Rasheed,Omer San*

Main category: cs.LG

TL;DR: The paper introduces a patch-based PCA-Net framework for reducing computational complexity in solving PDEs using neural operators. It achieves faster processing with high accuracy compared to global PCA.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the computational inefficiencies of traditional PCA methods when applied to high-dimensional solution fields in PDEs.

Method: The framework decomposes solution fields into patches, applies PCA locally within patches, and trains neural operators in reduced PCA space. Two patch approaches (local-to-global and local-to-local) and refinements such as overlapping patches and CNN-based two-step processes are assessed.

Result: Patch-based PCA reduces computational complexity while ensuring high accuracy, achieving a processing time improvement of 3.7 to 4 times compared to global PCA.

Conclusion: Patch-based PCA offers an efficient and accurate solution for operator learning in PDE-based systems and mitigates computational challenges in high-dimensional problems.

Abstract: Neural operator learning has emerged as a powerful approach for solving
partial differential equations (PDEs) in a data-driven manner. However,
applying principal component analysis (PCA) to high-dimensional solution fields
incurs significant computational overhead. To address this, we propose a
patch-based PCA-Net framework that decomposes the solution fields into smaller
patches, applies PCA within each patch, and trains a neural operator in the
reduced PCA space. We investigate two different patch-based approaches that
balance computational efficiency and reconstruction accuracy: (1)
local-to-global patch PCA, and (2) local-to-local patch PCA. The trade-off
between computational cost and accuracy is analyzed, highlighting the
advantages and limitations of each approach. Furthermore, within each approach,
we explore two refinements for the most computationally efficient method: (i)
introducing overlapping patches with a smoothing filter and (ii) employing a
two-step process with a convolutional neural network (CNN) for refinement. Our
results demonstrate that patch-based PCA significantly reduces computational
complexity while maintaining high accuracy, reducing end-to-end pipeline
processing time by a factor of 3.7 to 4 times compared to global PCA, thefore
making it a promising technique for efficient operator learning in PDE-based
systems.

</details>


### [253] [A Coopetitive-Compatible Data Generation Framework for Cross-silo Federated Learning](https://arxiv.org/abs/2509.18120)
*Thanh Linh Nguyen,Quoc-Viet Pham*

Main category: cs.LG

TL;DR: This paper introduces CoCoGen, a framework that addresses challenges in cross-silo federated learning involving statistical heterogeneity and economic competition, using generative AI and game theory.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of collaborative AI training among organizations that are rivals, aiming to mitigate utility loss due to economic competition and improve system-wide social welfare.

Method: The authors propose CoCoGen, which combines generative AI and potential game theory to model, analyze, and optimize collaborative learning considering both statistical heterogeneity and economic competition.

Result: Experiments using the Fashion-MNIST dataset show that CoCoGen performs better than baseline methods, demonstrating the impact of heterogeneity and competition on organizational behavior.

Conclusion: CoCoGen provides an effective framework for enabling coopetitive cross-silo federated learning, enhancing both collaboration and individual organizations' utility under complex competitive conditions.

Abstract: Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or
banks) to collaboratively train artificial intelligence (AI) models while
preserving data privacy by keeping data local. While prior work has primarily
addressed statistical heterogeneity across organizations, a critical challenge
arises from economic competition, where organizations may act as market rivals,
making them hesitant to participate in joint training due to potential utility
loss (i.e., reduced net benefit). Furthermore, the combined effects of
statistical heterogeneity and inter-organizational competition on
organizational behavior and system-wide social welfare remain underexplored. In
this paper, we propose CoCoGen, a coopetitive-compatible data generation
framework, leveraging generative AI (GenAI) and potential game theory to model,
analyze, and optimize collaborative learning under heterogeneous and
competitive settings. Specifically, CoCoGen characterizes competition and
statistical heterogeneity through learning performance and utility-based
formulations and models each training round as a weighted potential game. We
then derive GenAI-based data generation strategies that maximize social
welfare. Experimental results on the Fashion-MNIST dataset reveal how varying
heterogeneity and competition levels affect organizational behavior and
demonstrate that CoCoGen consistently outperforms baseline methods.

</details>


### [254] [Prompt Optimization Meets Subspace Representation Learning for Few-shot Out-of-Distribution Detection](https://arxiv.org/abs/2509.18111)
*Faizul Rakib Sayem,Shahana Ibrahim*

Main category: cs.LG

TL;DR: This paper introduces a novel framework using context optimization and subspace representation learning to enhance OOD detection for vision-language models.


<details>
  <summary>Details</summary>
Motivation: Existing OOD detection methods utilizing VLMs rely primarily on softmax probabilities, missing out on the potential of learned feature embeddings. This limitation undercuts their performance in open-world environments.

Method: The paper proposes a CoOp-based framework that integrates subspace representation learning with prompt tuning. The method separates ID and OOD features through projection into spaces spanned by prompt vectors and orthogonal null spaces, respectively.

Result: The proposed framework demonstrates effectiveness in experiments conducted on real-world datasets, yielding strong OOD detection performance and high ID classification accuracy.

Conclusion: The framework improves ID-OOD separability using VLMs and offers an efficient end-to-end learning criterion for reliable OOD detection.

Abstract: The reliability of artificial intelligence (AI) systems in open-world
settings depends heavily on their ability to flag out-of-distribution (OOD)
inputs unseen during training. Recent advances in large-scale vision-language
models (VLMs) have enabled promising few-shot OOD detection frameworks using
only a handful of in-distribution (ID) samples. However, existing prompt
learning-based OOD methods rely solely on softmax probabilities, overlooking
the rich discriminative potential of the feature embeddings learned by VLMs
trained on millions of samples. To address this limitation, we propose a novel
context optimization (CoOp)-based framework that integrates subspace
representation learning with prompt tuning. Our approach improves ID-OOD
separability by projecting the ID features into a subspace spanned by prompt
vectors, while projecting ID-irrelevant features into an orthogonal null space.
To train such OOD detection framework, we design an easy-to-handle end-to-end
learning criterion that ensures strong OOD detection performance as well as
high ID classification accuracy. Experiments on real-world datasets showcase
the effectiveness of our approach.

</details>


### [255] [Large language models surpass domain-specific architectures for antepartum electronic fetal monitoring analysis](https://arxiv.org/abs/2509.18112)
*Sheng Wong,Ravi Shankar,Beth Albert,Gabriel Davis Jones*

Main category: cs.LG

TL;DR: This study evaluates and compares AI approaches, including LLMs and CTG-specific architectures, for automated antepartum CTG analysis, finding that fine-tuned LLMs outperform others.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges of subjective and variable clinical interpretations in electronic fetal monitoring (EFM)/cardiotocography (CTG) assessment, which requires sophisticated analysis of complex time-series data.

Method: The researchers conducted a comprehensive comparison of various AI methodologies, including time-series FMs, LLMs, and CTG-specific architectures, using over 500 real-world CTG recordings.

Result: Fine-tuned LLMs demonstrate superior performance compared to both foundation models and CTG-specific domain approaches in CTG analysis.

Conclusion: Fine-tuned LLMs provide a promising alternative method for clinical CTG interpretation, offering insights for future AI applications in prenatal care.

Abstract: Foundation models (FMs) and large language models (LLMs) demonstrate
remarkable capabilities across diverse domains through training on massive
datasets. These models have demonstrated exceptional performance in healthcare
applications, yet their potential for electronic fetal monitoring
(EFM)/cardiotocography (CTG) analysis, a critical technology for evaluating
fetal well-being, remains largely underexplored. Antepartum CTG interpretation
presents unique challenges due to the complex nature of fetal heart rate (FHR)
patterns and uterine activity, requiring sophisticated analysis of long
time-series data. The assessment of CTG is heavily based on subjective clinical
interpretation, often leading to variability in diagnostic accuracy and
deviation from timely pregnancy care. This study presents the first
comprehensive comparison of state-of-the-art AI approaches for automated
antepartum CTG analysis. We systematically compare time-series FMs and LLMs
against established CTG-specific architectures. Our evaluation encompasses over
500 CTG recordings of varying durations reflecting real-world clinical
recordings, providing robust performance benchmarks across different modelling
paradigms. Our results demonstrate that fine-tuned LLMs achieve superior
performance compared to both foundation models and domain-specific approaches,
offering a promising alternative pathway for clinical CTG interpretation. These
findings provide critical insights into the relative strengths of different AI
methodologies for fetal monitoring applications and establish a foundation for
future clinical AI development in prenatal care.

</details>


### [256] [A Study of Skews, Imbalances, and Pathological Conditions in LLM Inference Deployment on GPU Clusters detectable from DPU](https://arxiv.org/abs/2509.18114)
*Javed I. Khan an Henry Uwabor Moye*

Main category: cs.LG

TL;DR: This paper discusses enhancing large language model (LLM) inference efficiency using Data Processing Units (DPUs) to detect and mitigate load imbalances across multi-GPU tensor-parallel setups.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing runtime inefficiencies, particularly load imbalance and latency issues, during the decode phase of autoregressive inference in large transformer-based LLMs.

Method: The proposed method leverages BlueField-3 Data Processing Units (DPUs) to offload monitoring tasks and analyze GPU telemetry and inter-node communication patterns to provide actionable feedback to inference controllers and schedulers.

Result: The framework effectively detects and mitigates load imbalances in multi-node tensor-parallel setups, improving inference performance.

Conclusion: Using DPUs for real-time load balancing in large-scale LLM inference is a promising approach to enhance computational efficiency and mitigate performance skews in multi-GPU systems.

Abstract: Autoregressive inference in large transformer-based language models (LLMs)
presents significant challenges for runtime efficiency, particularly during the
decode phase where load imbalance across GPU shards can cause throughput
degradation and latency spikes. A DPU-assisted framework leveraged by
BlueField-3 Data Processing Units can enable real-time detection and mitigation
of load imbalance in multi-node tensor-parallel inference. By offloading
monitoring tasks to the DPU and analyzing GPU telemetry and inter-node
communication patterns, the resulting system can provide actionable feedback to
inference controllers and schedulers. The goal of this study is three-fold i)
identify the reported skews/imbalances/pathological conditions that arise in
muti-GPU execution of a) LLM tensor computing (both during training and
inference), b) identify their impact on computational performance, and c) make
a critical assessment if those can be tracked for potential mitigation from a
DPU network.

</details>


### [257] [FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy Federated Learning in Healthcare AI](https://arxiv.org/abs/2509.19120)
*Ferdinand Kahenga,Antoine Bagula,Sajal K. Das,Patrick Sello*

Main category: cs.LG

TL;DR: The paper introduces FedFiTS, a new framework for Federated Learning (FL) that addresses challenges like non-IID data, client unreliability, and adversarial attacks using trust and fairness-aware mechanisms.


<details>
  <summary>Details</summary>
Motivation: To overcome persistent challenges in Federated Learning deployments, especially in sensitive domains like healthcare, which involve non-IID data, unreliable clients, and adversarial manipulations.

Method: FedFiTS employs a three-phase strategy combining free-for-all training, natural selection, and slotted team participation. It uses dynamic client scoring, adaptive thresholding, and cohort-based scheduling to ensure efficient convergence and robustness.

Result: Experiments on diverse datasets demonstrate that FedFiTS outperforms existing baseline methods (e.g., FedAvg) in terms of accuracy, time-to-target, and resistance to poisoning attacks.

Conclusion: FedFiTS enhances scalable and secure FL through trust-aware aggregation and fairness-focused client selection, making it ideal for real-world applications in healthcare and cross-domain scenarios.

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for
privacy-preserving model training, yet deployments in sensitive domains such as
healthcare face persistent challenges from non-IID data, client unreliability,
and adversarial manipulation. This paper introduces FedFiTS, a trust and
fairness-aware selective FL framework that advances the FedFaSt line by
combining fitness-based client election with slotted aggregation. FedFiTS
implements a three-phase participation strategy-free-for-all training, natural
selection, and slotted team participation-augmented with dynamic client
scoring, adaptive thresholding, and cohort-based scheduling to balance
convergence efficiency with robustness. A theoretical convergence analysis
establishes bounds for both convex and non-convex objectives under standard
assumptions, while a communication-complexity analysis shows reductions
relative to FedAvg and other baselines. Experiments on diverse datasets-medical
imaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular
agricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently
outperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and
resilience to poisoning attacks. By integrating trust-aware aggregation with
fairness-oriented client selection, FedFiTS advances scalable and secure FL,
making it well suited for real-world healthcare and cross-domain deployments.

</details>


### [258] [Towards Scalable and Structured Spatiotemporal Forecasting](https://arxiv.org/abs/2509.18115)
*Hongyi Chen,Xiucheng Li,Xinyang Chen,Jing Li,Kehai Chen,Liqiang Nie*

Main category: cs.LG

TL;DR: The paper introduces a Spatial Balance Attention block to improve spatiotemporal forecasting by balancing local and global spatial correlations through subgraph partitioning and multiscale analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to effectively capture both local and global spatial correlations in spatiotemporal forecasting tasks while ensuring scalability and ease of implementation.

Method: The proposed method involves partitioning spatial graphs into subgraphs and applying Intra-subgraph Attention for local correlations and Inter-subgraph Attention for global correlations. Additionally, a multiscale model progressively increases subgraph scales.

Result: The model demonstrates up to 7.7% performance improvement over baseline methods in experimental evaluation on real-world spatiotemporal datasets.

Conclusion: The Spatial Balance Attention block and the associated multiscale model are efficient, scalable, and successful at capturing structured spatial correlations, significantly enhancing forecasting performance.

Abstract: In this paper, we propose a novel Spatial Balance Attention block for
spatiotemporal forecasting. To strike a balance between obeying spatial
proximity and capturing global correlation, we partition the spatial graph into
a set of subgraphs and instantiate Intra-subgraph Attention to learn local
spatial correlation within each subgraph; to capture the global spatial
correlation, we further aggregate the nodes to produce subgraph representations
and achieve message passing among the subgraphs via Inter-subgraph Attention.
Building on the proposed Spatial Balance Attention block, we develop a
multiscale spatiotemporal forecasting model by progressively increasing the
subgraph scales. The resulting model is both scalable and able to produce
structured spatial correlation, and meanwhile, it is easy to implement. We
evaluate its efficacy and efficiency against the existing models on real-world
spatiotemporal datasets from medium to large sizes. The experimental results
show that it can achieve performance improvements up to 7.7% over the baseline
methods at low running costs.

</details>


### [259] [KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots](https://arxiv.org/abs/2509.18141)
*Yao Zhao,Haoyue Sun,Yantian Ding,Yanxun Xu*

Main category: cs.LG

TL;DR: KM-GPT is a fully automated AI-powered tool for reconstructing individual patient data (IPD) from Kaplan-Meier plots, streamlining evidence synthesis in clinical research.


<details>
  <summary>Details</summary>
Motivation: Existing methods for reconstructing IPD rely on manual processes that are error-prone and limited in scalability, necessitating an automated solution.

Method: KM-GPT combines advanced image preprocessing, multi-modal GPT-5 reasoning, and iterative reconstruction algorithms. It is equipped with a web interface and an AI assistant for ease of use.

Result: The tool demonstrated superior accuracy and was successfully applied in meta-analysis cases, exemplified by gastric cancer immunotherapy trials, showing its effectiveness.

Conclusion: KM-GPT significantly advances clinical research by automating IPD reconstruction, improving scalability, accuracy, and accessibility for evidence synthesis in trials.

Abstract: Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots
provides valuable insights for evidence synthesis in clinical research.
However, existing approaches often rely on manual digitization, which is
error-prone and lacks scalability. To address these limitations, we develop
KM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD
directly from KM plots with high accuracy, robustness, and reproducibility.
KM-GPT integrates advanced image preprocessing, multi-modal reasoning powered
by GPT-5, and iterative reconstruction algorithms to generate high-quality IPD
without manual input or intervention. Its hybrid reasoning architecture
automates the conversion of unstructured information into structured data flows
and validates data extraction from complex KM plots. To improve accessibility,
KM-GPT is equipped with a user-friendly web interface and an integrated AI
assistant, enabling researchers to reconstruct IPD without requiring
programming expertise. KM-GPT was rigorously evaluated on synthetic and
real-world datasets, consistently demonstrating superior accuracy. To
illustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer
immunotherapy trials, reconstructing IPD to facilitate evidence synthesis and
biomarker-based subgroup analyses. By automating traditionally manual processes
and providing a scalable, web-based solution, KM-GPT transforms clinical
research by leveraging reconstructed IPD to enable more informed downstream
analyses, supporting evidence-based decision-making.

</details>


### [260] [FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders for Robust Adaptation under Label Scarcity](https://arxiv.org/abs/2509.19220)
*Ferdinand Kahenga,Antoine Bagula,Patrick Sello,Sajal K. Das*

Main category: cs.LG

TL;DR: FedFusion addresses challenges in federated learning, such as heterogeneous feature spaces and label scarcity, by merging domain adaptation and frugal labeling strategies while maintaining performance under non-IID and real-world conditions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle practical challenges in federated learning, such as diverse feature spaces among clients, non-IID data distributions, and limited label availability.

Method: FedFusion introduces diversity-aware encoders, pseudo-labeling guided by teacher clients, domain adaptation, and a frugal-labelling pipeline for robust federated learning. It uses techniques like similarity-weighted classifier coupling and cluster-aware pairing.

Result: FedFusion achieves superior performance in accuracy, robustness, and fairness across benchmarks under IID, non-IID, and sparse-label regimes while maintaining communication and computation efficiency compared to state-of-the-art methods.

Conclusion: The framework demonstrates that integrating personalization, domain adaptation, and efficient labeling significantly enhances the robustness of federated learning in challenging real-world scenarios.

Abstract: Federated learning in practice must contend with heterogeneous feature
spaces, severe non-IID data, and scarce labels across clients. We present
FedFusion, a federated transfer-learning framework that unifies domain
adaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn,
DivEn-mix, DivEn-c). Labelled teacher clients guide learner clients via
confidence-filtered pseudo-labels and domain-adaptive transfer, while clients
maintain personalised encoders tailored to local data. To preserve global
coherence under heterogeneity, FedFusion employs similarity-weighted classifier
coupling (with optional cluster-wise averaging), mitigating dominance by
data-rich sites and improving minority-client performance. The frugal-labelling
pipeline combines self-/semi-supervised pretext training with selective
fine-tuning, reducing annotation demands without sharing raw data. Across
tabular and imaging benchmarks under IID, non-IID, and label-scarce regimes,
FedFusion consistently outperforms state-of-the-art baselines in accuracy,
robustness, and fairness while maintaining comparable communication and
computation budgets. These results show that harmonising personalisation,
domain adaptation, and label efficiency is an effective recipe for robust
federated learning under real-world constraints.

</details>


### [261] [Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization](https://arxiv.org/abs/2509.18116)
*Nathan Egbuna,Saatvik Gaur,Sunishchal Dev,Ashwinee Panda,Maheep Chaudhary*

Main category: cs.LG

TL;DR: Test-time optimization, traditionally computation-heavy, is made practical with Amortized Latent Steering (ALS), which achieves high efficiency with constant inference costs.


<details>
  <summary>Details</summary>
Motivation: To address the prohibitive computation costs associated with test-time optimization mechanisms such as iterative refinement and latent space optimization.

Method: ALS computes an offline vector that calibrates hidden states during inference without multiple backward passes, simplifying optimization to a constant cost operation.

Result: ALS delivers 2—5x speedup over traditional methods while achieving comparable or superior accuracy, with notable performance enhancements on GSM8K and MATH-500 benchmarks.

Conclusion: ALS demonstrates that latent optimization benefits can be captured efficiently offline, enabling high-performance reasoning techniques to become practically deployable.

Abstract: Test-time optimization remains impractical at scale due to prohibitive
inference costs\textemdash techniques like iterative refinement and multi-step
verification can require $10$--$100\times$ more compute per query than standard
decoding. Latent space test-time optimization methods like LatentSeek offer a
more direct approach by steering hidden representations, but still demand
expensive per-query optimization loops with multiple backward passes. We
propose Amortized Latent Steering (ALS), which collapses this iterative
optimization into a single offline-computed vector applied at constant cost
during inference. ALS computes the mean difference between hidden states from
successful versus unsuccessful generations, then uses this direction to
calibrate the model's hidden representations: when decoding drifts away from
the success manifold, ALS nudges activations back toward it. Across GSM8K and
MATH-$500$ benchmarks, ALS achieves $2$--$5\times$ speedup over iterative
methods while matching or surpassing greedy Chain-of-Thought (CoT) and
Self-Consistency baselines, yielding up to 101\% improvement in
efficiency--accuracy trade-off. These results show that much of latent
optimization's benefit can be captured offline, making sophisticated reasoning
techniques viable for production deployment. Code is available
at~\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}

</details>


### [262] [Robust and continuous machine learning of usage habits to adapt digital interfaces to user needs](https://arxiv.org/abs/2509.18117)
*Eric Petit,Denis Chêne*

Main category: cs.LG

TL;DR: Machine learning algorithm designs adaptive interfaces by modeling individual user behavior, focusing on dynamic learning and task representation.


<details>
  <summary>Details</summary>
Motivation: To address the need for interfaces capable of adapting to individual users and their evolving behaviors rather than relying on group-based preferences.

Method: Employing Bayesian statistics for online incremental learning to model individual user navigation and generate task models that adapt in real-time.

Result: Simulations demonstrate the effectiveness of the approach in both stable and changing environments.

Conclusion: The method advances adaptive systems, enhancing user experience with dynamic and personalized navigation interfaces.

Abstract: The paper presents a machine learning approach to design digital interfaces
that can dynamically adapt to different users and usage strategies. The
algorithm uses Bayesian statistics to model users' browsing behavior, focusing
on their habits rather than group preferences. It is distinguished by its
online incremental learning, allowing reliable predictions even with little
data and in the case of a changing environment. This inference method generates
a task model, providing a graphical representation of navigation with the usage
statistics of the current user. The algorithm learns new tasks while preserving
prior knowledge. The theoretical framework is described, and simulations show
the effectiveness of the approach in stationary and non-stationary
environments. In conclusion, this research paves the way for adaptive systems
that improve the user experience by helping them to better navigate and act on
their interface.

</details>


### [263] [MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents](https://arxiv.org/abs/2509.18119)
*Yifan Xu,Xiao Liu,Xinghan Liu,Jiaqi Fu,Hanchen Zhang,Bohao Jing,Shudan Zhang,Yuting Wang,Wenyi Zhao,Yuxiao Dong*

Main category: cs.LG

TL;DR: MOBILERL introduces a new framework for training mobile GUI agents using reinforcement learning tailored to handle tasks with varying difficulties, significantly improving performance.


<details>
  <summary>Details</summary>
Motivation: To address challenges in training mobile GUI agents caused by uneven task difficulty and inefficient environment sampling.

Method: The MOBILERL framework incorporates the ADAGRPO algorithm with techniques like difficulty-adaptive positive replay, failure curriculum filtering, and shortest path reward adjustment to stabilize and optimize training.

Result: Improvements achieved in success rates: 75.8% on AndroidWorld and 46.8% on AndroidLab, showcasing effectiveness across diverse apps and tasks.

Conclusion: MOBILERL demonstrates enhanced performance and efficiency for mobile GUI agents, providing a robust solution that is also available open-source for further adoption and development.

Abstract: Building general-purpose graphical user interface (GUI) agents has become
increasingly promising with the progress in vision language models. However,
developing effective mobile GUI agents with reinforcement learning (RL) remains
challenging due to the heavy-tailed distribution of task difficulty and the
inefficiency of large-scale environment sampling. We present an online agentic
reinforcement learning framework MOBILERL to enhance GUI agents in mobile
environments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO)
algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and
failure curriculum filtering to adapt the model to different task difficulties.
We introduce the shortest path reward adjustment strategy to reshape rewards
concerning the task length in multi-turn agentic tasks. Those strategies
jointly stabilize RL training, improve sample efficiency, and generate strong
performance across diverse mobile apps and tasks. We apply MOBILERL to two open
models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B
model achieves state-of-the-art results in terms of success rates on both
AndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted
in the AutoGLM products, and also open-sourced at
https://github.com/THUDM/MobileRL.

</details>


### [264] [Fast Linear Solvers via AI-Tuned Markov Chain Monte Carlo-based Matrix Inversion](https://arxiv.org/abs/2509.18452)
*Anton Lebedev,Won Kyung Lee,Soumyadip Ghosh,Olha I. Yaman,Vassilis Kalantzis,Yingdong Lu,Tomasz Nowicki,Shashanka Ubaru,Lior Horesh,Vassil Alexandrov*

Main category: cs.LG

TL;DR: The paper introduces an AI-based framework for optimizing Markov Chain Monte Carlo (MCMC) parameters, enhancing preconditioning for solving large, sparse linear systems using Krylov subspace solvers.


<details>
  <summary>Details</summary>
Motivation: Slow convergence in solving ill-conditioned large, sparse linear systems necessitates better preconditioning techniques for Krylov subspace solvers.

Method: The framework includes a graph neural surrogate for predicting preconditioning speed and a Bayesian acquisition function to optimize MCMC parameter selection.

Result: The framework improves preconditioning on ill-conditioned systems and reduces the search budget by 50% compared to conventional methods, decreasing required solver iterations by 10%.

Conclusion: AI-driven optimization of MCMC parameters offers an efficient pathway to improve Krylov subspace solver performance in large-scale systems.

Abstract: Large, sparse linear systems are pervasive in modern science and engineering,
and Krylov subspace solvers are an established means of solving them. Yet
convergence can be slow for ill-conditioned matrices, so practical deployments
usually require preconditioners. Markov chain Monte Carlo (MCMC)-based matrix
inversion can generate such preconditioners and accelerate Krylov iterations,
but its effectiveness depends on parameters whose optima vary across matrices;
manual or grid search is costly. We present an AI-driven framework recommending
MCMC parameters for a given linear system. A graph neural surrogate predicts
preconditioning speed from $A$ and MCMC parameters. A Bayesian acquisition
function then chooses the parameter sets most likely to minimise iterations. On
a previously unseen ill-conditioned system, the framework achieves better
preconditioning with 50\% of the search budget of conventional methods,
yielding about a 10\% reduction in iterations to convergence. These results
suggest a route for incorporating MCMC-based preconditioners into large-scale
systems.

</details>


### [265] [Prediction of Coffee Ratings Based On Influential Attributes Using SelectKBest and Optimal Hyperparameters](https://arxiv.org/abs/2509.18124)
*Edmund Agyemang,Lawrence Agbota,Vincent Agbenyeavu,Peggy Akabuah,Bismark Bimpong,Christopher Attafuah*

Main category: cs.LG

TL;DR: The research investigates using machine learning to predict coffee ratings based on textual and numerical attributes from user reviews. Ensemble methods and rigorous optimization demonstrated superior prediction results.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance coffee quality assessments by leveraging machine learning and combining user review data to complement traditional expert evaluations.

Method: Six supervised machine learning models were employed (including Random Forest, XGBoost) with preprocessing (text cleaning, TF-IDF, SelectKBest), followed by hyperparameter tuning, and evaluated using metrics like F1-score and AUC.

Result: Ensemble models and the Multi-layer Perceptron showed superior accuracy, outperforming simpler classifiers regarding F1-score, Gmean, and AUC metrics.

Conclusion: The study underscores the importance of data preprocessing, feature selection, and model optimization to create reliable systems for sensory product analysis such as coffee rating predictions.

Abstract: This study explores the application of supervised machine learning algorithms
to predict coffee ratings based on a combination of influential textual and
numerical attributes extracted from user reviews. Through careful data
preprocessing including text cleaning, feature extraction using TF-IDF, and
selection with SelectKBest, the study identifies key factors contributing to
coffee quality assessments. Six models (Decision Tree, KNearest Neighbors,
Multi-layer Perceptron, Random Forest, Extra Trees, and XGBoost) were trained
and evaluated using optimized hyperparameters. Model performance was assessed
primarily using F1-score, Gmean, and AUC metrics. Results demonstrate that
ensemble methods (Extra Trees, Random Forest, and XGBoost), as well as
Multi-layer Perceptron, consistently outperform simpler classifiers (Decision
Trees and K-Nearest Neighbors) in terms of evaluation metrics such as F1
scores, G-mean and AUC. The findings highlight the essence of rigorous feature
selection and hyperparameter tuning in building robust predictive systems for
sensory product evaluation, offering a data driven approach to complement
traditional coffee cupping by expertise of trained professionals.

</details>


### [266] [NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment](https://arxiv.org/abs/2509.18125)
*Harsha Koduri*

Main category: cs.LG

TL;DR: NurseSchedRL is a reinforcement learning framework designed to optimize nurse-patient assignments considering skills, fatigue, patient needs, and constraints, outperforming traditional scheduling methods.


<details>
  <summary>Details</summary>
Motivation: Healthcare systems struggle with efficiently allocating nursing resources amidst skill variability, patient demands, and staff burnout.

Method: Reinforcement learning using Proximal Policy Optimization (PPO) with structured encoding, constrained action masking, and attention-based skill representations.

Result: Improved scheduling efficiency, better skill-patient alignment, and reduced nurse fatigue in realistic simulations.

Conclusion: Reinforcement learning can significantly enhance healthcare workforce management in complex and constrained scenarios.

Abstract: Healthcare systems face increasing pressure to allocate limited nursing
resources efficiently while accounting for skill heterogeneity, patient acuity,
staff fatigue, and continuity of care. Traditional optimization and heuristic
scheduling methods struggle to capture these dynamic, multi-constraint
environments. I propose NurseSchedRL, a reinforcement learning framework for
nurse-patient assignment that integrates structured state encoding, constrained
action masking, and attention-based representations of skills, fatigue, and
geographical context. NurseSchedRL uses Proximal Policy Optimization (PPO) with
feasibility masks to ensure assignments respect real-world constraints, while
dynamically adapting to patient arrivals and varying nurse availability. In
simulation with realistic nurse and patient data, NurseSchedRL achieves
improved scheduling efficiency, better alignment of skills to patient needs,
and reduced fatigue compared to baseline heuristic and unconstrained RL
approaches. These results highlight the potential of reinforcement learning for
decision support in complex, high-stakes healthcare workforce management.

</details>


### [267] [Anomaly Detection in Electric Vehicle Charging Stations Using Federated Learning](https://arxiv.org/abs/2509.18126)
*Bishal K C,Amr Hilal,Pawan Thapa*

Main category: cs.LG

TL;DR: This paper evaluates the use of Federated Learning (FL) in enhancing security at IoT-based Electric Vehicle Charging Stations (EVCS), focusing on anomaly detection while addressing heterogeneity challenges.


<details>
  <summary>Details</summary>
Motivation: The study aims to secure IoT-based EV charging stations against cyber threats, considering privacy concerns of centralized systems and practical limitations in FL approaches such as data and system heterogeneity.

Method: Experiments were conducted using FL algorithms FedAvg and FedAvgM under both IID and non-IID data and system heterogeneity to evaluate their effectiveness in anomaly detection in EVCS.

Result: FedAvg performs better than centralized models under IID settings but its performance declines in non-IID and heterogeneous environments. FedAvgM outperforms FedAvg in these settings, achieving better convergence and detection accuracy.

Conclusion: Federated Learning is effective in dealing with heterogeneity in IoT-based EVCS without major performance sacrifices, with FedAvgM emerging as a robust and privacy-focused approach for EVCS security.

Abstract: Federated Learning (FL) is a decentralized training framework widely used in
IoT ecosystems that preserves privacy by keeping raw data local, making it
ideal for IoT-enabled cyber-physical systems with sensing and communication
like Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric
Vehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle
infrastructure, securing these IoT-based charging stations against cyber
threats has become critical. Centralized Intrusion Detection Systems (IDS)
raise privacy concerns due to sensitive network and user data, making FL a
promising alternative. However, current FL-based IDS evaluations overlook
practical challenges such as system heterogeneity and non-IID data. To address
these challenges, we conducted experiments to evaluate the performance of
federated learning for anomaly detection in EV charging stations under system
and data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization
approaches, to analyze their effectiveness in anomaly detection. Under IID
settings, FedAvg achieves superior performance to centralized models using the
same neural network. However, performance degrades with non-IID data and system
heterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous
settings, showing better convergence and higher anomaly detection accuracy. Our
results demonstrate that FL can handle heterogeneity in IoT-based EVCS without
significant performance loss, with FedAvgM as a promising solution for robust,
privacy-preserving EVCS security.

</details>


### [268] [Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework](https://arxiv.org/abs/2509.18127)
*Jiaqi Weng,Han Zheng,Hanyu Zhang,Qinqin He,Jialing Tao,Hui Xue,Zhixuan Chu,Xiting Wang*

Main category: cs.LG

TL;DR: This paper introduces Safe-SAIL, a framework for improving the safety of large language models (LLMs) by interpreting sparse autoencoder (SAE) features to identify and explain safety-related behaviors.


<details>
  <summary>Details</summary>
Motivation: The widespread use of large language models in real-world scenarios poses safety risks, necessitating tools for better understanding and addressing potentially harmful behaviors exhibited by these models.

Method: The proposed Safe-SAIL framework involves identifying sparse autoencoders (SAEs) with strong concept-specific interpretability, explaining their safety-related neurons, and optimizing the process to make interpretation more scalable.

Result: Safe-SAIL enhances the interpretation of atomic features in SAEs, focusing on safety-critical behaviors such as toxicity and regulation violations. It also introduces tools and methods for empirical safety risk analysis.

Conclusion: Safe-SAIL provides a pathway to better understand and mitigate the safety concerns surrounding large language models, supporting advancements in safe AI deployment.

Abstract: Increasing deployment of large language models (LLMs) in real-world
applications raises significant safety concerns. Most existing safety research
focuses on evaluating LLM outputs or specific safety tasks, limiting their
ability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs)
facilitate interpretability research to clarify model behavior by explaining
single-meaning atomic features decomposed from entangled signals. jHowever,
prior applications on SAEs do not interpret features with fine-grained
safety-related con- cepts, thus inadequately addressing safety-critical
behaviors, such as generating toxic responses and violating safety regu-
lations. For rigorous safety analysis, we must extract a rich and diverse set
of safety-relevant features that effectively capture these high-risk behaviors,
yet face two challenges: identifying SAEs with the greatest potential for
generating safety concept-specific neurons, and the prohibitively high cost of
detailed feature explanation. In this paper, we pro- pose Safe-SAIL, a
framework for interpreting SAE features within LLMs to advance mechanistic
understanding in safety domains. Our approach systematically identifies SAE
with best concept-specific interpretability, explains safety-related neurons,
and introduces efficient strategies to scale up the in- terpretation process.
We will release a comprehensive toolkit including SAE checkpoints and
human-readable neuron ex- planations, which supports empirical analysis of
safety risks to promote research on LLM safety.

</details>


### [269] [Diagonal Linear Networks and the Lasso Regularization Path](https://arxiv.org/abs/2509.18766)
*Raphaël Berthier*

Main category: cs.LG

TL;DR: The paper studies diagonal linear networks and analyzes their training trajectory, connecting it to the lasso regularization path.


<details>
  <summary>Details</summary>
Motivation: To explore and better understand the implicit regularization in diagonal linear networks and its relationship to the lasso regularization path.

Method: Theoretical analysis of training trajectories, rigorous results, and simulations under a monotonicity assumption.

Result: Established both exact and approximate connections between the training trajectory of diagonal linear networks and the lasso regularization path.

Conclusion: Training time in diagonal linear networks corresponds to an inverse regularization parameter, showcasing a close relationship with the lasso regularization path.

Abstract: Diagonal linear networks are neural networks with linear activation and
diagonal weight matrices. Their theoretical interest is that their implicit
regularization can be rigorously analyzed: from a small initialization, the
training of diagonal linear networks converges to the linear predictor with
minimal 1-norm among minimizers of the training loss. In this paper, we deepen
this analysis showing that the full training trajectory of diagonal linear
networks is closely related to the lasso regularization path. In this
connection, the training time plays the role of an inverse regularization
parameter. Both rigorous results and simulations are provided to illustrate
this conclusion. Under a monotonicity assumption on the lasso regularization
path, the connection is exact while in the general case, we show an approximate
connection.

</details>


### [270] [Accounting for Uncertainty in Machine Learning Surrogates: A Gauss-Hermite Quadrature Approach to Reliability Analysis](https://arxiv.org/abs/2509.18128)
*Amirreza Tootchi,Xiaoping Du*

Main category: cs.LG

TL;DR: The paper proposes a Gauss-Hermite quadrature approach to decouple epistemic and aleatory uncertainties in machine learning surrogates for reliability analysis, ensuring more accurate predictions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of combined epistemic (model approximation) and aleatory (input variability) uncertainties in machine learning models used for physics-based reliability analysis.

Method: A Gauss-Hermite quadrature approach combined with First and Second Order Reliability Methods to evaluate and integrate conditional failure probabilities, effectively decoupling nested uncertainties.

Result: The approach demonstrated computational efficiency and provided more accurate and trustworthy predictions in three test examples compared to traditional methods that neglect model uncertainty.

Conclusion: The proposed method successfully manages both types of uncertainties, offering a reliable and computationally efficient solution for reliability analysis using machine learning surrogates.

Abstract: Machine learning surrogates are increasingly employed to replace expensive
computational models for physics-based reliability analysis. However, their use
introduces epistemic uncertainty from model approximation errors, which couples
with aleatory uncertainty in model inputs, potentially compromising the
accuracy of reliability predictions. This study proposes a Gauss-Hermite
quadrature approach to decouple these nested uncertainties and enable more
accurate reliability analysis. The method evaluates conditional failure
probabilities under aleatory uncertainty using First and Second Order
Reliability Methods and then integrates these probabilities across realizations
of epistemic uncertainty. Three examples demonstrate that the proposed approach
maintains computational efficiency while yielding more trustworthy predictions
than traditional methods that ignore model uncertainty.

</details>


### [271] [Central Limit Theorems for Asynchronous Averaged Q-Learning](https://arxiv.org/abs/2509.18964)
*Xingtu Liu*

Main category: cs.LG

TL;DR: The paper derives central limit theorems for Polyak-Ruppert averaged Q-learning under asynchronous updates, focusing on convergence properties.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to understand the theoretical behavior of Q-learning under asynchronous updates, particularly by establishing convergence rates and functional properties.

Method: The authors present a non-asymptotic and functional analysis of Q-learning, examining Wasserstein distance, Brownian motion convergence, and their dependency on factors like iterations, state-action space, and discount factor.

Result: Central limit theorems are established, with explicit dependence on system parameters in the non-asymptotic regime and weak convergence in the functional sense.

Conclusion: The results provide a deeper theoretical understanding of Q-learning's convergence under asynchronous updates, with implications for performance and design optimizations.

Abstract: This paper establishes central limit theorems for Polyak-Ruppert averaged
Q-learning under asynchronous updates. We present a non-asymptotic central
limit theorem, where the convergence rate in Wasserstein distance explicitly
reflects the dependence on the number of iterations, state-action space size,
the discount factor, and the quality of exploration. In addition, we derive a
functional central limit theorem, showing that the partial-sum process
converges weakly to a Brownian motion.

</details>


### [272] [Research on Metro Transportation Flow Prediction Based on the STL-GRU Combined Model](https://arxiv.org/abs/2509.18130)
*Zijie Zhou,Huichen Ma*

Main category: cs.LG

TL;DR: This paper introduces a metro passenger flow prediction model integrating the STL decomposition method and GRU, demonstrating improved accuracy over existing models like LSTM and STL-LSTM.


<details>
  <summary>Details</summary>
Motivation: The study seeks to enhance metro passenger flow prediction to optimize operations and provide reliable support for intelligent transportation decisions.

Method: STL-GRU Model: Combines Seasonal and Trend decomposition using Loess (STL) with Gated Recurrent Unit (GRU). Key steps include data preprocessing, travel path identification using graph algorithms, STL decomposition, outlier correction using the 3σ principle, and training with the Keras deep learning library.

Result: The model achieves significantly better prediction accuracy compared to LSTM, GRU, and STL-LSTM, with MAPE reductions of 2.3, 1.36, and 6.42 percentage points for different day categories.

Conclusion: The STL-GRU model effectively improves metro transfer passenger flow prediction, providing a robust tool for intelligent transportation systems.

Abstract: In the metro intelligent transportation system, accurate transfer passenger
flow prediction is a key link in optimizing operation plans and improving
transportation efficiency. To further improve the theory of metro internal
transfer passenger flow prediction and provide more reliable support for
intelligent operation decisions, this paper innovatively proposes a metro
transfer passenger flow prediction model that integrates the Seasonal and Trend
decomposition using Loess (STL) method and Gated Recurrent Unit (GRU).In
practical application, the model first relies on the deep learning library
Keras to complete the construction and training of the GRU model, laying the
foundation for subsequent prediction; then preprocesses the original metro card
swiping data, uses the graph-based depth-first search algorithm to identify
passengers' travel paths, and further constructs the transfer passenger flow
time series; subsequently adopts the STL time series decomposition algorithm to
decompose the constructed transfer passenger flow time series into trend
component, periodic component and residual component, and uses the 3{\sigma}
principle to eliminate and fill the outliers in the residual component, and
finally completes the transfer passenger flow prediction.Taking the transfer
passenger flow data of a certain metro station as the research sample, the
validity of the model is verified. The results show that compared with Long
Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the combined model of
STL time series decomposition method and Long Short-Term Memory (STL-LSTM), the
STL-GRU combined prediction model significantly improves the prediction
accuracy of transfer passenger flow on weekdays (excluding Fridays), Fridays
and rest days, with the mean absolute percentage error (MAPE) of the prediction
results reduced by at least 2.3, 1.36 and 6.42 percentage points respectively.

</details>


### [273] [Two ways to knowledge?](https://arxiv.org/abs/2509.18131)
*Jean-Michel Tucny,Abhisek Ganguly,Santosh Ansumali,Sauro Succi*

Main category: cs.LG

TL;DR: Transformer-based machine learning applications exhibit weight matrices with random-like characteristics, lacking direct correlation to physical problem structures.


<details>
  <summary>Details</summary>
Motivation: To examine the relationship between weight matrices of transformer models and physical/mathematical structure in scientific problems.

Method: Analyzing weight matrices of transformer models applied to two representative physical problems, alongside conceptual parallels to path-integration techniques.

Result: Weight matrices appear random-like and have no straightforward link to physical or mathematical structures in the studied problems.

Conclusion: Machine learning and the scientific method are distinct paradigms; utilizing one without insight can be hazardous, and meaningful explainability in neural networks remains elusive.

Abstract: It is shown that the weight matrices of transformer-based machine learning
applications to the solution of two representative physical applications show a
random-like character which bears no directly recognizable link to the physical
and mathematical structure of the physical problem under study. This suggests
that machine learning and the scientific method may represent two distinct and
potentially complementary paths to knowledge, even though a strict notion of
explainability in terms of direct correspondence between network parameters and
physical structures may remain out of reach. It is also observed that drawing a
parallel between transformer operation and (generalized) path-integration
techniques may account for the random-like nature of the weights, but still
does not resolve the tension with explainability. We conclude with some general
comments on the hazards of gleaning knowledge without the benefit of Insight.

</details>


### [274] [DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment](https://arxiv.org/abs/2509.19104)
*Sharan Sahu,Martin T. Wells*

Main category: cs.LG

TL;DR: The paper introduces DRO-REBEL, a robust method to address overoptimization in RLHF for LLMs, showcasing theoretical guarantees and experimental robustness.


<details>
  <summary>Details</summary>
Motivation: Addressing overoptimization in reinforcement learning with human feedback (RLHF), where models overfit due to reward misalignment and behavioral drift.

Method: Proposes DRO-REBEL, relying on robust REBEL updates with Wasserstein, KL, and chi-squared divergence measures, deriving practical SGD algorithms for better performance.

Result: Demonstrated theoretical guarantees, tight estimation bounds, and strong empirical performance across various benchmarks and experimental setups, particularly by using chi-squared REBEL.

Conclusion: DRO-REBEL offers a robust solution to RLHF challenges, balancing estimation rates and coverage, and achieving minimized parametric performance rates with experimental validation.

Abstract: Reinforcement learning with human feedback (RLHF) has become crucial for
aligning Large Language Models (LLMs) with human intent. However, existing
offline RLHF approaches suffer from overoptimization, where models overfit to
reward misspecification and drift from preferred behaviors observed during
training. We introduce DRO-REBEL, a unified family of robust REBEL updates with
type-$p$ Wasserstein, KL, and $\chi^2$ ambiguity sets. Using Fenchel duality,
each update reduces to a simple relative-reward regression, preserving
scalability and avoiding PPO-style clipping or auxiliary value networks. Under
standard linear-reward and log-linear policy classes with a data-coverage
condition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants
than prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$
rate via a localized Rademacher complexity analysis. The same analysis closes
the gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal
parametric rates. We derive practical SGD algorithms for all three divergences:
gradient regularization (Wasserstein), importance weighting (KL), and a fast
1-D dual solve ($\chi^2$). Experiments on Emotion Alignment, the large-scale
ArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong
worst-case robustness across unseen preference mixtures, model sizes, and data
scales, with $\chi^2$-REBEL showing consistently strong empirical performance.
A controlled radius--coverage study validates a no-free-lunch trade-off: radii
shrinking faster than empirical divergence concentration rates achieve
minimax-optimal parametric rates but forfeit coverage, while
coverage-guaranteeing radii incur $O(n^{-1/4})$ rates.

</details>


### [275] [Self-Evolving LLMs via Continual Instruction Tuning](https://arxiv.org/abs/2509.18133)
*Le Huang,Jiazheng Kang,Cheng Hou,Zhe Zhao,Zhenxiang Yan,Chuan Shi,Ting Bai*

Main category: cs.LG

TL;DR: This paper introduces MoE-CL, a continual learning framework designed to address the challenges of catastrophic forgetting and task generalization in large language models through a mixture-of-experts approach.


<details>
  <summary>Details</summary>
Motivation: In industrial contexts, large language models need continual adaptation to handle evolving tasks and dynamic data distributions, but current methods struggle with catastrophic forgetting and generalization issues.

Method: MoE-CL involves a dual-expert system: (1) dedicated LoRA experts for task-specific retention and (2) a shared LoRA expert for cross-task generalization, enhanced by a task-aware discriminator integrated into a GAN framework to filter task-relevant knowledge.

Result: Experiments on standard (MTL5) and industrial (Tencent3) benchmarks showed MoE-CL's effectiveness in continual instruction tuning. Real-world A/B testing on Tencent Video led to a 15.3% reduction in manual content compliance review costs.

Conclusion: MoE-CL provides a scalable and efficient solution for the continual learning challenges of large industrial language models, balancing retention and generalization. It proves practical for real-world deployments requiring stable transfer and continual adaptation.

Abstract: In real-world industrial settings, large language models (LLMs) must learn
continually to keep pace with diverse and evolving tasks, requiring
self-evolution to refine knowledge under dynamic data distributions. However,
existing continual learning (CL) approaches, such as replay and parameter
isolation, often suffer from catastrophic forgetting: training on new tasks
degrades performance on earlier ones by overfitting to the new distribution and
weakening generalization.We propose MoE-CL, a parameter-efficient adversarial
mixture-of-experts framework for industrial-scale, self-evolving continual
instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated
LoRA expert per task to preserve task-specific knowledge via parameter
independence, mitigating forgetting; and (2) a shared LoRA expert to enable
cross-task transfer. To prevent transferring task-irrelevant noise through the
shared pathway, we integrate a task-aware discriminator within a GAN. The
discriminator encourages the shared expert to pass only task-aligned
information during sequential training. Through adversarial learning, the
shared expert acquires generalized representations that mimic the
discriminator, while dedicated experts retain task-specific details, balancing
knowledge retention and cross-task generalization and thereby supporting
self-evolution.Extensive experiments on the public MTL5 benchmark and an
industrial Tencent3 benchmark validate the effectiveness of MoE-CL for
continual instruction tuning. In real-world A/B testing for content compliance
review on the Tencent Video platform, MoE-CL reduced manual review costs by
15.3%. These results demonstrate that MoE-CL is practical for large-scale
industrial deployment where continual adaptation and stable transfer are
critical.

</details>


### [276] [Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws](https://arxiv.org/abs/2509.19189)
*Binghui Li,Fengling Chen,Zixun Huang,Lean Wang,Lei Wu*

Main category: cs.LG

TL;DR: The paper introduces a Functional Scaling Law (FSL) that models the training dynamics of LLMs with various learning rate schedules, offering insights into improving LLM training efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing scaling law studies have primarily focused on final loss metrics, neglecting training dynamics and learning rate schedules (LRS), which can significantly impact training efficiency.

Method: The authors used a teacher-student kernel regression setup trained with online SGD, applying intrinsic time and stochastic differential equation modeling to propose FSL, which characterizes population risk evolution and the effects of LRSs.

Result: The FSL framework offers insights into the efficiency of different LRSs (e.g., higher-capacity models and learning rate decay techniques) in both data- and compute-limited scenarios. Experiments on models with 0.1B to 1B parameters validate these findings.

Conclusion: FSL serves as a powerful tool for understanding and optimizing LLM pre-training dynamics, providing theoretical and empirical validation for common practices in LRS design and training efficiency improvements.

Abstract: Scaling laws have played a cornerstone role in guiding the training of large
language models (LLMs). However, most existing works on scaling laws primarily
focus on the final-step loss, overlooking the loss dynamics during the training
process and, crucially, the impact of learning rate schedule (LRS). In this
paper, we aim to bridge this gap by studying a teacher-student kernel
regression setup trained via online stochastic gradient descent (SGD).
Leveraging a novel intrinsic time viewpoint and stochastic differential
equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL),
which characterizes the evolution of population risk during the training
process for general LRSs. Remarkably, the impact of the LRSs is captured
through an explicit convolution-type functional term, making their effects
fully tractable. To illustrate the utility of FSL, we analyze three widely used
LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under
both data-limited and compute-limited regimes. We provide theoretical
justification for widely adopted empirical practices in LLMs pre-training such
as (i) higher-capacity models are more data- and compute-efficient; (ii)
learning rate decay can improve training efficiency; (iii) WSD-like schedules
can outperform direct-decay schedules. Lastly, we explore the practical
relevance of FSL as a surrogate model for fitting, predicting and optimizing
the loss curves in LLM pre-training, with experiments conducted across model
sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen
the understanding of LLM pre-training dynamics and provide insights for
improving large-scale model training.

</details>


### [277] [A Weighted Gradient Tracking Privacy-Preserving Method for Distributed Optimization](https://arxiv.org/abs/2509.18134)
*Furan Xie,Bing Liu,Li Chai*

Main category: cs.LG

TL;DR: The paper addresses the privacy concerns in distributed optimization using gradient tracking and introduces a new algorithm to eliminate privacy risks while ensuring convergence.


<details>
  <summary>Details</summary>
Motivation: To tackle the inherent privacy leakage risk in gradient tracking methods used in distributed optimization processes.

Method: Proposed a weighted gradient tracking algorithm with decaying weight factors to address privacy leakage, and analyzed its convergence under time-varying heterogeneous step sizes.

Result: The algorithm is proven to converge to the optimal solution under mild assumptions and is validated through numerical simulations on distributed estimation and CNN training.

Conclusion: The proposed method effectively removes privacy leakage in gradient tracking while maintaining optimization performance.

Abstract: This paper investigates the privacy-preserving distributed optimization
problem, aiming to protect agents' private information from potential attackers
during the optimization process. Gradient tracking, an advanced technique for
improving the convergence rate in distributed optimization, has been applied to
most first-order algorithms in recent years. We first reveal the inherent
privacy leakage risk associated with gradient tracking. Building upon this
insight, we propose a weighted gradient tracking distributed privacy-preserving
algorithm, eliminating the privacy leakage risk in gradient tracking using
decaying weight factors. Then, we characterize the convergence of the proposed
algorithm under time-varying heterogeneous step sizes. We prove the proposed
algorithm converges precisely to the optimal solution under mild assumptions.
Finally, numerical simulations validate the algorithm's effectiveness through a
classical distributed estimation problem and the distributed training of a
convolutional neural network.

</details>


### [278] [SDGF: Fusing Static and Multi-Scale Dynamic Correlations for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.18135)
*Shaoxun Wang,Xingjun Zhang,Qianyang Li,Jiawei Cao,Zhendong Tan*

Main category: cs.LG

TL;DR: The paper introduces the Static-Dynamic Graph Fusion (SDGF) network to improve multivariate time series forecasting by capturing multi-scale inter-series correlations.


<details>
  <summary>Details</summary>
Motivation: Multivariate time series forecasting requires addressing complex and evolving inter-series correlations across temporal scales, which existing methods struggle to model effectively.

Method: The SDGF network employs a dual-path graph learning approach: a static graph for stable long-term dependencies and a dynamic graph, built using Multi-level Wavelet Decomposition, to capture multi-scale features. An attention-gated module fuses these graphs, complemented by a multi-kernel dilated convolutional network.

Result: The model was tested through experiments on multiple benchmark datasets and demonstrated effectiveness in improving forecasting performance.

Conclusion: The SDGF network intelligently integrates multi-scale inter-series dependencies using complementary graph structures, leading to enhanced temporal pattern understanding and forecasting accuracy.

Abstract: Inter-series correlations are crucial for accurate multivariate time series
forecasting, yet these relationships often exhibit complex dynamics across
different temporal scales. Existing methods are limited in modeling these
multi-scale dependencies and struggle to capture their intricate and evolving
nature. To address this challenge, this paper proposes a novel Static-Dynamic
Graph Fusion network (SDGF), whose core lies in capturing multi-scale
inter-series correlations through a dual-path graph structure learning
approach. Specifically, the model utilizes a static graph based on prior
knowledge to anchor long-term, stable dependencies, while concurrently
employing Multi-level Wavelet Decomposition to extract multi-scale features for
constructing an adaptively learned dynamic graph to capture associations at
different scales. We design an attention-gated module to fuse these two
complementary sources of information intelligently, and a multi-kernel dilated
convolutional network is then used to deepen the understanding of temporal
patterns. Comprehensive experiments on multiple widely used real-world
benchmark datasets demonstrate the effectiveness of our proposed model.

</details>


### [279] [From Parameters to Performance: A Data-Driven Study on LLM Structure and Development](https://arxiv.org/abs/2509.18136)
*Suqing Wang,Zuchao Li,Luohe Shi,Bo Du,Hai Zhao,Yun Li,Qianren Wang*

Main category: cs.LG

TL;DR: This paper investigates how structural configurations of LLMs impact their performance using a large dataset and systematic data-mining methods.


<details>
  <summary>Details</summary>
Motivation: Understanding the link between structural configurations of LLMs and their performance to optimize future model development.

Method: Collected a large dataset of open-source LLM structures and their performance, analyzed data using benchmarks, and verified findings with interpretability techniques.

Result: Identified the influence of structural choices on LLM performance through benchmark analysis and interpretability methods.

Conclusion: This research provides data-driven insights to improve the optimization and targeted development of future LLMs, and releases a useful public dataset.

Abstract: Large language models (LLMs) have achieved remarkable success across various
domains, driving significant technological advancements and innovations.
Despite the rapid growth in model scale and capability, systematic, data-driven
research on how structural configurations affect performance remains scarce. To
address this gap, we present a large-scale dataset encompassing diverse
open-source LLM structures and their performance across multiple benchmarks.
Leveraging this dataset, we conduct a systematic, data mining-driven analysis
to validate and quantify the relationship between structural configurations and
performance. Our study begins with a review of the historical development of
LLMs and an exploration of potential future trends. We then analyze how various
structural choices impact performance across benchmarks and further corroborate
our findings using mechanistic interpretability techniques. By providing
data-driven insights into LLM optimization, our work aims to guide the targeted
development and application of future models. We will release our dataset at
https://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset

</details>


### [280] [LoRALib: A Standardized Benchmark for Evaluating LoRA-MoE Methods](https://arxiv.org/abs/2509.18137)
*Shaoheng Wang,Yao Lu,Yuqi Li,Yaxin Gao,Jiaqi Nie,Shanqing Yu,Yingli Tian,Qi Xuan*

Main category: cs.LG

TL;DR: A unified benchmark, LoRALib, is introduced to standardize datasets and conduct fair comparisons of LoRA-MoE methods, showcasing its advantages and limitations.


<details>
  <summary>Details</summary>
Motivation: Existing LoRA-MoE methods lack unified standards in datasets, hyperparameters, and evaluation methods, making fair comparisons difficult.

Method: Created a benchmark called LoRALib encompassing standardized datasets, fine-tuned hyperparameters, 680 LoRA modules with 17 architectures, and conducting extensive experiments using OpenCompass.

Result: Comparison between 3 representative LoRA-MoE methods showed LoRAMoE outperformed others, and prioritization of task-relevant LoRAs improved results.

Conclusion: LoRAMoE shows strong performance, and prioritizing relevant LoRAs enhances its capabilities; benchmarks promote progress in the fine-tuning field.

Abstract: As a parameter efficient fine-tuning (PEFT) method, low-rank adaptation
(LoRA) can save significant costs in storage and computing, but its strong
adaptability to a single task is often accompanied by insufficient cross-task
generalization capabilities. To improve this, existing work combines LoRA with
mixture-of-experts (MoE) to enhance the model's adaptability through expert
modules and routing mechanisms. However, existing LoRA-MoE methods lack unified
standards in models, datasets, hyperparameters, and evaluation methods, making
it difficult to conduct fair comparisons between different methods. To this
end, we proposed a unified benchmark named LoRALib. Specifically, we
standardized datasets from $40$ downstream tasks into a unified format,
fine-tuned them using the same hyperparameters and obtained $680$ LoRA modules
across $17$ model architectures. Based on this LoRA library, we conduct
large-scale experiments on $3$ representative LoRA-MoE methods and different
LoRA selection mechanisms using the open-sourced testing tool OpenCompass.
Extensive experiments show that LoRAMoE performs best, and that prioritizing
LoRAs relevant to the target task can further improve the performance of MoE.
We hope these findings will inspire future work. Our datasets and LoRA library
are available at https://huggingface.co/datasets/YaoLuzjut/LoRAOcean_dataset
and https://huggingface.co/YaoLuzjut/models.

</details>


### [281] [Rank-Induced PL Mirror Descent: A Rank-Faithful Second-Order Algorithm for Sleeping Experts](https://arxiv.org/abs/2509.18138)
*Tiantian Zhang*

Main category: cs.LG

TL;DR: RIPLM is a new algorithm for the sleeping experts setting, using rank-induced Plackett-Luce parameterization to maintain equivalence with rank benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address limitations of prior approaches that operate on expert identities by introducing an algorithm that updates within rank-induced distributions and remains rank-faithful and variance-adaptive.

Method: Developing the RIPLM algorithm, which utilizes rank-induced Plackett-Luce parameterization and ensures equivalence with rank benchmarks in each round of updates.

Result: The proposed algorithm maintains its distributional updates within rank-induced classes, achieving rank-faithfulness and variance-adaptiveness.

Conclusion: RIPLM represents a novel advancement in sleeping experts algorithms by fulfilling both rank-faithfulness and variance-adaptive criteria.

Abstract: We introduce a new algorithm, \emph{Rank-Induced Plackett--Luce Mirror
Descent (RIPLM)}, which leverages the structural equivalence between the
\emph{rank benchmark} and the \emph{distributional benchmark} established in
\citet{BergamOzcanHsu2022}. Unlike prior approaches that operate on expert
identities, RIPLM updates directly in the \emph{rank-induced Plackett--Luce
(PL)} parameterization. This ensures that the algorithm's played distributions
remain within the class of rank-induced distributions at every round,
preserving the equivalence with the rank benchmark. To our knowledge, RIPLM is
the first algorithm that is both (i) \emph{rank-faithful} and (ii)
\emph{variance-adaptive} in the sleeping experts setting.

</details>


### [282] [Comparative Analysis of FOLD-SE vs. FOLD-R++ in Binary Classification and XGBoost in Multi-Category Classification](https://arxiv.org/abs/2509.18139)
*Akshay Murthy,Shawn Sebastian,Manil Shangle,Huaduo Wang,Sopam Dasgupta,Gopal Gupta*

Main category: cs.LG

TL;DR: The paper evaluates the performance of rule-based algorithms FOLD-SE and FOLD-R++, comparing them with XGBoost for binary and multi-category classification tasks. FOLD-SE demonstrates competitive results and provides interpretable rule sets.


<details>
  <summary>Details</summary>
Motivation: The study addresses the need to balance accuracy, efficiency, and interpretability in machine learning models, which is often compromised in high-performing, non-transparent models like neural networks.

Method: Comparative analysis was conducted using binary and multi-category classification tasks. Metrics such as accuracy, F1 score, and processing time were used to evaluate FOLD-SE against FOLD-R++ and XGBoost.

Result: FOLD-SE outperformed FOLD-R++ in binary classification and showed better precision and efficiency than XGBoost in multi-category tasks, while maintaining interpretability through its rule sets.

Conclusion: FOLD-SE bridges the gap between explainability and performance in machine learning, making it a promising alternative to black-box models for various classification tasks.

Abstract: Recently, the demand for Machine Learning (ML) models that can balance
accuracy, efficiency, and interpreability has grown significantly.
Traditionally, there has been a tradeoff between accuracy and explainability in
predictive models, with models such as Neural Networks achieving high accuracy
on complex datasets while sacrificing internal transparency. As such, new
rule-based algorithms such as FOLD-SE have been developed that provide tangible
justification for predictions in the form of interpretable rule sets. The
primary objective of this study was to compare FOLD-SE and FOLD-R++, both
rule-based classifiers, in binary classification and evaluate how FOLD-SE
performs against XGBoost, a widely used ensemble classifier, when applied to
multi-category classification. We hypothesized that because FOLD-SE can
generate a condensed rule set in a more explainable manner, it would lose
upwards of an average of 3 percent in accuracy and F1 score when compared with
XGBoost and FOLD-R++ in multiclass and binary classification, respectively. The
research used data collections for classification, with accuracy, F1 scores,
and processing time as the primary performance measures. Outcomes show that
FOLD-SE is superior to FOLD-R++ in terms of binary classification by offering
fewer rules but losing a minor percentage of accuracy and efficiency in
processing time; in tasks that involve multi-category classifications, FOLD-SE
is more precise and far more efficient compared to XGBoost, in addition to
generating a comprehensible rule set. The results point out that FOLD-SE is a
better choice for both binary tasks and classifications with multiple
categories. Therefore, these results demonstrate that rule-based approaches
like FOLD-SE can bridge the gap between explainability and performance,
highlighting their potential as viable alternatives to black-box models in
diverse classification tasks.

</details>


### [283] [A Machine Learning Framework for Pathway-Driven Therapeutic Target Discovery in Metabolic Disorders](https://arxiv.org/abs/2509.18140)
*Iram Wajahat,Amritpal Singh,Fazel Keshtkar,Syed Ahmad Chan Bukhari*

Main category: cs.LG

TL;DR: The study introduced a machine learning framework integrating predictive modeling and gene-agnostic pathway mapping to predict type 2 diabetes mellitus (T2DM) risk among Pima Indians and identify novel therapeutic targets. Key predictors, associated pathways, and tailor-made therapeutic strategies were proposed.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the disproportionately high burden of type 2 diabetes mellitus (T2DM) among genetically predisposed populations, specifically the Pima Indians, by introducing an innovative framework to improve detection and treatment.

Method: A machine learning framework combining logistic regression and t-tests, along with principal component analysis (PCA), was employed to predict T2DM risk. Pathway mapping strategies linked predictors to biological signaling networks for insights into mechanisms.

Result: The predictive model achieved 78.43% accuracy. Pathway mapping revealed associations with insulin signaling, AMPK, and PPAR pathways, leading to proposed therapeutic strategies validated by enrichment analyses.

Conclusion: The paper provides a scalable, gene-agnostic framework for precision medicine in metabolic disorders, enabling early detection and targeted interventions. This approach could guide tailored prevention strategies for high-risk populations.

Abstract: Metabolic disorders, particularly type 2 diabetes mellitus (T2DM), represent
a significant global health burden, disproportionately impacting genetically
predisposed populations such as the Pima Indians (a Native American tribe from
south central Arizona). This study introduces a novel machine learning (ML)
framework that integrates predictive modeling with gene-agnostic pathway
mapping to identify high-risk individuals and uncover potential therapeutic
targets. Using the Pima Indian dataset, logistic regression and t-tests were
applied to identify key predictors of T2DM, yielding an overall model accuracy
of 78.43%. To bridge predictive analytics with biological relevance, we
developed a pathway mapping strategy that links identified predictors to
critical signaling networks, including insulin signaling, AMPK, and PPAR
pathways. This approach provides mechanistic insights without requiring direct
molecular data. Building upon these connections, we propose therapeutic
strategies such as dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1
modulators, and phytochemical, further validated through pathway enrichment
analyses. Overall, this framework advances precision medicine by offering
interpretable and scalable solutions for early detection and targeted
intervention in metabolic disorders. The key contributions of this work are:
(1) development of an ML framework combining logistic regression and principal
component analysis (PCA) for T2DM risk prediction; (2) introduction of a
gene-agnostic pathway mapping approach to generate mechanistic insights; and
(3) identification of novel therapeutic strategies tailored for high-risk
populations.

</details>


### [284] [AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for Spatio-Temporal Imputation](https://arxiv.org/abs/2509.18144)
*Yubo Yang,Yichen Zhu,Bo Jiang*

Main category: cs.LG

TL;DR: AdaSTI is a novel spatio-temporal imputation method leveraging conditional diffusion models and adaptive dependency modeling.


<details>
  <summary>Details</summary>
Motivation: Spatio-temporal data often has missing values due to issues like sensor failures, necessitating improved imputation methods. Diffusion models have shown promise but struggle with error accumulation and variability in dependencies.

Method: AdaSTI employs a BiS4PI network for pre-imputation, a Spatio-Temporal Conditionalizer (STC) network for extracting conditional information, and a Noise-Aware Spatio-Temporal (NAST) network to handle variant dependencies via a gated attention mechanism.

Result: AdaSTI demonstrated substantial improvements, reducing imputation errors by up to 46.4% across three real-world datasets compared to previous methods.

Conclusion: By addressing dependency variability and introducing adaptive mechanisms, AdaSTI establishes itself as a superior approach for spatio-temporal imputation tasks.

Abstract: Spatio-temporal data abounds in domain like traffic and environmental
monitoring. However, it often suffers from missing values due to sensor
malfunctions, transmission failures, etc. Recent years have seen continued
efforts to improve spatio-temporal data imputation performance. Recently
diffusion models have outperformed other approaches in various tasks, including
spatio-temporal imputation, showing competitive performance. Extracting and
utilizing spatio-temporal dependencies as conditional information is vital in
diffusion-based methods. However, previous methods introduce error accumulation
in this process and ignore the variability of the dependencies in the noisy
data at different diffusion steps. In this paper, we propose AdaSTI (Adaptive
Dependency Model in Diffusion-based Spatio-Temporal Imputation), a novel
spatio-temporal imputation approach based on conditional diffusion model.
Inside AdaSTI, we propose a BiS4PI network based on a bi-directional S4 model
for pre-imputation with the imputed result used to extract conditional
information by our designed Spatio-Temporal Conditionalizer (STC)network. We
also propose a Noise-Aware Spatio-Temporal (NAST) network with a gated
attention mechanism to capture the variant dependencies across diffusion steps.
Extensive experiments on three real-world datasets show that AdaSTI outperforms
existing methods in all the settings, with up to 46.4% reduction in imputation
error.

</details>


### [285] [Early Prediction of Multi-Label Care Escalation Triggers in the Intensive Care Unit Using Electronic Health Records](https://arxiv.org/abs/2509.18145)
*Syed Ahmad Chan Bukhari,Amritpal Singh,Shifath Hossain,Iram Wajahat*

Main category: cs.LG

TL;DR: The paper introduces a multi-label classification framework predicting ICU patients’ clinical decline in the first 24 hours to anticipate care escalation triggers (CETs). XGBoost outperforms baseline models with strong predictive metrics, highlighting clinically relevant predictors.


<details>
  <summary>Details</summary>
Motivation: ICU patients often face multidimensional clinical deterioration, which traditional models like SOFA or MEWS fail to address adequately. The paper aims to develop a tool for timely and multi-dimensional risk detection.

Method: The study uses ICU data from the MIMIC-IV database. It defines CETs through rule-based criteria and applies machine learning models, especially XGBoost, to predict these triggers from first 24-hour patient data. Metrics include F1-scores and Hamming loss.

Result: XGBoost provided the best results with F1-scores of 0.66 (respiratory), 0.72 (hemodynamic), 0.76 (renal), and 0.62 (neurological). Clinical predictors, like respiratory rate and creatinine, were most influential.

Conclusion: The framework offers an interpretable, efficient predictive tool for clinical deterioration without requiring complex modeling, with significant potential for early clinical alert systems in ICUs.

Abstract: Intensive Care Unit (ICU) patients often present with complex, overlapping
signs of physiological deterioration that require timely escalation of care.
Traditional early warning systems, such as SOFA or MEWS, are limited by their
focus on single outcomes and fail to capture the multi-dimensional nature of
clinical decline. This study proposes a multi-label classification framework to
predict Care Escalation Triggers (CETs), including respiratory failure,
hemodynamic instability, renal compromise, and neurological deterioration,
using the first 24 hours of ICU data. Using the MIMIC-IV database, CETs are
defined through rule-based criteria applied to data from hours 24 to 72 (for
example, oxygen saturation below 90, mean arterial pressure below 65 mmHg,
creatinine increase greater than 0.3 mg/dL, or a drop in Glasgow Coma Scale
score greater than 2). Features are extracted from the first 24 hours and
include vital sign aggregates, laboratory values, and static demographics. We
train and evaluate multiple classification models on a cohort of 85,242 ICU
stays (80 percent training: 68,193; 20 percent testing: 17,049). Evaluation
metrics include per-label precision, recall, F1-score, and Hamming loss.
XGBoost, the best performing model, achieves F1-scores of 0.66 for respiratory,
0.72 for hemodynamic, 0.76 for renal, and 0.62 for neurologic deterioration,
outperforming baseline models. Feature analysis shows that clinically relevant
parameters such as respiratory rate, blood pressure, and creatinine are the
most influential predictors, consistent with the clinical definitions of the
CETs. The proposed framework demonstrates practical potential for early,
interpretable clinical alerts without requiring complex time-series modeling or
natural language processing.

</details>


### [286] [ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks](https://arxiv.org/abs/2509.18147)
*Xinyu Mu,Hui Dou,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: The paper proposes ConceptFlow, a framework enabling concept-based interpretability in CNNs by tracing how semantic concepts evolve across layers and filters.


<details>
  <summary>Details</summary>
Motivation: Existing concept-based interpretability methods overlook individual filter roles and the propagation of semantic concepts in CNNs.

Method: ConceptFlow introduces concept attentions for filter-level semantic interpretation and conceptual pathways for modeling hierarchical concept transitions across layers.

Result: Experiments show ConceptFlow provides semantically meaningful insights into CNN reasoning, validating its components' effectiveness.

Conclusion: ConceptFlow enhances understanding of CNNs' internal logic, enabling detailed and human-aligned explanations through concept propagation and hierarchical modeling.

Abstract: Concept-based interpretability for Convolutional Neural Networks (CNNs) aims
to align internal model representations with high-level semantic concepts, but
existing approaches largely overlook the semantic roles of individual filters
and the dynamic propagation of concepts across layers. To address these
limitations, we propose ConceptFlow, a concept-based interpretability framework
that simulates the internal "thinking path" of a model by tracing how concepts
emerge and evolve across layers. ConceptFlow comprises two key components: (i)
concept attentions, which associate each filter with relevant high-level
concepts to enable localized semantic interpretation, and (ii) conceptual
pathways, derived from a concept transition matrix that quantifies how concepts
propagate and transform between filters. Together, these components offer a
unified and structured view of internal model reasoning. Experimental results
demonstrate that ConceptFlow yields semantically meaningful insights into model
reasoning, validating the effectiveness of concept attentions and conceptual
pathways in explaining decision behavior. By modeling hierarchical conceptual
pathways, ConceptFlow provides deeper insight into the internal logic of CNNs
and supports the generation of more faithful and human-aligned explanations.

</details>


### [287] [Sparse Training Scheme for Multimodal LLM](https://arxiv.org/abs/2509.18150)
*Kean Shi,Liang Chen,Haozhe Zhao,Baobao Chang*

Main category: cs.LG

TL;DR: The paper introduces a Sparse Training Scheme (STS) for Multimodal Large Language Models (MLLMs) to improve training efficiency, using visual token compression and dynamic layer skipping.


<details>
  <summary>Details</summary>
Motivation: Training MLLMs is inefficient due to long input sequences from multimodal data and low utilization of computations during inter-layer processing.

Method: The Sparse Training Scheme (STS) involves a Visual Token Compressor to reduce visual data complexity and a Layer Dynamic Skipper to skip unnecessary layers dynamically during training.

Result: The STS framework shows effectiveness and efficiency, as confirmed by extensive evaluations on multiple benchmarks across various MLLM architectures.

Conclusion: STS improves training efficiency for MLLMs while maintaining performance, making it adaptable for diverse architectures.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated outstanding
performance across a variety of domains. However, training MLLMs is often
inefficient due to the significantly longer input sequences introduced by
multimodal data and the low utilization of inter-layer computations. To address
this challenge, we shift the focus to the training process itself and propose a
novel training-efficient framework based on sparse representations, termed the
Sparse Training Scheme (STS). This scheme consists of two key components: the
Visual Token Compressor, which reduces the information load by compressing
visual tokens, and the Layer Dynamic Skipper, which mitigates the computational
overhead by dynamically skipping unnecessary layers in the language model
during both forward and backward passes. Our approach is broadly applicable to
diverse MLLM architectures and has been extensively evaluated on multiple
benchmarks, demonstrating its effectiveness and efficiency.

</details>


### [288] [HyperNAS: Enhancing Architecture Representation for NAS Predictor via Hypernetwork](https://arxiv.org/abs/2509.18151)
*Jindi Lv,Yuhao Zhou,Yuxin Tian,Qing Ye,Wentao Feng,Jiancheng Lv*

Main category: cs.LG

TL;DR: The paper introduces HyperNAS, a neural predictor framework enhancing architecture representation learning to boost efficiency in Neural Architecture Search (NAS), achieving state-of-the-art performance with minimal samples.


<details>
  <summary>Details</summary>
Motivation: Neural Architecture Search (NAS) processes are hindered by time-intensive evaluations, and the existing neural predictors struggle with generalization due to their inability to effectively capture complex relationships among architectures.

Method: HyperNAS is structured around two main components: 1) a global encoding scheme for capturing comprehensive macro-structure information and 2) a shared hypernetwork to analyze inter-architecture patterns. It also employs dynamic adaptive multi-task loss for stable training and personalized exploration.

Result: Extensive experiments across five search spaces, including vision transformers (ViTs), demonstrate HyperNAS's effectiveness, especially in few-shot scenarios. It achieves new state-of-the-art accuracies of 97.60% on CIFAR-10 and 82.4% on ImageNet, while requiring at least 5 times fewer data samples.

Conclusion: HyperNAS enhances the efficiency and accuracy of neural predictors for NAS by leveraging improved architecture representations and a robust training strategy, offering a significant reduction in the need for training samples while achieving superior performance.

Abstract: Time-intensive performance evaluations significantly impede progress in
Neural Architecture Search (NAS). To address this, neural predictors leverage
surrogate models trained on proxy datasets, allowing for direct performance
predictions for new architectures. However, these predictors often exhibit poor
generalization due to their limited ability to capture intricate relationships
among various architectures. In this paper, we propose HyperNAS, a novel neural
predictor paradigm for enhancing architecture representation learning. HyperNAS
consists of two primary components: a global encoding scheme and a shared
hypernetwork. The global encoding scheme is devised to capture the
comprehensive macro-structure information, while the shared hypernetwork serves
as an auxiliary task to enhance the investigation of inter-architecture
patterns. To ensure training stability, we further develop a dynamic adaptive
multi-task loss to facilitate personalized exploration on the Pareto front.
Extensive experiments across five representative search spaces, including ViTs,
demonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For
instance, HyperNAS strikes new state-of-the-art results, with 97.60\% top-1
accuracy on CIFAR-10 and 82.4\% top-1 accuracy on ImageNet, using at least
5.0$\times$ fewer samples.

</details>


### [289] [WLFM: A Well-Logs Foundation Model for Multi-Task and Cross-Well Geological Interpretation](https://arxiv.org/abs/2509.18152)
*Zhenyu Qi,Qing Yu,Jichen Wang,Yun-Bo Zhao,Zerui Li,Wenjun Lv*

Main category: cs.LG

TL;DR: WLFM is a foundation model pretrained on well-log data from 1,200 wells, achieving state-of-the-art accuracy in lithology classification and porosity estimation through self-supervised learning and few-shot fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in well-log interpretation due to heterogeneous tool responses, noisy data, and limited labeled information, proposing a model to enhance accuracy and scalability.

Method: WLFM involves tokenization of log patches, self-supervised pretraining using masked-token modeling and contrastive learning, and multi-task adaptation via few-shot fine-tuning.

Result: The model achieves 0.0041 MSE in porosity estimation and 74.13% accuracy in lithology classification, with fine-tuning improving results further. The model also shows strong performance in curve reconstruction and potential for boundary detection.

Conclusion: WLFM establishes itself as a scalable and interpretable geological AI backbone, with potential applications in integrating logs, seismic, and textual data for broader subsurface characterization tasks.

Abstract: Well-log interpretation is fundamental for subsurface characterization but
remains challenged by heterogeneous tool responses, noisy signals, and limited
labels. We propose WLFM, a foundation model pretrained on multi-curve logs from
1200 wells, comprising three stages: tokenization of log patches into
geological tokens, self-supervised pretraining with masked-token modeling and
stratigraphy-aware contrastive learning, and multi-task adaptation with
few-shot fine-tuning. WLFM consistently outperforms state-of-the-art baselines,
achieving 0.0041 MSE in porosity estimation and 74.13\% accuracy in lithology
classification, while WLFM-Finetune further improves to 0.0038 MSE and 78.10\%
accuracy. Beyond predictive accuracy, WLFM exhibits emergent layer-awareness,
learns a reusable geological vocabulary, and reconstructs masked curves with
reasonable fidelity, though systematic offsets are observed in shallow and
ultra-deep intervals. Although boundary detection is not explicitly evaluated
here, clustering analyses suggest strong potential for future extension. These
results establish WLFM as a scalable, interpretable, and transferable backbone
for geological AI, with implications for multi-modal integration of logs,
seismic, and textual data.

</details>


### [290] [A deep reinforcement learning platform for antibiotic discovery](https://arxiv.org/abs/2509.18153)
*Hanqun Cao,Marcelo D. T. Torres,Jingjie Zhang,Zijun Gao,Fang Wu,Chunbin Gu,Jure Leskovec,Yejin Choi,Cesar de la Fuente-Nunez,Guangyong Chen,Pheng-Ann Heng*

Main category: cs.LG

TL;DR: The study introduces ApexAmphion, a deep-learning system for designing new antibiotics, showing 100% success in creating peptides with strong antimicrobial activities.


<details>
  <summary>Details</summary>
Motivation: Antimicrobial resistance is a growing threat, with projections of 10 million deaths annually by 2050. This stresses the need for innovative strategies to discover effective antibiotics.

Method: The researchers utilized a 6.4-billion-parameter protein language model combined with reinforcement learning, fine-tuning it on peptide data and optimizing designs using a composite reward system.

Result: 100 designed peptides were tested, with all showing low minimum inhibitory concentration values and 99 of them demonstrating broad-spectrum activity against clinically critical bacteria.

Conclusion: This framework is effective in rapidly designing diverse and powerful antibiotic candidates, highlighting its potential as a scalable solution for addressing AMR.

Abstract: Antimicrobial resistance (AMR) is projected to cause up to 10 million deaths
annually by 2050, underscoring the urgent need for new antibiotics. Here we
present ApexAmphion, a deep-learning framework for de novo design of
antibiotics that couples a 6.4-billion-parameter protein language model with
reinforcement learning. The model is first fine-tuned on curated peptide data
to capture antimicrobial sequence regularities, then optimised with proximal
policy optimization against a composite reward that combines predictions from a
learned minimum inhibitory concentration (MIC) classifier with differentiable
physicochemical objectives. In vitro evaluation of 100 designed peptides showed
low MIC values (nanomolar range in some cases) for all candidates (100% hit
rate). Moreover, 99 our of 100 compounds exhibited broad-spectrum antimicrobial
activity against at least two clinically relevant bacteria. The lead molecules
killed bacteria primarily by potently targeting the cytoplasmic membrane. By
unifying generation, scoring and multi-objective optimization with deep
reinforcement learning in a single pipeline, our approach rapidly produces
diverse, potent candidates, offering a scalable route to peptide antibiotics
and a platform for iterative steering toward potency and developability within
hours.

</details>


### [291] [MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe](https://arxiv.org/abs/2509.18154)
*Tianyu Yu,Zefan Wang,Chongyi Wang,Fuwei Huang,Wenshuo Ma,Zhihui He,Tianchi Cai,Weize Chen,Yuxiang Huang,Yuanqian Zhao,Bokai Xu,Junbo Cui,Yingjing Xu,Liqing Ruan,Luoyuan Zhang,Hanyu Liu,Jingkun Tang,Hongyuan Liu,Qining Guo,Wenhao Hu,Bingxiang He,Jie Zhou,Jie Cai,Ji Qi,Zonghao Guo,Chi Chen,Guoyang Zeng,Yuxuan Li,Ganqu Cui,Ning Ding,Xu Han,Yuan Yao,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: MiniCPM-V 4.5 is an 8B parameter model aiming to improve efficiency and performance in Multimodal Large Language Models (MLLMs), outperforming larger models with significant resource savings.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the bottleneck in training and inference efficiency of MLLMs, aiming to make them more accessible and scalable.

Method: MiniCPM-V 4.5 introduces a unified 3D-Resampler architecture, a streamlined learning paradigm for document knowledge, and a hybrid reinforcement learning strategy for reasoning tasks.

Result: Experimental evaluation demonstrates MiniCPM-V 4.5 surpasses models like GPT-4o-latest and Qwen2.5-VL 72B in performance, efficiency, and resource optimization.

Conclusion: The model achieves state-of-the-art performance at a smaller scale with impressive GPU and inference-time savings, making it a cost-effective and powerful solution for MLLMs.

Abstract: Multimodal Large Language Models (MLLMs) are undergoing rapid progress and
represent the frontier of AI development. However, their training and inference
efficiency have emerged as a core bottleneck in making MLLMs more accessible
and scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B
parameter model designed for high efficiency and strong performance. We
introduce three core improvements in model architecture, data strategy and
training method: a unified 3D-Resampler model architecture for highly compact
encoding over images and videos, a unified learning paradigm for document
knowledge and text recognition without heavy data engineering, and a hybrid
reinforcement learning strategy for proficiency in both short and long
reasoning modes. Comprehensive experimental results in OpenCompass evaluation
show that MiniCPM-V 4.5 surpasses widely used proprietary models such as
GPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL
72B. Notably, the strong performance is achieved with remarkable efficiency.
For example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves
state-of-the-art performance among models under 30B size, using just 46.7\% GPU
memory cost and 8.7\% inference time of Qwen2.5-VL 7B.

</details>


### [292] [Developing Training Procedures for Piecewise-linear Spline Activation Functions in Neural Networks](https://arxiv.org/abs/2509.18161)
*William H Patty*

Main category: cs.LG

TL;DR: The paper explores optimizing activation functions in neural networks for improved accuracy and efficiency using parameterized linear B-splines, demonstrating significant performance improvements in FNNs and CNNs over standard ReLU.


<details>
  <summary>Details</summary>
Motivation: Traditional activation functions like ReLU, tanh, or sigmoid, while empirically validated, may not be optimal for specific neural network tasks. Optimizing activation functions can enhance model efficiency and accuracy.

Method: The authors propose and evaluate 9 training methodologies for dual-optimization dynamics in neural networks using parameterized linear B-spline activation functions.

Result: Experiments show reductions in error rates by up to 94% in FNNs and 51% in CNNs compared to ReLU-based models. However, these benefits come with increased training complexity and latency.

Conclusion: Parameterized activation functions can yield significant performance improvements, but they introduce additional development challenge and complexity in real-world applications.

Abstract: Activation functions in neural networks are typically selected from a set of
empirically validated, commonly used static functions such as ReLU, tanh, or
sigmoid. However, by optimizing the shapes of a network's activation functions,
we can train models that are more parameter-efficient and accurate by assigning
more optimal activations to the neurons. In this paper, I present and compare 9
training methodologies to explore dual-optimization dynamics in neural networks
with parameterized linear B-spline activation functions. The experiments
realize up to 94% lower end model error rates in FNNs and 51% lower rates in
CNNs compared to traditional ReLU-based models. These gains come at the cost of
additional development and training complexity as well as end model latency.

</details>


### [293] [A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge](https://arxiv.org/abs/2509.18162)
*Meraryslan Meraliyev,Cemil Turan,Shirali Kadyrov*

Main category: cs.LG

TL;DR: The paper explores a truck-drone last-mile delivery model using reinforcement learning for optimal scheduling.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency of last-mile logistics by using hybrid truck-drone systems with explicit battery recharge constraints.

Method: The study combines an ALNS-based truck tour optimization with reinforcement learning to schedule drone sorties under constraints, using feasibility masks and a timeline simulator.

Result: The proposed method outperforms ALNS by 2.73% and ties or surpasses a neural-network-based method in total completion time.

Conclusion: The reinforcement learning approach effectively optimizes truck-drone delivery systems, demonstrating balance and practical implementation support for replication.

Abstract: We study last-mile delivery with one truck and one drone under explicit
battery management: the drone flies at twice the truck speed; each sortie must
satisfy an endurance budget; after every delivery the drone recharges on the
truck before the next launch. We introduce a hybrid reinforcement learning (RL)
solver that couples an ALNS-based truck tour (with 2/3-opt and Or-opt) with a
small pointer/attention policy that schedules drone sorties. The policy decodes
launch--serve--rendezvous triplets with hard feasibility masks for endurance
and post-delivery recharge; a fast, exact timeline simulator enforces
launch/recovery handling and computes the true makespan used by masked
greedy/beam decoding. On Euclidean instances with $N{=}50$, $E{=}0.7$, and
$R{=}0.1$, the method achieves an average makespan of \textbf{5.203}$\pm$0.093,
versus \textbf{5.349}$\pm$0.038 for ALNS and \textbf{5.208}$\pm$0.124 for NN --
i.e., \textbf{2.73\%} better than ALNS on average and within \textbf{0.10\%} of
NN. Per-seed, the RL scheduler never underperforms ALNS on the same instance
and ties or beats NN on two of three seeds. A decomposition of the makespan
shows the expected truck--wait trade-off across heuristics; the learned
scheduler balances both to minimize the total completion time. We provide a
config-first implementation with plotting and significance-test utilities to
support replication.

</details>


### [294] [DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns](https://arxiv.org/abs/2509.18164)
*Ranfei Chen,Ming Chen*

Main category: cs.LG

TL;DR: This paper introduces DSFT, a strategy for improving the numerical and logical pattern understanding of diffusion large language models (dLLMs), resulting in 5-10% performance gains on mathematical tasks and 2% on logical tasks.


<details>
  <summary>Details</summary>
Motivation: dLLMs face challenges in performing numerically and logically sensitive tasks, as current training methods fail to comprehensively address mathematical and logical pattern learning.

Method: The authors propose DSFT, which modifies the masking strategy and loss function to better guide dLLMs in understanding mathematical and logical patterns. DSFT can complement existing training methods such as pre-training and reinforcement learning.

Result: By applying DSFT to models like LLaDA and Dream series, it showed improvements of 5-10% on mathematical problems and around 2% in logical tasks, even with small-scale data.

Conclusion: DSFT offers an efficient, adaptable method to enhance mathematical and logical pattern understanding in dLLMs, providing actionable insights and compatibility with various training methods.

Abstract: Diffusion large language models (dLLMs) have emerged as a new architecture
following auto regressive models. Their denoising process offers a powerful
generative advantage, but they present significant challenges in learning and
understanding numerically sensitive mathematical and order-sensitive logical
tasks. Current training methods, including pre-training, fine-tuning, and
reinforcement learning, focus primarily on improving general knowledge
retention and reasoning abilities, but lack a comprehensive understanding of
mathematical and logical patterns. We propose DSFT, a simple yet effective
Diffusion SFT strategy, by adjusting the masking strategy and loss function,
guiding models to understand mathematical and logical patterns. This strategy
can be flexibly combined with pre-training, reinforcement learning, and other
training methods. Validated on models such as LLaDA and Dream series, we prove
that DSFT on small-scale data can achieve improvements of 5-10% and
approximately 2% on mathematical and logical problems, respectively. This
inspiring masking approach offers insights for future learning of specific
patterns, which can be easily and efficiently combined with other training
methods and applied to various dLLMs. Our code is publicly available at
https://anonymous.4open.science/r/DSFT-0FFB/

</details>


### [295] [MobiGPT: A Foundation Model for Mobile Wireless Networks](https://arxiv.org/abs/2509.18166)
*Xiaoqian Qi,Haoye Chai,Yong Li*

Main category: cs.LG

TL;DR: The paper introduces MobiGPT, a versatile foundation model for mobile data forecasting that outperforms existing models in accuracy and transferability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of complex and costly forecasting models designed for specific data types in large-scale and diverse mobile networks.

Method: The authors developed MobiGPT, integrating a unified model architecture, soft-prompt learning for handling various data types, and a temporal masking mechanism for different forecasting tasks.

Result: MobiGPT demonstrated 7.27–27.37% better forecasting accuracy compared to other models and showed more than 21.51% improvement in zero/few-shot scenarios using real-world datasets.

Conclusion: MobiGPT is a highly generalizable and effective foundation model for multi-type mobile data forecasting, offering substantial accuracy and transferability in diverse scenarios.

Abstract: With the rapid development of mobile communication technologies, future
mobile networks will offer vast services and resources for commuting,
production, daily life, and entertainment. Accurate and efficient forecasting
of mobile data (e.g., cell traffic, user behavior, channel quality) helps
operators monitor network state changes, orchestrate wireless resources, and
schedule infrastructure and users, thereby improving supply efficiency and
service quality. However, current forecasting paradigms rely on customized
designs with tailored models for exclusive data types. Such approaches increase
complexity and deployment costs under large-scale, heterogeneous networks
involving base stations, users, and channels. In this paper, we design a
foundation model for mobile data forecasting, MobiGPT, with a unified structure
capable of forecasting three data types: base station traffic, user app usage,
and channel quality. We propose a soft-prompt learning method to help the model
understand features of different data types, and introduce a temporal masking
mechanism to guide the model through three forecasting tasks: short-term
prediction, long-term prediction, and distribution generation, supporting
diverse optimization scenarios. Evaluations on real-world datasets with over
100,000 samples show that MobiGPT achieves accurate multi-type forecasting.
Compared to existing models, it improves forecasting accuracy by 27.37%,
20.08%, and 7.27%, reflecting strong generalization. Moreover, MobiGPT exhibits
superior zero/few-shot performance in unseen scenarios, with over 21.51%
improvement, validating its strong transferability as a foundation model.

</details>


### [296] [PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning](https://arxiv.org/abs/2509.18169)
*Hengbo Xiao,Jingyuan Fan,Xin Tong,Jingzhao Zhang,Chao Lu,Guannan He*

Main category: cs.LG

TL;DR: The paper proposes the PiMoE architecture to integrate computation and reasoning in LLMs efficiently, improving upon current approaches.


<details>
  <summary>Details</summary>
Motivation: Large language models currently lack intrinsic high-precision numerical computation, leading to inefficiencies in communication and scalability when incorporating external experts.

Method: PiMoE integrates computational capabilities by training separate experts, a text-to-computation module, and a router, enabling token-level decision-making for seamless computation and reasoning.

Result: PiMoE achieved higher accuracy than fine-tuned LLMs and improved response latency, token usage, and energy efficiency compared to multi-agent systems.

Conclusion: PiMoE represents a scalable and efficient method for combining computation and reasoning, applicable in scientific and industrial intelligent systems.

Abstract: Complex systems typically rely on high-precision numerical computation to
support decisions, but current large language models (LLMs) cannot yet
incorporate such computations as an intrinsic and interpretable capability with
existing architectures. Mainstream multi-agent approaches can leverage external
experts, but inevitably introduce communication overhead and suffer from
inefficient multimodal emergent capability and limited scalability. To this
end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and
inference architecture for integrating computation and reasoning. Instead of
the workflow paradigm of tool invocation, PiMoE endogenously integrates
computational capabilities into neural networks after separately training
experts, a text-to-computation module, and a router. At inference, the router
directs computation and reasoning at the token level, thereby enabling
iterative alternation within a single chain of thought. We evaluate PiMoE on
two reasoning-computation tasks against LLM finetuning and the multi-agent
system approaches. Results show that the PiMoE architecture achieves not only
higher accuracy than directly finetuning LLMs but also significant improvements
in response latency, token usage, and GPU energy consumption compared with
mainstream multi-agent approaches. PiMoE offers an efficient, interpretable,
and scalable paradigm for next-generation scientific or industrial intelligent
systems.

</details>


### [297] [FedIA: A Plug-and-Play Importance-Aware Gradient Pruning Aggregation Method for Domain-Robust Federated Graph Learning on Node Classification](https://arxiv.org/abs/2509.18171)
*Zhanting Zhou,KaHou Tam,Zeqin Wu,Pengzhao Sun,Jinbo Wang,Fengli Zhang*

Main category: cs.LG

TL;DR: The paper explores Federated Graph Learning (FGL) under domain skew, proposing a projection-first approach called FedIA, which improves convergence and accuracy by denoising gradient signals before aggregation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address stability and effectiveness issues in federated graph learning caused by domain-specific noisy gradient signals, particularly under domain skew.

Method: The FedIA framework introduces a two-stage process: (i) server-side top-$\rho$ mask that prioritizes roughly the top 5% of most informative coordinates and (ii) influence-regularised momentum weights to suppress outlier clients.

Result: FedIA achieves smoother convergence, greater stability, and higher accuracy compared to nine strong baseline methods across both domain-similar and domain-diverse graphs.

Conclusion: Dynamic projection techniques like FedIA maintain optimal convergence rates while being deployable with minimal overhead.

Abstract: Federated Graph Learning (FGL) under domain skew -- as observed on platforms
such as \emph{Twitch Gamers} and multilingual \emph{Wikipedia} networks --
drives client models toward incompatible representations, rendering naive
aggregation both unstable and ineffective. We find that the culprit is not the
weighting scheme but the \emph{noisy gradient signal}: empirical analysis of
baseline methods suggests that a vast majority of gradient dimensions can be
dominated by domain-specific variance. We therefore shift focus from
"aggregation-first" to a \emph{projection-first} strategy that denoises client
updates \emph{before} they are combined. The proposed FedIA framework realises
this \underline{I}mportance-\underline{A}ware idea through a two-stage,
plug-and-play pipeline: (i) a server-side top-$\rho$ mask keeps only the most
informative about 5% of coordinates, and (ii) a lightweight
influence-regularised momentum weight suppresses outlier clients. FedIA adds
\emph{no extra uplink traffic and only negligible server memory}, making it
readily deployable. On both homogeneous (Twitch Gamers) and heterogeneous
(Wikipedia) graphs, it yields smoother, more stable convergence and higher
final accuracy than nine strong baselines. A convergence sketch further shows
that dynamic projection maintains the optimal
$\mathcal{O}(\sigma^{2}/\sqrt{T})$ rate.

</details>


### [298] [SBVR: Summation of BitVector Representation for Efficient LLM Quantization](https://arxiv.org/abs/2509.18172)
*Wonjun Bang,Jongseok Park,Hongseung Yu,Kyungmin Bin,Kyunghan Lee*

Main category: cs.LG

TL;DR: The paper proposes SBVR, a hardware-friendly LLM quantization method, which achieves efficient compression and faster inference by leveraging Gaussian-like distribution and custom CUDA kernels.


<details>
  <summary>Details</summary>
Motivation: Existing Post-Training Quantization (PTQ) methods face challenges like uniform representation mismatch and degraded inference speed due to GPU memory access limitations.

Method: The SBVR method maps LLM weights to non-uniform Gaussian-like representation points for better compression and includes a custom CUDA kernel for matrix-vector computation without decompression.

Result: SBVR achieves state-of-the-art perplexity and accuracy while delivering a 2.21x-3.04x token-generation speedup over FP16 models for 4-bit quantization.

Conclusion: SBVR addresses key limitations of existing PTQ methods by combining compression efficiency with hardware-aligned speed improvements, enabling effective LLM deployment.

Abstract: With the advent of large language models (LLMs), numerous Post-Training
Quantization (PTQ) strategies have been proposed to alleviate deployment
barriers created by their enormous parameter counts. Quantization achieves
compression by limiting the number of representable points in the data.
Therefore, the key to achieving efficient quantization is selecting the optimal
combination of representation points, or codes, for the given data. Existing
PTQ solutions adopt two major approaches to this problem: Round-To-Nearest
(RTN)-based methods and codebook-based methods. RTN-based methods map LLM
weights onto uniformly distributed integer grids, failing to account for the
Gaussian-like weight distribution of LLM weights. Codebook-based methods
mitigate this issue by constructing distribution-aware codebooks; however, they
suffer from random and strided memory access patterns, resulting in degraded
inference speed that is exacerbated by the limited size of GPU L1 cache. To
overcome these limitations, we propose a novel LLM quantization method, SBVR
(Summation of BitVector Representation), that enables Gaussian-like code
representation in a hardware-friendly manner for fast inference. SBVR maps
weight values to non-uniform representation points whose distribution follows
the actual distribution of LLM weights, enabling more accurate compression.
Additionally, we design a custom CUDA kernel that allows matrix-vector
multiplication directly in the SBVR format without decompression, thereby
enabling high-performance execution of SBVR-compressed models. Our evaluations
of SBVR on various models demonstrate state-of-the-art perplexity and accuracy
benchmark performance while delivering a 2.21x- 3.04x end-to-end
token-generation speedup over naive FP16 models in the 4-bit quantization
regime.

</details>


### [299] [TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route](https://arxiv.org/abs/2509.18173)
*Hongyi Luo,Qing Cheng,Daniel Matos,Hari Krishna Gadi,Yanfeng Zhang,Lu Liu,Yongliang Wang,Niclas Zeller,Daniel Cremers,Liqiu Meng*

Main category: cs.LG

TL;DR: The paper explores the geospatial cognitive abilities of LLMs by developing a benchmark with 36,000 routes from 12 cities globally, assessing their performance in route reversal tasks, and introducing a tool named PathBuilder.


<details>
  <summary>Details</summary>
Motivation: While humans process geospatial information seamlessly via natural language, the capabilities of LLMs in this domain are largely unknown, with previous research limited by subjective metrics and insufficient evaluation datasets.

Method: The authors developed a benchmark, created an evaluation dataset of 36,000 routes, introduced the tool PathBuilder for interpreting geospatial and linguistic data, and proposed a robust evaluation framework for analyzing state-of-the-art LLMs.

Result: Findings reveal LLMs struggle with route reversal tasks, often failing to return to the starting point or generate optimal routes. Other challenges include low robustness and overconfidence in incorrect outputs.

Conclusion: The study highlights significant limitations in LLMs' geospatial reasoning abilities, underscoring the need for improved models and evaluation mechanisms to enhance their performance in geospatial tasks.

Abstract: Humans can interpret geospatial information through natural language, while
the geospatial cognition capabilities of Large Language Models (LLMs) remain
underexplored. Prior research in this domain has been constrained by
non-quantifiable metrics, limited evaluation datasets and unclear research
hierarchies. Therefore, we propose a large-scale benchmark and conduct a
comprehensive evaluation of the geospatial route cognition of LLMs. We create a
large-scale evaluation dataset comprised of 36000 routes from 12 metropolises
worldwide. Then, we introduce PathBuilder, a novel tool for converting natural
language instructions into navigation routes, and vice versa, bridging the gap
between geospatial information and natural language. Finally, we propose a new
evaluation framework and metrics to rigorously assess 11 state-of-the-art
(SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs
exhibit limitation to reverse routes: most reverse routes neither return to the
starting point nor are similar to the optimal route. Additionally, LLMs face
challenges such as low robustness in route generation and high confidence for
their incorrect answers. Code\ \&\ Data available here:
\href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}

</details>


### [300] [Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought](https://arxiv.org/abs/2509.18200)
*Yu Ti Huang*

Main category: cs.LG

TL;DR: The paper introduces a multimodal chain-of-thought (MCoT) framework to accurately translate egocentric conversational orientations into allocentric directions, achieving high accuracy even with noisy transcripts.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the challenge of translating egocentric utterances into allocentric orientations in indoor or GPS-compromised environments, focusing on non-English and ASR-transcribed conversational scenarios.

Method: The paper proposes a structured three-step reasoning process in the MCoT framework, integrating ASR speech and landmark coordinates, while using a curriculum learning strategy on a mid-sized language model for progressive learning.

Result: The MCoT framework achieves 100% orientation accuracy on clean transcripts and 98.1% with ASR-transcribed input. It demonstrates robustness under noisy conditions, cross-domain evaluations, and linguistic variations.

Conclusion: MCoT shows promise as a structured and resource-efficient approach for multimodal spatial reasoning, pointing to advancements in interpretable conversational agents usable in various challenging environments.

Abstract: Conversational agents must translate egocentric utterances (e.g., "on my
right") into allocentric orientations (N/E/S/W). This challenge is particularly
critical in indoor or complex facilities where GPS signals are weak and
detailed maps are unavailable. While chain-of-thought (CoT) prompting has
advanced reasoning in language and vision tasks, its application to multimodal
spatial orientation remains underexplored. We introduce Conversational
Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese
conversational navigation projected from real-world environments, addressing
egocentric-to-allocentric reasoning in non-English and ASR-transcribed
scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which
integrates ASR-transcribed speech with landmark coordinates through a
structured three-step reasoning process: (1) extracting spatial relations, (2)
mapping coordinates to absolute directions, and (3) inferring user orientation.
A curriculum learning strategy progressively builds these capabilities on
Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of
resource-constrained settings. Experiments show that MCoT achieves 100%
orientation accuracy on clean transcripts and 98.1% with ASR transcripts,
substantially outperforming unimodal and non-structured baselines. Moreover,
MCoT demonstrates robustness under noisy conversational conditions, including
ASR recognition errors and multilingual code-switching. The model also
maintains high accuracy in cross-domain evaluation and resilience to linguistic
variation, domain shift, and referential ambiguity. These findings highlight
the potential of structured MCoT spatial reasoning as a path toward
interpretable and resource-efficient embodied navigation.

</details>


### [301] [Variational Task Vector Composition](https://arxiv.org/abs/2509.18208)
*Boyuan Zhang,Yingjun Du,Xiantong Zhen,Ling Shao*

Main category: cs.LG

TL;DR: The paper introduces variational task vector composition with a Bayesian framework for integrating knowledge from multiple tasks, showing better performance in efficiency and interpretability.


<details>
  <summary>Details</summary>
Motivation: To enhance task vector composition by addressing structural redundancy and high variance in high-dimensional spaces, aiming for better generalization without additional inference costs.

Method: The authors propose a Bayesian framework with a Spike-and-Slab prior for sparsity, and a gated sampling mechanism to construct a controllable posterior filtered by uncertainty and importance.

Result: The method outperforms existing approaches across different datasets by selectively leveraging the most reliable components of task vectors.

Conclusion: Variational task vector composition provides a more efficient, stable, and interpretable approach for integrating knowledge across tasks, setting a new benchmark in the field.

Abstract: Task vectors capture how a model changes during fine-tuning by recording the
difference between pre-trained and task-specific weights. The composition of
task vectors, a key operator in task arithmetic, enables models to integrate
knowledge from multiple tasks without incurring additional inference costs. In
this paper, we propose variational task vector composition, where composition
coefficients are taken as latent variables and estimated in a Bayesian
inference framework. Unlike previous methods that operate at the task level,
our framework focuses on sample-specific composition. Motivated by the
observation of structural redundancy in task vectors, we introduce a
Spike-and-Slab prior that promotes sparsity and preserves only the most
informative components. To further address the high variance and sampling
inefficiency in sparse, high-dimensional spaces, we develop a gated sampling
mechanism that constructs a controllable posterior by filtering the composition
coefficients based on both uncertainty and importance. This yields a more
stable and interpretable variational framework by deterministically selecting
reliable task components, reducing sampling variance while improving
transparency and generalization. Experimental results demonstrate that our
method consistently outperforms existing approaches across all datasets by
selectively leveraging the most reliable and informative components in task
vectors. These findings highlight the practical value of our approach,
establishing a new standard for efficient and effective task vector
composition.

</details>


### [302] [MolPILE - large-scale, diverse dataset for molecular representation learning](https://arxiv.org/abs/2509.18353)
*Jakub Adamczyk,Jakub Poziemski,Franciszek Job,Mateusz Król,Maciej Makowski*

Main category: cs.LG

TL;DR: The paper introduces MolPILE, a highly curated dataset of 222 million molecular compounds, aimed at improving molecular representation learning.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the recognized limitations in existing small molecule datasets, which restrict the effectiveness of molecular representation learning and the generalization ability of models in chemoinformatics.

Method: The authors compiled MolPILE by automating a curation pipeline to extract and standardize data from 6 large-scale molecular databases, aiming for size, diversity, and quality.

Result: Retraining existing molecular representation models on MolPILE demonstrated improved generalization capabilities compared to prior datasets.

Conclusion: MolPILE serves as a standardized, large-scale resource akin to ImageNet for molecular chemistry, addressing critical dataset shortcomings for machine learning model training.

Abstract: The size, diversity, and quality of pretraining datasets critically determine
the generalization ability of foundation models. Despite their growing
importance in chemoinformatics, the effectiveness of molecular representation
learning has been hindered by limitations in existing small molecule datasets.
To address this gap, we present MolPILE, large-scale, diverse, and rigorously
curated collection of 222 million compounds, constructed from 6 large-scale
databases using an automated curation pipeline. We present a comprehensive
analysis of current pretraining datasets, highlighting considerable
shortcomings for training ML models, and demonstrate how retraining existing
models on MolPILE yields improvements in generalization performance. This work
provides a standardized resource for model training, addressing the pressing
need for an ImageNet-like dataset in molecular chemistry.

</details>


### [303] [FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction](https://arxiv.org/abs/2509.18362)
*Yuxuan Cai,Xiaozhuan Liang,Xinghua Wang,Jin Ma,Haijin Liang,Jinwen Luo,Xinyu Zuo,Lisheng Duan,Yuyang Yin,Xi Chen*

Main category: cs.LG

TL;DR: FastMTP enhances Large Language Model inference efficiency by improving Multi-Token Prediction quality and reducing computational overhead, achieving significant speedups.


<details>
  <summary>Details</summary>
Motivation: Address the bottleneck created by autoregressive generation in Large Language Models and explore Multi-Token Prediction as a means to improve LLM inference.

Method: Fine-tune a Multi-Token Prediction head with position-shared weights on self-distilled data; align MTP training with inference; integrate dynamic vocabulary compression.

Result: FastMTP achieves an average speedup of 2.03x compared to standard next token prediction with lossless output quality, outperforms vanilla MTP by 82%, and reduces computational overhead.

Conclusion: FastMTP is an effective and deployable solution for accelerating LLM inference, requiring minimal additional training and integrating seamlessly into existing systems.

Abstract: As large language models (LLMs) become increasingly powerful, the sequential
nature of autoregressive generation creates a fundamental throughput bottleneck
that limits the practical deployment. While Multi-Token Prediction (MTP) has
demonstrated remarkable benefits for model training efficiency and performance,
its inherent potential for inference acceleration remains largely unexplored.
This paper introduces FastMTP, a simple yet effective method that improves
multi-step draft quality by aligning MTP training with its inference pattern,
significantly enhancing speculative decoding performance. Our approach
fine-tunes a single MTP head with position-shared weights on self-distilled
data, enabling it to capture dependencies among consecutive future tokens and
maintain high acceptance rates across multiple recursive draft steps. By
integrating language-aware dynamic vocabulary compression into the MTP head, we
further reduce computational overhead in the drafting process. Experimental
results across seven diverse benchmarks demonstrate that FastMTP achieves an
average of 2.03x speedup compared to standard next token prediction with
lossless output quality, outperforming vanilla MTP by 82%. FastMTP requires
only lightweight training and seamlessly integrates with existing inference
frameworks, offering a practical and rapidly deployable solution for
accelerating LLM inference.

</details>


### [304] [Multi-Worker Selection based Distributed Swarm Learning for Edge IoT with Non-i.i.d. Data](https://arxiv.org/abs/2509.18367)
*Zhuoyu Yao,Yue Wang,Songyang Zhang,Yingshu Li,Zhipeng Cai,Zhi Tian*

Main category: cs.LG

TL;DR: The paper addresses challenges in Distributed Swarm Learning (DSL) due to non-i.i.d. data and proposes the M-DSL algorithm, which improves global model updates in heterogeneous edge networks.


<details>
  <summary>Details</summary>
Motivation: The main motivation is to overcome the performance degradation of DSL caused by non-i.i.d. data in edge Internet of Things (IoT) scenarios and to investigate the theoretical impact of data heterogeneity on model training accuracy.

Method: The authors propose the M-DSL algorithm, introduce a new non-i.i.d. degree metric to quantify data heterogeneity, and design a multi-worker selection strategy for effective global model updates. They analyze convergence behavior theoretically and validate the system via experiments on heterogeneous datasets.

Result: The M-DSL algorithm consistently outperforms existing DSL benchmarks in terms of performance improvement, learning efficiency, and adaptability to heterogeneous datasets.

Conclusion: The study concludes that accounting for data heterogeneity via the proposed M-DSL algorithm enhances learning outcomes in distributed systems, offering scalability and robustness for edge IoT environments.

Abstract: Recent advances in distributed swarm learning (DSL) offer a promising
paradigm for edge Internet of Things. Such advancements enhance data privacy,
communication efficiency, energy saving, and model scalability. However, the
presence of non-independent and identically distributed (non-i.i.d.) data pose
a significant challenge for multi-access edge computing, degrading learning
performance and diverging training behavior of vanilla DSL. Further, there
still lacks theoretical guidance on how data heterogeneity affects model
training accuracy, which requires thorough investigation. To fill the gap, this
paper first study the data heterogeneity by measuring the impact of non-i.i.d.
datasets under the DSL framework. This then motivates a new multi-worker
selection design for DSL, termed M-DSL algorithm, which works effectively with
distributed heterogeneous data. A new non-i.i.d. degree metric is introduced
and defined in this work to formulate the statistical difference among local
datasets, which builds a connection between the measure of data heterogeneity
and the evaluation of DSL performance. In this way, our M-DSL guides effective
selection of multiple works who make prominent contributions for global model
updates. We also provide theoretical analysis on the convergence behavior of
our M-DSL, followed by extensive experiments on different heterogeneous
datasets and non-i.i.d. data settings. Numerical results verify performance
improvement and network intelligence enhancement provided by our M-DSL beyond
the benchmarks.

</details>


### [305] [GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability](https://arxiv.org/abs/2509.18376)
*Burouj Armgaan,Eshan Jain,Harsh Pandey,Mahesh Chandran,Sayan Ranu*

Main category: cs.LG

TL;DR: GnnXemplar is a novel global explanation method for Graph Neural Networks (GNNs) using representative nodes and language-based rules for interpretability.


<details>
  <summary>Details</summary>
Motivation: The opaque nature of GNNs restricts their trust and broader adoption, particularly due to the lack of effective global explanation methods.

Method: GnnXemplar identifies exemplar nodes through reverse k-nearest neighbors coverage maximization and derives natural language rules using self-refining prompts with large language models.

Result: GnnXemplar exhibits superior performance compared to existing methods in fidelity, scalability, and human interpretability, evidenced by experiments and user studies.

Conclusion: GnnXemplar addresses key limitations in global explanation methods for GNNs, paving the way for clearer, scalable, and human-understandable interpretations of GNN predictions.

Abstract: Graph Neural Networks (GNNs) are widely used for node classification, yet
their opaque decision-making limits trust and adoption. While local
explanations offer insights into individual predictions, global explanation
methods, those that characterize an entire class, remain underdeveloped.
Existing global explainers rely on motif discovery in small graphs, an approach
that breaks down in large, real-world settings where subgraph repetition is
rare, node attributes are high-dimensional, and predictions arise from complex
structure-attribute interactions. We propose GnnXemplar, a novel global
explainer inspired from Exemplar Theory from cognitive science. GnnXemplar
identifies representative nodes in the GNN embedding space, exemplars, and
explains predictions using natural language rules derived from their
neighborhoods. Exemplar selection is framed as a coverage maximization problem
over reverse k-nearest neighbors, for which we provide an efficient greedy
approximation. To derive interpretable rules, we employ a self-refining prompt
strategy using large language models (LLMs). Experiments across diverse
benchmarks show that GnnXemplar significantly outperforms existing methods in
fidelity, scalability, and human interpretability, as validated by a user study
with 60 participants.

</details>


### [306] [Graph Enhanced Trajectory Anomaly Detection](https://arxiv.org/abs/2509.18386)
*Jonathan Kabala Mbuya,Dieter Pfoser,Antonios Anastasopoulos*

Main category: cs.LG

TL;DR: The paper introduces a novel anomaly detection framework, GETAD, that incorporates road network topology and graph structures to improve anomaly detection in trajectories.


<details>
  <summary>Details</summary>
Motivation: Existing trajectory anomaly detection methods lack the ability to consider the constraints and connectivity of road networks and usually treat trajectories only as sequences of locations.

Method: The approach integrates Graph Attention Networks and Transformer-based decoders to model trajectory data while leveraging road network topology and augmenting graph embeddings with positional encodings. A multiobjective loss function ensures realistic representations.

Result: GETAD outperforms existing methods in experiments on real-world and synthetic datasets, particularly excelling in identifying subtle anomalies in road-constrained contexts.

Conclusion: Modeling trajectories with graph structure and contextual semantics enhances anomaly detection, making it more accurate and context-aware in road-constrained environments.

Abstract: Trajectory anomaly detection is essential for identifying unusual and
unexpected movement patterns in applications ranging from intelligent
transportation systems to urban safety and fraud prevention.
  Existing methods only consider limited aspects of the trajectory nature and
its movement space by treating trajectories as sequences of sampled locations,
with sampling determined by positioning technology, e.g., GPS, or by high-level
abstractions such as staypoints. Trajectories are analyzed in Euclidean space,
neglecting the constraints and connectivity information of the underlying
movement network, e.g., road or transit networks.
  The proposed Graph Enhanced Trajectory Anomaly Detection (GETAD) framework
tightly integrates road network topology, segment semantics, and historical
travel patterns to model trajectory data. GETAD uses a Graph Attention Network
to learn road-aware embeddings that capture both physical attributes and
transition behavior, and augments these with graph-based positional encodings
that reflect the spatial layout of the road network.
  A Transformer-based decoder models sequential movement, while a
multiobjective loss function combining autoregressive prediction and supervised
link prediction ensures realistic and structurally coherent representations.
  To improve the robustness of anomaly detection, we introduce Confidence
Weighted Negative Log Likelihood (CW NLL), an anomaly scoring function that
emphasizes high-confidence deviations.
  Experiments on real-world and synthetic datasets demonstrate that GETAD
achieves consistent improvements over existing methods, particularly in
detecting subtle anomalies in road-constrained environments. These results
highlight the benefits of incorporating graph structure and contextual
semantics into trajectory modeling, enabling more precise and context-aware
anomaly detection.

</details>


### [307] [Towards Provable Emergence of In-Context Reinforcement Learning](https://arxiv.org/abs/2509.18389)
*Jiuqi Wang,Rohan Chandra,Shangtong Zhang*

Main category: cs.LG

TL;DR: This paper investigates why reinforcement learning (RL) algorithms trained using standard techniques enable in-context RL (ICRL), where pretrained agents can dynamically adapt to new tasks without updating network parameters by conditioning on contextual input.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to understand the phenomenon of ICRL, where RL agents, once pretrained, can adapt to new tasks without requiring parameter updates, an observation raising the question of how standard RL pretraining practices enable this capability.

Method: The authors hypothesize that network parameters capable of ICRL are minimizers of the pretraining loss. They provide initial support for this hypothesis through a case study that includes theoretical proof involving the pretraining of Transformers for policy evaluation.

Result: The study demonstrates that one of the global minimizers of the pretraining loss in Transformers enables in-context temporal difference learning, supporting the hypothesis.

Conclusion: The findings suggest that the ICRL phenomenon can be attributed to the alignment of pretrained network parameters with the global minimizers of the pretraining loss, laying the groundwork for further exploration of this capability.

Abstract: Typically, a modern reinforcement learning (RL) agent solves a task by
updating its neural network parameters to adapt its policy to the task.
Recently, it has been observed that some RL agents can solve a wide range of
new out-of-distribution tasks without parameter updates after pretraining on
some task distribution. When evaluated in a new task, instead of making
parameter updates, the pretrained agent conditions its policy on additional
input called the context, e.g., the agent's interaction history in the new
task. The agent's performance increases as the information in the context
increases, with the agent's parameters fixed. This phenomenon is typically
called in-context RL (ICRL). The pretrained parameters of the agent network
enable the remarkable ICRL phenomenon. However, many ICRL works perform the
pretraining with standard RL algorithms. This raises the central question this
paper aims to address: Why can the RL pretraining algorithm generate network
parameters that enable ICRL? We hypothesize that the parameters capable of ICRL
are minimizers of the pretraining loss. This work provides initial support for
this hypothesis through a case study. In particular, we prove that when a
Transformer is pretrained for policy evaluation, one of the global minimizers
of the pretraining loss can enable in-context temporal difference learning.

</details>


### [308] [Development of Deep Learning Optimizers: Approaches, Concepts, and Update Rules](https://arxiv.org/abs/2509.18396)
*Doğay Altınel*

Main category: cs.LG

TL;DR: The paper reviews various deep learning optimization algorithms, from earlier methods like Stochastic Gradient Descent to modern ones like AdamW, Sophia, and Muon, summarizing their update rules, contributions, and hyperparameters.


<details>
  <summary>Details</summary>
Motivation: Understanding how different deep learning optimizers contribute to neural network training and identifying areas where optimization can be improved.

Method: Chronological examination of optimizers, providing detailed explanations of their update rules, hyperparameters, and distinctive features.

Result: The study highlights the advancements and diversity in deep learning optimization approaches, showcasing their individual contributions and challenges.

Conclusion: A comprehensive resource is presented for understanding and improving deep learning optimization techniques, while identifying challenges for future research.

Abstract: Deep learning optimizers are optimization algorithms that enable deep neural
networks to learn. The effectiveness of learning is highly dependent on the
optimizer employed in the training process. Alongside the rapid advancement of
deep learning, a wide range of optimizers with different approaches have been
developed. This study aims to provide a review of various optimizers that have
been proposed and received attention in the literature. From Stochastic
gradient descent to the most recent ones such as Momentum, AdamW, Sophia, and
Muon in chronological order, optimizers are examined individually, and their
distinctive features are highlighted in the study. The update rule of each
optimizer is presented in detail, with an explanation of the associated
concepts and variables. The techniques applied by these optimizers, their
contributions to the optimization process, and their default hyperparameter
settings are also discussed. In addition, insights are offered into the open
challenges encountered in the optimization of deep learning models. Thus, a
comprehensive resource is provided both for understanding the current state of
optimizers and for identifying potential areas of future development.

</details>


### [309] [Explicit Path CGR: Maintaining Sequence Fidelity in Geometric Representations](https://arxiv.org/abs/2509.18408)
*Sarwan Ali*

Main category: cs.LG

TL;DR: The paper introduces Reverse-CGR (R-CGR), a method that enables biological sequence recovery in Chaos Game Representation (CGR) by preserving both positional and character details while generating interpretable visualizations for classification.


<details>
  <summary>Details</summary>
Motivation: Traditional CGR methods lose sequence information during geometric mapping, limiting effectiveness in biological sequence analysis.

Method: Reverse-CGR explicitly encodes path information alongside rational arithmetic precision, preserving complete sequence details and allowing for sequence reconstruction.

Result: The method achieved competitive performance in biological sequence classification tasks and generated feature-rich images suitable for deep learning.

Conclusion: R-CGR offers a novel, interpretable bioinformatics tool allowing both accuracy in classification tasks and complete sequence recovery through enhanced geometric representations.

Abstract: We present a novel information-preserving Chaos Game Representation (CGR)
method, also called Reverse-CGR (R-CGR), for biological sequence analysis that
addresses the fundamental limitation of traditional CGR approaches - the loss
of sequence information during geometric mapping. Our method introduces
complete sequence recovery through explicit path encoding combined with
rational arithmetic precision control, enabling perfect sequence reconstruction
from stored geometric traces. Unlike purely geometric approaches, our
reversibility is achieved through comprehensive path storage that maintains
both positional and character information at each step. We demonstrate the
effectiveness of R-CGR on biological sequence classification tasks, achieving
competitive performance compared to traditional sequence-based methods while
providing interpretable geometric visualizations. The approach generates
feature-rich images suitable for deep learning while maintaining complete
sequence information through explicit encoding, opening new avenues for
interpretable bioinformatics analysis where both accuracy and sequence recovery
are essential.

</details>


### [310] [Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors](https://arxiv.org/abs/2509.18433)
*Chang Liu,Ladda Thiamwong,Yanjie Fu,Rui Xie*

Main category: cs.LG

TL;DR: The paper introduces KANDI, a method that combines Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse Reinforcement Learning, aimed at promoting physical activity in older adults and addressing key challenges in offline reinforcement learning in healthcare.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle significant challenges in applying offline reinforcement learning in healthcare, such as difficulty in defining direct rewards, limitations of inverse reinforcement learning in complex environments, and misalignment of learned policies with human behavior.

Method: KANDI leverages Kolmogorov-Arnold Networks for flexible reward estimation from low-fall-risk older adults and employs diffusion-based policies within an Actor-Critic framework for efficient action refinement in offline RL.

Result: KANDI was evaluated on wearable activity monitoring data from a clinical trial (PEER study) and outperformed state-of-the-art methods on the D4RL benchmark, proving its efficacy in promoting physical activity among older adults.

Conclusion: KANDI demonstrates strong potential to overcome offline reinforcement learning challenges in healthcare, specifically in activity promotion intervention programs, making it a promising solution for healthcare applications.

Abstract: Utilizing offline reinforcement learning (RL) with real-world clinical data
is getting increasing attention in AI for healthcare. However, implementation
poses significant challenges. Defining direct rewards is difficult, and inverse
RL (IRL) struggles to infer accurate reward functions from expert behavior in
complex environments. Offline RL also encounters challenges in aligning learned
policies with observed human behavior in healthcare applications. To address
challenges in applying offline RL to physical activity promotion for older
adults at high risk of falls, based on wearable sensor activity monitoring, we
introduce Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse
Reinforcement Learning (KANDI). By leveraging the flexible function
approximation in Kolmogorov-Arnold Networks, we estimate reward functions by
learning free-living environment behavior from low-fall-risk older adults
(experts), while diffusion-based policies within an Actor-Critic framework
provide a generative approach for action refinement and efficiency in offline
RL. We evaluate KANDI using wearable activity monitoring data in a two-arm
clinical trial from our Physio-feedback Exercise Program (PEER) study,
emphasizing its practical application in a fall-risk intervention program to
promote physical activity among older adults. Additionally, KANDI outperforms
state-of-the-art methods on the D4RL benchmark. These results underscore
KANDI's potential to address key challenges in offline RL for healthcare
applications, offering an effective solution for activity promotion
intervention strategies in healthcare.

</details>


### [311] [MeshODENet: A Graph-Informed Neural Ordinary Differential Equation Neural Network for Simulating Mesh-Based Physical Systems](https://arxiv.org/abs/2509.18445)
*Kangzheng Liu,Leixin Ma*

Main category: cs.LG

TL;DR: The paper introduces MeshODENet, a framework combining GNNs and Neural ODEs for improved long-term predictive accuracy and computational efficiency in structural mechanics simulations.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical solvers for complex physical systems are computationally expensive for many-query tasks, and existing methods using GNNs for long-term predictions suffer from error accumulation and instability.

Method: The MeshODENet framework integrates spatial reasoning capabilities of Graph Neural Networks (GNNs) with the continuous-time modeling approach of Neural Ordinary Differential Equations (ODEs).

Result: The proposed method demonstrates superior performance in terms of long-term predictive accuracy, stability, and computational efficiency compared to baseline models in simulations involving elastic bodies undergoing non-linear deformations.

Conclusion: MeshODENet is a powerful and generalizable surrogate modeling approach that accelerates the analysis and modeling of complex structural systems, surpassing traditional and baseline methods.

Abstract: The simulation of complex physical systems using a discretized mesh is a
cornerstone of applied mechanics, but traditional numerical solvers are often
computationally prohibitive for many-query tasks. While Graph Neural Networks
(GNNs) have emerged as powerful surrogate models for mesh-based data, their
standard autoregressive application for long-term prediction is often plagued
by error accumulation and instability. To address this, we introduce
MeshODENet, a general framework that synergizes the spatial reasoning of GNNs
with the continuous-time modeling of Neural Ordinary Differential Equations. We
demonstrate the framework's effectiveness and versatility on a series of
challenging structural mechanics problems, including one- and two-dimensional
elastic bodies undergoing large, non-linear deformations. The results
demonstrate that our approach significantly outperforms baseline models in
long-term predictive accuracy and stability, while achieving substantial
computational speed-ups over traditional solvers. This work presents a powerful
and generalizable approach for developing data-driven surrogates to accelerate
the analysis and modeling of complex structural systems.

</details>


### [312] [GluMind: Multimodal Parallel Attention and Knowledge Retention for Robust Cross-Population Blood Glucose Forecasting](https://arxiv.org/abs/2509.18457)
*Ebrahim Farahmand,Reza Rahimi Azghan,Nooshin Taheri Chatrudi,Velarie Yaa Ansu-Baidoo,Eric Kim,Gautham Krishna Gudur,Mohit Malu,Owen Krueger,Edison Thomaz,Giulia Pedrielli,Pavan Turaga,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: GluMind is a transformer-based multimodal framework for long-term blood glucose forecasting, utilizing innovative attention mechanisms and a knowledge retention module.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve long-term blood glucose forecasting by integrating multimodal physiological and behavioral signals while addressing issues like varying data sampling rates and catastrophic forgetting.

Method: GluMind employs cross-attention to integrate diverse physiological and behavioral signals and multi-scale attention for capturing long-range temporal dependencies. It also incorporates a knowledge retention technique to handle continual learning scenarios.

Result: Experimental evaluation on the AIREADI dataset demonstrates that GluMind outperforms state-of-the-art models, achieving approximately 15% and 9% improvements in RMSE and MAE, respectively.

Conclusion: GluMind enhances predictive performance and adaptability in continual learning settings, showing promise for robust blood glucose monitoring and forecasting.

Abstract: This paper proposes GluMind, a transformer-based multimodal framework
designed for continual and long-term blood glucose forecasting. GluMind devises
two attention mechanisms, including cross-attention and multi-scale attention,
which operate in parallel and deliver accurate predictive performance.
Cross-attention effectively integrates blood glucose data with other
physiological and behavioral signals such as activity, stress, and heart rate,
addressing challenges associated with varying sampling rates and their adverse
impacts on robust prediction. Moreover, the multi-scale attention mechanism
captures long-range temporal dependencies. To mitigate catastrophic forgetting,
GluMind incorporates a knowledge retention technique into the transformer-based
forecasting model. The knowledge retention module not only enhances the model's
ability to retain prior knowledge but also boosts its overall forecasting
performance. We evaluate GluMind on the recently released AIREADI dataset,
which contains behavioral and physiological data collected from healthy people,
individuals with prediabetes, and those with type 2 diabetes. We examine the
performance stability and adaptability of GluMind in learning continuously as
new patient cohorts are introduced. Experimental results show that GluMind
consistently outperforms other state-of-the-art forecasting models, achieving
approximately 15% and 9% improvements in root mean squared error (RMSE) and
mean absolute error (MAE), respectively.

</details>


### [313] [Discrete-time diffusion-like models for speech synthesis](https://arxiv.org/abs/2509.18470)
*Xiaozhou Tan,Minghui Zhao,Mattias Cross,Anton Ragni*

Main category: cs.LG

TL;DR: The paper introduces discrete-time diffusion-like processes for speech generation, addressing limitations in continuous-time methods.


<details>
  <summary>Details</summary>
Motivation: To overcome the inefficiencies and mismatches of continuous-time diffusion models in speech generation during training and inference.

Method: The authors propose discrete-time processes that utilize various noise types, such as additive Gaussian, multiplicative Gaussian, blurring, and mixed noises.

Result: The discrete-time processes achieve comparable speech quality to continuous methods while being more efficient and consistent in training and inference.

Conclusion: Discrete-time processes can serve as a viable alternative to continuous-time models, optimizing performance and matching conditions across training and inference.

Abstract: Diffusion models have attracted a lot of attention in recent years. These
models view speech generation as a continuous-time process. For efficient
training, this process is typically restricted to additive Gaussian noising,
which is limiting. For inference, the time is typically discretized, leading to
the mismatch between continuous training and discrete sampling conditions.
Recently proposed discrete-time processes, on the other hand, usually do not
have these limitations, may require substantially fewer inference steps, and
are fully consistent between training/inference conditions. This paper explores
some diffusion-like discrete-time processes and proposes some new variants.
These include processes applying additive Gaussian noise, multiplicative
Gaussian noise, blurring noise and a mixture of blurring and Gaussian noises.
The experimental results suggest that discrete-time processes offer comparable
subjective and objective speech quality to their widely popular continuous
counterpart, with more efficient and consistent training and inference schemas.

</details>


### [314] [Individualized non-uniform quantization for vector search](https://arxiv.org/abs/2509.18471)
*Mariano Tepper,Ted Willke*

Main category: cs.LG

TL;DR: This paper proposes NVQ, a new vector compression method to enhance computational and storage efficiency for high-dimensional embedding vectors.


<details>
  <summary>Details</summary>
Motivation: High-dimensional embedding vectors are important for semantic similarity searches, but they are computationally and spatially expensive to retrieve and store.

Method: The paper introduces Non-Uniform Vector Quantization (NVQ), which uses novel parsimonious nonlinearities to build quantizers individually tailored for each vector.

Result: The experimental results demonstrate that NVQ improves accuracy versus state-of-the-art methods while maintaining minimal computational cost.

Conclusion: NVQ is an effective and efficient vector compression technique for handling high-dimensional embedding vectors with competitive accuracy and reduced computational footprint.

Abstract: Embedding vectors are widely used for representing unstructured data and
searching through it for semantically similar items. However, the large size of
these vectors, due to their high-dimensionality, creates problems for modern
vector search techniques: retrieving large vectors from memory/storage is
expensive and their footprint is costly. In this work, we present NVQ
(non-uniform vector quantization), a new vector compression technique that is
computationally and spatially efficient in the high-fidelity regime. The core
in NVQ is to use novel parsimonious and computationally efficient
nonlinearities for building non-uniform vector quantizers. Critically, these
quantizers are \emph{individually} learned for each indexed vector. Our
experimental results show that NVQ exhibits improved accuracy compared to the
state of the art with a minimal computational cost.

</details>


### [315] [SimpleFold: Folding Proteins is Simpler than You Think](https://arxiv.org/abs/2509.18480)
*Yuyang Wang,Jiarui Lu,Navdeep Jaitly,Josh Susskind,Miguel Angel Bautista*

Main category: cs.LG

TL;DR: SimpleFold is a model that challenges traditional protein folding model architectures by using standard transformer blocks. It achieves competitive results while being simpler and more efficient.


<details>
  <summary>Details</summary>
Motivation: The motivation is to question whether the complex architectural designs in existing protein folding models are necessary and to explore the potential of leveraging general-purpose generative models for this task.

Method: SimpleFold is based on transformer blocks combined with adaptive layers and trained using a generative flow-matching objective with an additional structural term. It avoids traditional computationally expensive modules and uses a dataset of approximately 9M protein structures and PDB data.

Result: SimpleFold-3B achieves competitive performance on protein folding benchmarks, demonstrates strong ensemble prediction abilities, and operates efficiently on consumer-level hardware.

Conclusion: SimpleFold shows that effective protein folding models can be built without relying on domain-specific architectural designs, opening up new avenues for model design in this field.

Abstract: Protein folding models have achieved groundbreaking results typically via a
combination of integrating domain knowledge into the architectural blocks and
training pipelines. Nonetheless, given the success of generative models across
different but related problems, it is natural to question whether these
architectural designs are a necessary condition to build performant models. In
this paper, we introduce SimpleFold, the first flow-matching based protein
folding model that solely uses general purpose transformer blocks. Protein
folding models typically employ computationally expensive modules involving
triangular updates, explicit pair representations or multiple training
objectives curated for this specific domain. Instead, SimpleFold employs
standard transformer blocks with adaptive layers and is trained via a
generative flow-matching objective with an additional structural term. We scale
SimpleFold to 3B parameters and train it on approximately 9M distilled protein
structures together with experimental PDB data. On standard folding benchmarks,
SimpleFold-3B achieves competitive performance compared to state-of-the-art
baselines, in addition SimpleFold demonstrates strong performance in ensemble
prediction which is typically difficult for models trained via deterministic
reconstruction objectives. Due to its general-purpose architecture, SimpleFold
shows efficiency in deployment and inference on consumer-level hardware.
SimpleFold challenges the reliance on complex domain-specific architectures
designs in protein folding, opening up an alternative design space for future
progress.

</details>


### [316] [Physics-informed time series analysis with Kolmogorov-Arnold Networks under Ehrenfest constraints](https://arxiv.org/abs/2509.18483)
*Abhijit Sen,Illya V. Lukin,Kurt Jacobs,Lev Kaplan,Andrii G. Sotnikov,Denys I. Bondar*

Main category: cs.LG

TL;DR: The paper introduces Kolmogorov Arnold Networks (KANs) enhanced with physics-informed loss functions to improve the modeling of quantum dynamical responses.


<details>
  <summary>Details</summary>
Motivation: Quantum dynamics evolve in high-dimensional spaces, making traditional numerical modeling computationally expensive. Existing neural methods struggle with data efficiency and physical interpretability.

Method: The authors developed KANs incorporating physics-informed loss functions to respect the Ehrenfest theorems and introduced a Chain of KANs architecture for embedding temporal causality.

Result: KANs showed higher accuracy with less data (just 5.4% of the samples needed by other methods). The Chain of KANs architecture further improved time series modeling.

Conclusion: The physics-informed KAN approach enhances computational efficiency, physical consistency, and data performance compared to traditional methods in quantum dynamics forecasting.

Abstract: The prediction of quantum dynamical responses lies at the heart of modern
physics. Yet, modeling these time-dependent behaviors remains a formidable
challenge because quantum systems evolve in high-dimensional Hilbert spaces,
often rendering traditional numerical methods computationally prohibitive.
While large language models have achieved remarkable success in sequential
prediction, quantum dynamics presents a fundamentally different challenge:
forecasting the entire temporal evolution of quantum systems rather than merely
the next element in a sequence. Existing neural architectures such as recurrent
and convolutional networks often require vast training datasets and suffer from
spurious oscillations that compromise physical interpretability. In this work,
we introduce a fundamentally new approach: Kolmogorov Arnold Networks (KANs)
augmented with physics-informed loss functions that enforce the Ehrenfest
theorems. Our method achieves superior accuracy with significantly less
training data: it requires only 5.4 percent of the samples (200) compared to
Temporal Convolution Networks (3,700). We further introduce the Chain of KANs,
a novel architecture that embeds temporal causality directly into the model
design, making it particularly well-suited for time series modeling. Our
results demonstrate that physics-informed KANs offer a compelling advantage
over conventional black-box models, maintaining both mathematical rigor and
physical consistency while dramatically reducing data requirements.

</details>


### [317] [Hybrid Data can Enhance the Utility of Synthetic Data for Training Anti-Money Laundering Models](https://arxiv.org/abs/2509.18499)
*Rachel Chung,Pratyush Nidhi Sharma,Mikko Siponen,Rohit Vadodaria,Luke Smith*

Main category: cs.LG

TL;DR: The paper highlights the use of hybrid datasets, blending synthetic and real-world features, to train anti-money laundering (AML) models while preserving privacy and enhancing performance.


<details>
  <summary>Details</summary>
Motivation: Money laundering is a major global challenge, yet access to relevant data for training detection models is limited due to privacy concerns.

Method: The paper introduces hybrid datasets, combining synthetic data with publicly accessible real-world features to train AML models effectively.

Result: Hybrid datasets improve the utility of AML models compared to purely synthetic datasets, while maintaining privacy.

Conclusion: Using hybrid datasets provides a viable solution for improving AML model training, balancing privacy protection with enhanced detection capabilities.

Abstract: Money laundering is a critical global issue for financial institutions.
Automated Anti-money laundering (AML) models, like Graph Neural Networks (GNN),
can be trained to identify illicit transactions in real time. A major issue for
developing such models is the lack of access to training data due to privacy
and confidentiality concerns. Synthetically generated data that mimics the
statistical properties of real data but preserves privacy and confidentiality
has been proposed as a solution. However, training AML models on purely
synthetic datasets presents its own set of challenges. This article proposes
the use of hybrid datasets to augment the utility of synthetic datasets by
incorporating publicly available, easily accessible, and real-world features.
These additions demonstrate that hybrid datasets not only preserve privacy but
also improve model utility, offering a practical pathway for financial
institutions to enhance AML systems.

</details>


### [318] [APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation](https://arxiv.org/abs/2509.18521)
*Yuzhen Zhou,Jiajun Li,Yusheng Su,Gowtham Ramesh,Zilin Zhu,Xiang Long,Chenyang Zhao,Jin Pan,Xiaodong Yu,Ze Wang,Kangrui Du,Jialian Wu,Ximeng Sun,Jiang Liu,Qiaolin Yu,Hao Chen,Zicheng Liu,Emad Barsoum*

Main category: cs.LG

TL;DR: APRIL improves RL training efficiency by addressing GPU idle time caused by long-tail rollout responses, achieving better throughput and accuracy.


<details>
  <summary>Details</summary>
Motivation: RL training for LLMs is computationally intensive, especially in rollout generation, which limits scalability due to inefficient GPU utilization.

Method: APRIL over-provisions rollout requests, halts once target responses are obtained, and repurposes partial rollouts to reduce GPU idle time during training.

Result: APRIL enhances rollout throughput by up to 44%, boosts convergence speed, and achieves up to 8% higher final accuracy across RL tasks.

Conclusion: APRIL combines system and algorithm optimizations to improve RL efficiency and scalability, integrating seamlessly into existing frameworks and hardware setups.

Abstract: Reinforcement learning (RL) has become a cornerstone in advancing large-scale
pre-trained language models (LLMs). Successive generations, including GPT-o
series, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale
RL training to enhance reasoning and coding capabilities. To meet the
community's growing RL needs, numerous RL frameworks have been proposed. Most
of these frameworks primarily rely on inference engines for rollout generation
and training engines for policy updates. However, RL training remains
computationally expensive, with rollout generation accounting for more than 90%
of total runtime. In addition, its efficiency is often constrained by the
long-tail distribution of rollout response lengths, where a few lengthy
responses stall entire batches, leaving GPUs idle and underutilized. As model
and rollout sizes continue to grow, this bottleneck increasingly limits
scalability. To address this challenge, we propose Active Partial Rollouts in
Reinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the
rollout phase, APRIL over-provisions rollout requests, terminates once the
target number of responses is reached, and recycles incomplete responses for
continuation in future steps. This strategy ensures that no rollouts are
discarded while substantially reducing GPU idle time. Experiments show that
APRIL improves rollout throughput by at most 44% across commonly used RL
algorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves at most 8%
higher final accuracy across tasks. Moreover, APRIL is both framework and
hardware agnostic, already integrated into the slime RL framework, and
deployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies
system-level and algorithmic considerations in proposing APRIL, with the aim of
advancing RL training efficiency and inspiring further optimizations in RL
systems.

</details>


### [319] [Reverse-Complement Consistency for DNA Language Models](https://arxiv.org/abs/2509.18529)
*Mingqian Ma*

Main category: cs.LG

TL;DR: The paper introduces RCCR, a method to ensure that DNA models make consistent predictions for sequences and their reverse complements, improving reliability without reducing accuracy.


<details>
  <summary>Details</summary>
Motivation: DNA sequences and their reverse complements often carry the same biological meaning, but existing DNA models make inconsistent predictions for them, reducing reliability.

Method: The authors propose Reverse-Complement Consistency Regularization (RCCR), which penalizes prediction discrepancies between a DNA sequence and its reverse complement during model fine-tuning.

Result: RCCR significantly improves the consistency of predictions between sequences and their reverse complements across different models and tasks, while also maintaining or enhancing task accuracy.

Conclusion: RCCR provides a robust, biologically-informed, and efficient fine-tuning approach to improve the reliability of DNA language models on diverse applications.

Abstract: A fundamental property of DNA is that the reverse complement (RC) of a
sequence often carries identical biological meaning. However, state-of-the-art
DNA language models frequently fail to capture this symmetry, producing
inconsistent predictions for a sequence and its RC counterpart, which
undermines their reliability. In this work, we introduce Reverse-Complement
Consistency Regularization (RCCR), a simple and model-agnostic fine-tuning
objective that directly penalizes the divergence between a model's prediction
on a sequence and the aligned prediction on its reverse complement. We evaluate
RCCR across three diverse backbones (Nucleotide Transformer, HyenaDNA,
DNABERT-2) on a wide range of genomic tasks, including sequence classification,
scalar regression, and profile prediction. Our experiments show that RCCR
substantially improves RC robustness by dramatically reducing prediction flips
and errors, all while maintaining or improving task accuracy compared to
baselines such as RC data augmentation and test-time averaging. By integrating
a key biological prior directly into the learning process, RCCR produces a
single, intrinsically robust, and computationally efficient model fine-tuning
recipe for diverse biology tasks.

</details>


### [320] [Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts](https://arxiv.org/abs/2509.18542)
*Qi Wang,Hanyang Peng,Yue Yu*

Main category: cs.LG

TL;DR: The paper introduces Symphony-MoE, a framework for constructing Mixture-of-Experts (MoE) models from multiple pre-trained models, overcoming parameter misalignment and boosting performance in multi-domain tasks.


<details>
  <summary>Details</summary>
Motivation: Current approaches to Mixture-of-Experts (MoE) models face limitations in expert diversity due to reliance on a single pre-trained dense model.

Method: The paper proposes Symphony-MoE, a two-stage framework: (1) a training-free phase using a layer-aware fusion strategy and activation-based functional alignment to harmonize disparate pre-trained models, and (2) lightweight router training to coordinate the architecture.

Result: Symphony-MoE effectively integrates experts from multiple heterogeneous pre-trained models, achieving superior performance in multi-domain and out-of-distribution generalization tasks compared to baseline methods.

Conclusion: Symphony-MoE successfully addresses expert diversity limitations and parameter misalignment in MoE models, demonstrating its potential in multi-domain learning scenarios.

Abstract: Mixture-of-Experts (MoE) models enable scalable performance by activating
large parameter sets sparsely, minimizing computational overhead. To circumvent
the prohibitive cost of training MoEs from scratch, recent work employs
upcycling, reusing a single pre-trained dense model by replicating its
feed-forward network (FFN) layers into experts. However, this limits expert
diversity, as all experts originate from a single pre-trained dense model. This
paper addresses this limitation by constructing powerful MoE models using
experts sourced from multiple identically-architected but disparate pre-trained
models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact
that these source models occupy disparate, dissonant regions of the parameter
space, making direct upcycling prone to severe performance degradation. To
overcome this, we propose Symphony-MoE, a novel two-stage framework designed to
harmonize these models into a single, coherent expert mixture. First, we
establish this harmony in a training-free manner: we construct a shared
backbone via a layer-aware fusion strategy and, crucially, alleviate parameter
misalignment among experts using activation-based functional alignment.
Subsequently, a single lightweight stage of router training coordinates the
entire architecture. Experiments demonstrate that our method successfully
integrates experts from heterogeneous sources, achieving an MoE model that
significantly surpasses baselines in multi-domain tasks and out-of-distribution
generalization.

</details>


### [321] [Global Minimizers of Sigmoid Contrastive Loss](https://arxiv.org/abs/2509.18552)
*Kiril Bangachev,Guy Bresler,Iliyas Noman,Yury Polyanskiy*

Main category: cs.LG

TL;DR: Explores the theoretical advantages of using trainable temperature and bias in SigLIP models, introduces $(m, b_{rel})$-Constellations, and proposes an improved reparameterization of sigmoid loss.


<details>
  <summary>Details</summary>
Motivation: Understand and improve contrastive pretraining for representation alignment, inspired by recent advancements like SigLIP and SigLIP2.

Method: The paper introduces $(m, b_{rel})$-Constellations, relates them to spherical codes, and reparameterizes sigmoid loss with explicit relative bias for better training.

Result: Demonstrates why SigLIP performs well, identifies modality gaps, and suggests necessary dimensions for effective representation.

Conclusion: $(m, b_{rel})$-Constellations provide a theoretical foundation for SigLIP's success, and the proposed sigmoid loss reparameterization enhances training dynamics.

Abstract: The meta-task of obtaining and aligning representations through contrastive
pretraining is steadily gaining importance since its introduction in CLIP and
ALIGN. In this paper we theoretically explain the advantages of synchronizing
with trainable inverse temperature and bias under the sigmoid loss, as
implemented in the recent SigLIP and SigLIP2 models of Google DeepMind.
Temperature and bias can drive the loss function to zero for a rich class of
configurations that we call $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations. $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations are a novel combinatorial object
related to spherical codes and are parametrized by a margin $\mathsf{m}$ and
relative bias $\mathsf{b}_{\mathsf{rel}}$. We use our characterization of
constellations to theoretically justify the success of SigLIP on retrieval, to
explain the modality gap present in SigLIP, and to identify the necessary
dimension for producing high-quality representations. Finally, we propose a
reparameterization of the sigmoid loss with explicit relative bias, which
improves training dynamics in experiments with synthetic data.

</details>


### [322] [Explainable Graph Neural Networks: Understanding Brain Connectivity and Biomarkers in Dementia](https://arxiv.org/abs/2509.18568)
*Niharika Tewari,Nguyen Linh Dan Le,Mujie Liu,Jing Ren,Ziqi Xu,Tabinda Sarwar,Veeky Baths,Feng Xia*

Main category: cs.LG

TL;DR: Dementia diagnosis is complicated due to its heterogeneity, but Explainable Graph Neural Networks (XGNNs) show promise in analyzing brain networks and biomarkers. This paper reviews XGNN applications in dementia research and discusses challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges in diagnosing and differentiating dementia subtypes, leveraging XGNNs to overcome issues like data scarcity and limited interpretability.

Method: The paper provides a comprehensive review of existing research on XGNNs applied to dementia, including taxonomy of explainability methods, model comparisons, and analysis of clinical challenges.

Result: Applications of XGNNs have demonstrated potential in diagnosing dementia-related diseases using brain biomarkers and network analysis, but they face challenges in generalizability and integration with LLMs.

Conclusion: This review emphasizes the progress and open problems in XGNNs, aiming to enhance their clinical significance, scalability, and trustworthiness in dementia research.

Abstract: Dementia is a progressive neurodegenerative disorder with multiple
etiologies, including Alzheimer's disease, Parkinson's disease, frontotemporal
dementia, and vascular dementia. Its clinical and biological heterogeneity
makes diagnosis and subtype differentiation highly challenging. Graph Neural
Networks (GNNs) have recently shown strong potential in modeling brain
connectivity, but their limited robustness, data scarcity, and lack of
interpretability constrain clinical adoption. Explainable Graph Neural Networks
(XGNNs) have emerged to address these barriers by combining graph-based
learning with interpretability, enabling the identification of disease-relevant
biomarkers, analysis of brain network disruptions, and provision of transparent
insights for clinicians. This paper presents the first comprehensive review
dedicated to XGNNs in dementia research. We examine their applications across
Alzheimer's disease, Parkinson's disease, mild cognitive impairment, and
multi-disease diagnosis. A taxonomy of explainability methods tailored for
dementia-related tasks is introduced, alongside comparisons of existing models
in clinical scenarios. We also highlight challenges such as limited
generalizability, underexplored domains, and the integration of Large Language
Models (LLMs) for early detection. By outlining both progress and open
problems, this review aims to guide future work toward trustworthy, clinically
meaningful, and scalable use of XGNNs in dementia research.

</details>


### [323] [Interaction Topological Transformer for Multiscale Learning in Porous Materials](https://arxiv.org/abs/2509.18573)
*Dong Chen,Jian Liu,Chun-Long Chen,Guo-Wei Wei*

Main category: cs.LG

TL;DR: The paper introduces the Interaction Topological Transformer (ITT), a framework for modeling and predicting properties of porous materials by capturing relationships across multiple scales and levels of structure.


<details>
  <summary>Details</summary>
Motivation: Modeling porous materials is challenging due to their complex multiscale structure-property relationships and the limitations of sparse and unevenly distributed data across material families.

Method: The proposed ITT framework uses interaction topology to capture multi-scale and multi-level material information. It integrates extracted features through a Transformer architecture and employs a two-stage training process: self-supervised pretraining and supervised fine-tuning.

Result: ITT demonstrates state-of-the-art, accurate, and transferable predictions for adsorption, transport, and stability properties of porous materials.

Conclusion: The ITT framework offers a scalable and effective method for predictive modeling, enabling learning-guided discovery in diverse porous materials.

Abstract: Porous materials exhibit vast structural diversity and support critical
applications in gas storage, separations, and catalysis. However, predictive
modeling remains challenging due to the multiscale nature of structure-property
relationships, where performance is governed by both local chemical
environments and global pore-network topology. These complexities, combined
with sparse and unevenly distributed labeled data, hinder generalization across
material families. We propose the Interaction Topological Transformer (ITT), a
unified data-efficient framework that leverages novel interaction topology to
capture materials information across multiple scales and multiple levels,
including structural, elemental, atomic, and pairwise-elemental organization.
ITT extracts scale-aware features that reflect both compositional and
relational structure within complex porous frameworks, and integrates them
through a built-in Transformer architecture that supports joint reasoning
across scales. Trained using a two-stage strategy, i.e., self-supervised
pretraining on 0.6 million unlabeled structures followed by supervised
fine-tuning, ITT achieves state-of-the-art, accurate, and transferable
predictions for adsorption, transport, and stability properties. This framework
provides a principled and scalable path for learning-guided discovery in
structurally and chemically diverse porous materials.

</details>


### [324] [DS-Diffusion: Data Style-Guided Diffusion Model for Time-Series Generation](https://arxiv.org/abs/2509.18584)
*Mingchun Sun,Rongqiang Zhao,Jie Liu*

Main category: cs.LG

TL;DR: This paper introduces DS-Diffusion, a style-guided diffusion model for time series generation that addresses retraining needs, distributional bias, and interpretability issues in existing models.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models for time series generation require retraining for specific conditions, face distributional biases, and have unintuitive inference processes.

Method: The proposed DS-Diffusion employs style-guided kernels to eliminate retraining needs and introduces a Time-information based Hierarchical Denoising (THD) mechanism to minimize distributional bias and improve interpretability.

Result: DS-Diffusion outperforms state-of-the-art models like ImagenTime by achieving 5.56% and 61.55% reductions in predictive and discriminative scores, respectively. It also reduces distributional bias and enhances inference interpretability.

Conclusion: DS-Diffusion offers an innovative, flexible, and interpretable framework for time series generation that addresses key limitations of existing diffusion models, demonstrating superior performance and adaptability through comprehensive evaluations.

Abstract: Diffusion models are the mainstream approach for time series generation
tasks. However, existing diffusion models for time series generation require
retraining the entire framework to introduce specific conditional guidance.
There also exists a certain degree of distributional bias between the generated
data and the real data, which leads to potential model biases in downstream
tasks. Additionally, the complexity of diffusion models and the latent spaces
leads to an uninterpretable inference process. To address these issues, we
propose the data style-guided diffusion model (DS-Diffusion). In the
DS-Diffusion, a diffusion framework based on style-guided kernels is developed
to avoid retraining for specific conditions. The time-information based
hierarchical denoising mechanism (THD) is developed to reduce the
distributional bias between the generated data and the real data. Furthermore,
the generated samples can clearly indicate the data style from which they
originate. We conduct comprehensive evaluations using multiple public datasets
to validate our approach. Experimental results show that, compared to the
state-of-the-art model such as ImagenTime, the predictive score and the
discriminative score decrease by 5.56% and 61.55%, respectively. The
distributional bias between the generated data and the real data is further
reduced, the inference process is also more interpretable. Moreover, by
eliminating the need to retrain the diffusion model, the flexibility and
adaptability of the model to specific conditions are also enhanced.

</details>


### [325] [Reflect before Act: Proactive Error Correction in Language Models](https://arxiv.org/abs/2509.18607)
*Qiuhai Zeng,Sarvesh Rajkumar,Di Wang,Narendra Gyanchandani,Wenbo Yan*

Main category: cs.LG

TL;DR: The REBACT method improves LLM-based decision-making by incorporating a reflection step before actions, achieving notable performance increases in several interactive environments.


<details>
  <summary>Details</summary>
Motivation: To address challenges in error accumulation and the lack of robust self-correction in LLM-based decision-making systems.

Method: REBACT introduces a reflection step between actions in LLM workflows to provide immediate error correction and adaptability to feedback.

Result: REBACT shows significant performance improvements, increasing success rates up to 24% on WebShop, 6.72% on ALFWorld, and 0.5% on TextCraft.

Conclusion: REBACT effectively enhances decision-making accuracy and adaptability in LLMs with minimal computational cost.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
interactive decision-making tasks, but existing methods often struggle with
error accumulation and lack robust self-correction mechanisms. We introduce
"Reflect before Act" (REBACT), a novel approach that enhances LLM-based
decision-making by introducing a critical reflect step prior to taking the next
action. This approach allows for immediate error correction, ensuring smooth
action path and adaptibity to environment feedback. We evaluate REBACT on three
diverse interactive environments: ALFWorld, WebShop, and TextCraft. Our results
demonstrate that REBACT significantly outperforms strong baselines, improving
success rates by up to 24% on WebShop (achieving 61%), 6.72% on ALFWorld
(achieving 98.51%), and 0.5% on TextCraft (achieving 99.5%) using
Claude3.5-sonnet as the underlying LLM. Further analysis reveals that REBACT's
performance improvements are achieved with only a few modification steps,
demonstrating its computational efficiency.

</details>


### [326] [Flow marching for a generative PDE foundation model](https://arxiv.org/abs/2509.18611)
*Zituo Chen,Sili Deng*

Main category: cs.LG

TL;DR: This paper introduces "Flow Marching," a generative PDE foundation model designed to improve long-term stability and uncertainty-awareness in modeling dynamical systems. Leveraging pretraining on a large dataset of spatiotemporal trajectories, the method combines neural operator learning and flow matching with key innovations like P2VAE and FMT to enhance efficiency and scalability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in existing PDE foundation models, which often rely on deterministic approaches and struggle with generative flexibility, long-term stability, and computational efficiency, particularly for science and engineering applications.

Method: The researchers propose "Flow Marching," which bridges neural operator learning with flow matching to improve accuracy and generative capabilities. The approach combines noise-level sampling, Physics-Pretrained Variational Autoencoders (P2VAEs) for compact latent representation, and Flow Marching Transformers (FMTs) for efficient processing using latent temporal pyramids.

Result: The approach achieves up to 15x greater computational efficiency compared to full-length video diffusion models and is pretrained on a large dataset of ~2.5M trajectories from 12 PDE families. The results demonstrate effectiveness in long-term rollout stability, few-shot adaptation on turbulence datasets, and uncertainty-aware ensemble generation.

Conclusion: The paper concludes that generative PDE foundation models, supported by innovations like Flow Marching, P2VAE, and FMT, are critical for achieving scalable, stable, and uncertainty-aware modeling capabilities, essential for complex and practical applications in dynamical systems.

Abstract: Pretraining on large-scale collections of PDE-governed spatiotemporal
trajectories has recently shown promise for building generalizable models of
dynamical systems. Yet most existing PDE foundation models rely on
deterministic Transformer architectures, which lack generative flexibility for
many science and engineering applications. We propose Flow Marching, an
algorithm that bridges neural operator learning with flow matching motivated by
an analysis of error accumulation in physical dynamical systems, and we build a
generative PDE foundation model on top of it. By jointly sampling the noise
level and the physical time step between adjacent states, the model learns a
unified velocity field that transports a noisy current state toward its clean
successor, reducing long-term rollout drift while enabling uncertainty-aware
ensemble generations. Alongside this core algorithm, we introduce a
Physics-Pretrained Variational Autoencoder (P2VAE) to embed physical states
into a compact latent space, and an efficient Flow Marching Transformer (FMT)
that combines a diffusion-forcing scheme with latent temporal pyramids,
achieving up to 15x greater computational efficiency than full-length video
diffusion models and thereby enabling large-scale pretraining at substantially
reduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE
families and train suites of P2VAEs and FMTs at multiple scales. On downstream
evaluation, we benchmark on unseen Kolmogorov turbulence with few-shot
adaptation, demonstrate long-term rollout stability over deterministic
counterparts, and present uncertainty-stratified ensemble results, highlighting
the importance of generative PDE foundation models for real-world applications.

</details>


### [327] [HyperAdapt: Simple High-Rank Adaptation](https://arxiv.org/abs/2509.18629)
*Abel Gurung,Joseph Campbell*

Main category: cs.LG

TL;DR: HyperAdapt is a parameter-efficient fine-tuning method that applies row- and column-wise scaling to pre-trained weight matrices, achieving competitive performance with far fewer trainable parameters.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning large foundation models is resource-intensive, and there is a need for methods that reduce memory and computational requirements without sacrificing performance.

Method: HyperAdapt introduces diagonal matrices for row- and column-wise scaling of pre-trained weight matrices. It utilizes only $n+m$ trainable parameters for an $n \times m$ matrix while producing high-rank updates.

Result: HyperAdapt achieves performance that matches or closely approaches full fine-tuning and other state-of-the-art parameter-efficient fine-tuning (PEFT) methods, while requiring significantly fewer trainable parameters.

Conclusion: HyperAdapt demonstrates that highly efficient fine-tuning can maintain competitive performance, making it a promising approach for resource-constrained applications of foundation models.

Abstract: Foundation models excel across diverse tasks, but adapting them to
specialized applications often requires fine-tuning, an approach that is memory
and compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate
this by updating only a small subset of weights. In this paper, we introduce
HyperAdapt, a parameter-efficient fine-tuning method that significantly reduces
the number of trainable parameters compared to state-of-the-art methods like
LoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying
row- and column-wise scaling through diagonal matrices, thereby inducing a
high-rank update while requiring only $n+m$ trainable parameters for an $n
\times m$ matrix. Theoretically, we establish an upper bound on the rank of
HyperAdapt's updates, and empirically, we confirm that it consistently induces
high-rank transformations across model layers. Experiments on GLUE, arithmetic
reasoning, and commonsense reasoning benchmarks with models up to 14B
parameters demonstrate that HyperAdapt matches or nearly matches the
performance of full fine-tuning and state-of-the-art PEFT methods while using
orders of magnitude fewer trainable parameters.

</details>


### [328] [Subspace Clustering of Subspaces: Unifying Canonical Correlation Analysis and Subspace Clustering](https://arxiv.org/abs/2509.18653)
*Paris A. Karakasis,Nicholas D. Sidiropoulos*

Main category: cs.LG

TL;DR: The paper introduces Subspace Clustering of Subspaces (SCoS), a new method for clustering tall matrices based on their column spaces, surpassing traditional subspace clustering approaches. Key results show improved accuracy and robustness in challenging applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of traditional subspace clustering techniques, which rely on vectorized data and overlook the structural representation of data as matrices.

Method: The method utilizes Block Term Decomposition (BTD) of a tensor built from input matrices, enabling simultaneous cluster membership and subspace estimation. Scalable algorithms for large datasets are proposed.

Result: Results from experiments on hyperspectral imaging datasets show higher accuracy and noise robustness compared to traditional subspace clustering methods.

Conclusion: The framework extends subspace clustering to matrix-based representations and proves effective for high-dimensional applications, demonstrating improved performance in noise-heavy scenarios.

Abstract: We introduce a novel framework for clustering a collection of tall matrices
based on their column spaces, a problem we term Subspace Clustering of
Subspaces (SCoS). Unlike traditional subspace clustering methods that assume
vectorized data, our formulation directly models each data sample as a matrix
and clusters them according to their underlying subspaces. We establish
conceptual links to Subspace Clustering and Generalized Canonical Correlation
Analysis (GCCA), and clarify key differences that arise in this more general
setting. Our approach is based on a Block Term Decomposition (BTD) of a
third-order tensor constructed from the input matrices, enabling joint
estimation of cluster memberships and partially shared subspaces. We provide
the first identifiability results for this formulation and propose scalable
optimization algorithms tailored to large datasets. Experiments on real-world
hyperspectral imaging datasets demonstrate that our method achieves superior
clustering accuracy and robustness, especially under high noise and
interference, compared to existing subspace clustering techniques. These
results highlight the potential of the proposed framework in challenging
high-dimensional applications where structure exists beyond individual data
vectors.

</details>


### [329] [Towards Rational Pesticide Design with Graph Machine Learning Models for Ecotoxicology](https://arxiv.org/abs/2509.18703)
*Jakub Adamczyk*

Main category: cs.LG

TL;DR: This paper explores using graph machine learning for rational pesticide design, introducing a large dataset and evaluating ML methods for toxicity prediction.


<details>
  <summary>Details</summary>
Motivation: To develop safer, eco-friendly pesticides by leveraging advanced machine learning techniques inspired by drug discovery.

Method: Created a curated dataset (ApisTox) and evaluated various ML models for molecular graph classification, identifying gaps in their generalization to agrochemical contexts.

Result: ML methods used in medicinal chemistry underperform in agrochemical applications, highlighting the need for domain-specific approaches.

Conclusion: Future research should focus on domain-specific model development and comprehensive benchmark creation tailored to pesticide discovery.

Abstract: This research focuses on rational pesticide design, using graph machine
learning to accelerate the development of safer, eco-friendly agrochemicals,
inspired by in silico methods in drug discovery. With an emphasis on
ecotoxicology, the initial contributions include the creation of ApisTox, the
largest curated dataset on pesticide toxicity to honey bees. We conducted a
broad evaluation of machine learning (ML) models for molecular graph
classification, including molecular fingerprints, graph kernels, GNNs, and
pretrained transformers. The results show that methods successful in medicinal
chemistry often fail to generalize to agrochemicals, underscoring the need for
domain-specific models and benchmarks. Future work will focus on developing a
comprehensive benchmarking suite and designing ML models tailored to the unique
challenges of pesticide discovery.

</details>


### [330] [A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications](https://arxiv.org/abs/2509.18714)
*Zhenyu Tao,Wei Xu,Xiaohu You*

Main category: cs.LG

TL;DR: The paper proposes a generalized bisimulation metric (GBSM) for comparing pairs of Markov decision processes (MDPs) and applies it to policy transfer and other Reinforcement Learning (RL) challenges.


<details>
  <summary>Details</summary>
Motivation: To extend the bisimulation metric (BSM) to address challenges in multi-MDP scenarios such as policy transfer, which current BSM approaches are limited in addressing.

Method: The authors formalize the generalized bisimulation metric (GBSM) by establishing its fundamental mathematical properties (such as symmetry and triangle inequality) and use this to analyze RL tasks with theoretical bounds and sample complexity.

Result: The paper derives tighter explicit bounds for policy transfer, state aggregation, and sampling-based methods using GBSM compared to standard BSM. It also provides a closed-form sample complexity for estimation, validated by numerical experiments.

Conclusion: The proposed GBSM proves to be a powerful tool for multi-MDP scenarios, enabling more efficient policy transfer and related tasks with improved theoretical guarantees and empirical results.

Abstract: The bisimulation metric (BSM) is a powerful tool for computing state
similarities within a Markov decision process (MDP), revealing that states
closer in BSM have more similar optimal value functions. While BSM has been
successfully utilized in reinforcement learning (RL) for tasks like state
representation learning and policy exploration, its application to multiple-MDP
scenarios, such as policy transfer, remains challenging. Prior work has
attempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis
of its mathematical properties has limited further theoretical progress. In
this work, we formally establish a generalized bisimulation metric (GBSM)
between pairs of MDPs, which is rigorously proven with the three fundamental
properties: GBSM symmetry, inter-MDP triangle inequality, and the distance
bound on identical state spaces. Leveraging these properties, we theoretically
analyse policy transfer, state aggregation, and sampling-based estimation in
MDPs, obtaining explicit bounds that are strictly tighter than those derived
from the standard BSM. Additionally, GBSM provides a closed-form sample
complexity for estimation, improving upon existing asymptotic results based on
BSM. Numerical results validate our theoretical findings and demonstrate the
effectiveness of GBSM in multi-MDP scenarios.

</details>


### [331] [LLM-Enhanced Self-Evolving Reinforcement Learning for Multi-Step E-Commerce Payment Fraud Risk Detection](https://arxiv.org/abs/2509.18719)
*Bo Qu,Zhurong Wang,Daisuke Yagi,Zhen Xu,Yang Zhao,Yinan Shan,Frank Zahradnik*

Main category: cs.LG

TL;DR: The paper proposes a novel e-commerce fraud detection approach by integrating reinforcement learning (RL) and large language models (LLMs), using transaction risks framed as a Markov Decision Process.


<details>
  <summary>Details</summary>
Motivation: Improving e-commerce payment fraud detection by addressing limitations in manual reward function crafting for RL models.

Method: Combines RL with LLMs to iteratively refine reward functions in fraud detection through a Markov Decision Process model.

Result: Achieved better fraud detection accuracy, robustness, and resilience in experiments using real-world data.

Conclusion: The framework demonstrates the potential for LLMs to advance reinforcement learning applications in industrial use cases.

Abstract: This paper presents a novel approach to e-commerce payment fraud detection by
integrating reinforcement learning (RL) with Large Language Models (LLMs). By
framing transaction risk as a multi-step Markov Decision Process (MDP), RL
optimizes risk detection across multiple payment stages. Crafting effective
reward functions, essential for RL model success, typically requires
significant human expertise due to the complexity and variability in design.
LLMs, with their advanced reasoning and coding capabilities, are well-suited to
refine these functions, offering improvements over traditional methods. Our
approach leverages LLMs to iteratively enhance reward functions, achieving
better fraud detection accuracy and demonstrating zero-shot capability.
Experiments with real-world data confirm the effectiveness, robustness, and
resilience of our LLM-enhanced RL framework through long-term evaluations,
underscoring the potential of LLMs in advancing industrial RL applications.

</details>


### [332] [Theory of periodic convolutional neural network](https://arxiv.org/abs/2509.18744)
*Yuqing Liu*

Main category: cs.LG

TL;DR: Introduces periodic CNNs, proving their ability to approximate functions with specific ridge structures, making them effective for high-dimensional data with periodicity.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional CNNs in approximating functions on periodic or high-dimensional ridge-like structures.

Method: Developed periodic boundary conditions for convolutional layers and established a theoretical approximation power through mathematical analysis.

Result: Proved the ability of periodic CNNs to approximate ridge functions depending on $d-1$ linear variables in $d$-dimensional space.

Conclusion: Periodic CNNs are highly effective for problems with intrinsic high dimensionality and periodic structures, expanding theoretical and practical utility of CNNs.

Abstract: We introduce a novel convolutional neural network architecture, termed the
\emph{periodic CNN}, which incorporates periodic boundary conditions into the
convolutional layers. Our main theoretical contribution is a rigorous
approximation theorem: periodic CNNs can approximate ridge functions depending
on $d-1$ linear variables in a $d$-dimensional input space, while such
approximation is impossible in lower-dimensional ridge settings ($d-2$ or fewer
variables). This result establishes a sharp characterization of the expressive
power of periodic CNNs. Beyond the theory, our findings suggest that periodic
CNNs are particularly well-suited for problems where data naturally admits a
ridge-like structure of high intrinsic dimension, such as image analysis on
wrapped domains, physics-informed learning, and materials science. The work
thus both expands the mathematical foundation of CNN approximation theory and
highlights a class of architectures with surprising and practically relevant
approximation capabilities.

</details>


### [333] [MOMEMTO: Patch-based Memory Gate Model in Time Series Foundation Model](https://arxiv.org/abs/2509.18751)
*Samuel Yoon,Jongwon Kim,Juyoung Ha,Young Myoung Ko*

Main category: cs.LG

TL;DR: The paper introduces MOMEMTO, a deep learning model for time series anomaly detection, equipped with a memory module to handle over-generalization issues in foundation models.


<details>
  <summary>Details</summary>
Motivation: Address limitations in reconstruction-based deep models for time series anomaly detection, including over-generalization and high training costs in prior memory-based methods.

Method: Propose MOMEMTO, a time series foundation model (TFM) with a patch-based memory module to store normal patterns. This module uses latent representations from a pre-trained encoder, organizes patches, and updates them via attention mechanisms, enabling multi-domain joint training.

Result: Experimental evaluation on 23 univariate benchmark datasets shows that MOMEMTO outperforms baseline methods in AUC and VUS metrics and improves its foundation model’s few-shot learning capability.

Conclusion: MOMEMTO successfully mitigates over-generalization in TFMs for anomaly detection, enhances their performance in multi-domain and few-shot settings, and reduces training costs.

Abstract: Recently reconstruction-based deep models have been widely used for time
series anomaly detection, but as their capacity and representation capability
increase, these models tend to over-generalize, often reconstructing unseen
anomalies accurately. Prior works have attempted to mitigate this by
incorporating a memory architecture that stores prototypes of normal patterns.
Nevertheless, these approaches suffer from high training costs and have yet to
be effectively integrated with time series foundation models (TFMs). To address
these challenges, we propose \textbf{MOMEMTO}, a TFM for anomaly detection,
enhanced with a patch-based memory module to mitigate over-generalization. The
memory module is designed to capture representative normal patterns from
multiple domains and enables a single model to be jointly fine-tuned across
multiple datasets through a multi-domain training strategy. MOMEMTO initializes
memory items with latent representations from a pre-trained encoder, organizes
them into patch-level units, and updates them via an attention mechanism. We
evaluate our method using 23 univariate benchmark datasets. Experimental
results demonstrate that MOMEMTO, as a single model, achieves higher scores on
AUC and VUS metrics compared to baseline methods, and further enhances the
performance of its backbone TFM, particularly in few-shot learning scenarios.

</details>


### [334] [Probabilistic Machine Learning for Uncertainty-Aware Diagnosis of Industrial Systems](https://arxiv.org/abs/2509.18810)
*Arman Mohammadi,Mattias Krysander,Daniel Jung,Erik Frisk*

Main category: cs.LG

TL;DR: This paper proposes a framework using ensemble probabilistic machine learning to enhance fault diagnostics by quantifying and automating prediction uncertainty, thus improving diagnostic reliability.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the shortcomings of deep neural networks in evaluating their confidence in prediction tasks—a critical need for consistency-based diagnosis that is sensitive to false alarms.

Method: The authors utilize ensemble probabilistic machine learning to create a diagnostic framework capable of improving data-driven consistency-based diagnosis by automating prediction uncertainty quantification.

Result: The approach has been tested through ablation and comparative analyses across multiple case studies, demonstrating consistent improvements in diagnostic metrics.

Conclusion: The framework effectively enhances fault diagnostics by addressing prediction uncertainty, thereby providing more reliable and accurate consistency-based diagnosis.

Abstract: Deep neural networks has been increasingly applied in fault diagnostics,
where it uses historical data
  to capture systems behavior, bypassing the need for high-fidelity physical
models.
  However, despite their competence in prediction tasks, these models often
struggle with
  the evaluation of their confidence. This matter is particularly
  important in consistency-based diagnosis where decision logic is highly
sensitive to false alarms.
  To address this challenge, this work presents a diagnostic framework that
uses
  ensemble probabilistic machine learning to
  improve diagnostic characteristics of data driven consistency based diagnosis
  by quantifying and automating the prediction uncertainty.
  The proposed method is evaluated across several case studies using both
ablation
  and comparative analyses, showing consistent improvements across a range of
diagnostic metrics.

</details>


### [335] [Training-Free Data Assimilation with GenCast](https://arxiv.org/abs/2509.18811)
*Thomas Savary,François Rozet,Gilles Louppe*

Main category: cs.LG

TL;DR: The paper introduces a general and lightweight methodology for data assimilation using pre-trained diffusion models, specifically applied to example scenarios like global weather forecasting.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve state estimation for dynamical systems from noisy observations by leveraging diffusion models and avoiding additional training.

Method: A lightweight data assimilation technique based on particle filters, integrating pre-trained diffusion models for application, such as weather prediction.

Result: The methodology was successfully demonstrated using GenCast, a diffusion-based model for generating ensemble weather forecasts.

Conclusion: The proposed method effectively uses pre-trained diffusion models for data assimilation in various scenarios without additional training.

Abstract: Data assimilation is widely used in many disciplines such as meteorology,
oceanography, and robotics to estimate the state of a dynamical system from
noisy observations. In this work, we propose a lightweight and general method
to perform data assimilation using diffusion models pre-trained for emulating
dynamical systems. Our method builds on particle filters, a class of data
assimilation algorithms, and does not require any further training. As a
guiding example throughout this work, we illustrate our methodology on GenCast,
a diffusion-based model that generates global ensemble weather forecasts.

</details>


### [336] [Graph-based Clustering Revisited: A Relaxation of Kernel $k$-Means Perspective](https://arxiv.org/abs/2509.18826)
*Wenlong Lyu,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.LG

TL;DR: The paper introduces LoRD, a probabilistic clustering model improving constraints in kernel k-means, and B-LoRD, which incorporates block diagonal regularization to enhance clustering performance.


<details>
  <summary>Details</summary>
Motivation: Graph-based clustering methods often rely on excessive relaxations of constraints, which might limit their clustering effectiveness. The paper aims to address these limitations by exploring stronger theoretical and practical clustering approaches.

Method: The authors propose LoRD, which adjusts the orthonormal constraint for probabilistic clustering, and B-LoRD, which adds block diagonal regularization to enhance results. They transform non-convex doubly stochastic constraints into linear convex constraints to ensure numerical solvability, and use a gradient descent optimization approach.

Result: Employing extensive experiments, the proposed models (LoRD and B-LoRD) demonstrate significant effectiveness in clustering tasks, showcasing enhanced clustering accuracy.

Conclusion: LoRD and B-LoRD address the limitations present in traditional graph-based clustering methods, providing theoretically backed and experimentally validated improvements in clustering performance.

Abstract: The well-known graph-based clustering methods, including spectral clustering,
symmetric non-negative matrix factorization, and doubly stochastic
normalization, can be viewed as relaxations of the kernel $k$-means approach.
However, we posit that these methods excessively relax their inherent low-rank,
nonnegative, doubly stochastic, and orthonormal constraints to ensure numerical
feasibility, potentially limiting their clustering efficacy. In this paper,
guided by our theoretical analyses, we propose \textbf{Lo}w-\textbf{R}ank
\textbf{D}oubly stochastic clustering (\textbf{LoRD}), a model that only
relaxes the orthonormal constraint to derive a probabilistic clustering
results. Furthermore, we theoretically establish the equivalence between
orthogonality and block diagonality under the doubly stochastic constraint. By
integrating \textbf{B}lock diagonal regularization into LoRD, expressed as the
maximization of the Frobenius norm, we propose \textbf{B-LoRD}, which further
enhances the clustering performance. To ensure numerical solvability, we
transform the non-convex doubly stochastic constraint into a linear convex
constraint through the introduction of a class probability parameter. We
further theoretically demonstrate the gradient Lipschitz continuity of our LoRD
and B-LoRD enables the proposal of a globally convergent projected gradient
descent algorithm for their optimization. Extensive experiments validate the
effectiveness of our approaches. The code is publicly available at
https://github.com/lwl-learning/LoRD.

</details>


### [337] [Shared-Weights Extender and Gradient Voting for Neural Network Expansion](https://arxiv.org/abs/2509.18842)
*Nikolas Chatzis,Ioannis Kordonis,Manos Theodosis,Petros Maragos*

Main category: cs.LG

TL;DR: This paper introduces methods to dynamically expand neural network capacity during training without neuron inactivity issues, achieving improved performance.


<details>
  <summary>Details</summary>
Motivation: To enable seamless and effective expansion of neural networks during training, avoiding the issues of neuron inactivity and inefficient capacity usage.

Method: The paper presents two methods: the Shared-Weights Extender (SWE) for integrating new neurons by coupling them with existing ones, and the Steepest Voting Distributor (SVoD) for gradient-based neuron allocation across layers.

Result: Through benchmarking on four datasets, the proposed methods suppressed neuron inactivity and outperformed alternative network expansion approaches and baselines.

Conclusion: The proposed SWE and SVoD methods successfully address neuron inactivity challenges and enhance performance of neural network expansion during training.

Abstract: Expanding neural networks during training is a promising way to augment
capacity without retraining larger models from scratch. However, newly added
neurons often fail to adjust to a trained network and become inactive,
providing no contribution to capacity growth. We propose the Shared-Weights
Extender (SWE), a novel method explicitly designed to prevent inactivity of new
neurons by coupling them with existing ones for smooth integration. In
parallel, we introduce the Steepest Voting Distributor (SVoD), a gradient-based
method for allocating neurons across layers during deep network expansion. Our
extensive benchmarking on four datasets shows that our method can effectively
suppress neuron inactivity and achieve better performance compared to other
expanding methods and baselines.

</details>


### [338] [NGRPO: Negative-enhanced Group Relative Policy Optimization](https://arxiv.org/abs/2509.18851)
*Gongrui Nan,Siye Chen,Jing Huang,Mengyu Lu,Dexun Wang,Chunmei Xie,Weiqi Xiong,Xianzhou Zeng,Qixuan Zhou,Yadong Li,Xingzhong Xu*

Main category: cs.LG

TL;DR: NGRPO is proposed to address limitations in GRPO, a RLVR algorithm, enhancing LLMs' reasoning by converting homogeneous errors into learning signals.


<details>
  <summary>Details</summary>
Motivation: GRPO struggles with learning from homogeneous groups of incorrect responses, which leads to null gradients and loss of valuable learning data.

Method: NGRPO employs Advantage Calibration and Asymmetric Clipping to address the issue, transforming homogeneous errors into learning signals and stabilizing exploration pressure.

Result: Experiments on Qwen2.5-Math-7B show NGRPO surpasses algorithms like PPO, GRPO, and DAPO on mathematical tasks such as MATH500, AMC23, and AIME2025.

Conclusion: NGRPO effectively tackles GRPO's limitations, improving mathematical reasoning abilities in LLMs through robust learning from homogeneous errors.

Abstract: RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs)
across various tasks. However, GRPO, a representative RLVR algorithm, suffers
from a critical limitation: when all responses within a group are either
entirely correct or entirely incorrect, the model fails to learn from these
homogeneous responses. This is particularly problematic for homogeneously
incorrect groups, where GRPO's advantage function yields a value of zero,
leading to null gradients and the loss of valuable learning signals. To
overcome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy
Optimization), an algorithm designed to convert homogeneous errors into robust
learning signals. First, NGRPO introduces Advantage Calibration. This mechanism
hypothesizes the existence of a virtual maximum-reward sample during advantage
calculation, thereby altering the mean and variance of rewards within a group
and ensuring that the advantages for homogeneously incorrect samples are no
longer zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the
update magnitude for positive samples while imposing stricter constraints on
that of negative samples. This serves to stabilize the exploration pressure
introduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B
demonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO,
DAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and
AIME2025. These results validate NGRPO's ability to learn from homogeneous
errors, leading to stable and substantial improvements in mathematical
reasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.

</details>


### [339] [Exploring Heterophily in Graph-level Tasks](https://arxiv.org/abs/2509.18893)
*Qinhan Hou,Yilun Zheng,Xichun Zhang,Sitao Luan,Jing Tang*

Main category: cs.LG

TL;DR: This paper investigates the understudied impact of heterophily on graph-level learning tasks, emphasizing its role in motif-based labeling tasks and proposing frequency-adaptive GNN models.


<details>
  <summary>Details</summary>
Motivation: Heterophily has been primarily studied in node-level tasks, but its role in graph-level tasks has not been explored. The paper aims to bridge this gap with a theoretical and empirical study, particularly focusing on motif-based tasks.

Method: The paper introduces a taxonomy for graph-level labeling schemes and analyzes motif detection in the context of local structural labeling. Using energy-based gradient flow analysis, it explores the role of mixed-frequency dynamics in motif detection. Experiments validate theoretical claims on synthetic and real-world datasets.

Result: The findings reveal that motif detection in graph-level tasks requires mixed-frequency dynamics, and frequency-adaptive GNN models outperform frequency-dominated ones on tasks involving heterophily.

Conclusion: The study provides a theoretical framework for understanding heterophily in graph-level tasks and outlines architectural considerations for designing effective GNN models.

Abstract: While heterophily has been widely studied in node-level tasks, its impact on
graph-level tasks remains unclear. We present the first analysis of heterophily
in graph-level learning, combining theoretical insights with empirical
validation. We first introduce a taxonomy of graph-level labeling schemes, and
focus on motif-based tasks within local structure labeling, which is a popular
labeling scheme. Using energy-based gradient flow analysis, we reveal a key
insight: unlike frequency-dominated regimes in node-level tasks, motif
detection requires mixed-frequency dynamics to remain flexible across multiple
spectral components. Our theory shows that motif objectives are inherently
misaligned with global frequency dominance, demanding distinct architectural
considerations. Experiments on synthetic datasets with controlled heterophily
and real-world molecular property prediction support our findings, showing that
frequency-adaptive model outperform frequency-dominated models. This work
establishes a new theoretical understanding of heterophily in graph-level
learning and offers guidance for designing effective GNN architectures.

</details>


### [340] [Enhancing the Effectiveness and Durability of Backdoor Attacks in Federated Learning through Maximizing Task Distinction](https://arxiv.org/abs/2509.18904)
*Zhaoxin Wang,Handing Wang,Cong Tian,Yaochu Jin*

Main category: cs.LG

TL;DR: This paper presents a federated learning backdoor attack method that dynamically optimizes the backdoor trigger to enhance attack persistence and robustness.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of existing backdoor attacks in federated learning, which often tightly couple the main and backdoor tasks, making them vulnerable to defenses and dilution by honest updates.

Method: The proposed approach uses a min-max optimization framework. The inner layer focuses on maximizing the performance gap between poisoned and benign samples, while the outer process injects adaptive backdoor triggers into the local model.

Result: Experimental evaluations on computer vision and natural language tasks demonstrate that the proposed method outperforms six existing backdoor attack techniques and is effective even under six defense algorithms.

Conclusion: The method successfully enhances the persistence and adaptability of backdoor triggers, offering a significant improvement over prior approaches and expanding the scope of backdoor attack strategies in federated learning settings.

Abstract: Federated learning allows multiple participants to collaboratively train a
central model without sharing their private data. However, this distributed
nature also exposes new attack surfaces. In particular, backdoor attacks allow
attackers to implant malicious behaviors into the global model while
maintaining high accuracy on benign inputs. Existing attacks usually rely on
fixed patterns or adversarial perturbations as triggers, which tightly couple
the main and backdoor tasks. This coupling makes them vulnerable to dilution by
honest updates and limits their persistence under federated defenses. In this
work, we propose an approach to decouple the backdoor task from the main task
by dynamically optimizing the backdoor trigger within a min-max framework. The
inner layer maximizes the performance gap between poisoned and benign samples,
ensuring that the contributions of benign users have minimal impact on the
backdoor. The outer process injects the adaptive triggers into the local model.
We evaluate our method on both computer vision and natural language tasks, and
compare it with six backdoor attack methods under six defense algorithms.
Experimental results show that our method achieves good attack performance and
can be easily integrated into existing backdoor attack techniques.

</details>


### [341] [Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning](https://arxiv.org/abs/2509.18930)
*Alex Schutz,Victor-Alexandru Darvariu,Efimia Panagiotaki,Bruno Lacerda,Nick Hawes*

Main category: cs.LG

TL;DR: A new GNARL framework is proposed to extend Neural Algorithmic Reasoning (NAR) using reinforcement learning to overcome its limitations in solving algorithmic tasks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses key limitations of NAR, including its inability to realize valid solutions without post-processing, poor performance on NP-hard problems, and lack of applicability to problems without strong expert algorithms.

Method: The authors reframe NAR as a Markov Decision Process to utilize imitation and reinforcement learning, developing the GNARL framework suitable for graph-based problems and proposing a new learning architecture.

Result: GNARL achieves high accuracy on CLRS-30 problems, matches or exceeds NAR's performance on NP-hard problems, and showcases applicability even without expert algorithms.

Conclusion: The GNARL framework enhances NAR's capabilities using RL methodologies, offering broader application potential to algorithmic problem-solving tasks.

Abstract: Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks
to execute classic algorithms by supervised learning. Despite its successes,
important limitations remain: inability to construct valid solutions without
post-processing and to reason about multiple correct ones, poor performance on
combinatorial NP-hard problems, and inapplicability to problems for which
strong algorithms are not yet known. To address these limitations, we reframe
the problem of learning algorithm trajectories as a Markov Decision Process,
which imposes structure on the solution construction procedure and unlocks the
powerful tools of imitation and reinforcement learning (RL). We propose the
GNARL framework, encompassing the methodology to translate problem formulations
from NAR to RL and a learning architecture suitable for a wide range of
graph-based problems. We achieve very high graph accuracy results on several
CLRS-30 problems, performance matching or exceeding much narrower NAR
approaches for NP-hard problems and, remarkably, applicability even when
lacking an expert algorithm.

</details>


### [342] [Towards Privacy-Aware Bayesian Networks: A Credal Approach](https://arxiv.org/abs/2509.18949)
*Niccolò Rocchi,Fabio Stella,Cassio de Campos*

Main category: cs.LG

TL;DR: Bayesian networks are vulnerable to tracing attacks, compromising privacy, and current solutions involve noisy parameter adjustments which reduce model accuracy. Credal networks are proposed as a privacy-aware alternative that preserves utility while mitigating privacy risks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the issue of safeguarding sensitive information in Bayesian networks, which are commonly used across various fields, while maintaining their functionality and ensuring protection against tracing attacks.

Method: The authors adapt tracing attacks to credal networks and identify the information that needs to be concealed to prevent adversaries from deducing learned Bayesian networks. They also adjust hyperparameters of CNs to analyze trade-offs between privacy and utility.

Result: Credal networks mask the learned Bayesian networks effectively, reducing tracing attack success probability, and numerical experiments demonstrate privacy gains through CN hyperparameter tuning.

Conclusion: Credal networks provide a balanced approach to creating privacy-aware probabilistic models that protect sensitive data while maintaining their inference utility, addressing the limitations of noise-based protection methods.

Abstract: Bayesian networks (BN) are probabilistic graphical models that enable
efficient knowledge representation and inference. These have proven effective
across diverse domains, including healthcare, bioinformatics and economics. The
structure and parameters of a BN can be obtained by domain experts or directly
learned from available data. However, as privacy concerns escalate, it becomes
increasingly critical for publicly released models to safeguard sensitive
information in training data. Typically, released models do not prioritize
privacy by design. In particular, tracing attacks from adversaries can combine
the released BN with auxiliary data to determine whether specific individuals
belong to the data from which the BN was learned. State-of-the-art protection
tecniques involve introducing noise into the learned parameters. While this
offers robust protection against tracing attacks, it significantly impacts the
model's utility, in terms of both the significance and accuracy of the
resulting inferences. Hence, high privacy may be attained at the cost of
releasing a possibly ineffective model. This paper introduces credal networks
(CN) as a novel solution for balancing the model's privacy and utility. After
adapting the notion of tracing attacks, we demonstrate that a CN enables the
masking of the learned BN, thereby reducing the probability of successful
attacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve
meaningful inferences while safeguarding privacy. Moreover, we identify key
learning information that must be concealed to prevent attackers from
recovering the underlying BN. Finally, we conduct a set of numerical
experiments to analyze how privacy gains can be modulated by tuning the CN
hyperparameters. Our results confirm that CNs provide a principled, practical,
and effective approach towards the development of privacy-aware probabilistic
graphical models.

</details>


### [343] [Lift What You Can: Green Online Learning with Heterogeneous Ensembles](https://arxiv.org/abs/2509.18962)
*Kirsten Köbschall,Sebastian Buschjäger,Raphael Fischer,Lisa Hartung,Stefan Kramer*

Main category: cs.LG

TL;DR: The paper proposes HEROS, a heterogeneous online ensemble method that optimizes predictive performance under resource constraints to enable sustainable online learning.


<details>
  <summary>Details</summary>
Motivation: Current ensemble methods for stream mining prioritize predictive performance but neglect computational sustainability, which is increasingly important.

Method: HEROS utilizes diverse models and introduces a Markov decision process for balancing accuracy and sustainability. Policies, including the novel $
\zeta$-policy, are proposed for selecting models.

Result: The $
\zeta$-policy achieves near-optimal performance while consuming fewer resources, validated through experiments on 11 datasets where it often outperforms competitors.

Conclusion: HEROS and its $
\zeta$-policy represent a significant step toward sustainable online learning by delivering accuracy with reduced computational costs.

Abstract: Ensemble methods for stream mining necessitate managing multiple models and
updating them as data distributions evolve. Considering the calls for more
sustainability, established methods are however not sufficiently considerate of
ensemble members' computational expenses and instead overly focus on predictive
capabilities. To address these challenges and enable green online learning, we
propose heterogeneous online ensembles (HEROS). For every training step, HEROS
chooses a subset of models from a pool of models initialized with diverse
hyperparameter choices under resource constraints to train. We introduce a
Markov decision process to theoretically capture the trade-offs between
predictive performance and sustainability constraints. Based on this framework,
we present different policies for choosing which models to train on incoming
data. Most notably, we propose the novel $\zeta$-policy, which focuses on
training near-optimal models at reduced costs. Using a stochastic model, we
theoretically prove that our $\zeta$-policy achieves near optimal performance
while using fewer resources compared to the best performing policy. In our
experiments across 11 benchmark datasets, we find empiric evidence that our
$\zeta$-policy is a strong contribution to the state-of-the-art, demonstrating
highly accurate performance, in some cases even outperforming competitors, and
simultaneously being much more resource-friendly.

</details>


### [344] [Otters: An Energy-Efficient SpikingTransformer via Optical Time-to-First-Spike Encoding](https://arxiv.org/abs/2509.18968)
*Zhanglu Yan,Jiayi Mao,Qianhui Liu,Fanfan Li,Gang Pan,Tao Luo,Bowen Zhu,Weng-Fai Wong*

Main category: cs.LG

TL;DR: This paper presents a hardware-software co-design leveraging physical hardware properties for energy-efficient spiking neural networks (SNNs) with time-to-first-spike encoding.


<details>
  <summary>Details</summary>
Motivation: SNNs are energy-efficient but face high computational costs due to temporal decay functions and synaptic weight multiplications.

Method: The method involves repurposing a hardware anomaly in optoelectronic devices to naturally compute temporal decay, combined with novel neural network-to-SNN conversion algorithms for complex architectures.

Result: State-of-the-art accuracy across seven GLUE datasets and a 1.77× improvement in energy efficiency compared to previous SNN approaches.

Conclusion: The study introduces a paradigm shift in SNN design, using physical device physics for computational primitives while providing open-source codes and data for accessibility.

Abstract: Spiking neural networks (SNNs) promise high energy efficiency, particularly
with time-to-first-spike (TTFS) encoding, which maximizes sparsity by emitting
at most one spike per neuron. However, such energy advantage is often
unrealized because inference requires evaluating a temporal decay function and
subsequent multiplication with the synaptic weights. This paper challenges this
costly approach by repurposing a physical hardware `bug', namely, the natural
signal decay in optoelectronic devices, as the core computation of TTFS. We
fabricated a custom indium oxide optoelectronic synapse, showing how its
natural physical decay directly implements the required temporal function. By
treating the device's analog output as the fused product of the synaptic weight
and temporal decay, optoelectronic synaptic TTFS (named Otters) eliminates
these expensive digital operations. To use the Otters paradigm in complex
architectures like the transformer, which are challenging to train directly due
to the sparsity issue, we introduce a novel quantized neural network-to-SNN
conversion algorithm. This complete hardware-software co-design enables our
model to achieve state-of-the-art accuracy across seven GLUE benchmark datasets
and demonstrates a 1.77$\times$ improvement in energy efficiency over previous
leading SNNs, based on a comprehensive analysis of compute, data movement, and
memory access costs using energy measurements from a commercial 22nm process.
Our work thus establishes a new paradigm for energy-efficient SNNs, translating
fundamental device physics directly into powerful computational primitives. All
codes and data are open source.

</details>


### [345] [Learning From Simulators: A Theory of Simulation-Grounded Learning](https://arxiv.org/abs/2509.18990)
*Carson Dudley,Marisa Eisenberg*

Main category: cs.LG

TL;DR: SGNNs use synthetic data and achieve state-of-the-art predictions for under-observed phenomena, explained through Bayesian inference and validated experimentally.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of making accurate predictions in data-limited settings where real-world labels are minimal or unavailable.

Method: Develop a theoretical foundation for SGNNs based on amortized Bayesian inference, derive generalization bounds under model misspecification, and propose novel mechanistic interpretability.

Result: SGNNs were shown in numerical experiments to recover latent parameters, remain robust under mismatches, and achieve superior performance to classical tools like AIC in specific tasks.

Conclusion: SGNNs are a principled, practical framework for scientific prediction in scenarios with limited data, leveraging synthetic simulations for robust inference and interpretability.

Abstract: Simulation-Grounded Neural Networks (SGNNs) are predictive models trained
entirely on synthetic data from mechanistic simulations. They have achieved
state-of-the-art performance in domains where real-world labels are limited or
unobserved, but lack a formal underpinning.
  We present the foundational theory of simulation-grounded learning. We show
that SGNNs implement amortized Bayesian inference under a simulation prior and
converge to the Bayes-optimal predictor. We derive generalization bounds under
model misspecification and prove that SGNNs can learn unobservable scientific
quantities that empirical methods provably cannot. We also formalize a novel
form of mechanistic interpretability uniquely enabled by SGNNs: by attributing
predictions to the simulated mechanisms that generated them, SGNNs yield
posterior-consistent, scientifically grounded explanations.
  We provide numerical experiments to validate all theoretical predictions.
SGNNs recover latent parameters, remain robust under mismatch, and outperform
classical tools: in a model selection task, SGNNs achieve half the error of AIC
in distinguishing mechanistic dynamics. These results establish SGNNs as a
principled and practical framework for scientific prediction in data-limited
regimes.

</details>


### [346] [CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank Structure](https://arxiv.org/abs/2509.18993)
*Boao Kong,Junzhu Liang,Yuxi Liu,Renjia Deng,Kun Yuan*

Main category: cs.LG

TL;DR: CR-Net is introduced as an advanced low-rank framework aimed at improving parameter efficiency and memory/computational demands in large language model pre-training.


<details>
  <summary>Details</summary>
Motivation: Existing low-rank architectures for LLM pre-training suffer from three major issues: reduced performance, high computational requirements, and insufficient memory savings.

Method: The CR-Net framework employs a dual-path design that reconstructs layer activations using previous-layer outputs combined with low-rank residuals, maintaining high-rank information while optimizing parameters.

Result: Extensive pre-training experiments show CR-Net achieves superior performance compared to state-of-the-art low-rank methods across model scales, along with reduced computational and memory demands.

Conclusion: CR-Net effectively addresses the shortcomings of current low-rank methods by combining innovation in architecture and memory strategies, establishing its utility in efficient LLM pre-training.

Abstract: Low-rank architectures have become increasingly important for efficient large
language model (LLM) pre-training, providing substantial reductions in both
parameter complexity and memory/computational demands. Despite these
advantages, current low-rank methods face three critical shortcomings: (1)
compromised model performance, (2) considerable computational overhead, and (3)
limited activation memory savings. To address these limitations, we propose
Cross-layer Low-Rank residual Network (CR-Net), an innovative
parameter-efficient framework inspired by our discovery that inter-layer
activation residuals possess low-rank properties. CR-Net implements this
insight through a dual-path architecture that efficiently reconstructs layer
activations by combining previous-layer outputs with their low-rank
differences, thereby maintaining high-rank information with minimal parameters.
We further develop a specialized activation recomputation strategy tailored for
CR-Net that dramatically reduces memory requirements. Extensive pre-training
experiments across model scales from 60M to 7B parameters demonstrate that
CR-Net consistently outperforms state-of-the-art low-rank frameworks while
requiring fewer computational resources and less memory.

</details>


### [347] [Theoretical Foundations of Representation Learning using Unlabeled Data: Statistics and Optimization](https://arxiv.org/abs/2509.18997)
*Pascal Esser,Maximilian Fleissner,Debarghya Ghoshdastidar*

Main category: cs.LG

TL;DR: The paper reviews developments and theoretical advances in unsupervised representation learning and highlights its contributions to understanding these techniques.


<details>
  <summary>Details</summary>
Motivation: Unsupervised representation learning, especially using modern deep learning principles like self-supervision and denoising autoencoders, lacks comprehensive theoretical characterization.

Method: The work combines mathematical approaches from statistics and optimization to study and explain the success behind these models and behaviors.

Result: Recent progress in theoretical understanding of foundation models is summarized, along with the authors' contributions towards this investigation.

Conclusion: New insights and advances in understanding the representations learned by unsupervised models enable better explanations for their performance and emergent behaviors.

Abstract: Representation learning from unlabeled data has been extensively studied in
statistics, data science and signal processing with a rich literature on
techniques for dimension reduction, compression, multi-dimensional scaling
among others. However, current deep learning models use new principles for
unsupervised representation learning that cannot be easily analyzed using
classical theories. For example, visual foundation models have found tremendous
success using self-supervision or denoising/masked autoencoders, which
effectively learn representations from massive amounts of unlabeled data.
However, it remains difficult to characterize the representations learned by
these models and to explain why they perform well for diverse prediction tasks
or show emergent behavior. To answer these questions, one needs to combine
mathematical tools from statistics and optimization. This paper provides an
overview of recent theoretical advances in representation learning from
unlabeled data and mentions our contributions in this direction.

</details>


### [348] [Fully Learnable Neural Reward Machines](https://arxiv.org/abs/2509.19017)
*Hazem Dewidar,Elena Umili*

Main category: cs.LG

TL;DR: The paper introduces Fully Learnable Neural Reward Machines (FLNRM), which learn symbolic representations and automaton structures end-to-end for Non-Markovian RL, outperforming RNN-based methods.


<details>
  <summary>Details</summary>
Motivation: The need to overcome restrictive assumptions in existing approaches like predefined symbol grounding and prior temporal task knowledge in Non-Markovian RL.

Method: An end-to-end learning framework for Neural Reward Machines (NRM) that integrates with deep RL systems, learning both symbolic representations and automaton structures without prior knowledge.

Result: The FLNRMs outperform RNN-based methods in solving Non-Markovian RL tasks, offering better performance and explainability.

Conclusion: FLNRM provides a user-friendly and more explainable framework for solving complex RL tasks while addressing limitations of existing symbolic formalism-based approaches.

Abstract: Non-Markovian Reinforcement Learning (RL) tasks present significant
challenges, as agents must reason over entire trajectories of state-action
pairs to make optimal decisions. A common strategy to address this is through
symbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which
provide a structured way to express temporally extended objectives. However,
these approaches often rely on restrictive assumptions -- such as the
availability of a predefined Symbol Grounding (SG) function mapping raw
observations to high-level symbolic representations, or prior knowledge of the
temporal task. In this work, we propose a fully learnable version of Neural
Reward Machines (NRM), which can learn both the SG function and the automaton
end-to-end, removing any reliance on prior knowledge. Our approach is therefore
as easily applicable as classic deep RL (DRL) approaches, while being far more
explainable, because of the finite and compact nature of automata. Furthermore,
we show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL,
our method outperforms previous approaches based on Recurrent Neural Networks
(RNNs).

</details>


### [349] [OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval via Latent Space Alignment](https://arxiv.org/abs/2509.19018)
*Teng Xiao,Zuchao Li,Lefei Zhang*

Main category: cs.LG

TL;DR: OmniBridge is a multimodal framework that integrates vision-language understanding, generation, and retrieval into a unified architecture, achieving competitive or state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current multimodal solutions lack integration across tasks or incur high computational costs due to training LLMs from scratch. This paper aims to address these challenges by creating a unified, efficient system.

Method: OmniBridge utilizes pretrained LLMs, introducing a bidirectional latent alignment module. It employs a two-stage training strategy: supervised fine-tuning and latent space alignment, coupled with semantic-guided diffusion training.

Result: OmniBridge shows competitive or state-of-the-art results across benchmarks for multimodal tasks, validating its design for unified cross-modal reasoning and representation.

Conclusion: OmniBridge effectively addresses multimodal integration challenges by leveraging pretrained LLMs and efficient training strategies, providing a unified architecture for diverse multimodal tasks.

Abstract: Recent advances in multimodal large language models (LLMs) have led to
significant progress in understanding, generation, and retrieval tasks.
However, current solutions often treat these tasks in isolation or require
training LLMs from scratch, resulting in high computational costs and limited
generalization across modalities. In this work, we present OmniBridge, a
unified and modular multimodal framework that supports vision-language
understanding, generation, and retrieval within a unified architecture.
OmniBridge adopts a language-centric design that reuses pretrained LLMs and
introduces a lightweight bidirectional latent alignment module. To address the
challenge of task interference, we propose a two-stage decoupled training
strategy: supervised fine-tuning and latent space alignment for aligning LLM
behavior with multimodal reasoning, and semantic-guided diffusion training to
align cross-modal latent spaces via learnable query embeddings. Extensive
experiments across a wide range of benchmarks demonstrate that OmniBridge
achieves competitive or state-of-the-art performance in all three tasks.
Moreover, our results highlight the effectiveness of latent space alignment for
unifying multimodal modeling under a shared representation space. Code and
models are released at https://github.com/xiao-xt/OmniBridge.

</details>


### [350] [Improving Credit Card Fraud Detection through Transformer-Enhanced GAN Oversampling](https://arxiv.org/abs/2509.19032)
*Kashaf Ul Emaan*

Main category: cs.LG

TL;DR: The paper introduces a Transformer-based GAN hybrid model to improve credit card fraud detection, addressing data imbalance and surpassing existing resampling techniques.


<details>
  <summary>Details</summary>
Motivation: The study addresses the acute issue of credit card fraud detection, focusing on overcoming the class imbalance in transaction datasets, as existing resampling methods like SMOTE and advanced techniques (CTGAN, TVAE) struggle with complex fraud patterns.

Method: A hybrid model is proposed, combining GANs with Transformer encoder blocks. GANs enhance the generation of realistic fraudulent samples, while the Transformer’s self-attention mechanism captures rich feature interactions.

Result: The Transformer-based GAN outperformed conventional resampling and generative strategies when tested on the Credit Card Fraud Detection dataset. Substantial improvements were observed in Recall, F1-score, and AUC metrics using classifiers like LR, RF, XGBoost, and SVM.

Conclusion: The proposed hybrid model is effective at addressing severe class imbalances in fraud detection and demonstrates strong potential for practical deployment in financial security applications.

Abstract: Detection of credit card fraud is an acute issue of financial security
because transaction datasets are highly lopsided, with fraud cases being only a
drop in the ocean. Balancing datasets using the most popular methods of
traditional oversampling such as the Synthetic Minority Oversampling Technique
(SMOTE) generally create simplistic synthetic samples that are not readily
applicable to complex fraud patterns. Recent industry advances that include
Conditional Tabular Generative Adversarial Networks (CTGAN) and Tabular
Variational Autoencoders (TVAE) have demonstrated increased efficiency in
tabular synthesis, yet all these models still exhibit issues with
high-dimensional dependence modelling. Now we will present our hybrid approach
where we use a Generative Adversarial Network (GAN) with a Transformer encoder
block to produce realistic fraudulent transactions samples. The GAN
architecture allows training realistic generators adversarial, and the
Transformer allows the model to learn rich feature interactions by
self-attention. Such a hybrid strategy overcomes the limitations of SMOTE,
CTGAN, and TVAE by producing a variety of high-quality synthetic minority
classes samples. We test our algorithm on the publicly-available Credit Card
Fraud Detection dataset and compare it to conventional and generative
resampling strategies with a variety of classifiers, such as Logistic
Regression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and
Support Vector Machine (SVM). Findings indicate that our Transformer-based GAN
shows substantial gains in Recall, F1-score and Area Under the Receiver
Operating Characteristic Curve (AUC), which indicates that it is effective in
overcoming the severe class imbalance inherent in the task of fraud detection.

</details>


### [351] [Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks](https://arxiv.org/abs/2509.19044)
*Yang Li,Chenyu Wang,Tingrui Wang,Yongwei Wang,Haonan Li,Zhunga Liu,Quan Pan*

Main category: cs.LG

TL;DR: The paper proposes JAD, a latent diffusion model that improves black-box adversarial attack efficiency and cross-architecture transferability using joint attention distillation.


<details>
  <summary>Details</summary>
Motivation: Current black-box adversarial attack methods face challenges like dependency on specific architectures, limited transferability, and high query costs.

Method: JAD uses a latent diffusion model guided by attention maps distilled from both CNNs and ViTs. It focuses on commonly sensitive image regions to create architecture-agnostic adversarial examples.

Result: The proposed method demonstrated improved attack generalization, efficiency in generating adversarial samples, and effective transferability across different model types.

Conclusion: JAD provides a robust paradigm for black-box adversarial attacks, addressing key limitations in existing approaches while being efficient and architecture-agnostic.

Abstract: Black-box adversarial attacks remain challenging due to limited access to
model internals. Existing methods often depend on specific network
architectures or require numerous queries, resulting in limited
cross-architecture transferability and high query costs. To address these
limitations, we propose JAD, a latent diffusion model framework for black-box
adversarial attacks. JAD generates adversarial examples by leveraging a latent
diffusion model guided by attention maps distilled from both a convolutional
neural network (CNN) and a Vision Transformer (ViT) models. By focusing on
image regions that are commonly sensitive across architectures, this approach
crafts adversarial perturbations that transfer effectively between different
model types. This joint attention distillation strategy enables JAD to be
architecture-agnostic, achieving superior attack generalization across diverse
models. Moreover, the generative nature of the diffusion framework yields high
adversarial sample generation efficiency by reducing reliance on iterative
queries. Experiments demonstrate that JAD offers improved attack
generalization, generation efficiency, and cross-architecture transferability
compared to existing methods, providing a promising and effective paradigm for
black-box adversarial attacks.

</details>


### [352] [Beyond Backpropagation: Exploring Innovative Algorithms for Energy-Efficient Deep Neural Network Training](https://arxiv.org/abs/2509.19063)
*Przemysław Spyra*

Main category: cs.LG

TL;DR: This paper analyzes and benchmarks BP-free training methods (FF, CaFo, MF), demonstrating that MF surpasses BP in classification accuracy, energy efficiency, and training speed.


<details>
  <summary>Details</summary>
Motivation: The paper investigates BP-free DNN training techniques as a solution to the computational demands and environmental impacts of backpropagation.

Method: A comparative framework optimizes and benchmarks FF, CaFo, and MF against BP-trained models using native architectures, early stopping criteria, and energy measurements.

Result: MF outperforms BP in accuracy, reduces energy consumption by up to 41%, and training time by 34%. Hardware-level analysis reveals efficiency improvements.

Conclusion: Mono-Forward (MF) blends accuracy and sustainability, offering a roadmap for energy-efficient deep learning models while challenging traditional optimization assumptions.

Abstract: The rising computational and energy demands of deep neural networks (DNNs),
driven largely by backpropagation (BP), challenge sustainable AI development.
This paper rigorously investigates three BP-free training methods: the
Forward-Forward (FF), Cascaded-Forward (CaFo), and Mono-Forward (MF)
algorithms, tracing their progression from foundational concepts to a
demonstrably superior solution.
  A robust comparative framework was established: each algorithm was
implemented on its native architecture (MLPs for FF and MF, a CNN for CaFo) and
benchmarked against an equivalent BP-trained model. Hyperparameters were
optimized with Optuna, and consistent early stopping criteria were applied
based on validation performance, ensuring all models were optimally tuned
before comparison.
  Results show that MF not only competes with but consistently surpasses BP in
classification accuracy on its native MLPs. Its superior generalization stems
from converging to a more favorable minimum in the validation loss landscape,
challenging the assumption that global optimization is required for
state-of-the-art results. Measured at the hardware level using the NVIDIA
Management Library (NVML) API, MF reduces energy consumption by up to 41% and
shortens training time by up to 34%, translating to a measurably smaller carbon
footprint as estimated by CodeCarbon.
  Beyond this primary result, we present a hardware-level analysis that
explains the efficiency gains: exposing FF's architectural inefficiencies,
validating MF's computationally lean design, and challenging the assumption
that all BP-free methods are inherently more memory-efficient. By documenting
the evolution from FF's conceptual groundwork to MF's synthesis of accuracy and
sustainability, this work offers a clear, data-driven roadmap for future
energy-efficient deep learning.

</details>


### [353] [Diffusion Bridge Variational Inference for Deep Gaussian Processes](https://arxiv.org/abs/2509.19078)
*Jian Xu,Qibin Zhao,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: The paper introduces DBVI, an improved method for posterior inference in DGPs, outperforming existing approaches in accuracy, speed, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Posterior inference for deep Gaussian processes (DGPs) is challenging due to inducing variable complexities, and existing methods like DDVI struggle with slow convergence due to inefficient trajectories.

Method: DBVI extends DDVI by using a learnable, data-dependent initialization for reverse diffusion, parameterized through an amortized neural network and optimized using ELBO gradients.

Result: DBVI demonstrates superior performance in predictive accuracy, convergence speed, and posterior quality across tasks such as regression, classification, and image reconstruction.

Conclusion: Through its principled design, DBVI improves sample efficiency and the quality of posterior inference while maintaining scalability and mathematical rigor.

Abstract: Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian
modeling but pose substantial challenges for posterior inference, especially
over inducing variables. Denoising diffusion variational inference (DDVI)
addresses this by modeling the posterior as a time-reversed diffusion from a
simple Gaussian prior. However, DDVI's fixed unconditional starting
distribution remains far from the complex true posterior, resulting in
inefficient inference trajectories and slow convergence. In this work, we
propose Diffusion Bridge Variational Inference (DBVI), a principled extension
of DDVI that initiates the reverse diffusion from a learnable, data-dependent
initial distribution. This initialization is parameterized via an amortized
neural network and progressively adapted using gradients from the ELBO
objective, reducing the posterior gap and improving sample efficiency. To
enable scalable amortization, we design the network to operate on the inducing
inputs, which serve as structured, low-dimensional summaries of the dataset and
naturally align with the inducing variables' shape. DBVI retains the
mathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time
SDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We
derive a tractable training objective under this formulation and implement DBVI
for scalable inference in large-scale DGPs. Across regression, classification,
and image reconstruction tasks, DBVI consistently outperforms DDVI and other
variational baselines in predictive accuracy, convergence speed, and posterior
quality.

</details>


### [354] [Graph Neural Networks with Similarity-Navigated Probabilistic Feature Copying](https://arxiv.org/abs/2509.19084)
*Asela Hevapathige*

Main category: cs.LG

TL;DR: AxelGNN is a novel GNN model inspired by Axelrod's cultural dissemination model, addressing key limitations of existing GNNs like oversmoothing and inflexibility in heterogeneous graphs.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome fundamental limitations of current GNNs, such as feature oversmoothing, difficulty in handling heterogenous relationships, and the lack of flexibility due to processing feature vectors as indivisible units.

Method: AxelGNN introduces similarity-gated probabilistic interactions for adaptive convergence/divergence, trait-level copying for fine-grained feature aggregation, and global polarization to maintain node distinctiveness across representation clusters.

Result: Through extensive experiments on node classification and influence estimation benchmarks, AxelGNN consistently performs better or matches state-of-the-art GNN methods over diverse graph structures with varying characteristics.

Conclusion: AxelGNN successfully addresses critical issues in GNNs by leveraging bistable convergence dynamics, making it adaptable to both homophilic and heterophilic graphs within a single unified architecture.

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable success across
various graph-based tasks. However, they face some fundamental limitations:
feature oversmoothing can cause node representations to become
indistinguishable in deeper networks, they struggle to effectively manage
heterogeneous relationships where connected nodes differ significantly, and
they process entire feature vectors as indivisible units, which limits
flexibility. We seek to address these limitations. We propose AxelGNN, a novel
GNN architecture inspired by Axelrod's cultural dissemination model that
addresses these limitations through a unified framework. AxelGNN incorporates
similarity-gated probabilistic interactions that adaptively promote convergence
or divergence based on node similarity, implements trait-level copying
mechanisms for fine-grained feature aggregation at the segment level, and
maintains global polarization to preserve node distinctiveness across multiple
representation clusters. The model's bistable convergence dynamics naturally
handle both homophilic and heterophilic graphs within a single architecture.
Extensive experiments on node classification and influence estimation
benchmarks demonstrate that AxelGNN consistently outperforms or matches
state-of-the-art GNN methods across diverse graph structures with varying
homophily-heterophily characteristics.

</details>


### [355] [Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer Learning](https://arxiv.org/abs/2509.19098)
*Adrien Prevost,Timothee Mathieu,Odalric-Ambrym Maillard*

Main category: cs.LG

TL;DR: The paper extends multi-armed bandit problem analysis to a transfer learning context and presents a new algorithm, KL-UCB-Transfer, which outperforms baselines under specific conditions.


<details>
  <summary>Details</summary>
Motivation: To improve multi-armed bandit algorithms by integrating transfer learning, enabling better performance when prior knowledge from source distributions is available.

Method: Derived theoretical bounds on cumulative regret while incorporating transfer learning parameters and proposed the KL-UCB-Transfer algorithm based on these insights.

Result: KL-UCB-Transfer matches the theoretical regret bounds under Gaussian assumptions and outperforms baselines in simulations when source and target distributions are adequately similar.

Conclusion: The integration of transfer learning into bandit problems can significantly improve performance by leveraging prior knowledge, as demonstrated by the KL-UCB-Transfer algorithm.

Abstract: We study the non-contextual multi-armed bandit problem in a transfer learning
setting: before any pulls, the learner is given N'_k i.i.d. samples from each
source distribution nu'_k, and the true target distributions nu_k lie within a
known distance bound d_k(nu_k, nu'_k) <= L_k. In this framework, we first
derive a problem-dependent asymptotic lower bound on cumulative regret that
extends the classical Lai-Robbins result to incorporate the transfer parameters
(d_k, L_k, N'_k). We then propose KL-UCB-Transfer, a simple index policy that
matches this new bound in the Gaussian case. Finally, we validate our approach
via simulations, showing that KL-UCB-Transfer significantly outperforms the
no-prior baseline when source and target distributions are sufficiently close.

</details>


### [356] [Algorithms for Adversarially Robust Deep Learning](https://arxiv.org/abs/2509.19100)
*Alexander Robey*

Main category: cs.LG

TL;DR: The thesis examines improving robustness in deep learning models for safety-critical applications across adversarial examples, domain generalization, and language model jailbreaking.


<details>
  <summary>Details</summary>
Motivation: To address vulnerabilities in deep learning models in safety-critical applications and enhance their robustness against adversarial exploitation.

Method: The paper introduces novel technical results, training paradigms, and certification algorithms for adversarial examples, state-of-the-art algorithms for domain generalization in various applications, and new attack and defense strategies for jailbreaking large language models.

Result: Progress is made in robustifying computer vision models against adversarial attacks, achieving superior generalization in domain generalization tasks, and advancing the understanding of robustness in large language models.

Conclusion: These contributions push the frontier of designing robust algorithms for deep learning models, addressing challenges in adversarial exploitation and generalization to unseen domains.

Abstract: Given the widespread use of deep learning models in safety-critical
applications, ensuring that the decisions of such models are robust against
adversarial exploitation is of fundamental importance. In this thesis, we
discuss recent progress toward designing algorithms that exhibit desirable
robustness properties. First, we discuss the problem of adversarial examples in
computer vision, for which we introduce new technical results, training
paradigms, and certification algorithms. Next, we consider the problem of
domain generalization, wherein the task is to train neural networks to
generalize from a family of training distributions to unseen test
distributions. We present new algorithms that achieve state-of-the-art
generalization in medical imaging, molecular identification, and image
classification. Finally, we study the setting of jailbreaking large language
models (LLMs), wherein an adversarial user attempts to design prompts that
elicit objectionable content from an LLM. We propose new attacks and defenses,
which represent the frontier of progress toward designing robust language-based
agents.

</details>


### [357] [Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation](https://arxiv.org/abs/2509.19112)
*Hugo Math,Rainer Lienhart*

Main category: cs.LG

TL;DR: The paper presents CARGO, a scalable causal discovery method for high-dimensional event sequences, using pretrained Transformer models to infer one-shot causal graphs and aggregate them for label reasoning.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the unsolved challenge of understanding causality in complex event sequences, essential for domains like healthcare and automotive diagnostics.

Method: CARGO uses two pretrained causal Transformers as foundation models for event sequences, infers causal graphs per sequence in parallel, and aggregates them using adaptive frequency fusion to construct global Markov boundaries.

Result: On a real-world automotive fault prediction dataset with over 29,100 event types and 474 labels, CARGO demonstrated effective structured reasoning capabilities.

Conclusion: CARGO facilitates efficient and scalable probabilistic reasoning in sparse, high-dimensional event sequences without the high cost of traditional methods.

Abstract: Understanding causality in event sequences where outcome labels such as
diseases or system failures arise from preceding events like symptoms or error
codes is critical. Yet remains an unsolved challenge across domains like
healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label
causal discovery method for sparse, high-dimensional event sequences comprising
of thousands of unique event types. Using two pretrained causal Transformers as
domain-specific foundation models for event sequences. CARGO infers in
parallel, per sequence one-shot causal graphs and aggregates them using an
adaptive frequency fusion to reconstruct the global Markov boundaries of
labels. This two-stage approach enables efficient probabilistic reasoning at
scale while bypassing the intractable cost of full-dataset conditional
independence testing. Our results on a challenging real-world automotive fault
prediction dataset with over 29,100 unique event types and 474 imbalanced
labels demonstrate CARGO's ability to perform structured reasoning.

</details>


### [358] [Analysis on distribution and clustering of weight](https://arxiv.org/abs/2509.19122)
*Chunming Ye,Wenquan Tian,Yalan Gao,Songzhou Li*

Main category: cs.LG

TL;DR: This paper analyzes weight characteristics in large language models using novel vector approaches to understand their similarities and differences, before and after fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Large language models are extensively researched, specifically in understanding architecture and parameter characteristics.

Method: Two novel vectors—Standard-Deviation Vector (based on normal distribution of weights) and Clustering Vector (using K-Means grouping on singular values)—are proposed to analyze and compare weight characteristics in models.

Result: The proposed vectors effectively differentiate between models and identify family similarities. Fine-tuning datasets influence the Standard-Deviation Vector but not the Clustering Vector.

Conclusion: The proposed methodologies provide clear insights into weight characteristics and highlight the consistency of correlation characteristics regardless of fine-tuning.

Abstract: The study on architecture and parameter characteristics remains the hot topic
in the research of large language models. In this paper we concern with the
characteristics of weight which are used to analyze the correlations and
differences between models. Two kinds of vectors-standard deviation vector and
clustering vector-are proposed to describe features of models. In the first
case, the weights are assumed to follow normal distribution. The standard
deviation values of projection matrices are normalized to form
Standard-Deviation Vector, representing the distribution characteristics of
models. In the second case, the singular values from each weight projection
matrix are extracted and grouped by K-Means algorithm. The grouped data with
the same type matrix are combined as Clustering Vector to represent the
correlation characteristics of models' weights. The study reveals that these
two vectors can effectively distinguish between different models and clearly
show the similarities among models of the same family. Moreover, after
conducting LoRA fine-tuning with different datasets and models, it is found
that the distribution of weights represented by standard deviation vector is
directly influenced by the dataset, but the correlations between different
weights represented by clustering vector remain unaffected and maintain a high
consistency with the pre-trained model.

</details>


### [359] [PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio](https://arxiv.org/abs/2509.19128)
*Alexandre Piché,Ehsan Kamaloo,Rafael Pardinas,Dzmitry Bahdanau*

Main category: cs.LG

TL;DR: The paper introduces PipelineRL, a reinforcement learning approach for training large language models (LLMs) that enhances hardware efficiency and maintains fresh training data, achieving significant speedups during training.


<details>
  <summary>Details</summary>
Motivation: Scaling reinforcement learning techniques for large language models is challenging due to the trade-off between hardware utilization and generating high-quality, on-policy data.

Method: PipelineRL combines concurrent asynchronous data generation and model training, implementing in-flight weight updates to ensure minimal disruption and maximize data freshness and hardware utilization.

Result: PipelineRL improves learning speed by approximately 2x on long-form reasoning tasks using 128 H100 GPUs compared to traditional RL methods, while maintaining highly on-policy data.

Conclusion: PipelineRL offers a scalable and efficient method for training LLMs, demonstrating both high performance and practicality for real-world applications, with an open-source implementation available.

Abstract: Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning
capabilities of Large Language Models (LLMs). However, effectively scaling
these RL methods presents significant challenges, primarily due to the
difficulty in maintaining high AI accelerator utilization without generating
stale, off-policy data that harms common RL algorithms. This paper introduces
PipelineRL, an approach designed to achieve a superior trade-off between
hardware efficiency and data on-policyness for LLM training. PipelineRL employs
concurrent asynchronous data generation and model training, distinguished by
the novel in-flight weight updates. This mechanism allows the LLM generation
engine to receive updated model weights with minimal interruption during the
generation of token sequences, thereby maximizing both the accelerator
utilization and the freshness of training data. Experiments conducted on
long-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL
achieves approximately $\sim 2x$ faster learning compared to conventional RL
baselines while maintaining highly on-policy training data. A scalable and
modular open-source implementation of PipelineRL is also released as a key
contribution.

</details>


### [360] [GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility Understanding](https://arxiv.org/abs/2509.19135)
*Wenying Luo,Zhiyuan Lin,Wenhao Xu,Minghao Liu,Zhi Li*

Main category: cs.LG

TL;DR: The paper introduces GSTM-HMU, a framework for analyzing human mobility by modeling its semantic and temporal complexities.


<details>
  <summary>Details</summary>
Motivation: To better understand human mobility patterns by capturing the semantic and temporal regularities, and to improve predictive and interpretative abilities.

Method: The framework, GSTM-HMU, consists of four key components: Spatio-Temporal Concept Encoder (STCE), Cognitive Trajectory Memory (CTM), Lifestyle Concept Bank (LCB), and task-specific generative heads.

Result: Extensive experiments on four real-world datasets demonstrated improved performance over strong baselines on tasks such as next-location prediction, trajectory-user identification, and time estimation.

Conclusion: GSTM-HMU effectively models complex mobility data, improving performance and interpretability while paving the way for generative modeling in human mobility analysis.

Abstract: Human mobility traces, often recorded as sequences of check-ins, provide a
unique window into both short-term visiting patterns and persistent lifestyle
regularities. In this work we introduce GSTM-HMU, a generative spatio-temporal
framework designed to advance mobility analysis by explicitly modeling the
semantic and temporal complexity of human movement. The framework consists of
four key innovations. First, a Spatio-Temporal Concept Encoder (STCE)
integrates geographic location, POI category semantics, and periodic temporal
rhythms into unified vector representations. Second, a Cognitive Trajectory
Memory (CTM) adaptively filters historical visits, emphasizing recent and
behaviorally salient events in order to capture user intent more effectively.
Third, a Lifestyle Concept Bank (LCB) contributes structured human preference
cues, such as activity types and lifestyle patterns, to enhance
interpretability and personalization. Finally, task-oriented generative heads
transform the learned representations into predictions for multiple downstream
tasks. We conduct extensive experiments on four widely used real-world
datasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate
performance on three benchmark tasks: next-location prediction, trajectory-user
identification, and time estimation. The results demonstrate consistent and
substantial improvements over strong baselines, confirming the effectiveness of
GSTM-HMU in extracting semantic regularities from complex mobility data. Beyond
raw performance gains, our findings also suggest that generative modeling
provides a promising foundation for building more robust, interpretable, and
generalizable systems for human mobility intelligence.

</details>


### [361] [Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation Functions](https://arxiv.org/abs/2509.19159)
*Qingfeng Lan,Gautham Vasan,A. Rupam Mahmood*

Main category: cs.LG

TL;DR: The paper investigates how activation functions in neural networks affect catastrophic forgetting in reinforcement learning and proposes new activation functions ("elephant activation functions") to improve performance.


<details>
  <summary>Details</summary>
Motivation: Despite progress in addressing catastrophic forgetting through algorithms, the architectural factors in neural networks contributing to this issue, particularly the role of activation functions, remain underexplored.

Method: The study examines the influence of activation functions on training dynamics and catastrophic forgetting, revealing the importance of gradient sparsity. It introduces 'elephant activation functions' which produce sparse outputs and gradients.

Result: Replacing classical activation functions with the proposed 'elephant activation functions' enhances neural network resilience to catastrophic forgetting, improving both sample efficiency and memory efficiency in reinforcement learning.

Conclusion: Elephant activation functions effectively mitigate catastrophic forgetting by leveraging both output and gradient sparsity in neural networks, offering a simple yet impactful architectural improvement for reinforcement learning.

Abstract: Catastrophic forgetting has remained a significant challenge for efficient
reinforcement learning for decades (Ring 1994, Rivest and Precup 2003). While
recent works have proposed effective methods to mitigate this issue, they
mainly focus on the algorithmic side. Meanwhile, we do not fully understand
what architectural properties of neural networks lead to catastrophic
forgetting. This study aims to fill this gap by studying the role of activation
functions in the training dynamics of neural networks and their impact on
catastrophic forgetting in reinforcement learning setup. Our study reveals
that, besides sparse representations, the gradient sparsity of activation
functions also plays an important role in reducing forgetting. Based on this
insight, we propose a new class of activation functions, elephant activation
functions, that can generate both sparse outputs and sparse gradients. We show
that by simply replacing classical activation functions with elephant
activation functions in the neural networks of value-based algorithms, we can
significantly improve the resilience of neural networks to catastrophic
forgetting, thus making reinforcement learning more sample-efficient and
memory-efficient.

</details>


### [362] [A Validation Strategy for Deep Learning Models: Evaluating and Enhancing Robustness](https://arxiv.org/abs/2509.19197)
*Abdul-Rauf Nuhu,Parham Kebria,Vahid Hemmati,Benjamin Lartey,Mahmoud Nabil Mahmoud,Abdollah Homaifar,Edward Tunstel*

Main category: cs.LG

TL;DR: This paper introduces a new approach for robustness validation in deep learning models, focusing on identifying samples in the training set most susceptible to perturbations and using them to assess and improve model reliability.


<details>
  <summary>Details</summary>
Motivation: Deep learning models often perform well on clean datasets but face significant challenges when exposed to adversarial or common corruption distortions, which can compromise their reliability.

Method: The proposed method extracts 'weak robust' samples from the training dataset through local robustness analysis and uses these to evaluate the model's vulnerabilities. These samples act as early indicators of robustness issues and guide targeted performance improvements.

Result: Experiments on CIFAR-10, CIFAR-100, and ImageNet show that evaluating models on weak robust samples enables better understanding of their robustness and leads to improvements under adversarial and corruption scenarios.

Conclusion: The framework provides a direct way to identify and leverage weak robust samples from training data for more precise robustness validation, enhancing overall model reliability against distortions.

Abstract: Data-driven models, especially deep learning classifiers often demonstrate
great success on clean datasets. Yet, they remain vulnerable to common data
distortions such as adversarial and common corruption perturbations. These
perturbations can significantly degrade performance, thereby challenging the
overall reliability of the models. Traditional robustness validation typically
relies on perturbed test datasets to assess and improve model performance. In
our framework, however, we propose a validation approach that extracts "weak
robust" samples directly from the training dataset via local robustness
analysis. These samples, being the most susceptible to perturbations, serve as
an early and sensitive indicator of the model's vulnerabilities. By evaluating
models on these challenging training instances, we gain a more nuanced
understanding of its robustness, which informs targeted performance
enhancement. We demonstrate the effectiveness of our approach on models trained
with CIFAR-10, CIFAR-100, and ImageNet, highlighting how robustness validation
guided by weak robust samples can drive meaningful improvements in model
reliability under adversarial and common corruption scenarios.

</details>


### [363] [PPG-Distill: Efficient Photoplethysmography Signals Analysis via Foundation Model Distillation](https://arxiv.org/abs/2509.19215)
*Juntong Ni,Saurabh Kataria,Shengpu Tang,Carl Yang,Xiao Hu,Wei Jin*

Main category: cs.LG

TL;DR: PPG-Distill introduces a novel distillation framework, effectively improving the performance of lightweight PPG models for wearable devices while significantly enhancing efficiency.


<details>
  <summary>Details</summary>
Motivation: Develop a solution to overcome challenges in deploying large PPG models on resource-constrained devices for wearable health monitoring.

Method: PPG-Distill employs a knowledge distillation framework that includes prediction-, feature-, and patch-level distillation alongside innovative morphology and rhythm distillation techniques.

Result: The framework enhances smaller PPG models, yielding up to 21.8% better performance, 7X faster inference, and 19X reduced memory usage.

Conclusion: PPG-Distill enables efficient and effective PPG model deployment on wearable devices, making health monitoring more accessible in constrained environments.

Abstract: Photoplethysmography (PPG) is widely used in wearable health monitoring, yet
large PPG foundation models remain difficult to deploy on resource-limited
devices. We present PPG-Distill, a knowledge distillation framework that
transfers both global and local knowledge through prediction-, feature-, and
patch-level distillation. PPG-Distill incorporates morphology distillation to
preserve local waveform patterns and rhythm distillation to capture inter-patch
temporal structures. On heart rate estimation and atrial fibrillation
detection, PPG-Distill improves student performance by up to 21.8% while
achieving 7X faster inference and reducing memory usage by 19X, enabling
efficient PPG analysis on wearables

</details>


### [364] [Video Killed the Energy Budget: Characterizing the Latency and Power Regimes of Open Text-to-Video Models](https://arxiv.org/abs/2509.19222)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: This paper examines the computational and energy costs of text-to-video (T2V) generation systems, providing a benchmark and insights into their efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand and optimize the energy consumption and latency of state-of-the-art T2V models, which are computationally taxing.

Method: They developed an analytical model to predict computational scaling laws, validated it through experiments on WAN2.1-T2V, and extended analysis across six T2V systems to compare energy and runtime performance.

Result: The paper confirms quadratic growth in computational demand with spatial and temporal dimensions, linear growth with denoising steps, and provides comparative runtime and energy profiles for different T2V models.

Conclusion: The findings offer practical insights for creating more sustainable generative video systems and serve as a benchmark for evaluating state-of-the-art T2V models.

Abstract: Recent advances in text-to-video (T2V) generation have enabled the creation
of high-fidelity, temporally coherent clips from natural language prompts. Yet
these systems come with significant computational costs, and their energy
demands remain poorly understood. In this paper, we present a systematic study
of the latency and energy consumption of state-of-the-art open-source T2V
models. We first develop a compute-bound analytical model that predicts scaling
laws with respect to spatial resolution, temporal length, and denoising steps.
We then validate these predictions through fine-grained experiments on
WAN2.1-T2V, showing quadratic growth with spatial and temporal dimensions, and
linear scaling with the number of denoising steps. Finally, we extend our
analysis to six diverse T2V models, comparing their runtime and energy profiles
under default settings. Our results provide both a benchmark reference and
practical insights for designing and deploying more sustainable generative
video systems.

</details>


### [365] [Study Design and Demystification of Physics Informed Neural Networks for Power Flow Simulation](https://arxiv.org/abs/2509.19233)
*Milad Leyli-abadi,Antoine Marot,Jérôme Picault*

Main category: cs.LG

TL;DR: This paper explores various hybrid machine learning approaches for power grid simulations, integrating physical laws to enhance performance in real-time applications.


<details>
  <summary>Details</summary>
Motivation: The study investigates how hybrid machine learning models can improve the efficiency and physical adherence of power flow simulations, crucial for maintaining grid stability amidst growing uncertainties in energy systems.

Method: The authors conduct an ablation study on hybrid models by embedding physical constraints into machine learning architectures. They systematically evaluate these models using a custom benchmarking pipeline (LIPS) across different criteria.

Result: The study shows that integrating physical knowledge through hybridization strategies enhances model performance in terms of accuracy, compliance with physical laws, industrial applicability, and generalization to out-of-distribution scenarios.

Conclusion: Embedding physical constraints in machine learning models proves beneficial for power flow simulations, offering better performance and practical applicability. The study also provides open-source implementations for reproducibility.

Abstract: In the context of the energy transition, with increasing integration of
renewable sources and cross-border electricity exchanges, power grids are
encountering greater uncertainty and operational risk. Maintaining grid
stability under varying conditions is a complex task, and power flow simulators
are commonly used to support operators by evaluating potential actions before
implementation. However, traditional physical solvers, while accurate, are
often too slow for near real-time use. Machine learning models have emerged as
fast surrogates, and to improve their adherence to physical laws (e.g.,
Kirchhoff's laws), they are often trained with embedded constraints which are
also known as physics-informed or hybrid models. This paper presents an
ablation study to demystify hybridization strategies, ranging from
incorporating physical constraints as regularization terms or unsupervised
losses, and exploring model architectures from simple multilayer perceptrons to
advanced graph-based networks enabling the direct optimization of physics
equations. Using our custom benchmarking pipeline for hybrid models called
LIPS, we evaluate these models across four dimensions: accuracy, physical
compliance, industrial readiness, and out-of-distribution generalization. The
results highlight how integrating physical knowledge impacts performance across
these criteria. All the implementations are reproducible and provided in the
corresponding Github page.

</details>


### [366] [Stability and Generalization of Adversarial Diffusion Training](https://arxiv.org/abs/2509.19234)
*Hesam Hosseini,Ying Cao,Ali H. Sayed*

Main category: cs.LG

TL;DR: The paper offers a stability-based analysis for generalization in decentralized adversarial training with convex losses.


<details>
  <summary>Details</summary>
Motivation: Address the lack of understanding of generalization properties in adversarial training under decentralized settings.

Method: Analyzes the generalization error using a stability framework, deriving bounds, and validates findings with experiments on logistic regression.

Result: The generalization error increases with adversarial perturbation strength and the number of training steps, validated by experiments.

Conclusion: Provides new insights into the generalization behavior of adversarial training in decentralized systems, consistent with existing findings for single-agent systems.

Abstract: Algorithmic stability is an established tool for analyzing generalization.
While adversarial training enhances model robustness, it often suffers from
robust overfitting and an enlarged generalization gap. Although recent work has
established the convergence of adversarial training in decentralized networks,
its generalization properties remain unexplored. This work presents a
stability-based generalization analysis of adversarial training under the
diffusion strategy for convex losses. We derive a bound showing that the
generalization error grows with both the adversarial perturbation strength and
the number of training steps, a finding consistent with single-agent case but
novel for decentralized settings. Numerical experiments on logistic regression
validate these theoretical predictions.

</details>


### [367] [What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT](https://arxiv.org/abs/2509.19284)
*Yunzhen Feng,Julia Kempe,Cheng Zhang,Parag Jain,Anthony Hartshorn*

Main category: cs.LG

TL;DR: The paper critiques the assumption that longer chain-of-thought reasoning is better, introducing Failed-Step Fraction (FSF) as a superior metric for evaluating reasoning quality.


<details>
  <summary>Details</summary>
Motivation: Evaluate the effectiveness of chain-of-thought (CoT) reasoning approaches in large reasoning models (LRMs) and address the assumption that longer CoTs inherently produce better results.

Method: Introduce the Failed-Step Fraction (FSF) metric, conduct systematic evaluations on math and scientific tasks across ten LRMs, and design interventions to rank and edit CoTs based on FSF.

Result: Discover that FSF outperforms CoT length and review ratio in predicting correctness, and show that reducing failed branches in CoT improves reasoning accuracy.

Conclusion: Effective CoTs fail less often, making structure-aware approaches more viable for improving reasoning in LRMs rather than indiscriminately lengthening CoT reasoning.

Abstract: Large reasoning models (LRMs) spend substantial test-time compute on long
chain-of-thought (CoT) traces, but what *characterizes* an effective CoT
remains unclear. While prior work reports gains from lengthening CoTs and
increasing review (revisiting earlier steps) via appended *wait* tokens, recent
studies suggest that shorter thinking can outperform longer traces. We
therefore conduct a systematic evaluation across ten LRMs on math and
scientific reasoning. Contrary to the "longer-is-better" narrative, we find
that both naive CoT lengthening and increased review are associated with
*lower* accuracy.
  As CoT unfolds step by step, token-level metrics can conflate verbosity with
process quality. We introduce a graph view of CoT to extract structure and
identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of
steps in abandoned branches-that consistently outpredicts length and review
ratio for correctness across models. To probe causality, we design two
interventions. First, we rank candidate CoTs by each metric at test time, where
FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed
branches, which significantly improves accuracy, indicating that failed
branches bias subsequent reasoning. Taken together, these results characterize
effective CoTs as those that *fail less* and support *structure-aware*
test-time scaling over indiscriminately generating long CoT.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [368] [Smart Cellular Bricks for Decentralized Shape Classification and Damage Recovery](https://arxiv.org/abs/2509.18659)
*Rodrigo Moreno,Andres Faina,Shyam Sudhakaran,Kathryn Walker,Sebastian Risi*

Main category: cs.NE

TL;DR: Researchers developed a decentralized modular system using simple cubic bricks with onboard processing and sensing to infer their shape and detect damage, inspired by biological self-recognition strategies.


<details>
  <summary>Details</summary>
Motivation: Inspired by biological capabilities for self-recognition and regeneration, the study aimed to develop physical systems that mimic these decentralized processes.

Method: The system employs Neural Cellular Automata (NCA), where each brick executes the same neural network algorithm locally without any centralized state or positioning information.

Result: The bricks collectively classify diverse 3D shapes with high accuracy and robustness to faults while localizing and addressing structural damage.

Conclusion: This decentralized modular system is a step towards robust, adaptive, bio-inspired tools for self-recognition and damage recovery in physical systems.

Abstract: Biological systems possess remarkable capabilities for self-recognition and
morphological regeneration, often relying solely on local interactions.
Inspired by these decentralized processes, we present a novel system of
physical 3D bricks--simple cubic units equipped with local communication,
processing, and sensing--that are capable of inferring their global shape class
and detecting structural damage. Leveraging Neural Cellular Automata (NCA), a
learned, fully-distributed algorithm, our system enables each module to
independently execute the same neural network without access to any global
state or positioning information. We demonstrate the ability of collections of
hundreds of these cellular bricks to accurately classify a variety of 3D shapes
through purely local interactions. The approach shows strong robustness to
out-of-distribution shape variations and high tolerance to communication faults
and failed modules. In addition to shape inference, the same decentralized
framework is extended to detect missing or damaged components, allowing the
collective to localize structural disruptions and to guide a recovery process.
This work provides a physical realization of large-scale, decentralized
self-recognition and damage detection, advancing the potential of robust,
adaptive, and bio-inspired modular systems.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [369] [Static Estimation of Reuse Profiles for Arrays in Nested Loops](https://arxiv.org/abs/2509.18684)
*Abdur Razzak,Atanu Barai,Nandakishore Santhi,Abdel-Hameed A. Badawy*

Main category: cs.PF

TL;DR: The paper develops a static analysis framework that predicts reuse profiles and cache hit rates for arrays in nested loops, achieving near-dynamic profiling accuracy with far greater speed.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for calculating Reuse Distance Histograms (RDHs) rely on runtime memory trace collection, which is resource-heavy and unsuitable for early optimization or large-scale applications.

Method: The proposed framework analyzes loop bounds, access patterns in smaller problem sizes, and predictive equations to statically estimate reuse distances and cache hit rates without runtime information.

Result: The static predictor demonstrates comparable accuracy to the dynamic tool PARDA, while drastically improving analysis speed.

Conclusion: This work provides a fast, accurate alternative to dynamic reuse profiling, making it suitable for integration into compilers and performance modeling tools.

Abstract: Efficient memory access patterns play a crucial role in determining the
overall performance of applications by exploiting temporal and spatial
locality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is
a widely used metric to quantify temporal locality, measuring the distance
between consecutive accesses to the same memory location. Traditionally,
calculating RDH requires program execution and memory trace collection to
obtain dynamic memory access behavior. This trace collection is often
time-consuming, resource-intensive, and unsuitable for early-stage optimization
or large-scale applications. Static prediction, on the other hand, offers a
significant speedup in estimating RDH and cache hit rates. However, these
approaches lack accuracy, since the predictions come without running the
program and knowing the complete memory access pattern, more specifically when
arrays are used inside nested loops. This paper presents a novel static
analysis framework for predicting the reuse profiles of array references in
programs with nested loop structures, without requiring any runtime
information. By analyzing loop bounds, access patterns in smaller problem
sizes, and predictive equations, our method predicts access patterns of arrays
and estimates reuse distances and cache hit rate at compile time. This paper
extends our previous study by incorporating more analysis and improving
prediction by addressing previously unhandled reuse patterns. We evaluate our
technique against a widely accepted traditional trace-driven profiling tool,
Parallel Reuse Distance Analysis (PARDA). The results demonstrate that our
static predictor achieves comparable accuracy while offering
orders-of-magnitude improvement in the analysis speed. This work offers a
practical alternative to dynamic reuse profiling and paves the way for
integration into compilers and static performance modeling tools.

</details>


### [370] [Confidential LLM Inference: Performance and Cost Across CPU and GPU TEEs](https://arxiv.org/abs/2509.18886)
*Marcin Chrapek,Marcin Copik,Etienne Mettaz,Torsten Hoefler*

Main category: cs.PF

TL;DR: This study evaluates the use of Trusted Execution Environments (TEEs) in CPUs and GPUs to secure end-to-end inference for Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The need to secure LLMs, which handle sensitive data and proprietary datasets, for adoption in privacy-sensitive industries like healthcare and finance.

Method: The authors conduct evaluations for LLM inference pipelines within Intel CPU TEEs (TDX and SGX) and NVIDIA GPU (H100 Confidential Compute GPUs), analyzing throughput, latency, and performance over various configurations.

Result: The study finds that CPU TEEs impose under 10% throughput and 20% latency overheads, further reduced by AMX, while GPU TEEs show 4-8% throughput penalties that decrease as batch/input sizes grow.

Conclusion: This work demonstrates the practicality and performance of TEEs for securing LLM workloads, offering insights into cost and security trade-offs between CPU and GPU implementations.

Abstract: Large Language Models (LLMs) are increasingly deployed on converged Cloud and
High-Performance Computing (HPC) infrastructure. However, as LLMs handle
confidential inputs and are fine-tuned on costly, proprietary datasets, their
heightened security requirements slow adoption in privacy-sensitive sectors
such as healthcare and finance. We investigate methods to address this gap and
propose Trusted Execution Environments (TEEs) as a solution for securing
end-to-end LLM inference. We validate their practicality by evaluating these
compute-intensive workloads entirely within CPU and GPU TEEs. On the CPU side,
we conduct an in-depth study running full Llama2 inference pipelines (7B, 13B,
70B) inside Intel's TDX and SGX, accelerated by Advanced Matrix Extensions
(AMX). We derive 12 insights, including that across various data types, batch
sizes, and input lengths, CPU TEEs impose under 10% throughput and 20% latency
overheads, further reduced by AMX. We run LLM inference on NVIDIA H100
Confidential Compute GPUs, contextualizing our CPU findings and observing
throughput penalties of 4-8% that diminish as batch and input sizes grow. By
comparing performance, cost, and security trade-offs, we show how CPU TEEs can
be more cost-effective or secure than their GPU counterparts. To our knowledge,
our work is the first to comprehensively demonstrate the performance and
practicality of modern TEEs across both CPUs and GPUs for enabling confidential
LLMs (cLLMs).

</details>


### [371] [Glass-Box Analysis for Computer Systems: Transparency Index, Shapley Attribution, and Markov Models of Branch Prediction](https://arxiv.org/abs/2509.19027)
*Faruk Alpay,Hamdi Alakkad*

Main category: cs.PF

TL;DR: The paper introduces methods for glass-box analysis in computer systems, involving quantification of performance variance, attribution of throughput, and analytic frameworks for branch predictors.


<details>
  <summary>Details</summary>
Motivation: To enhance transparency in computer systems analysis by quantifying internal features and their influence.

Method: The proposed methods include: GTI for performance variance quantification, ETD using Shapley values for throughput attribution, and Markov analytic frameworks for branch prediction analysis.

Result: Tools and methodologies are developed with mathematical guarantees such as error bounds, stability under noise, and exact analytical outcomes.

Conclusion: The paper provides new frameworks that advance transparent and analytically grounded assessments in computer systems design and performance.

Abstract: We formalize glass-box analysis for computer systems and introduce three
principled tools. First, the Glass-Box Transparency Index (GTI) quantifies the
fraction of performance variance explainable by internal features and comes
equipped with bounds, invariances, cross-validated estimation, and bootstrap
confidence intervals. Second, Explainable Throughput Decomposition (ETD) uses
Shapley values to provide an efficiency-preserving attribution of throughput,
together with non-asymptotic Monte Carlo error guarantees and convexity
(Jensen) gap bounds. Third, we develop an exact Markov analytic framework for
branch predictors, including a closed-form misprediction rate for a two-bit
saturating counter under a two-state Markov branch process and its i.i.d.
corollary. Additionally, we establish an identifiability theorem for recovering
event rates from aggregated hardware counters and provide stability bounds
under noise.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [372] [A Verified Compiler for Quantum Simulation](https://arxiv.org/abs/2509.18583)
*Liyi Li,Fenfen An,Federico Zahariev,Zhi Xiang Chong,Amr Sabry,Mark Gordon*

Main category: cs.PL

TL;DR: QBlue is a formally verified framework for compiling Hamiltonian simulations using second quantization, ensuring correctness and flexibility for quantum computing.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing Hamiltonian simulation compilers, which lack programmability and formal correctness guarantees, by developing a safer, more expressive framework.

Method: Introduces QBlue, a high-level framework using second quantization, a type system for safety, and formal verification using the Rocq proof framework to ensure correctness of compilation.

Result: QBlue offers end-to-end verified compilation for Hamiltonian simulations with support for both digital and analog quantum circuits.

Conclusion: QBlue ensures both expressiveness and correctness in Hamiltonian simulation, representing a significant advancement in high-level quantum programming and verified compilation.

Abstract: Hamiltonian simulation is a central application of quantum computing, with
significant potential in modeling physical systems and solving complex
optimization problems. Existing compilers for such simulations typically focus
on low-level representations based on Pauli operators, limiting programmability
and offering no formal guarantees of correctness across the compilation
pipeline. We introduce QBlue, a high-level, formally verified framework for
compiling Hamiltonian simulations. QBlue is based on the formalism of second
quantization, which provides a natural and expressive way to describe quantum
particle systems using creation and annihilation operators. To ensure safety
and correctness, QBlue includes a type system that tracks particle types and
enforces Hermitian structure. The framework supports compilation to both
digital and analog quantum circuits and captures multiple layers of semantics,
from static constraints to dynamic evolution. All components of QBlue,
including its language design, type system, and compilation correctness, are
fully mechanized in the Rocq proof framework, making it the first end-to-end
verified compiler for second-quantized Hamiltonian simulation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [373] [PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies](https://arxiv.org/abs/2509.18282)
*Jesse Zhang,Marius Memmel,Kevin Kim,Dieter Fox,Jesse Thomason,Fabio Ramos,Erdem Bıyık,Abhishek Gupta,Anqi Li*

Main category: cs.RO

TL;DR: The paper introduces PEEK, a method that leverages vision-language models to improve robotic manipulation policies through a transferable and scalable point-based representation.


<details>
  <summary>Details</summary>
Motivation: Robotic manipulation often struggles with generalization since policies must simultaneously handle multiple aspects like attending to objects, taking actions, and executing them.

Method: PEEK fine-tunes vision-language models to predict a point-based representation, including end-effector paths and task-specific masks, which are transferable to different robot architectures.

Result: PEEK shows significant improvements in zero-shot generalization, including a 41.4x performance boost for policies trained in simulation and notable gains across different robot scales.

Conclusion: PEEK reduces the burden on manipulation policies by allowing vision-language models to handle semantic and visual complexity, leading to better generalization and scalable robotic applications.

Abstract: Robotic manipulation policies often fail to generalize because they must
simultaneously learn where to attend, what actions to take, and how to execute
them. We argue that high-level reasoning about where and what can be offloaded
to vision-language models (VLMs), leaving policies to specialize in how to act.
We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which
fine-tunes VLMs to predict a unified point-based intermediate representation:
1. end-effector paths specifying what actions to take, and 2. task-relevant
masks indicating where to focus. These annotations are directly overlaid onto
robot observations, making the representation policy-agnostic and transferable
across architectures. To enable scalable training, we introduce an automatic
annotation pipeline, generating labeled data across 20+ robot datasets spanning
9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot
generalization, including a 41.4x real-world improvement for a 3D policy
trained only in simulation, and 2-3.5x gains for both large VLAs and small
manipulation policies. By letting VLMs absorb semantic and visual complexity,
PEEK equips manipulation policies with the minimal cues they need--where, what,
and how. Website at https://peek-robot.github.io/.

</details>


### [374] [Fine-Tuning Robot Policies While Maintaining User Privacy](https://arxiv.org/abs/2509.18311)
*Benjamin A. Christie,Sagar Parekh,Dylan P. Losey*

Main category: cs.RO

TL;DR: The paper introduces PRoP, a novel framework for enabling personalized yet private robot policies by incorporating user-specific keys that transform robot policy weights.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns during robot policy personalization, where users inadvertently expose their preferences and habits.

Method: The PRoP framework utilizes unique user-specific keys to mathematically transform weights within the robot's neural network, ensuring both personalization and privacy.

Result: PRoP effectively personalizes robot behaviors while preserving privacy, demonstrating superiority over encoder-based approaches across various learning models and tasks.

Conclusion: PRoP provides a robust, model-agnostic solution for privacy-aware personalization in robotics, retaining original architectures and outperforming alternative methods.

Abstract: Recent works introduce general-purpose robot policies. These policies provide
a strong prior over how robots should behave -- e.g., how a robot arm should
manipulate food items. But in order for robots to match an individual person's
needs, users typically fine-tune these generalized policies -- e.g., showing
the robot arm how to make their own preferred dinners. Importantly, during the
process of personalizing robots, end-users leak data about their preferences,
habits, and styles (e.g., the foods they prefer to eat). Other agents can
simply roll-out the fine-tuned policy and see these personally-trained
behaviors. This leads to a fundamental challenge: how can we develop robots
that personalize actions while keeping learning private from external agents?
We here explore this emerging topic in human-robot interaction and develop
PRoP, a model-agnostic framework for personalized and private robot policies.
Our core idea is to equip each user with a unique key; this key is then used to
mathematically transform the weights of the robot's network. With the correct
key, the robot's policy switches to match that user's preferences -- but with
incorrect keys, the robot reverts to its baseline behaviors. We show the
general applicability of our method across multiple model types in imitation
learning, reinforcement learning, and classification tasks. PRoP is practically
advantageous because it retains the architecture and behaviors of the original
policy, and experimentally outperforms existing encoder-based approaches. See
videos and code here: https://prop-icra26.github.io.

</details>


### [375] [Haptic Communication in Human-Human and Human-Robot Co-Manipulation](https://arxiv.org/abs/2509.18327)
*Katherine H. Allen,Chris Rogers,Elaine S. Short*

Main category: cs.RO

TL;DR: Examined human-human and human-robot object manipulation using shared motion and haptic communication, highlighting the fluency gap between the two.


<details>
  <summary>Details</summary>
Motivation: To explore haptic communication in shared object manipulation and improve robot collaborators' ability to deliver anthropomorphic signals.

Method: Analyzed motion profiles using low-cost IMU sensors in human-human and human-robot interactions, supplemented with participant feedback from surveys.

Result: Human-human collaboration showed superior fluency, and IMU motion data clearly differentiated motion profiles between the conditions.

Conclusion: Highlighting the fluency gap places emphasis on designing robots capable of sending and receiving more human-like haptic signals for improved collaboration.

Abstract: When a human dyad jointly manipulates an object, they must communicate about
their intended motion plans. Some of that collaboration is achieved through the
motion of the manipulated object itself, which we call "haptic communication."
In this work, we captured the motion of human-human dyads moving an object
together with one participant leading a motion plan about which the follower is
uninformed. We then captured the same human participants manipulating the same
object with a robot collaborator. By tracking the motion of the shared object
using a low-cost IMU, we can directly compare human-human shared manipulation
to the motion of those same participants interacting with the robot.
Intra-study and post-study questionnaires provided participant feedback on the
collaborations, indicating that the human-human collaborations are
significantly more fluent, and analysis of the IMU data indicates that it
captures objective differences in the motion profiles of the conditions. The
differences in objective and subjective measures of accuracy and fluency
between the human-human and human-robot trials motivate future research into
improving robot assistants for physical tasks by enabling them to send and
receive anthropomorphic haptic signals.

</details>


### [376] [The Landform Contextual Mesh: Automatically Fusing Surface and Orbital Terrain for Mars 2020](https://arxiv.org/abs/2509.18330)
*Marsette Vona*

Main category: cs.RO

TL;DR: The paper discusses a 3D terrain visualization system called Landform contextual mesh, combining data from Mars rover imagery and orbital maps for Mars exploration.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to enhance mission planning and public engagement through advanced visualization tools that integrate diverse datasets.

Method: Landform contextual mesh combines 2D and 3D data from Mars rover images and orbital data via automated processing.

Result: 3D terrain visuals are created and provided to scientists for planning, and a subset is shared publicly via a dedicated website.

Conclusion: The system supports scientific planning and public outreach, demonstrating innovation in data fusion and visualization for Mars exploration.

Abstract: The Landform contextual mesh fuses 2D and 3D data from up to thousands of
Mars 2020 rover images, along with orbital elevation and color maps from Mars
Reconnaissance Orbiter, into an interactive 3D terrain visualization.
Contextual meshes are built automatically for each rover location during
mission ground data system processing, and are made available to mission
scientists for tactical and strategic planning in the Advanced Science
Targeting Tool for Robotic Operations (ASTTRO). A subset of them are also
deployed to the "Explore with Perseverance" public access website.

</details>


### [377] [Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation](https://arxiv.org/abs/2509.18342)
*Rajitha de Silva,Jonathan Cox,James R. Heselden,Marija Popovic,Cesar Cadena,Riccardo Polvara*

Main category: cs.RO

TL;DR: This paper introduces a semantic particle filter for improved robot localization in vineyards using object-level detections and structural constraints.


<details>
  <summary>Details</summary>
Motivation: To address the failure of traditional LiDAR-based localization in vineyards due to repetitive row structures and perceptual aliasing.

Method: Combines object-level detections of vine trunks and support poles with LiDAR scans, incorporates semantic walls for pseudo-structural constraints, and uses a noisy GPS prior for headland regions.

Result: The proposed approach successfully maintains localization within vineyard rows, recovers from deviations, and outperforms established vision-based SLAM methods like RTAB-Map.

Conclusion: The semantic particle filter improves localization reliability in structured outdoor environments like vineyards where conventional methods struggle.

Abstract: Accurate localisation is critical for mobile robots in structured outdoor
environments, yet LiDAR-based methods often fail in vineyards due to repetitive
row geometry and perceptual aliasing. We propose a semantic particle filter
that incorporates stable object-level detections, specifically vine trunks and
support poles into the likelihood estimation process. Detected landmarks are
projected into a birds eye view and fused with LiDAR scans to generate semantic
observations. A key innovation is the use of semantic walls, which connect
adjacent landmarks into pseudo-structural constraints that mitigate row
aliasing. To maintain global consistency in headland regions where semantics
are sparse, we introduce a noisy GPS prior that adaptively supports the filter.
Experiments in a real vineyard demonstrate that our approach maintains
localisation within the correct row, recovers from deviations where AMCL fails,
and outperforms vision-based SLAM methods such as RTAB-Map.

</details>


### [378] [AD-VF: LLM-Automatic Differentiation Enables Fine-Tuning-Free Robot Planning from Formal Methods Feedback](https://arxiv.org/abs/2509.18384)
*Yunhao Yang,Junyuan Hong,Gabriel Jacob Perin,Zhiwen Fan,Li Yin,Zhangyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: This paper introduces LAD-VF, a framework to enhance LLM-driven planning by leveraging formal verification feedback for automated prompt refinement, improving compliance and success rates in robotics tasks without costly fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of deploying LLM-driven planning systems in real-world scenarios, where adherence to safety and regulatory constraints is critical, but current methods are hindered by hallucinations or weak model alignment.

Method: LAD-VF uses formal-verification-informed text loss and LLM-AutoDiff for iterative prompt engineering without altering model parameters, enabling scalable adaptation and compatibility with modular LLM architectures.

Result: Experiments in robot navigation and manipulation show LAD-VF improves specification compliance significantly, boosting success rates from 60% to over 90%.

Conclusion: LAD-VF offers a scalable, interpretable, fine-tuning-free method for trustworthy LLM-driven control systems, addressing safety and alignment challenges effectively.

Abstract: Large language models (LLMs) can translate natural language instructions into
executable action plans for robotics, autonomous driving, and other domains.
Yet, deploying LLM-driven planning in the physical world demands strict
adherence to safety and regulatory constraints, which current models often
violate due to hallucination or weak alignment. Traditional data-driven
alignment methods, such as Direct Preference Optimization (DPO), require costly
human labeling, while recent formal-feedback approaches still depend on
resource-intensive fine-tuning. In this paper, we propose LAD-VF, a
fine-tuning-free framework that leverages formal verification feedback for
automated prompt engineering. By introducing a formal-verification-informed
text loss integrated with LLM-AutoDiff, LAD-VF iteratively refines prompts
rather than model parameters. This yields three key benefits: (i) scalable
adaptation without fine-tuning; (ii) compatibility with modular LLM
architectures; and (iii) interpretable refinement via auditable prompts.
Experiments in robot navigation and manipulation tasks demonstrate that LAD-VF
substantially enhances specification compliance, improving success rates from
60% to over 90%. Our method thus presents a scalable and interpretable pathway
toward trustworthy, formally-verified LLM-driven control systems.

</details>


### [379] [Assistive Decision-Making for Right of Way Navigation at Uncontrolled Intersections](https://arxiv.org/abs/2509.18407)
*Navya Tiwari,Joseph Vazhaeparampil,Victoria Preston*

Main category: cs.RO

TL;DR: This paper develops a driver-assist framework for decision-making at uncontrolled intersections, using a POMDP approach. It evaluates four decision-making methods in simulations, demonstrating that probabilistic planners significantly outperform rule-based baselines in safety and navigation efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reduce the frequency of crashes at uncontrolled intersections caused by ambiguous rules, limited visibility, and erratic driver behavior. Existing solutions for autonomous vehicles lack retrofitting options for human-operated cars.

Method: The method involves formulating a driver-assistance framework as a Partially Observable Markov Decision Process (POMDP). Simulation tests include stochastic traffic scenarios and evaluations of four decision-making approaches (FSM, QMDP, POMCP, DESPOT).

Result: Probabilistic planners (QMDP, POMCP, DESPOT) performed better than the deterministic FSM baseline, achieving up to 97.5% collision-free navigation. POMCP emphasized safety, while DESPOT balanced efficiency and runtime.

Conclusion: The study concludes that integrating uncertainty-aware planning principles is vital for enhancing driver assistance systems. It encourages further research on real-time implementation, including sensor fusion and perception advancements in realistic environments.

Abstract: Uncontrolled intersections account for a significant fraction of roadway
crashes due to ambiguous right-of-way rules, occlusions, and unpredictable
driver behavior. While autonomous vehicle research has explored
uncertainty-aware decision making, few systems exist to retrofit human-operated
vehicles with assistive navigation support. We present a driver-assist
framework for right-of-way reasoning at uncontrolled intersections, formulated
as a Partially Observable Markov Decision Process (POMDP). Using a custom
simulation testbed with stochastic traffic agents, pedestrians, occlusions, and
adversarial scenarios, we evaluate four decision-making approaches: a
deterministic finite state machine (FSM), and three probabilistic planners:
QMDP, POMCP, and DESPOT. Results show that probabilistic planners outperform
the rule-based baseline, achieving up to 97.5 percent collision-free navigation
under partial observability, with POMCP prioritizing safety and DESPOT
balancing efficiency and runtime feasibility. Our findings highlight the
importance of uncertainty-aware planning for driver assistance and motivate
future integration of sensor fusion and environment perception modules for
real-time deployment in realistic traffic environments.

</details>


### [380] [Latent Action Pretraining Through World Modeling](https://arxiv.org/abs/2509.18428)
*Bahey Tharwat,Yara Nasser,Ali Abouzeid,Ian Reid*

Main category: cs.RO

TL;DR: The paper presents LAWM, a model-agnostic framework that enables self-supervised pretraining of imitation learning models using unlabeled video data, outperforming existing methods in efficiency and effectiveness.


<details>
  <summary>Details</summary>
Motivation: Robotic manipulation models require extensive labeled datasets for training, which is labor-intensive and challenging, limiting their real-world deployment. This work aims to address these limitations by leveraging self-supervised learning methods.

Method: The authors propose LAWM, a self-supervised pretraining framework that models latent action representations from unlabeled video data through world modeling. This framework is task-agnostic and sources data from diverse robot or human activities.

Result: The framework outperforms existing models using ground-truth robotics actions or comparable pretraining methods on the LIBERO benchmark as well as in real-world experiments, while reducing model size for practical application.

Conclusion: LAWM represents an efficient and scalable solution for pretraining robotic manipulation models, addressing challenges in deployment and boosting performance in diverse tasks and environments.

Abstract: Vision-Language-Action (VLA) models have gained popularity for learning
robotic manipulation tasks that follow language instructions. State-of-the-art
VLAs, such as OpenVLA and $\pi_{0}$, were trained on large-scale, manually
labeled action datasets collected through teleoperation. More recent
approaches, including LAPA and villa-X, introduce latent action representations
that enable unsupervised pretraining on unlabeled datasets by modeling abstract
visual changes between frames. Although these methods have shown strong
results, their large model sizes make deployment in real-world settings
challenging. In this work, we propose LAWM, a model-agnostic framework to
pretrain imitation learning models in a self-supervised way, by learning latent
action representations from unlabeled video data through world modeling. These
videos can be sourced from robot recordings or videos of humans performing
actions with everyday objects. Our framework is designed to be effective for
transferring across tasks, environments, and embodiments. It outperforms models
trained with ground-truth robotics actions and similar pretraining methods on
the LIBERO benchmark and real-world setup, while being significantly more
efficient and practical for real-world settings.

</details>


### [381] [PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction](https://arxiv.org/abs/2509.18447)
*Rishabh Madan,Jiawei Lin,Mahika Goel,Angchen Xie,Xiaoyu Liang,Marcus Lee,Justin Guo,Pranav N. Thakkar,Rohan Banerjee,Jose Barreiros,Kate Tsui,Tom Silver,Tapomayukh Bhattacharjee*

Main category: cs.RO

TL;DR: The paper presents PrioriTouch, a framework for multi-contact human-robot interaction prioritizing user preferences, improving assistance tasks' safety and comfort.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of adapting robots to user preferences in multi-contact scenarios, especially in caregiving tasks, where conflicting force requirements across body parts make standard solutions inadequate.

Method: Developed PrioriTouch, which combines a learning-to-rank approach with hierarchical operational space control, incorporating user studies, personalized thresholds, and simulation-in-the-loop exploration.

Result: Through simulation and real-world experiments, PrioriTouch demonstrated its ability to adapt to user preferences, maintain task performance, and improve safety and comfort.

Conclusion: PrioriTouch effectively balances user preferences and task objectives in complex multi-contact scenarios, making it valuable for caregiving and broader pHRI applications.

Abstract: Physical human-robot interaction (pHRI) requires robots to adapt to
individual contact preferences, such as where and how much force is applied.
Identifying preferences is difficult for a single contact; with whole-arm
interaction involving multiple simultaneous contacts between the robot and
human, the challenge is greater because different body parts can impose
incompatible force requirements. In caregiving tasks, where contact is frequent
and varied, such conflicts are unavoidable. With multiple preferences across
multiple contacts, no single solution can satisfy all objectives--trade-offs
are inherent, making prioritization essential. We present PrioriTouch, a
framework for ranking and executing control objectives across multiple
contacts. PrioriTouch can prioritize from a general collection of controllers,
making it applicable not only to caregiving scenarios such as bed bathing and
dressing but also to broader multi-contact settings. Our method combines a
novel learning-to-rank approach with hierarchical operational space control,
leveraging simulation-in-the-loop rollouts for data-efficient and safe
exploration. We conduct a user study on physical assistance preferences, derive
personalized comfort thresholds, and incorporate them into PrioriTouch. We
evaluate PrioriTouch through extensive simulation and real-world experiments,
demonstrating its ability to adapt to user contact preferences, maintain task
performance, and enhance safety and comfort. Website:
https://emprise.cs.cornell.edu/prioritouch.

</details>


### [382] [Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous Hands](https://arxiv.org/abs/2509.18455)
*Yunshuang Li,Yiyang Ling,Gaurav S. Sukhatme,Daniel Seita*

Main category: cs.RO

TL;DR: The paper introduces GD2P, a method to enable dexterous robotic hands to perform nonprehensile manipulation tasks, such as pushing and pulling, by leveraging geometry-aware learning and modeling.


<details>
  <summary>Details</summary>
Motivation: Robots often struggle with manipulating objects that are difficult to grasp due to their geometry or environmental constraints. Nonprehensile manipulation, particularly with dexterous hands, can offer improved performance if modeled effectively.

Method: The authors developed GD2P, which synthesizes and learns dexterous hand poses. It uses contact-guided sampling, physics simulations, and diffusion models conditioned on object geometry to predict viable poses. Testing involves real-world experiments and standard motion planning.

Result: 840 experiments on the Allegro Hand demonstrate GD2P’s effectiveness over baseline methods in nonprehensile manipulation. The approach is also successfully tested on a LEAP Hand, validating its adaptability for different hand designs.

Conclusion: GD2P provides a scalable solution for training dexterous robotic hands in nonprehensile manipulation. Open-source models and datasets are shared to foster further research and development.

Abstract: Nonprehensile manipulation, such as pushing and pulling, enables robots to
move, align, or reposition objects that may be difficult to grasp due to their
geometry, size, or relationship to the robot or the environment. Much of the
existing work in nonprehensile manipulation relies on parallel-jaw grippers or
tools such as rods and spatulas. In contrast, multi-fingered dexterous hands
offer richer contact modes and versatility for handling diverse objects to
provide stable support over the objects, which compensates for the difficulty
of modeling the dynamics of nonprehensile manipulation. Therefore, we propose
Geometry-aware Dexterous Pushing and Pulling (GD2P) for nonprehensile
manipulation with dexterous robotic hands. We study pushing and pulling by
framing the problem as synthesizing and learning pre-contact dexterous hand
poses that lead to effective manipulation. We generate diverse hand poses via
contact-guided sampling, filter them using physics simulation, and train a
diffusion model conditioned on object geometry to predict viable poses. At test
time, we sample hand poses and use standard motion planners to select and
execute pushing and pulling actions. We perform 840 real-world experiments with
an Allegro Hand, comparing our method to baselines. The results indicate that
GD2P offers a scalable route for training dexterous nonprehensile manipulation
policies. We further demonstrate GD2P on a LEAP Hand, highlighting its
applicability to different hand morphologies. Our pre-trained models and
dataset, including 1.3 million hand poses across 2.3k objects, will be
open-source to facilitate further research. Our project website is available
at: geodex2p.github.io.

</details>


### [383] [Application Management in C-ITS: Orchestrating Demand-Driven Deployments and Reconfigurations](https://arxiv.org/abs/2509.18793)
*Lukas Zanger,Bastian Lampe,Lennart Reiher,Lutz Eckstein*

Main category: cs.RO

TL;DR: This paper proposes a demand-driven application management framework leveraging cloud-native microservices and Kubernetes in cooperative intelligent transport systems (C-ITS). The approach automates deployment, scaling, and updates dynamically for better resource efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the computational and network traffic challenges in managing dynamic and scalable applications in cooperative intelligent transport systems (C-ITS).

Method: The framework utilizes cloud-native techniques, particularly Kubernetes, combined with the Robot Operating System (ROS 2), to automate and dynamically manage microservices based on demand.

Result: The proposed framework successfully demonstrated its operation in a C-ITS use case focused on collective environment perception. Source code for the prototype is made available.

Conclusion: The demand-driven approach improves resource efficiency and adaptability in C-ITS, highlighting its potential to support scalable and dynamic intelligent transport applications.

Abstract: Vehicles are becoming increasingly automated and interconnected, enabling the
formation of cooperative intelligent transport systems (C-ITS) and the use of
offboard services. As a result, cloud-native techniques, such as microservices
and container orchestration, play an increasingly important role in their
operation. However, orchestrating applications in a large-scale C-ITS poses
unique challenges due to the dynamic nature of the environment and the need for
efficient resource utilization. In this paper, we present a demand-driven
application management approach that leverages cloud-native techniques -
specifically Kubernetes - to address these challenges. Taking into account the
demands originating from different entities within the C-ITS, the approach
enables the automation of processes, such as deployment, reconfiguration,
update, upgrade, and scaling of microservices. Executing these processes on
demand can, for example, reduce computing resource consumption and network
traffic. A demand may include a request for provisioning an external supporting
service, such as a collective environment model. The approach handles changing
and new demands by dynamically reconciling them through our proposed
application management framework built on Kubernetes and the Robot Operating
System (ROS 2). We demonstrate the operation of our framework in the C-ITS use
case of collective environment perception and make the source code of the
prototypical framework publicly available at
https://github.com/ika-rwth-aachen/application_manager .

</details>


### [384] [A Counterfactual Reasoning Framework for Fault Diagnosis in Robot Perception Systems](https://arxiv.org/abs/2509.18460)
*Haeyoon Han,Mahdi Taheri,Soon-Jo Chung,Fred Y. Hadaegh*

Main category: cs.RO

TL;DR: This paper develops a counterfactual reasoning framework for fault detection and isolation in robotic perception systems, using analytical redundancy instead of extra sensors.


<details>
  <summary>Details</summary>
Motivation: Faulty perception systems are challenging in autonomous systems as errors propagate across modules and are tied to environmental context.

Method: The authors use counterfactual reasoning to perform analytical redundancy, introducing both passive and active fault detection methods via causal bandit analysis.

Result: In robot exploration scenarios, the framework successfully isolates faults from sensor damage, dynamic scenes, and perceptual degradation using vision-based navigation.

Conclusion: Effective Information-driven counterfactual reasoning enhances perception system fault detection and isolation without relying on physical redundancy.

Abstract: Perception systems provide a rich understanding of the environment for
autonomous systems, shaping decisions in all downstream modules. Hence,
accurate detection and isolation of faults in perception systems is important.
Faults in perception systems pose particular challenges: faults are often tied
to the perceptual context of the environment, and errors in their multi-stage
pipelines can propagate across modules. To address this, we adopt a
counterfactual reasoning approach to propose a framework for fault detection
and isolation (FDI) in perception systems. As opposed to relying on physical
redundancy (i.e., having extra sensors), our approach utilizes analytical
redundancy with counterfactual reasoning to construct perception reliability
tests as causal outcomes influenced by system states and fault scenarios.
Counterfactual reasoning generates reliability test results under hypothesized
faults to update the belief over fault hypotheses. We derive both passive and
active FDI methods. While the passive FDI can be achieved by belief updates,
the active FDI approach is defined as a causal bandit problem, where we utilize
Monte Carlo Tree Search (MCTS) with upper confidence bound (UCB) to find
control inputs that maximize a detection and isolation metric, designated as
Effective Information (EI). The mentioned metric quantifies the informativeness
of control inputs for FDI. We demonstrate the approach in a robot exploration
scenario, where a space robot performing vision-based navigation actively
adjusts its attitude to increase EI and correctly isolate faults caused by
sensor damage, dynamic scenes, and perceptual degradation.

</details>


### [385] [Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task](https://arxiv.org/abs/2509.18463)
*Jannick van Buuren,Roberto Giglio,Loris Roveda,Luka Peternel*

Main category: cs.RO

TL;DR: The paper proposes using deliberate mutations in reward functions for reinforcement learning to generate diverse robotic manipulation behaviors, demonstrated with a liquid pouring task.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance diversity in robotic skill development, inspired by the human motor control tradeoff model that balances accuracy, time, and effort.

Method: A reward function mutation framework was created by introducing Gaussian noise into reward function parameters. The framework was tested in a robot simulation environment using Proximal Policy Optimization.

Result: The mutated reward functions led to diverse learned policies, including variations of the pouring task and unexpected skills like rim cleaning and liquid mixing.

Conclusion: Mutating reward functions can guide robots to learn both task-specific and transferable skills, paving the way for generalized skill development in robotics.

Abstract: This paper explores how deliberate mutations of reward function in
reinforcement learning can produce diversified skill variations in robotic
manipulation tasks, examined with a liquid pouring use case. To this end, we
developed a new reward function mutation framework that is based on applying
Gaussian noise to the weights of the different terms in the reward function.
Inspired by the cost-benefit tradeoff model from human motor control, we
designed the reward function with the following key terms: accuracy, time, and
effort. The study was performed in a simulation environment created in NVIDIA
Isaac Sim, and the setup included Franka Emika Panda robotic arm holding a
glass with a liquid that needed to be poured into a container. The
reinforcement learning algorithm was based on Proximal Policy Optimization. We
systematically explored how different configurations of mutated weights in the
rewards function would affect the learned policy. The resulting policies
exhibit a wide range of behaviours: from variations in execution of the
originally intended pouring task to novel skills useful for unexpected tasks,
such as container rim cleaning, liquid mixing, and watering. This approach
offers promising directions for robotic systems to perform diversified learning
of specific tasks, while also potentially deriving meaningful skills for future
tasks.

</details>


### [386] [RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain](https://arxiv.org/abs/2509.18466)
*Junnosuke Kamohara,Feiyang Wu,Chinmayee Wamorkar,Seth Hutchinson,Ye Zhao*

Main category: cs.RO

TL;DR: This paper presents an RL-augmented MPC framework designed for enhanced bipedal robot locomotion on rough and slippery terrain.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional MPC in challenging terrains and RL's reliance on reward shaping, aiming to combine their strengths for improved bipedal locomotion.

Method: The approach parametrizes key components of single-rigid-body-dynamics-based MPC (system dynamics, swing leg controller, and gait frequency) while leveraging RL for adaptability.

Result: Simulations in NVIDIA IsaacLab show robust bipedal robot navigation over diverse terrains, such as stairs, stepping stones, and slippery surfaces.

Conclusion: The RL-augmented MPC framework outperforms traditional MPC and RL methods, demonstrating adaptability and robustness for bipedal locomotion in challenging environments.

Abstract: Model predictive control (MPC) has demonstrated effectiveness for humanoid
bipedal locomotion; however, its applicability in challenging environments,
such as rough and slippery terrain, is limited by the difficulty of modeling
terrain interactions. In contrast, reinforcement learning (RL) has achieved
notable success in training robust locomotion policies over diverse terrain,
yet it lacks guarantees of constraint satisfaction and often requires
substantial reward shaping. Recent efforts in combining MPC and RL have shown
promise of taking the best of both worlds, but they are primarily restricted to
flat terrain or quadrupedal robots. In this work, we propose an RL-augmented
MPC framework tailored for bipedal locomotion over rough and slippery terrain.
Our method parametrizes three key components of
single-rigid-body-dynamics-based MPC: system dynamics, swing leg controller,
and gait frequency. We validate our approach through bipedal robot simulations
in NVIDIA IsaacLab across various terrains, including stairs, stepping stones,
and low-friction surfaces. Experimental results demonstrate that our
RL-augmented MPC framework produces significantly more adaptive and robust
behaviors compared to baseline MPC and RL.

</details>


### [387] [Spatial Envelope MPC: High Performance Driving without a Reference](https://arxiv.org/abs/2509.18506)
*Siyuan Yu,Congkai Shen,Yufei Xi,James Dallas,Michael Thompson,John Subosits,Hiroshi Yasuda,Tulga Ersal*

Main category: cs.RO

TL;DR: The paper proposes a new envelope-based model predictive control framework for autonomous vehicles to enable high-performance driving without predefined references.


<details>
  <summary>Details</summary>
Motivation: High-performance autonomous driving requires a framework capable of safe operation at the vehicle's dynamic limits, especially when predefined reference trajectories are ineffective or infeasible.

Method: The authors introduce a computationally efficient vehicle dynamics model, a continuously differentiable mathematical formulation for the drivable envelope, and combine reinforcement learning with optimization techniques for envelope planning.

Result: The framework was validated in simulations and real-world experiments, showcasing high-performance results in tasks like racing, emergency collision avoidance, and off-road navigation.

Conclusion: The proposed framework demonstrates scalability and wide applicability, addressing limitations in reference-based autonomous driving systems and enabling performance across diverse scenarios.

Abstract: This paper presents a novel envelope based model predictive control (MPC)
framework designed to enable autonomous vehicles to handle high performance
driving across a wide range of scenarios without a predefined reference. In
high performance autonomous driving, safe operation at the vehicle's dynamic
limits requires a real time planning and control framework capable of
accounting for key vehicle dynamics and environmental constraints when
following a predefined reference trajectory is suboptimal or even infeasible.
State of the art planning and control frameworks, however, are predominantly
reference based, which limits their performance in such situations. To address
this gap, this work first introduces a computationally efficient vehicle
dynamics model tailored for optimization based control and a continuously
differentiable mathematical formulation that accurately captures the entire
drivable envelope. This novel model and formulation allow for the direct
integration of dynamic feasibility and safety constraints into a unified
planning and control framework, thereby removing the necessity for predefined
references. The challenge of envelope planning, which refers to maximally
approximating the safe drivable area, is tackled by combining reinforcement
learning with optimization techniques. The framework is validated through both
simulations and real world experiments, demonstrating its high performance
across a variety of tasks, including racing, emergency collision avoidance and
off road navigation. These results highlight the framework's scalability and
broad applicability across a diverse set of scenarios.

</details>


### [388] [LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA](https://arxiv.org/abs/2509.18576)
*Zeyi Kang,Liang He,Yanxin Zhang,Zuheng Ming,Kaixing Zhao*

Main category: cs.RO

TL;DR: This paper introduces LCMF, a lightweight framework for efficient multimodal fusion in robotic applications, showcasing competitive accuracy and reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in multimodal semantic learning, including effective fusion of diverse data types and computational efficiency in environments with limited resources.

Method: The proposed LCMF framework utilizes a multi-level cross-modal parameter-sharing mechanism integrated into the Mamba module, leveraging Cross-Attention and Selective State Space Models to enhance fusion and alignment.

Result: LCMF achieved 74.29% accuracy in VQA tasks and competitive performance in EQA video tasks among LLM Agents. It also reduced FLOPs by 4.35 times and utilized fewer parameters compared to baseline models.

Conclusion: LCMF provides an efficient multimodal fusion and decision-making solution suited for Human-Robot Interaction in resource-constrained environments.

Abstract: Multimodal semantic learning plays a critical role in embodied intelligence,
especially when robots perceive their surroundings, understand human
instructions, and make intelligent decisions. However, the field faces
technical challenges such as effective fusion of heterogeneous data and
computational efficiency in resource-constrained environments. To address these
challenges, this study proposes the lightweight LCMF cascaded attention
framework, introducing a multi-level cross-modal parameter sharing mechanism
into the Mamba module. By integrating the advantages of Cross-Attention and
Selective parameter-sharing State Space Models (SSMs), the framework achieves
efficient fusion of heterogeneous modalities and semantic complementary
alignment. Experimental results show that LCMF surpasses existing multimodal
baselines with an accuracy of 74.29% in VQA tasks and achieves competitive
mid-tier performance within the distribution cluster of Large Language Model
Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a
4.35-fold reduction in FLOPs relative to the average of comparable baselines
while using only 166.51M parameters (image-text) and 219M parameters
(video-text), providing an efficient solution for Human-Robot Interaction (HRI)
applications in resource-constrained scenarios with strong multimodal decision
generalization capabilities.

</details>


### [389] [VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation](https://arxiv.org/abs/2509.18592)
*Neel P. Bhatt,Yunhao Yang,Rohan Siva,Pranay Samineni,Daniel Milan,Zhangyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: VLN-Zero introduces a framework improving vision-language navigation by enabling zero-shot robust decision-making through efficient exploration and symbolic reasoning.


<details>
  <summary>Details</summary>
Motivation: Current vision-language navigation methods struggle with either exhaustive exploration or rigid policies, leading to poor generalization in new environments.

Method: The framework consists of two phases: exploration using structured prompts to create symbolic scene graphs, and deployment using a neurosymbolic planner and cache-enabled execution module for efficient task adaptation.

Result: VLN-Zero achieves a 2x higher success rate, faster goal-reaching, and reduced computational demands compared to leading zero-shot models and fine-tuned baselines.

Conclusion: The framework demonstrates strong scalability, efficiency, and generalization for autonomous decision-making in unseen environments.

Abstract: Rapid adaptation in unseen environments is essential for scalable real-world
autonomy, yet existing approaches rely on exhaustive exploration or rigid
navigation policies that fail to generalize. We present VLN-Zero, a two-phase
vision-language navigation framework that leverages vision-language models to
efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic
navigation. In the exploration phase, structured prompts guide VLM-based search
toward informative and diverse trajectories, yielding compact scene graph
representations. In the deployment phase, a neurosymbolic planner reasons over
the scene graph and environmental observations to generate executable plans,
while a cache-enabled execution module accelerates adaptation by reusing
previously computed task-location trajectories. By combining rapid exploration,
symbolic reasoning, and cache-enabled execution, the proposed framework
overcomes the computational inefficiency and poor generalization of prior
vision-language navigation methods, enabling robust and scalable
decision-making in unseen environments. VLN-Zero achieves 2x higher success
rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned
baselines, and reaches goal locations in half the time with 55% fewer VLM calls
on average compared to state-of-the-art models across diverse environments.
Codebase, datasets, and videos for VLN-Zero are available at:
https://vln-zero.github.io/.

</details>


### [390] [Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills](https://arxiv.org/abs/2509.18597)
*Yuan Meng,Zhenguo Sun,Max Fest,Xukun Li,Zhenshan Bing,Alois Knoll*

Main category: cs.RO

TL;DR: This paper introduces a new human-in-the-loop framework for robotic manipulation using large language models, focusing on encoding corrections into reusable skills to improve success rates and efficiency in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of current LLM-based robotic code generation methods like noise, restricted primitives, context limitations, and challenges with long-horizon tasks.

Method: Propose a human-in-the-loop framework that incorporates external memory, skill encoding, and Retrieval-Augmented Generation with a hint mechanism.

Result: Achieved a 93% success rate (up to 27% higher than baselines) and a 42% efficiency improvement in robotic task correction rounds, demonstrating robustness in extremely long-horizon tasks.

Conclusion: The proposed approach effectively solves challenges in existing methods, enabling enhanced generalization, efficiency, and capability in long-horizon robotic tasks.

Abstract: Large language models (LLMs)-based code generation for robotic manipulation
has recently shown promise by directly translating human instructions into
executable code, but existing methods remain noisy, constrained by fixed
primitives and limited context windows, and struggle with long-horizon tasks.
While closed-loop feedback has been explored, corrected knowledge is often
stored in improper formats, restricting generalization and causing catastrophic
forgetting, which highlights the need for learning reusable skills. Moreover,
approaches that rely solely on LLM guidance frequently fail in extremely
long-horizon scenarios due to LLMs' limited reasoning capability in the robotic
domain, where such issues are often straightforward for humans to identify. To
address these challenges, we propose a human-in-the-loop framework that encodes
corrections into reusable skills, supported by external memory and
Retrieval-Augmented Generation with a hint mechanism for dynamic reuse.
Experiments on Ravens, Franka Kitchen, and MetaWorld, as well as real-world
settings, show that our framework achieves a 0.93 success rate (up to 27%
higher than baselines) and a 42% efficiency improvement in correction rounds.
It can robustly solve extremely long-horizon tasks such as "build a house",
which requires planning over 20 primitives.

</details>


### [391] [End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning](https://arxiv.org/abs/2509.18608)
*Ana Luiza Mineiro,Francisco Affonso,Marcelo Becker*

Main category: cs.RO

TL;DR: This paper presents a deep reinforcement learning-based navigation system for under-canopy agricultural environments, relying on LiDAR data for control command generation.


<details>
  <summary>Details</summary>
Motivation: Navigation in under-canopy agricultural environments faces challenges from GNSS unreliability, cluttered rows, and lighting variability.

Method: The approach involves training a deep reinforcement learning policy entirely in simulation, incorporating a voxel-based downsampling technique to reduce the LiDAR input size by 95.83%, and bypassing labeled datasets or manual control interfaces.

Result: The model achieved a 100% success rate in straight-row plantation simulations, with performance declining as row curvature increased (tested on sinusoidal trajectories with varying frequencies and amplitudes).

Conclusion: The system demonstrates promise for efficient and reliable navigation in structured agricultural settings, leveraging LiDAR data for end-to-end policy learning.

Abstract: Reliable navigation in under-canopy agricultural environments remains a
challenge due to GNSS unreliability, cluttered rows, and variable lighting. To
address these limitations, we present an end-to-end learning-based navigation
system that maps raw 3D LiDAR data directly to control commands using a deep
reinforcement learning policy trained entirely in simulation. Our method
includes a voxel-based downsampling strategy that reduces LiDAR input size by
95.83%, enabling efficient policy learning without relying on labeled datasets
or manually designed control interfaces. The policy was validated in
simulation, achieving a 100% success rate in straight-row plantations and
showing a gradual decline in performance as row curvature increased, tested
across varying sinusoidal frequencies and amplitudes.

</details>


### [392] [PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving](https://arxiv.org/abs/2509.18609)
*Chengran Yuan,Zijian Lu,Zhanqi Zhang,Yimin Zhao,Zefan Huang,Shuo Sun,Jiawei Sun,Jiahui Li,Christina Dao Wen Lee,Dongen Li,Marcelo H. Ang Jr*

Main category: cs.RO

TL;DR: The paper presents the PIE framework for end-to-end motion planning in autonomous driving, improving perception, reasoning, and intention modeling to enhance scene interaction and trajectory inference.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address challenges in end-to-end autonomous driving systems, particularly the need for better scene understanding, effective prediction, and decision-making.

Method: PIE combines a bidirectional Mamba fusion for multimodal data handling, a reasoning-enhanced decoder for trajectory optimization, and an action-motion interaction module to leverage predictions of surrounding agents.

Result: In tests on the NAVSIM benchmark, PIE achieved an 88.9 PDM score and 85.6 EPDM score, outperforming previous state-of-the-art methods without using data augmentation or ensembles.

Conclusion: The results validate PIE's ability to generate feasible and high-quality ego trajectories, highlighting its potential for real-world autonomous driving applications.

Abstract: End-to-end motion planning is promising for simplifying complex autonomous
driving pipelines. However, challenges such as scene understanding and
effective prediction for decision-making continue to present substantial
obstacles to its large-scale deployment. In this paper, we present PIE, a
pioneering framework that integrates advanced perception, reasoning, and
intention modeling to dynamically capture interactions between the ego vehicle
and surrounding agents. It incorporates a bidirectional Mamba fusion that
addresses data compression losses in multimodal fusion of camera and LiDAR
inputs, alongside a novel reasoning-enhanced decoder integrating Mamba and
Mixture-of-Experts to facilitate scene-compliant anchor selection and optimize
adaptive trajectory inference. PIE adopts an action-motion interaction module
to effectively utilize state predictions of surrounding agents to refine ego
planning. The proposed framework is thoroughly validated on the NAVSIM
benchmark. PIE, without using any ensemble and data augmentation techniques,
achieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of
prior state-of-the-art methods. Comprehensive quantitative and qualitative
analyses demonstrate that PIE is capable of reliably generating feasible and
high-quality ego trajectories.

</details>


### [393] [SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones](https://arxiv.org/abs/2509.18610)
*Maximilian Adang,JunEn Low,Ola Shorinwa,Mac Schwager*

Main category: cs.RO

TL;DR: The paper introduces SINGER, a system for language-guided autonomous drone navigation using onboard sensing and computing. It employs a photorealistic simulator, trajectory-generation expert, and a lightweight visuomotor policy for closed-loop control, achieving better navigation performance and fewer collisions in real-world tests.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of open-vocabulary autonomous drone navigation, which is hindered by limited demonstrations, high real-time control demands, and unreliable external pose estimation.

Method: SINGER uses a photorealistic simulator for data generation, a multi-trajectory generation expert for creating collision-free navigation demonstrations, and a lightweight visuomotor policy for real-time drone control.

Result: SINGER demonstrates superior zero-shot sim-to-real transfer in navigating unseen environments and identifying unseen language-conditioned goal objects, outperforming existing baselines with better accuracy and fewer collisions.

Conclusion: SINGER successfully advances open-vocabulary drone navigation by using onboard sensing and compute, highlighting its robustness and potential for practical applications.

Abstract: Large vision-language models have driven remarkable progress in
open-vocabulary robot policies, e.g., generalist robot manipulation policies,
that enable robots to complete complex tasks specified in natural language.
Despite these successes, open-vocabulary autonomous drone navigation remains an
unsolved challenge due to the scarcity of large-scale demonstrations, real-time
control demands of drones for stabilization, and lack of reliable external pose
estimation modules. In this work, we present SINGER for language-guided
autonomous drone navigation in the open world using only onboard sensing and
compute. To train robust, open-vocabulary navigation policies, SINGER leverages
three central components: (i) a photorealistic language-embedded flight
simulator with minimal sim-to-real gap using Gaussian Splatting for efficient
data generation, (ii) an RRT-inspired multi-trajectory generation expert for
collision-free navigation demonstrations, and these are used to train (iii) a
lightweight end-to-end visuomotor policy for real-time closed-loop control.
Through extensive hardware flight experiments, we demonstrate superior
zero-shot sim-to-real transfer of our policy to unseen environments and unseen
language-conditioned goal objects. When trained on ~700k-1M observation action
pairs of language conditioned visuomotor data and deployed on hardware, SINGER
outperforms a velocity-controlled semantic guidance baseline by reaching the
query 23.33% more on average, and maintains the query in the field of view
16.67% more on average, with 10% fewer collisions.

</details>


### [394] [The Case for Negative Data: From Crash Reports to Counterfactuals for Reasonable Driving](https://arxiv.org/abs/2509.18626)
*Jay Patrikar,Apoorva Sharma,Sushant Veer,Boyi Li,Sebastian Scherer,Marco Pavone*

Main category: cs.RO

TL;DR: The paper develops a system to leverage real crash narrative data for enhancing autonomous driving decisions using a unified representation and precedent retrieval.


<details>
  <summary>Details</summary>
Motivation: Learning-based autonomous driving systems lack sufficient guidance near safety-performance boundaries due to training on incident-free data.

Method: The authors normalize crash narratives into ego-centric language and unify them with logs into a scene-action representation for retrieval. An additional counterfactual extension evaluates plausible alternatives and outcomes.

Result: Precedent retrieval improved recall of contextually preferred actions from 24% to 53% on the nuScenes benchmark. The counterfactual variant further sharpens near-risk decision-making.

Conclusion: Integrating crash-based retrieval and counterfactual reasoning enhances autonomous driving systems' calibration and decision-making near critical safety thresholds.

Abstract: Learning-based autonomous driving systems are trained mostly on incident-free
data, offering little guidance near safety-performance boundaries. Real crash
reports contain precisely the contrastive evidence needed, but they are hard to
use: narratives are unstructured, third-person, and poorly grounded to sensor
views. We address these challenges by normalizing crash narratives to
ego-centric language and converting both logs and crashes into a unified
scene-action representation suitable for retrieval. At decision time, our
system adjudicates proposed actions by retrieving relevant precedents from this
unified index; an agentic counterfactual extension proposes plausible
alternatives, retrieves for each, and reasons across outcomes before deciding.
On a nuScenes benchmark, precedent retrieval substantially improves
calibration, with recall on contextually preferred actions rising from 24% to
53%. The counterfactual variant preserves these gains while sharpening
decisions near risk.

</details>


### [395] [Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training](https://arxiv.org/abs/2509.18631)
*Shuo Cheng,Liqian Ma,Zhenyang Chen,Ajay Mandlekar,Caelan Garrett,Danfei Xu*

Main category: cs.RO

TL;DR: The paper introduces a co-training framework leveraging simulation with minimal real-world demonstrations using domain-invariant features and an Optimal Transport-inspired strategy.


<details>
  <summary>Details</summary>
Motivation: Real-world demonstrations for robot manipulation are costly, and scalability issues highlight simulation as an alternative. However, transferring simulated policies to the real world faces domain gap challenges.

Method: A co-training framework was proposed to learn domain-invariant, task-relevant feature spaces using Optimal Transport and Unbalanced OT-inspired losses, aligning distributions of observations and actions across domains.

Result: The method achieved up to a 30% improvement in real-world success rates and showed generalization to scenarios only encountered in simulation.

Conclusion: Simulation is effectively leveraged to enhance real-world robotic manipulation policies and reduce dependency on extensive real-world demonstrations, addressing simulation-to-reality gaps.

Abstract: Behavior cloning has shown promise for robot manipulation, but real-world
demonstrations are costly to acquire at scale. While simulated data offers a
scalable alternative, particularly with advances in automated demonstration
generation, transferring policies to the real world is hampered by various
simulation and real domain gaps. In this work, we propose a unified
sim-and-real co-training framework for learning generalizable manipulation
policies that primarily leverages simulation and only requires a few real-world
demonstrations. Central to our approach is learning a domain-invariant,
task-relevant feature space. Our key insight is that aligning the joint
distributions of observations and their corresponding actions across domains
provides a richer signal than aligning observations (marginals) alone. We
achieve this by embedding an Optimal Transport (OT)-inspired loss within the
co-training framework, and extend this to an Unbalanced OT framework to handle
the imbalance between abundant simulation data and limited real-world examples.
We validate our method on challenging manipulation tasks, showing it can
leverage abundant simulation data to achieve up to a 30% improvement in the
real-world success rate and even generalize to scenarios seen only in
simulation.

</details>


### [396] [Number Adaptive Formation Flight Planning via Affine Deformable Guidance in Narrow Environments](https://arxiv.org/abs/2509.18636)
*Yuan Zhou,Jialiang Hou,Guangtong Xu,Fei Gao*

Main category: cs.RO

TL;DR: The paper introduces a formation planning method for drone swarms in narrow environments using Deformable Virtual Structures (DVS), ensuring adaptivity to changing drone numbers and obstacles.


<details>
  <summary>Details</summary>
Motivation: Ensuring effective formation maintenance for drone swarms in narrow environments is challenging when the number of drones varies.

Method: The proposed method uses DVS with Lloyd and Hungarian algorithms for spatial partitioning and assignment. Trajectories are planned incorporating nonlinear optimization and are adaptable to environmental constraints. Distributed trajectory planning is also integrated for collision avoidance and feasibility.

Result: Simulations show the method enables up to 15% of drones to join or leave while maintaining formation, with faster recovery compared to existing methods. Real-world experiments demonstrate effectiveness and resilience.

Conclusion: The proposed approach enhances formation stability, adaptability, and rapid recovery, making it effective for drone swarm navigation in constrained environments.

Abstract: Formation maintenance with varying number of drones in narrow environments
hinders the convergence of planning to the desired configurations. To address
this challenge, this paper proposes a formation planning method guided by
Deformable Virtual Structures (DVS) with continuous spatiotemporal
transformation. Firstly, to satisfy swarm safety distance and preserve
formation shape filling integrity for irregular formation geometries, we employ
Lloyd algorithm for uniform $\underline{PA}$rtitioning and Hungarian algorithm
for $\underline{AS}$signment (PAAS) in DVS. Subsequently, a spatiotemporal
trajectory involving DVS is planned using primitive-based path search and
nonlinear trajectory optimization. The DVS trajectory achieves adaptive
transitions with respect to a varying number of drones while ensuring
adaptability to narrow environments through affine transformation. Finally,
each agent conducts distributed trajectory planning guided by desired
spatiotemporal positions within the DVS, while incorporating collision
avoidance and dynamic feasibility requirements. Our method enables up to 15\%
of swarm numbers to join or leave in cluttered environments while rapidly
restoring the desired formation shape in simulation. Compared to cutting-edge
formation planning method, we demonstrate rapid formation recovery capacity and
environmental adaptability. Real-world experiments validate the effectiveness
and resilience of our formation planning method.

</details>


### [397] [Do You Need Proprioceptive States in Visuomotor Policies?](https://arxiv.org/abs/2509.18644)
*Juntu Zhao,Wenbo Lu,Di Zhang,Yufeng Liu,Yushen Liang,Tianluo Zhang,Yifeng Cao,Junyuan Xie,Yingdong Hu,Shengjie Wang,Junliang Guo,Dequan Wang,Yang Gao*

Main category: cs.RO

TL;DR: The paper proposes and evaluates a State-free Policy for visuomotor control, which outperforms traditional state-based policies in spatial generalization, data efficiency, and cross-embodiment adaptation.


<details>
  <summary>Details</summary>
Motivation: To address the issue with traditional imitation-learning-based visuomotor policies that overfit to proprioceptive state inputs, leading to poor spatial generalization.

Method: The proposed State-free Policy excludes proprioceptive state inputs, instead using visual observations from dual wide-angle wrist cameras. It operates in the relative end-effector action space.

Result: The State-free Policy boosts success rates in spatial generalization for tasks like pick-and-place and shirt-folding, improving from 0% to 85% in height and 6% to 64% in horizontal generalization.

Conclusion: State-free Policies offer superior spatial generalization, data efficiency, and cross-embodiment adaptability, illustrating great promise for real-world robotic manipulation applications.

Abstract: Imitation-learning-based visuomotor policies have been widely used in robot
manipulation, where both visual observations and proprioceptive states are
typically adopted together for precise control. However, in this study, we find
that this common practice makes the policy overly reliant on the proprioceptive
state input, which causes overfitting to the training trajectories and results
in poor spatial generalization. On the contrary, we propose the State-free
Policy, removing the proprioceptive state input and predicting actions only
conditioned on visual observations. The State-free Policy is built in the
relative end-effector action space, and should ensure the full task-relevant
visual observations, here provided by dual wide-angle wrist cameras. Empirical
results demonstrate that the State-free policy achieves significantly stronger
spatial generalization than the state-based policy: in real-world tasks such as
pick-and-place, challenging shirt-folding, and complex whole-body manipulation,
spanning multiple robot embodiments, the average success rate improves from 0\%
to 85\% in height generalization and from 6\% to 64\% in horizontal
generalization. Furthermore, they also show advantages in data efficiency and
cross-embodiment adaptation, enhancing their practicality for real-world
deployment.

</details>


### [398] [SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer](https://arxiv.org/abs/2509.18648)
*Yarden As,Chengrui Qu,Benjamin Unger,Dongho Kang,Max van der Hart,Laixi Shi,Stelian Coros,Adam Wierman,Andreas Krause*

Main category: cs.RO

TL;DR: The paper introduces SPiDR, an algorithm designed to improve safety in sim-to-real transfer for RL by integrating domain randomization into safety constraints, demonstrated to maintain performance and ensure safety across benchmarks and platforms.


<details>
  <summary>Details</summary>
Motivation: Deploying RL in the real world often faces safety concerns due to discrepancies (sim-to-real gap) between simulations and real-world conditions. Existing robust safe RL methods are not always scalable or compatible with standard pipelines, whereas domain randomization is simplistic but often yields unsafe results.

Method: SPiDR leverages pessimistic domain randomization to address sim-to-real gaps by integrating uncertainty into safety constraints. This approach is designed to be scalable, versatile, and compatible with modern RL training pipelines.

Result: SPiDR is validated through extensive experiments on simulated benchmarks and two real-world robotic systems, demonstrating improved safety in sim-to-real transitions without sacrificing performance.

Conclusion: The study proposes SPiDR as an effective and scalable solution for safe sim-to-real transfer, combining the simplicity of domain randomization with provable safety guarantees, making it both practical and reliable for real-world RL applications.

Abstract: Safety remains a major concern for deploying reinforcement learning (RL) in
real-world applications. Simulators provide safe, scalable training
environments, but the inevitable sim-to-real gap introduces additional safety
concerns, as policies must satisfy constraints in real-world conditions that
differ from simulation. To address this challenge, robust safe RL techniques
offer principled methods, but are often incompatible with standard scalable
training pipelines. In contrast, domain randomization, a simple and popular
sim-to-real technique, stands out as a promising alternative, although it often
results in unsafe behaviors in practice. We present SPiDR, short for
Sim-to-real via Pessimistic Domain Randomization -- a scalable algorithm with
provable guarantees for safe sim-to-real transfer. SPiDR uses domain
randomization to incorporate the uncertainty about the sim-to-real gap into the
safety constraints, making it versatile and highly compatible with existing
training pipelines. Through extensive experiments on sim-to-sim benchmarks and
two distinct real-world robotic platforms, we demonstrate that SPiDR
effectively ensures safety despite the sim-to-real gap while maintaining strong
performance.

</details>


### [399] [Distributionally Robust Safe Motion Planning with Contextual Information](https://arxiv.org/abs/2509.18666)
*Kaizer Rahaman,Simran Kumari,Ashish R. Hota*

Main category: cs.RO

TL;DR: The paper proposes a robust method for collision avoidance using contextual information and distributional robustness, leveraging past data to predict trajectory distributions in motion planning.


<details>
  <summary>Details</summary>
Motivation: To improve collision avoidance systems by incorporating contextual information and addressing uncertainty in trajectory predictions.

Method: The approach embeds the conditional distribution of obstacle trajectories into a reproducing kernel Hilbert space using past data, defines an ambiguity set for uncertain distributions, and integrates these into the motion planning of the ego agent.

Result: Simulation results demonstrate that this method outperforms traditional approaches in collision avoidance, especially in challenging scenarios.

Conclusion: Incorporating contextual information and distributional robustness enhances collision avoidance performance and improves system reliability.

Abstract: We present a distributionally robust approach for collision avoidance by
incorporating contextual information. Specifically, we embed the conditional
distribution of future trajectory of the obstacle conditioned on the motion of
the ego agent in a reproducing kernel Hilbert space (RKHS) via the conditional
kernel mean embedding operator. Then, we define an ambiguity set containing all
distributions whose embedding in the RKHS is within a certain distance from the
empirical estimate of conditional mean embedding learnt from past data.
Consequently, a distributionally robust collision avoidance constraint is
formulated, and included in the receding horizon based motion planning
formulation of the ego agent. Simulation results show that the proposed
approach is more successful in avoiding collision compared to approaches that
do not include contextual information and/or distributional robustness in their
formulation in several challenging scenarios.

</details>


### [400] [N2M: Bridging Navigation and Manipulation by Learning Pose Preference from Rollout](https://arxiv.org/abs/2509.18671)
*Kaixin Chai,Hyunjun Lee,Joseph J. Lim*

Main category: cs.RO

TL;DR: The paper introduces a module called N2M to improve success rates in mobile manipulation tasks by guiding robots to favorable initial poses using ego-centric observations.


<details>
  <summary>Details</summary>
Motivation: Current navigation modules in mobile manipulation often neglect the importance of choosing favorable initial poses for downstream tasks, leading to reduced success rates.

Method: N2M is a transition module that uses ego-centric observations for real-time pose adaptation. It improves success rates by predicting optimal initial poses without relying on global data.

Result: In simulations and real-world experiments, N2M raised the success rate from 3% to 54% in one task and demonstrated high efficiency and generalizability even with minimal data in another.

Conclusion: N2M effectively tackles the misalignment between navigation and manipulation policies, showing substantial improvement in task success rates and adaptability across environments and hardware.

Abstract: In mobile manipulation, the manipulation policy has strong preferences for
initial poses where it is executed. However, the navigation module focuses
solely on reaching the task area, without considering which initial pose is
preferable for downstream manipulation. To address this misalignment, we
introduce N2M, a transition module that guides the robot to a preferable
initial pose after reaching the task area, thereby substantially improving task
success rates. N2M features five key advantages: (1) reliance solely on
ego-centric observation without requiring global or historical information; (2)
real-time adaptation to environmental changes; (3) reliable prediction with
high viewpoint robustness; (4) broad applicability across diverse tasks,
manipulation policies, and robot hardware; and (5) remarkable data efficiency
and generalizability. We demonstrate the effectiveness of N2M through extensive
simulation and real-world experiments. In the PnPCounterToCab task, N2M
improves the averaged success rate from 3% with the reachability-based baseline
to 54%. Furthermore, in the Toybox Handover task, N2M provides reliable
predictions even in unseen environments with only 15 data samples, showing
remarkable data efficiency and generalizability.

</details>


### [401] [3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space](https://arxiv.org/abs/2509.18676)
*Sangjun Noh,Dongwoo Nam,Kangmin Kim,Geonhyup Lee,Yeonguk Yu,Raeyoung Kang,Kyoobin Lee*

Main category: cs.RO

TL;DR: The paper introduces 3D Flow Diffusion Policy (3D FDP), a framework utilizing 3D flow for learning visuomotor policies to improve robot manipulation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of developing robotic manipulation policies that generalize across diverse objects and dynamic interactions, where traditional approaches often fail to consider localized motion cues critical for precise interactions.

Method: The proposed 3D FDP leverages 3D flow as an intermediate representation, predicts temporal trajectories for sampled points, and incorporates these flows into action generation using a unified diffusion architecture for scene-level reasoning.

Result: 3D FDP achieves state-of-the-art performance on the MetaWorld benchmark across 50 tasks, excelling in medium and hard settings, and outperforms baselines in real-robot tasks, especially in contact-rich and non-prehensile scenarios.

Conclusion: The study demonstrates that using 3D flow as a structural prior enables learning more generalizable and robust visuomotor policies for robotic manipulation, promoting progress in versatile and precise manipulation tasks.

Abstract: Learning robust visuomotor policies that generalize across diverse objects
and interaction dynamics remains a central challenge in robotic manipulation.
Most existing approaches rely on direct observation-to-action mappings or
compress perceptual inputs into global or object-centric features, which often
overlook localized motion cues critical for precise and contact-rich
manipulation. We present 3D Flow Diffusion Policy (3D FDP), a novel framework
that leverages scene-level 3D flow as a structured intermediate representation
to capture fine-grained local motion cues. Our approach predicts the temporal
trajectories of sampled query points and conditions action generation on these
interaction-aware flows, implemented jointly within a unified diffusion
architecture. This design grounds manipulation in localized dynamics while
enabling the policy to reason about broader scene-level consequences of
actions. Extensive experiments on the MetaWorld benchmark show that 3D FDP
achieves state-of-the-art performance across 50 tasks, particularly excelling
on medium and hard settings. Beyond simulation, we validate our method on eight
real-robot tasks, where it consistently outperforms prior baselines in
contact-rich and non-prehensile scenarios. These results highlight 3D flow as a
powerful structural prior for learning generalizable visuomotor policies,
supporting the development of more robust and versatile robotic manipulation.
Robot demonstrations, additional results, and code can be found at
https://sites.google.com/view/3dfdp/home.

</details>


### [402] [Query-Centric Diffusion Policy for Generalizable Robotic Assembly](https://arxiv.org/abs/2509.18686)
*Ziyi Xu,Haohong Lin,Shiqi Liu,Ding Zhao*

Main category: cs.RO

TL;DR: The paper proposes a robotic assembly framework, Query-centric Diffusion Policy (QDP), which bridges high-level planning and low-level control using structured queries and point cloud observations.


<details>
  <summary>Details</summary>
Motivation: Address the mismatch in hierarchical robotic assembly systems between high-level skill queries and low-level policy execution.

Method: Introduce structured queries (objects, contact points, skill info) and use a query-centric mechanism leveraging point cloud observations for hierarchical policy design.

Result: QDP demonstrates over 50% better success rates in skill precision and challenging long-horizon tasks in both simulation and real-world experiments.

Conclusion: QDP effectively enhances robustness and task success in robotic assembly by bridging planning and control with structured queries, offering significant performance improvements.

Abstract: The robotic assembly task poses a key challenge in building generalist robots
due to the intrinsic complexity of part interactions and the sensitivity to
noise perturbations in contact-rich settings. The assembly agent is typically
designed in a hierarchical manner: high-level multi-part reasoning and
low-level precise control. However, implementing such a hierarchical policy is
challenging in practice due to the mismatch between high-level skill queries
and low-level execution. To address this, we propose the Query-centric
Diffusion Policy (QDP), a hierarchical framework that bridges high-level
planning and low-level control by utilizing queries comprising objects, contact
points, and skill information. QDP introduces a query-centric mechanism that
identifies task-relevant components and uses them to guide low-level policies,
leveraging point cloud observations to improve the policy's robustness. We
conduct comprehensive experiments on the FurnitureBench in both simulation and
real-world settings, demonstrating improved performance in skill precision and
long-horizon success rate. In the challenging insertion and screwing tasks, QDP
improves the skill-wise success rate by over 50% compared to baselines without
structured queries.

</details>


### [403] [Learning Obstacle Avoidance using Double DQN for Quadcopter Navigation](https://arxiv.org/abs/2509.18734)
*Nishant Doshi,Amey Sutvani,Sanket Gujar*

Main category: cs.RO

TL;DR: The paper addresses autonomous aerial navigation in urban environments using reinforcement learning with depth sensor-equipped quadcopters.


<details>
  <summary>Details</summary>
Motivation: Urban navigation for aerial robots faces challenges like reduced GPS accuracy, narrow spaces, and moving obstacles, necessitating adaptive collision avoidance.

Method: Using Reinforcement Learning, the paper trains a virtual quadcopter equipped with a depth camera to navigate a simulated urban setting.

Result: The approach enables the quadcopter to effectively utilize depth sensor data for collision-free navigation in a simulated environment.

Conclusion: Reinforcement Learning and depth sensing can support reliable navigation for autonomous drones in challenging urban landscapes.

Abstract: One of the challenges faced by Autonomous Aerial Vehicles is reliable
navigation through urban environments. Factors like reduction in precision of
Global Positioning System (GPS), narrow spaces and dynamically moving obstacles
make the path planning of an aerial robot a complicated task. One of the skills
required for the agent to effectively navigate through such an environment is
to develop an ability to avoid collisions using information from onboard depth
sensors. In this paper, we propose Reinforcement Learning of a virtual
quadcopter robot agent equipped with a Depth Camera to navigate through a
simulated urban environment.

</details>


### [404] [MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning](https://arxiv.org/abs/2509.18757)
*Omar Rayyan,John Abanes,Mahmoud Hafez,Anthony Tzes,Fares Abu-Dakka*

Main category: cs.RO

TL;DR: The paper introduces MV-UMI, a framework combining egocentric and third-person views to improve robot manipulation learning from handheld grippers.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of using solely wrist-mounted egocentric cameras for robot data collection with handheld grippers.

Method: Integrates third-person perspective with egocentric camera views using the MV-UMI framework for improved scene understanding and reduced domain shifts between demonstration and deployment.

Result: Experimental evaluations show a performance improvement of approximately 47% in tasks requiring broad scene understanding, achieved without sacrificing cross-embodiment advantages.

Conclusion: MV-UMI enhances the range of manipulation tasks learnable through handheld gripper systems, confirming it as a scalable and effective solution for data collection.

Abstract: Recent advances in imitation learning have shown great promise for developing
robust robot manipulation policies from demonstrations. However, this promise
is contingent on the availability of diverse, high-quality datasets, which are
not only challenging and costly to collect but are often constrained to a
specific robot embodiment. Portable handheld grippers have recently emerged as
intuitive and scalable alternatives to traditional robotic teleoperation
methods for data collection. However, their reliance solely on first-person
view wrist-mounted cameras often creates limitations in capturing sufficient
scene contexts. In this paper, we present MV-UMI (Multi-View Universal
Manipulation Interface), a framework that integrates a third-person perspective
with the egocentric camera to overcome this limitation. This integration
mitigates domain shifts between human demonstration and robot deployment,
preserving the cross-embodiment advantages of handheld data-collection devices.
Our experimental results, including an ablation study, demonstrate that our
MV-UMI framework improves performance in sub-tasks requiring broad scene
understanding by approximately 47% across 3 tasks, confirming the effectiveness
of our approach in expanding the range of feasible manipulation tasks that can
be learned using handheld gripper systems, without compromising the
cross-embodiment advantages inherent to such systems.

</details>


### [405] [VGGT-DP: Generalizable Robot Control via Vision Foundation Models](https://arxiv.org/abs/2509.18778)
*Shijia Ge,Yinxin Zhang,Shuzhao Xie,Weixiang Zhang,Mingcai Zhou,Zhi Wang*

Main category: cs.RO

TL;DR: The paper introduces VGGT-DP, an imitation learning framework that enhances robotic manipulation by integrating geometric visual priors and proprioceptive feedback.


<details>
  <summary>Details</summary>
Motivation: Existing visual imitation learning frameworks struggle with spatial understanding and generalization due to limitations in visual encoder design.

Method: The authors propose VGGT-DP, which uses a Visual Geometry Grounded Transformer (VGGT) as the visual encoder and incorporates proprioceptive feedback. Techniques like frame-wise token reuse and random token pruning are introduced for efficiency and robustness.

Result: VGGT-DP outperforms strong baselines like DP and DP3 in precision-heavy and long-horizon tasks, as demonstrated on the MetaWorld platform.

Conclusion: By combining biologically inspired multi-modal inputs with efficient design improvements, VGGT-DP enhances spatial grounding and control in robotic tasks.

Abstract: Visual imitation learning frameworks allow robots to learn manipulation
skills from expert demonstrations. While existing approaches mainly focus on
policy design, they often neglect the structure and capacity of visual
encoders, limiting spatial understanding and generalization. Inspired by
biological vision systems, which rely on both visual and proprioceptive cues
for robust control, we propose VGGT-DP, a visuomotor policy framework that
integrates geometric priors from a pretrained 3D perception model with
proprioceptive feedback. We adopt the Visual Geometry Grounded Transformer
(VGGT) as the visual encoder and introduce a proprioception-guided visual
learning strategy to align perception with internal robot states, improving
spatial grounding and closed-loop control. To reduce inference latency, we
design a frame-wise token reuse mechanism that compacts multi-view tokens into
an efficient spatial representation. We further apply random token pruning to
enhance policy robustness and reduce overfitting. Experiments on challenging
MetaWorld tasks show that VGGT-DP significantly outperforms strong baselines
such as DP and DP3, particularly in precision-critical and long-horizon
scenarios.

</details>


### [406] [Human-Interpretable Uncertainty Explanations for Point Cloud Registration](https://arxiv.org/abs/2509.18786)
*Johannes A. Gaus,Loris Schneider,Yitian Shi,Jongseok Lee,Rania Rayyes,Rudolph Triebel*

Main category: cs.RO

TL;DR: The paper introduces GP-CA, a method for quantifying and explaining uncertainty in point cloud registration under challenging conditions, outperforming alternatives in accuracy, runtime, and robustness.


<details>
  <summary>Details</summary>
Motivation: Point cloud registration is challenging due to sensor noise, pose estimation errors, and partial overlaps caused by occlusion. Existing methods like ICP struggle in such uncertain conditions.

Method: The paper proposes Gaussian Process Concept Attribution (GP-CA), a novel model that quantifies registration uncertainty and attributes it to known error sources. It employs active learning to discover new error sources in real-world conditions.

Result: GP-CA was validated across three public datasets and a real-world robot experiment, demonstrating superior runtime, sample efficiency, and accuracy compared to state-of-the-art methods.

Conclusion: GP-CA enhances robotic perception by enabling robust and adaptive behaviors under uncertainty, outperforming existing techniques and showcasing its applicability in real-world contexts.

Abstract: In this paper, we address the point cloud registration problem, where
well-known methods like ICP fail under uncertainty arising from sensor noise,
pose-estimation errors, and partial overlap due to occlusion. We develop a
novel approach, Gaussian Process Concept Attribution (GP-CA), which not only
quantifies registration uncertainty but also explains it by attributing
uncertainty to well-known sources of errors in registration problems. Our
approach leverages active learning to discover new uncertainty sources in the
wild by querying informative instances. We validate GP-CA on three publicly
available datasets and in our real-world robot experiment. Extensive ablations
substantiate our design choices. Our approach outperforms other
state-of-the-art methods in terms of runtime, high sample-efficiency with
active learning, and high accuracy. Our real-world experiment clearly
demonstrates its applicability. Our video also demonstrates that GP-CA enables
effective failure-recovery behaviors, yielding more robust robotic perception.

</details>


### [407] [DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation](https://arxiv.org/abs/2509.18830)
*Suzannah Wistreich,Baiyu Shi,Stephen Tian,Samuel Clarke,Michael Nath,Chengyi Xu,Zhenan Bao,Jiajun Wu*

Main category: cs.RO

TL;DR: DexSkin is a soft capacitive skin enabling sensitive, localized tactile sensing for robotic manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Replicating human-like tactile sensing capabilities in robots for dexterous manipulation.

Method: A capacitive electronic skin integrated into robotic grippers to support data-driven learning in manipulation tasks.

Result: DexSkin successfully enables challenging manipulation tasks and model transferability across sensor instances.

Conclusion: DexSkin is practical and effective for real-world tactile sensing in robotic systems.

Abstract: Human skin provides a rich tactile sensing stream, localizing intentional and
unintentional contact events over a large and contoured region. Replicating
these tactile sensing capabilities for dexterous robotic manipulation systems
remains a longstanding challenge. In this work, we take a step towards this
goal by introducing DexSkin. DexSkin is a soft, conformable capacitive
electronic skin that enables sensitive, localized, and calibratable tactile
sensing, and can be tailored to varying geometries. We demonstrate its efficacy
for learning downstream robotic manipulation by sensorizing a pair of parallel
jaw gripper fingers, providing tactile coverage across almost the entire finger
surfaces. We empirically evaluate DexSkin's capabilities in learning
challenging manipulation tasks that require sensing coverage across the entire
surface of the fingers, such as reorienting objects in hand and wrapping
elastic bands around boxes, in a learning-from-demonstration framework. We then
show that, critically for data-driven approaches, DexSkin can be calibrated to
enable model transfer across sensor instances, and demonstrate its
applicability to online reinforcement learning on real robots. Our results
highlight DexSkin's suitability and practicality for learning real-world,
contact-rich manipulation. Please see our project webpage for videos and
visualizations: https://dex-skin.github.io/.

</details>


### [408] [Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation](https://arxiv.org/abs/2509.18865)
*Masato Kobayashi,Thanpimon Buamanee*

Main category: cs.RO

TL;DR: Bi-VLA is a new framework combining bilateral control-based imitation learning with vision-language fusion to enable handling multiple tasks within a single model, improving versatility and task success rates.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of conventional bilateral control methods, which require task-specific models and are unable to generalize across multiple tasks.

Method: Bi-VLA combines robot joint data (angle, velocity, torque) obtained from bilateral control with vision features and natural language instructions through SigLIP and FiLM-based fusion. It addresses multiple tasks using a unified model.

Result: Real-robot experiments demonstrated that Bi-VLA enhances task success rates compared to conventional methods by successfully interpreting combined vision-language cues.

Conclusion: Bi-VLA provides a versatile framework that addresses the single-task limitation of bilateral control approaches and demonstrates the efficacy of combining vision and language in improving task performance.

Abstract: We propose Bilateral Control-Based Imitation Learning via Vision-Language
Fusion for Action Generation (Bi-VLA), a novel framework that extends bilateral
control-based imitation learning to handle more than one task within a single
model. Conventional bilateral control methods exploit joint angle, velocity,
torque, and vision for precise manipulation but require task-specific models,
limiting their generality. Bi-VLA overcomes this limitation by utilizing robot
joint angle, velocity, and torque data from leader-follower bilateral control
with visual features and natural language instructions through SigLIP and
FiLM-based fusion. We validated Bi-VLA on two task types: one requiring
supplementary language cues and another distinguishable solely by vision.
Real-robot experiments showed that Bi-VLA successfully interprets
vision-language combinations and improves task success rates compared to
conventional bilateral control-based imitation learning. Our Bi-VLA addresses
the single-task limitation of prior bilateral approaches and provides empirical
evidence that combining vision and language significantly enhances versatility.
Experimental results validate the effectiveness of Bi-VLA in real-world tasks.
For additional material, please visit the website:
https://mertcookimg.github.io/bi-vla/

</details>


### [409] [Lang2Morph: Language-Driven Morphological Design of Robotic Hands](https://arxiv.org/abs/2509.18937)
*Yanyuan Qiao,Kieran Gilday,Yutong Xie,Josie Hughes*

Main category: cs.RO

TL;DR: Lang2Morph is introduced as a pipeline leveraging large language models (LLMs) for robotic hand design, translating task descriptions into practical hand parameters for 3D printing.


<details>
  <summary>Details</summary>
Motivation: The current methodologies for designing robotic hands are limited by expert heuristics, manual tuning, and resource-heavy optimization methods that often do not target dexterous hands. Large language models provide a new opportunity to automate design reasoning more effectively.

Method: Lang2Morph utilizes LLMs to map task descriptions into symbolic structures and OPH-compatible parameters. The pipeline consists of two stages: (i) Morphology Design, converting tasks into semantic tags and structural parameters; and (ii) Selection and Refinement, which evaluates and optimizes design candidates for specific tasks.

Result: Lang2Morph is evaluated across a variety of tasks, demonstrating the ability to generate diverse and task-specific robotic hand morphologies effectively.

Conclusion: This framework represents the first use of LLMs for task-conditioned robotic hand design, highlighting its potential for automating and optimizing task-relevant morphologies.

Abstract: Designing robotic hand morphologies for diverse manipulation tasks requires
balancing dexterity, manufacturability, and task-specific functionality. While
open-source frameworks and parametric tools support reproducible design, they
still rely on expert heuristics and manual tuning. Automated methods using
optimization are often compute-intensive, simulation-dependent, and rarely
target dexterous hands. Large language models (LLMs), with their broad
knowledge of human-object interactions and strong generative capabilities,
offer a promising alternative for zero-shot design reasoning. In this paper, we
present Lang2Morph, a language-driven pipeline for robotic hand design. It uses
LLMs to translate natural-language task descriptions into symbolic structures
and OPH-compatible parameters, enabling 3D-printable task-specific
morphologies. The pipeline consists of: (i) Morphology Design, which maps tasks
into semantic tags, structural grammars, and OPH-compatible parameters; and
(ii) Selection and Refinement, which evaluates design candidates based on
semantic alignment and size compatibility, and optionally applies LLM-guided
refinement when needed. We evaluate Lang2Morph across varied tasks, and results
show that our approach can generate diverse, task-relevant morphologies. To our
knowledge, this is the first attempt to develop an LLM-based framework for
task-conditioned robotic hand design.

</details>


### [410] [Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations](https://arxiv.org/abs/2509.18953)
*Hanqing Liu,Jiahuan Long,Junqi Wu,Jiacheng Hou,Huili Tang,Tingsong Jiang,Weien Zhou,Wen Yao*

Main category: cs.RO

TL;DR: This paper presents Eva-VLA, a framework for systematically assessing the real-world robustness of Vision-Language-Action (VLA) robotic systems by transforming discrete physical variations into continuous optimization problems. It identifies critical vulnerabilities in existing VLA models.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from a need to evaluate and enhance the robustness of VLA models for robotic manipulation in real-world scenarios where physical variations could drastically affect performance.

Method: The authors propose Eva-VLA, a unified framework. The approach involves decomposing real-world variations into three domains—3D object transformations, illumination variations, and adversarial patches—and employing a continuous black-box optimization framework to explore worst-case scenarios efficiently.

Result: The study reveals that current state-of-the-art VLA models are highly vulnerable to real-world variations, with failure rates exceeding 60% in all scenarios and reaching 97.8% in tasks involving object transformations.

Conclusion: The Eva-VLA framework highlights significant gaps between controlled lab performance and real-world readiness of VLA models. It provides insights and practical tools for improving the robustness of such robotic systems.

Abstract: Vision-Language-Action (VLA) models have emerged as promising solutions for
robotic manipulation, yet their robustness to real-world physical variations
remains critically underexplored. To bridge this gap, we propose Eva-VLA, the
first unified framework that systematically evaluates the robustness of VLA
models by transforming discrete physical variations into continuous
optimization problems. However, comprehensively assessing VLA robustness
presents two key challenges: (1) how to systematically characterize diverse
physical variations encountered in real-world deployments while maintaining
evaluation reproducibility, and (2) how to discover worst-case scenarios
without prohibitive real-world data collection costs efficiently. To address
the first challenge, we decompose real-world variations into three critical
domains: object 3D transformations that affect spatial reasoning, illumination
variations that challenge visual perception, and adversarial patches that
disrupt scene understanding. For the second challenge, we introduce a
continuous black-box optimization framework that transforms discrete physical
variations into parameter optimization, enabling systematic exploration of
worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models
across multiple benchmarks reveal alarming vulnerabilities: all variation types
trigger failure rates exceeding 60%, with object transformations causing up to
97.8% failure in long-horizon tasks. Our findings expose critical gaps between
controlled laboratory success and unpredictable deployment readiness, while the
Eva-VLA framework provides a practical pathway for hardening VLA-based robotic
manipulation models against real-world deployment challenges.

</details>


### [411] [Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation](https://arxiv.org/abs/2509.18954)
*Minoo Dolatabadi,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi*

Main category: cs.RO

TL;DR: The paper addresses inaccuracies in ICP-based localization by proposing a deep learning framework that predicts registration error covariance for LiDAR scans, improving pose estimation accuracy even without reference maps.


<details>
  <summary>Details</summary>
Motivation: ICP struggles with featureless environments and dynamic scenes, and current uncertainty prediction methods are based on oversimplified approaches. The authors aim to estimate registration error covariance for robust localization.

Method: They develop a deep learning-based framework that predicts ICP registration error covariance for LiDAR scans before matching, enabling integration with Kalman filtering.

Result: Experiments on the KITTI dataset show their method accurately predicts error covariance and reduces localization errors compared to existing techniques.

Conclusion: The proposed framework enhances localization robustness and accuracy by addressing limitations in ICP uncertainty estimation, making it effective in various scenarios.

Abstract: LiDAR-based localization and SLAM often rely on iterative matching
algorithms, particularly the Iterative Closest Point (ICP) algorithm, to align
sensor data with pre-existing maps or previous scans. However, ICP is prone to
errors in featureless environments and dynamic scenes, leading to inaccurate
pose estimation. Accurately predicting the uncertainty associated with ICP is
crucial for robust state estimation but remains challenging, as existing
approaches often rely on handcrafted models or simplified assumptions.
Moreover, a few deep learning-based methods for localizability estimation
either depend on a pre-built map, which may not always be available, or provide
a binary classification of localizable versus non-localizable, which fails to
properly model uncertainty. In this work, we propose a data-driven framework
that leverages deep learning to estimate the registration error covariance of
ICP before matching, even in the absence of a reference map. By associating
each LiDAR scan with a reliable 6-DoF error covariance estimate, our method
enables seamless integration of ICP within Kalman filtering, enhancing
localization accuracy and robustness. Extensive experiments on the KITTI
dataset demonstrate the effectiveness of our approach, showing that it
accurately predicts covariance and, when applied to localization using a
pre-built map or SLAM, reduces localization errors and improves robustness.

</details>


### [412] [Category-Level Object Shape and Pose Estimation in Less Than a Millisecond](https://arxiv.org/abs/2509.18979)
*Lorenzo Shaikewitz,Tim Nguyen,Luca Carlone*

Main category: cs.RO

TL;DR: This paper introduces a fast solver for object shape and pose estimation using semantic keypoints and a simple global optimality certificate.


<details>
  <summary>Details</summary>
Motivation: Efficient and accurate shape and pose estimation is crucial for advancing robotics applications such as manipulation and navigation.

Method: The method uses learned category-level semantic keypoints, a linear active shape model, and a self-consistent field iteration solver based on eigenvalue optimization.

Result: The solver runs in about 100 microseconds per iteration, demonstrating effectiveness on synthetic, public datasets, and real-world drone scenarios.

Conclusion: This approach offers a rapid and efficient solution for robust object shape and pose estimation with global optimality guarantees.

Abstract: Object shape and pose estimation is a foundational robotics problem,
supporting tasks from manipulation to scene understanding and navigation. We
present a fast local solver for shape and pose estimation which requires only
category-level object priors and admits an efficient certificate of global
optimality. Given an RGB-D image of an object, we use a learned front-end to
detect sparse, category-level semantic keypoints on the target object. We
represent the target object's unknown shape using a linear active shape model
and pose a maximum a posteriori optimization problem to solve for position,
orientation, and shape simultaneously. Expressed in unit quaternions, this
problem admits first-order optimality conditions in the form of an eigenvalue
problem with eigenvector nonlinearities. Our primary contribution is to solve
this problem efficiently with self-consistent field iteration, which only
requires computing a 4-by-4 matrix and finding its minimum eigenvalue-vector
pair at each iterate. Solving a linear system for the corresponding Lagrange
multipliers gives a simple global optimality certificate. One iteration of our
solver runs in about 100 microseconds, enabling fast outlier rejection. We test
our method on synthetic data and a variety of real-world settings, including
two public datasets and a drone tracking scenario. Code is released at
https://github.com/MIT-SPARK/Fast-ShapeAndPose.

</details>


### [413] [Pure Vision Language Action (VLA) Models: A Comprehensive Survey](https://arxiv.org/abs/2509.19012)
*Dapeng Zhang,Jin Sun,Chenghui Hu,Xiaoyan Wu,Zhenlong Yuan,Rui Zhou,Fei Shen,Qingguo Zhou*

Main category: cs.RO

TL;DR: This survey reviews Vision Language Action (VLA) models, classifying methods, exploring their applications, and suggesting future directions.


<details>
  <summary>Details</summary>
Motivation: To analyze and synthesize advancements in Vision Language Action (VLA) models, aiming to redefine robotics with generalized approaches.

Method: A comprehensive taxonomy and review of VLA research, including paradigms like autoregression, diffusion, reinforcement-based methods, and foundational resources.

Result: Detailed analysis of VLA applications, classification of paradigms, and synthesis of insights from over 300 studies.

Conclusion: The work identifies challenges and maps opportunities for scalable, general-purpose Vision Language Action models in robotics.

Abstract: The emergence of Vision Language Action (VLA) models marks a paradigm shift
from traditional policy-based control to generalized robotics, reframing Vision
Language Models (VLMs) from passive sequence generators into active agents for
manipulation and decision-making in complex, dynamic environments. This survey
delves into advanced VLA methods, aiming to provide a clear taxonomy and a
systematic, comprehensive review of existing research. It presents a
comprehensive analysis of VLA applications across different scenarios and
classifies VLA approaches into several paradigms: autoregression-based,
diffusion-based, reinforcement-based, hybrid, and specialized methods; while
examining their motivations, core strategies, and implementations in detail. In
addition, foundational datasets, benchmarks, and simulation platforms are
introduced. Building on the current VLA landscape, the review further proposes
perspectives on key challenges and future directions to advance research in VLA
models and generalizable robotics. By synthesizing insights from over three
hundred recent studies, this survey maps the contours of this rapidly evolving
field and highlights the opportunities and challenges that will shape the
development of scalable, general-purpose VLA methods.

</details>


### [414] [Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion](https://arxiv.org/abs/2509.19023)
*Shuai Liu,Meng Cheng Lau*

Main category: cs.RO

TL;DR: This paper introduces ROM-GRL, a two-stage reinforcement learning method for humanoid walking that doesn't require motion capture data.


<details>
  <summary>Details</summary>
Motivation: To develop humanoid walking techniques that don't rely on human demonstration or intricate reward shaping.

Method: Employs a two-stage process: training a reduced-order model (ROM) with proximal policy optimization and using its trajectories for guiding full-body humanoid control with Soft Actor-Critic enhanced by an adversarial discriminator.

Result: The proposed framework yielded stable, symmetric gaits with lower tracking errors compared to a reward-only baseline.

Conclusion: ROM-GRL demonstrates that lightweight ROM guidance can create efficient and naturalistic humanoid walking policies, bridging reward-based and imitation-based approaches without human demonstrations.

Abstract: We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a
two-stage reinforcement learning framework for humanoid walking that requires
no motion capture data or elaborate reward shaping. In the first stage, a
compact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via
Proximal Policy Optimization. This generates energy-efficient gait templates.
In the second stage, those dynamically consistent trajectories guide a
full-body policy trained with Soft Actor--Critic augmented by an adversarial
discriminator, ensuring the student's five-dimensional gait feature
distribution matches the ROM's demonstrations. Experiments at 1
meter-per-second and 4 meter-per-second show that ROM-GRL produces stable,
symmetric gaits with substantially lower tracking error than a pure-reward
baseline. By distilling lightweight ROM guidance into high-dimensional
policies, ROM-GRL bridges the gap between reward-only and imitation-based
locomotion methods, enabling versatile, naturalistic humanoid behaviors without
any human demonstrations.

</details>


### [415] [TacEva: A Performance Evaluation Framework For Vision-Based Tactile Sensors](https://arxiv.org/abs/2509.19037)
*Qingzheng Cong,Steven Oh,Wen Fan,Shan Luo,Kaspar Althoefer,Dandan Zhang*

Main category: cs.RO

TL;DR: TacEva introduces a standardized evaluation framework for Vision-Based Tactile Sensors (VBTSs) to address performance disparities and aid in optimization and task-specific selection.


<details>
  <summary>Details</summary>
Motivation: Existing VBTSs lack standardized metrics for consistent performance evaluation, making it difficult to optimize and choose sensors for specific tasks.

Method: TacEva framework defines performance metrics and designs structured experimental pipelines for the repeatable and consistent evaluation of VBTSs.

Result: Applied TacEva on diverse VBTSs, showcasing its ability to provide thorough evaluations and quantitative indicators for performance dimensions.

Conclusion: TacEva aids researchers in pre-selecting suitable VBTSs for tasks and offers insights for design optimization, advancing the utility of VBTSs in robotic applications.

Abstract: Vision-Based Tactile Sensors (VBTSs) are widely used in robotic tasks because
of the high spatial resolution they offer and their relatively low
manufacturing costs. However, variations in their sensing mechanisms,
structural dimension, and other parameters lead to significant performance
disparities between existing VBTSs. This makes it challenging to optimize them
for specific tasks, as both the initial choice and subsequent fine-tuning are
hindered by the lack of standardized metrics. To address this issue, TacEva is
introduced as a comprehensive evaluation framework for the quantitative
analysis of VBTS performance. The framework defines a set of performance
metrics that capture key characteristics in typical application scenarios. For
each metric, a structured experimental pipeline is designed to ensure
consistent and repeatable quantification. The framework is applied to multiple
VBTSs with distinct sensing mechanisms, and the results demonstrate its ability
to provide a thorough evaluation of each design and quantitative indicators for
each performance dimension. This enables researchers to pre-select the most
appropriate VBTS on a task by task basis, while also offering
performance-guided insights into the optimization of VBTS design. A list of
existing VBTS evaluation methods and additional evaluations can be found on our
website: https://stevenoh2003.github.io/TacEva/

</details>


### [416] [ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation](https://arxiv.org/abs/2509.19047)
*Geonhyup Lee,Yeongjin Lee,Kangmin Kim,Seongju Lee,Sangjun Noh,Seunghyeok Back,Kyoobin Lee*

Main category: cs.RO

TL;DR: This paper proposes ManipForce to capture high-frequency force-torque and RGB data during human demonstrations for precise manipulation tasks, along with FMT, a transformer-based policy achieving 83% task success.


<details>
  <summary>Details</summary>
Motivation: The need for more precise control in contact-rich manipulation tasks, which are not well addressed by vision-only demonstrations.

Method: ManipForce collects RGB and high-frequency force-torque data from demonstrations, while FMT uses frequency- and modality-aware embeddings combined via bi-directional cross-attention in a transformer diffusion policy.

Result: FMT trained on ManipForce demonstrations achieves an 83% average success rate across six manipulation tasks, outperforming RGB-only methods.

Conclusion: Utilizing high-frequency force-torque data and multimodal integration significantly enhances manipulation performance in precision tasks.

Abstract: Contact-rich manipulation tasks such as precision assembly require precise
control of interaction forces, yet existing imitation learning methods rely
mainly on vision-only demonstrations. We propose ManipForce, a handheld system
designed to capture high-frequency force-torque (F/T) and RGB data during
natural human demonstrations for contact-rich manipulation. Building on these
demonstrations, we introduce the Frequency-Aware Multimodal Transformer (FMT).
FMT encodes asynchronous RGB and F/T signals using frequency- and
modality-aware embeddings and fuses them via bi-directional cross-attention
within a transformer diffusion policy. Through extensive experiments on six
real-world contact-rich manipulation tasks - such as gear assembly, box
flipping, and battery insertion - FMT trained on ManipForce demonstrations
achieves robust performance with an average success rate of 83% across all
tasks, substantially outperforming RGB-only baselines. Ablation and
sampling-frequency analyses further confirm that incorporating high-frequency
F/T data and cross-modal integration improves policy performance, especially in
tasks demanding high precision and stable contact.

</details>


### [417] [SlicerROS2: A Research and Development Module for Image-Guided Robotic Interventions](https://arxiv.org/abs/2509.19076)
*Laura Connolly,Aravind S. Kumar,Kapi Ketan Mehta,Lidia Al-Zogbi,Peter Kazanzides,Parvin Mousavi,Gabor Fichtinger,Axel Krieger,Junichi Tokuda,Russell H. Taylor,Simon Leonard,Anton Deguet*

Main category: cs.RO

TL;DR: SlicerROS2 integrates 3D Slicer and ROS for enhanced medical robotics.


<details>
  <summary>Details</summary>
Motivation: To standardize integration for medical robotics research.

Method: Redesigned SlicerROS2 with modularity, Python API, and improved data handling.

Result: Demonstrated new design through four realistic applications.

Conclusion: SlicerROS2 boosts capabilities for image-guided robotic interventions.

Abstract: Image-guided robotic interventions involve the use of medical imaging in
tandem with robotics. SlicerROS2 is a software module that combines 3D Slicer
and robot operating system (ROS) in pursuit of a standard integration approach
for medical robotics research. The first release of SlicerROS2 demonstrated the
feasibility of using the C++ API from 3D Slicer and ROS to load and visualize
robots in real time. Since this initial release, we've rewritten and redesigned
the module to offer greater modularity, access to low-level features, access to
3D Slicer's Python API, and better data transfer protocols. In this paper, we
introduce this new design as well as four applications that leverage the core
functionalities of SlicerROS2 in realistic image-guided robotics scenarios.

</details>


### [418] [World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation](https://arxiv.org/abs/2509.19080)
*Zhennan Jiang,Kai Liu,Yuxin Qin,Shuai Tian,Yupeng Zheng,Mingcai Zhou,Chao Yu,Haoran Li,Dongbin Zhao*

Main category: cs.RO

TL;DR: The paper introduces World4RL, a framework using diffusion-based world models as simulators to refine robotic manipulation policies in simulated environments, bypassing real-world constraints.


<details>
  <summary>Details</summary>
Motivation: Robotic manipulation policies are constrained by the limited expert data available for imitation learning and the safety/cost challenges of reinforcement learning in real robots or sim-to-real transfer gaps from simulators.

Method: World4RL pre-trains a diffusion world model on diverse tasks and refines policies entirely within this model using techniques like two-hot action encoding and frozen model interactions for robotic manipulation.

Result: The method achieves higher success rates in robotic tasks compared to imitation learning and other baselines, showcasing high-fidelity simulation and robust policy refinement.

Conclusion: World4RL demonstrates the potential of diffusion-based simulation in enabling efficient, high-quality policy optimization for robotic manipulation without requiring risky or costly real-world interactions.

Abstract: Robotic manipulation policies are commonly initialized through imitation
learning, but their performance is limited by the scarcity and narrow coverage
of expert data. Reinforcement learning can refine polices to alleviate this
limitation, yet real-robot training is costly and unsafe, while training in
simulators suffers from the sim-to-real gap. Recent advances in generative
models have demonstrated remarkable capabilities in real-world simulation, with
diffusion models in particular excelling at generation. This raises the
question of how diffusion model-based world models can be combined to enhance
pre-trained policies in robotic manipulation. In this work, we propose
World4RL, a framework that employs diffusion-based world models as
high-fidelity simulators to refine pre-trained policies entirely in imagined
environments for robotic manipulation. Unlike prior works that primarily employ
world models for planning, our framework enables direct end-to-end policy
optimization. World4RL is designed around two principles: pre-training a
diffusion world model that captures diverse dynamics on multi-task datasets and
refining policies entirely within a frozen world model to avoid online
real-world interactions. We further design a two-hot action encoding scheme
tailored for robotic manipulation and adopt diffusion backbones to improve
modeling fidelity. Extensive simulation and real-world experiments demonstrate
that World4RL provides high-fidelity environment modeling and enables
consistent policy refinement, yielding significantly higher success rates
compared to imitation learning and other baselines. More visualization results
are available at https://world4rl.github.io/.

</details>


### [419] [FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation](https://arxiv.org/abs/2509.19102)
*Hongli Xu,Lei Zhang,Xiaoyue Hu,Boyang Zhong,Kaixin Bai,Zoltán-Csaba Márton,Zhenshan Bing,Zhaopeng Chen,Alois Christian Knoll,Jianwei Zhang*

Main category: cs.RO

TL;DR: The paper presents FunCanon, a framework that divides complex manipulation tasks into reusable action chunks, leveraging functional object alignment via vision language models to train a generalizing diffusion policy, FuncDiffuser.


<details>
  <summary>Details</summary>
Motivation: General-purpose robotic skills often fail to generalize across tasks due to task-specific training; thus, a scalable, generalizable approach is needed for complex manipulation.

Method: FunCanon divides long-horizon tasks into actor-verb-object action chunks using functional canonicalization, aided by cues from vision language models. These aligned datasets train a diffusion policy called FuncDiffuser.

Result: Experiments show category-level generalization, task reuse, and robust deployment in simulated and real-world benchmarks, demonstrating strong generalization and learning improvement.

Conclusion: Functional canonicalization using FunCanon with FuncDiffuser provides a scalable inductive bias, enabling imitation learning for complex robotic manipulation tasks that generalize across categories and tasks.

Abstract: General-purpose robotic skills from end-to-end demonstrations often leads to
task-specific policies that fail to generalize beyond the training
distribution. Therefore, we introduce FunCanon, a framework that converts
long-horizon manipulation tasks into sequences of action chunks, each defined
by an actor, verb, and object. These chunks focus policy learning on the
actions themselves, rather than isolated tasks, enabling compositionality and
reuse. To make policies pose-aware and category-general, we perform functional
object canonicalization for functional alignment and automatic manipulation
trajectory transfer, mapping objects into shared functional frames using
affordance cues from large vision language models. An object centric and action
centric diffusion policy FuncDiffuser trained on this aligned data naturally
respects object affordances and poses, simplifying learning and improving
generalization ability. Experiments on simulated and real-world benchmarks
demonstrate category-level generalization, cross-task behavior reuse, and
robust sim2real deployment, showing that functional canonicalization provides a
strong inductive bias for scalable imitation learning in complex manipulation
domains. Details of the demo and supplemental material are available on our
project website https://sites.google.com/view/funcanon.

</details>


### [420] [Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation](https://arxiv.org/abs/2509.19105)
*Sarvesh Prajapati,Ananya Trivedi,Nathaniel Hanson,Bruce Maxwell,Taskin Padir*

Main category: cs.RO

TL;DR: The paper proposes RS-Net, a neural network predicting spectral signatures and physical properties from RGB images to enhance robotic navigation.


<details>
  <summary>Details</summary>
Motivation: Existing navigation techniques struggle with visually similar surfaces that differ in material properties. While spectral sensors can help, their adoption is limited due to high costs and complex setups.

Method: The proposed RS-Net leverages a deep neural network to predict spectral signatures from RGB patches. These are then used to classify terrains and estimate physical properties like friction. This is integrated into robot motion planners.

Result: The framework enables robots to accurately navigate diverse terrains using only RGB sensors during operation while performing as if they had access to rich spectral data.

Conclusion: RS-Net bridges the gap between affordable RGB sensing and the material insights of spectral data. It simplifies robotic terrain classification and navigation.

Abstract: Successful navigation in outdoor environments requires accurate prediction of
the physical interactions between the robot and the terrain. To this end,
several methods rely on geometric or semantic labels to classify traversable
surfaces. However, such labels cannot distinguish visually similar surfaces
that differ in material properties. Spectral sensors enable inference of
material composition from surface reflectance measured across multiple
wavelength bands. Although spectral sensing is gaining traction in robotics,
widespread deployment remains constrained by the need for custom hardware
integration, high sensor costs, and compute-intensive processing pipelines. In
this paper, we present RGB Image to Spectral Signature Neural Network (RS-Net),
a deep neural network designed to bridge the gap between the accessibility of
RGB sensing and the rich material information provided by spectral data. RS-Net
predicts spectral signatures from RGB patches, which we map to terrain labels
and friction coefficients. The resulting terrain classifications are integrated
into a sampling-based motion planner for a wheeled robot operating in outdoor
environments. Likewise, the friction estimates are incorporated into a
contact-force-based MPC for a quadruped robot navigating slippery surfaces.
Thus, we introduce a framework that learns the task-relevant physical property
once during training and thereafter relies solely on RGB sensing at test time.
The code is available at https://github.com/prajapatisarvesh/RS-Net.

</details>


### [421] [BiGraspFormer: End-to-End Bimanual Grasp Transformer](https://arxiv.org/abs/2509.19142)
*Kangmin Kim,Seunghyeok Back,Geonhyup Lee,Sangbeom Lee,Sangjun Noh,Kyoobin Lee*

Main category: cs.RO

TL;DR: BiGraspFormer is an end-to-end framework designed for coordinated robot bimanual grasping, overcoming limitations of traditional methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address coordination challenges in robot bimanual grasping, such as collision risks and unbalanced force distribution.

Method: The authors introduce a Single-Guided Bimanual (SGB) strategy in a transformer framework that first generates single grasp candidates and then predicts bimanual poses using specialized attention mechanisms.

Result: Experimental results demonstrate BiGraspFormer's superior grasping performance compared to existing methods, with efficient inference speed (< 0.05s).

Conclusion: BiGraspFormer significantly improves bimanual grasping coordination, reliability, and speed, offering a robust solution for robotic manipulation tasks.

Abstract: Bimanual grasping is essential for robots to handle large and complex
objects. However, existing methods either focus solely on single-arm grasping
or employ separate grasp generation and bimanual evaluation stages, leading to
coordination problems including collision risks and unbalanced force
distribution. To address these limitations, we propose BiGraspFormer, a unified
end-to-end transformer framework that directly generates coordinated bimanual
grasps from object point clouds. Our key idea is the Single-Guided Bimanual
(SGB) strategy, which first generates diverse single grasp candidates using a
transformer decoder, then leverages their learned features through specialized
attention mechanisms to jointly predict bimanual poses and quality scores. This
conditioning strategy reduces the complexity of the 12-DoF search space while
ensuring coordinated bimanual manipulation. Comprehensive simulation
experiments and real-world validation demonstrate that BiGraspFormer
consistently outperforms existing methods while maintaining efficient inference
speed (<0.05s), confirming the effectiveness of our framework. Code and
supplementary materials are available at https://sites.google.com/bigraspformer

</details>


### [422] [A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination](https://arxiv.org/abs/2509.19168)
*Mark Gonzales,Ethan Oh,Joseph Moore*

Main category: cs.RO

TL;DR: This paper introduces a sampling-based planner that uses multimodal policy distributions for improved exploration and robustness, tested through simulations and hardware experiments.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in planning, such as local minima and computational complexity, especially in multi-robot systems.

Method: A receding-horizon, sampling-based planning approach leveraging cross-entropy optimization for multimodal policies.

Result: Simulations show improved success rates in challenging environments, and hardware tests confirm real-time feasibility.

Conclusion: The method enhances robustness, exploration, and collision-free multi-robot planning while reducing centralized optimization complexity.

Abstract: In this paper, we present a receding-horizon, sampling-based planner capable
of reasoning over multimodal policy distributions. By using the cross-entropy
method to optimize a multimodal policy under a common cost function, our
approach increases robustness against local minima and promotes effective
exploration of the solution space. We show that our approach naturally extends
to multi-robot collision-free planning, enables agents to share diverse
candidate policies to avoid deadlocks, and allows teams to minimize a global
objective without incurring the computational complexity of centralized
optimization. Numerical simulations demonstrate that employing multiple modes
significantly improves success rates in trap environments and in multi-robot
collision avoidance. Hardware experiments further validate the approach's
real-time feasibility and practical performance.

</details>


### [423] [MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human Demonstration to Robotic Deployment Gap](https://arxiv.org/abs/2509.19169)
*Tianyu Wu,Xudong Han,Haoran Sun,Zishang Zhang,Bangchao Huang,Chaoyang Song,Fang Wan*

Main category: cs.RO

TL;DR: The paper introduces MagiClaw, a versatile two-finger robotic end-effector designed to bridge the domain gap between human demonstration and robotic manipulation, equipped with advanced sensing capabilities for intuitive data collection and policy deployment.


<details>
  <summary>Details</summary>
Motivation: The domain gap in sensing and morphology often makes it challenging to transfer manipulation skills from humans to robots. The paper is motivated to address this challenge by creating a hardware system that ensures consistency between data collection and robotic policy deployment.

Method: The authors propose MagiClaw, which functions both as a handheld tool and a robotic end-effector. It incorporates Soft Polyhedral Network (SPN) fingers with embedded cameras for proprioception and utilizes an iPhone for exteroceptive sensing. The device streams synchronized multimodal data through a custom iOS app for teleoperation, policy learning, and mixed-reality control.

Result: The proposed system reduces the complexity of collecting high-fidelity, contact-rich datasets and accelerates the development of manipulation policies by providing a unified and immersive data collection and control architecture.

Conclusion: MagiClaw successfully bridges the gap between human data collection and robotic policy deployment, enabling easier development of generalizable manipulation skills through an innovative combination of hardware and software components.

Abstract: The transfer of manipulation skills from human demonstration to robotic
execution is often hindered by a "domain gap" in sensing and morphology. This
paper introduces MagiClaw, a versatile two-finger end-effector designed to
bridge this gap. MagiClaw functions interchangeably as both a handheld tool for
intuitive data collection and a robotic end-effector for policy deployment,
ensuring hardware consistency and reliability. Each finger incorporates a Soft
Polyhedral Network (SPN) with an embedded camera, enabling vision-based
estimation of 6-DoF forces and contact deformation. This proprioceptive data is
fused with exteroceptive environmental sensing from an integrated iPhone, which
provides 6D pose, RGB video, and LiDAR-based depth maps. Through a custom iOS
application, MagiClaw streams synchronized, multi-modal data for real-time
teleoperation, offline policy learning, and immersive control via mixed-reality
interfaces. We demonstrate how this unified system architecture lowers the
barrier to collecting high-fidelity, contact-rich datasets and accelerates the
development of generalizable manipulation policies. Please refer to the iOS app
at https://apps.apple.com/cn/app/magiclaw/id6661033548 for further details.

</details>


### [424] [Proactive-reactive detection and mitigation of intermittent faults in robot swarms](https://arxiv.org/abs/2509.19246)
*Sinan Oğuz,Emanuele Garone,Marco Dorigo,Mary Katherine Heinrich*

Main category: cs.RO

TL;DR: Intermittent faults in robot swarms are tackled using a proactive-reactive approach leveraging self-organized nervous systems for fault detection and mitigation.


<details>
  <summary>Details</summary>
Motivation: Intermittent faults are a significant challenge in robot swarms due to their transient and unpredictable nature, especially in self-organized networks.

Method: The paper introduces a proactive-reactive strategy combining dynamic backup paths, distributed consensus, and likelihood ratio tests for fault detection and mitigation.

Result: The approach was validated in scenarios with faulty positional data, showing high accuracy in fault detection and minimal false positives during formation control tasks.

Conclusion: The proposed method effectively addresses intermittent faults by preventing disruptions and ensuring reliable operation through innovative self-organized tactics.

Abstract: Intermittent faults are transient errors that sporadically appear and
disappear. Although intermittent faults pose substantial challenges to
reliability and coordination, existing studies of fault tolerance in robot
swarms focus instead on permanent faults. One reason for this is that
intermittent faults are prohibitively difficult to detect in the fully
self-organized ad-hoc networks typical of robot swarms, as their network
topologies are transient and often unpredictable. However, in the recently
introduced self-organizing nervous systems (SoNS) approach, robot swarms are
able to self-organize persistent network structures for the first time, easing
the problem of detecting intermittent faults. To address intermittent faults in
robot swarms that have persistent networks, we propose a novel
proactive-reactive strategy to detection and mitigation, based on
self-organized backup layers and distributed consensus in a multiplex network.
Proactively, the robots self-organize dynamic backup paths before faults occur,
adapting to changes in the primary network topology and the robots' relative
positions. Reactively, robots use one-shot likelihood ratio tests to compare
information received along different paths in the multiplex network, enabling
early fault detection. Upon detection, communication is temporarily rerouted in
a self-organized way, until the detected fault resolves. We validate the
approach in representative scenarios of faulty positional data occurring during
formation control, demonstrating that intermittent faults are prevented from
disrupting convergence to desired formations, with high fault detection
accuracy and low rates of false positives.

</details>


### [425] [Imitation-Guided Bimanual Planning for Stable Manipulation under Changing External Forces](https://arxiv.org/abs/2509.19261)
*Kuanqi Cai,Chunfeng Wang,Zeqi Li,Haowen Yao,Weinan Chen,Luis Figueredo,Aude Billard,Arash Ajoudani*

Main category: cs.RO

TL;DR: This paper proposes a framework to improve robotic manipulation by enabling smooth grasp transitions and motion efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of achieving smooth and adaptive robotic grasp transitions in dynamic environments, particularly under varying external forces.

Method: It introduces an Imitation-Guided Bimanual Planning Framework that leverages stable grasp manifold sampling and a dual-stage motion architecture combining Imitation Learning and Quadratic Programming.

Result: The framework significantly enhances grasp transition efficiency and robotic motion performance in force-intensive tasks.

Conclusion: The proposed approach effectively integrates stable grasp transitions and motion optimization to improve stability and dexterity in robotic manipulation.

Abstract: Robotic manipulation in dynamic environments often requires seamless
transitions between different grasp types to maintain stability and efficiency.
However, achieving smooth and adaptive grasp transitions remains a challenge,
particularly when dealing with external forces and complex motion constraints.
Existing grasp transition strategies often fail to account for varying external
forces and do not optimize motion performance effectively. In this work, we
propose an Imitation-Guided Bimanual Planning Framework that integrates
efficient grasp transition strategies and motion performance optimization to
enhance stability and dexterity in robotic manipulation. Our approach
introduces Strategies for Sampling Stable Intersections in Grasp Manifolds for
seamless transitions between uni-manual and bi-manual grasps, reducing
computational costs and regrasping inefficiencies. Additionally, a Hierarchical
Dual-Stage Motion Architecture combines an Imitation Learning-based Global Path
Generator with a Quadratic Programming-driven Local Planner to ensure real-time
motion feasibility, obstacle avoidance, and superior manipulability. The
proposed method is evaluated through a series of force-intensive tasks,
demonstrating significant improvements in grasp transition efficiency and
motion performance. A video demonstrating our simulation results can be viewed
at
\href{https://youtu.be/3DhbUsv4eDo}{\textcolor{blue}{https://youtu.be/3DhbUsv4eDo}}.

</details>


### [426] [SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration](https://arxiv.org/abs/2509.19292)
*Yang Jin,Jun Lv,Han Xue,Wendi Chen,Chuan Wen,Cewu Lu*

Main category: cs.RO

TL;DR: The paper introduces SOE, an approach for robotic manipulation that ensures safe and effective exploration by learning a latent action space.


<details>
  <summary>Details</summary>
Motivation: Robot policies often suffer from limited exploration capabilities due to action mode collapse, and existing methods relying on random perturbations are unsafe and unstable.

Method: SOE learns a latent representation for task-relevant factors and restricts exploration to valid actions. It integrates with arbitrary policies as a module, allowing safe and structured exploration, and supports human-guided intervention.

Result: The experiments, in simulation and real tasks, show that SOE achieves higher success rates, safer and smoother exploration, and better sample efficiency compared to prior methods.

Conclusion: On-manifold exploration is a robust approach for sample-efficient policy self-improvement, and SOE enables safer, more effective, and human-controllable exploration.

Abstract: Intelligent agents progress by continually refining their capabilities
through actively exploring environments. Yet robot policies often lack
sufficient exploration capability due to action mode collapse. Existing methods
that encourage exploration typically rely on random perturbations, which are
unsafe and induce unstable, erratic behaviors, thereby limiting their
effectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a
framework that enhances policy exploration and improvement in robotic
manipulation. SOE learns a compact latent representation of task-relevant
factors and constrains exploration to the manifold of valid actions, ensuring
safety, diversity, and effectiveness. It can be seamlessly integrated with
arbitrary policy models as a plug-in module, augmenting exploration without
degrading the base policy performance. Moreover, the structured latent space
enables human-guided exploration, further improving efficiency and
controllability. Extensive experiments in both simulation and real-world tasks
demonstrate that SOE consistently outperforms prior methods, achieving higher
task success rates, smoother and safer exploration, and superior sample
efficiency. These results establish on-manifold exploration as a principled
approach to sample-efficient policy self-improvement. Project website:
https://ericjin2002.github.io/SOE

</details>


### [427] [Residual Off-Policy RL for Finetuning Behavior Cloning Policies](https://arxiv.org/abs/2509.19301)
*Lars Ankile,Zhenyu Jiang,Rocky Duan,Guanya Shi,Pieter Abbeel,Anusha Nagabandi*

Main category: cs.RO

TL;DR: The paper proposes a combination of behavior cloning (BC) and reinforcement learning (RL) through residual learning to overcome challenges in robot control, achieving state-of-the-art performance on high-degree-of-freedom systems, including humanoid robots.


<details>
  <summary>Details</summary>
Motivation: Address limitations in behavior cloning (e.g., data dependency) and reinforcement learning (e.g., inefficiency and safety) for real-world robot control.

Method: Utilizes a residual learning framework where BC policies serve as black-box bases, and RL learns lightweight residual corrections with sparse binary rewards.

Result: Method effectively trains both simulated and real-world manipulation policies on high-degree-of-freedom robots, achieving successful RL for humanoid robots with dexterous hands.

Conclusion: The approach offers a practical pathway for deploying RL in real-world robotic tasks with sparse rewards, achieving significant advancements in vision-based and dexterous manipulation tasks.

Abstract: Recent advances in behavior cloning (BC) have enabled impressive visuomotor
control policies. However, these approaches are limited by the quality of human
demonstrations, the manual effort required for data collection, and the
diminishing returns from increasing offline data. In comparison, reinforcement
learning (RL) trains an agent through autonomous interaction with the
environment and has shown remarkable success in various domains. Still,
training RL policies directly on real-world robots remains challenging due to
sample inefficiency, safety concerns, and the difficulty of learning from
sparse rewards for long-horizon tasks, especially for high-degree-of-freedom
(DoF) systems. We present a recipe that combines the benefits of BC and RL
through a residual learning framework. Our approach leverages BC policies as
black-box bases and learns lightweight per-step residual corrections via
sample-efficient off-policy RL. We demonstrate that our method requires only
sparse binary reward signals and can effectively improve manipulation policies
on high-degree-of-freedom (DoF) systems in both simulation and the real world.
In particular, we demonstrate, to the best of our knowledge, the first
successful real-world RL training on a humanoid robot with dexterous hands. Our
results demonstrate state-of-the-art performance in various vision-based tasks,
pointing towards a practical pathway for deploying RL in the real world.
Project website: https://residual-offpolicy-rl.github.io

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [428] [CoRaCMG: Contextual Retrieval-Augmented Framework for Commit Message Generation](https://arxiv.org/abs/2509.18337)
*Bo Xiong,Linghao Zhang,Chong Wang,Peng Liang*

Main category: cs.SE

TL;DR: This paper introduces CoRaCMG, a framework combining retrieval and augmentation of similar diff-message pairs to guide LLMs in generating high-quality commit messages.


<details>
  <summary>Details</summary>
Motivation: Commit messages are often vague or incomplete, making them less useful. The paper aims to improve LLM-based commit message generation to reduce developers' effort and boost message quality.

Method: CoRaCMG includes three phases: retrieve similar diff-message pairs, augment the query diff with these pairs to create a structured prompt, and generate commit messages using LLMs. It applies this method to various language models.

Result: The experiments reveal significant performance improvements across metrics like BLEU, Rouge-L, METEOR, and CIDEr. For example, DeepSeek-R1 showed relative improvements of 76% in BLEU and 71% in CIDEr using a single example pair. GPT-4o achieved a BLEU score improvement of 89%.

Conclusion: CoRaCMG improves LLM performance by leveraging project-specific terminologies and human writing styles from retrieved pairs. Results plateau after three examples, highlighting limited gains beyond this.

Abstract: Commit messages play a key role in documenting the intent behind code
changes. However, they are often low-quality, vague, or incomplete, limiting
their usefulness. Commit Message Generation (CMG) aims to automatically
generate descriptive commit messages from code diffs to reduce developers'
effort and improve message quality. Although recent advances in LLMs have shown
promise in automating CMG, their performance remains limited. This paper aims
to enhance CMG performance by retrieving similar diff-message pairs to guide
LLMs to generate commit messages that are more precise and informative. We
proposed CoRaCMG, a Contextual Retrieval-augmented framework for Commit Message
Generation, structured in three phases: (1) Retrieve: retrieving the similar
diff-message pairs; (2) Augment: combining them with the query diff into a
structured prompt; and (3) Generate: generating commit messages corresponding
to the query diff via LLMs. CoRaCMG enables LLMs to learn project-specific
terminologies and writing styles from the retrieved diff-message pairs, thereby
producing high-quality commit messages. We evaluated our method on various
LLMs, including closed-source GPT models and open-source DeepSeek models.
Experimental results show that CoRaCMG significantly boosts LLM performance
across four metrics (BLEU, Rouge-L, METEOR, and CIDEr). Specifically,
DeepSeek-R1 achieves relative improvements of 76% in BLEU and 71% in CIDEr when
augmented with a single retrieved example pair. After incorporating the single
example pair, GPT-4o achieves the highest improvement rate, with BLEU
increasing by 89%. Moreover, performance gains plateau after more than three
examples are used, indicating diminishing returns. Further analysis shows that
the improvements are attributed to the model's ability to capture the
terminologies and writing styles of human-written commit messages from the
retrieved example pairs.

</details>


### [429] [Reading Between the Lines: Scalable User Feedback via Implicit Sentiment in Developer Prompts](https://arxiv.org/abs/2509.18361)
*Daye Nam,Malgorzata Salawa,Satish Chandra*

Main category: cs.SE

TL;DR: This paper proposes using sentiment analysis of developer prompts to evaluate satisfaction with conversational AI assistants, addressing limitations of both user studies and quantitative metrics.


<details>
  <summary>Details</summary>
Motivation: Evaluation of developer satisfaction with AI assistants is limited by the scalability of user studies and the reliability of large-scale quantitative metrics.

Method: The method involves sentiment analysis on developer prompts from usage logs to identify implicit satisfaction signals, analyzing data from 372 professional developers.

Result: The proposed approach identifies satisfaction signals in ~8% of all interactions, a rate over 13 times higher than explicit feedback, and demonstrates reasonable accuracy using off-the-shelf sentiment analysis tools.

Conclusion: This approach is a practical complement to existing feedback methods and paves the way for better understanding developer experiences at scale.

Abstract: Evaluating developer satisfaction with conversational AI assistants at scale
is critical but challenging. User studies provide rich insights, but are
unscalable, while large-scale quantitative signals from logs or in-product
ratings are often too shallow or sparse to be reliable. To address this gap, we
propose and evaluate a new approach: using sentiment analysis of developer
prompts to identify implicit signals of user satisfaction. With an analysis of
industrial usage logs of 372 professional developers, we show that this
approach can identify a signal in ~8% of all interactions, a rate more than 13
times higher than explicit user feedback, with reasonable accuracy even with an
off-the-shelf sentiment analysis approach. This new practical approach to
complement existing feedback channels would open up new directions for building
a more comprehensive understanding of the developer experience at scale.

</details>


### [430] [SC2Tools: StarCraft II Toolset and Dataset API](https://arxiv.org/abs/2509.18454)
*Andrzej Białecki,Piotr Białecki,Piotr Sowiński,Mateusz Budziak,Jan Gajewski*

Main category: cs.SE

TL;DR: This paper introduces `SC2Tools`, a modular toolkit designed to simplify data collection, preprocessing, and dataset creation for gaming and esports research, especially in StarCraft 2.


<details>
  <summary>Details</summary>
Motivation: Gaming and esports are significantly influenced by AI and ML, but scientists lack easy-to-use tools for working with data, which slows research progress.

Method: The authors developed `SC2Tools`, a modular toolset for creating and manipulating datasets, which also includes APIs for easy access to the data.

Result: `SC2Tools` facilitated the creation of one of the largest datasets for StarCraft 2 tournaments and provides APIs compatible with PyTorch and PyTorch Lightning.

Conclusion: By simplifying data-handling workflows, `SC2Tools` enables less technical researchers to participate in gaming and esports research, and lays foundational groundwork for standardizing experiment workflows in StarCraft 2.

Abstract: Computer games, as fully controlled simulated environments, have been
utilized in significant scientific studies demonstrating the application of
Reinforcement Learning (RL). Gaming and esports are key areas influenced by the
application of Artificial Intelligence (AI) and Machine Learning (ML) solutions
at scale. Tooling simplifies scientific workloads and is essential for
developing the gaming and esports research area.
  In this work, we present ``SC2Tools'', a toolset containing multiple
submodules responsible for working with, and producing larger datasets. We
provide a modular structure of the implemented tooling, leaving room for future
extensions where needed. Additionally, some of the tools are not StarCraft~2
exclusive and can be used with other types of data for dataset creation.
  The tools we present were leveraged in creating one of the largest
StarCraft~2 tournament datasets to date with a separate PyTorch and PyTorch
Lightning application programming interface (API) for easy access to the data.
  We conclude that alleviating the burden of data collection, preprocessing,
and custom code development is essential for less technically proficient
researchers to engage in the growing gaming and esports research area. Finally,
our solution provides some foundational work toward normalizing experiment
workflow in StarCraft~2

</details>


### [431] [Locking Down Science Gateways](https://arxiv.org/abs/2509.18548)
*Steven R Brandt,Max Morris,Patrick Diehl,Christopher Bowen,Jacob Tucker,Lauren Bristol,Golden G. Richard III*

Main category: cs.SE

TL;DR: The paper discusses using Linux's Landlock feature to secure scientific codes and science gateways, detailing its application to astrophysical simulation tools and demonstrating its feasibility.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the security needs of scientific codes and gateways, particularly the need to restrict network access after MPI initialization to protect against user-supplied security threats.

Method: The authors modify three scientific codes (Einstein Toolkit, Octo-Tiger, and FUKA) to use Landlock for enhancing security and implement a science gateway for FUKA utilizing Landlock instead of user authentication.

Result: The study finds Landlock to be useful in securing scientific applications, as it successfully locks down network access while maintaining functionality for astrophysical simulations.

Conclusion: Landlock is demonstrated as a viable tool for enforcing security on science gateways and scientific codes, allowing them to operate safely without relying on traditional user authentication methods.

Abstract: The most recent Linux kernels have a new feature for securing applications:
Landlock. Like Seccomp before it, Landlock makes it possible for a running
process to give up access to resources. For applications running as Science
Gateways, network access is required while starting up MPI, but for the sake of
security, it should be taken away prior to the reading of user-supplied
parameter files. We explore the usefulness of Landlock by modifying and locking
down three mature scientific codes: The Einstein Toolkit (a code that studies
the dynamics of relativistic astrophysics, e.g. neutron star collisions),
Octo-Tiger (a code for studying the dynamics of non-relativistic astrophysics,
e.g. white dwarfs), and FUKA (an initial data solver for relativistic codes).
Finally, we implement a fully-functioning FUKA science gateway that relies on
Landlock (instead of user authentication) for security.

</details>


### [432] [SR-Eval: Evaluating LLMs on Code Generation under Stepwise Requirement Refinement](https://arxiv.org/abs/2509.18808)
*Zexun Zhan,Shuzheng Gao,Ruida Hu,Cuiyun Gao*

Main category: cs.SE

TL;DR: This paper introduces SR-Eval, a benchmark to assess large language models (LLMs) in iterative code generation with evolving requirements, emphasizing the challenges in real-world development workflows.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to capture the iterative nature of real-world software development, limiting the understanding of LLMs' reliability in such workflows.

Method: The benchmark involves a multi-agent-based requirement generation pipeline and semantic-aware test case generation, enabling evaluation across Python and Java tasks with turn-specific discriminative tests.

Result: The study evaluates 11 LLMs, showing low completion rates (22.67% function-level, 20.00% repository-level tasks) and substantial influence of prompting strategies on performance.

Conclusion: Iterative code generation under stepwise requirement refinement remains a challenge for LLMs, necessitating advanced methods to improve their usability and effectiveness in real-world workflows.

Abstract: Large language models (LLMs) have achieved remarkable progress in code
generation. However, existing benchmarks mainly formalize the task as a static,
single-turn problem, overlooking the stepwise requirement changes and iterative
workflows in real-world software development. This mismatch limits the
understanding of how well LLMs can support real-world development workflows.
Constructing such iterative benchmarks is challenging due to the lack of public
interaction traces and the difficulty of creating discriminative, turn-specific
test cases.
  To bridge this gap, we present SR-Eval, a benchmark specifically designed to
assess LLMs on iterative code generation under Stepwise requirements
Refinement. SR-Eval spans both function-level and repository-level tasks in
Python and Java, enabling fine-grained and progressive evaluation across
evolving requirements. The construction of SR-Eval follows a carefully designed
pipeline that first leverages a multi-agent-based requirement generation method
to simulate the development process and recover the multi-round interaction
process from final requirements, then employs a semantic-aware discriminative
test case generation component to ensure discriminative and consistent
evaluation at each turn. SR-Eval comprises 443 multi-turn tasks and 1,857
questions at both function and repository levels. Using SR-Eval, we evaluate 11
representative LLMs with three prompting strategies that simulate different
usage patterns. Results show that iterative code generation under stepwise
requirement refinement remains highly challenging: the best-performing model
achieves only 22.67% completion rate on function-level tasks and 20.00% on
repository-level tasks. We further observe that prompting strategies
substantially influence performance, highlighting the need for the development
of advanced methods.

</details>


### [433] [On the Soundness and Consistency of LLM Agents for Executing Test Cases Written in Natural Language](https://arxiv.org/abs/2509.19136)
*Sébastien Salva,Redha Taguelmimt*

Main category: cs.SE

TL;DR: The paper explores the use of natural language (NL) test cases for GUI testing, using large language models (LLMs), and proposes an algorithm for addressing unsoundness and inconsistency issues. Experimental evaluation reveals potential and current limitations.


<details>
  <summary>Details</summary>
Motivation: Enable easier and cost-effective GUI testing through NL test cases executed by LLMs, addressing challenges like unsoundness and execution inconsistency.

Method: Proposed a guardrail-augmented algorithm with specialized agents to verify NL test case execution, introduced evaluation metrics for LLM execution capability and consistency, and experimented with eight publicly available LLMs.

Result: Meta Llama 3.1 70B showed acceptable execution capabilities and high consistency (above 3-sigma level); prototype tools, test suites, and results were provided.

Conclusion: LLMs can successfully execute NL test cases but face limitations. The proposed algorithm enhances reliability for industrial settings, marking a step forward in GUI application testing.

Abstract: The use of natural language (NL) test cases for validating graphical user
interface (GUI) applications is emerging as a promising direction to manually
written executable test scripts, which are costly to develop and difficult to
maintain. Recent advances in large language models (LLMs) have opened the
possibility of the direct execution of NL test cases by LLM agents. This paper
investigates this direction, focusing on the impact on NL test case unsoundness
and on test case execution consistency. NL test cases are inherently unsound,
as they may yield false failures due to ambiguous instructions or unpredictable
agent behaviour. Furthermore, repeated executions of the same NL test case may
lead to inconsistent outcomes, undermining test reliability. To address these
challenges, we propose an algorithm for executing NL test cases with guardrail
mechanisms and specialised agents that dynamically verify the correct execution
of each test step. We introduce measures to evaluate the capabilities of LLMs
in test execution and one measure to quantify execution consistency. We propose
a definition of weak unsoundness to characterise contexts in which NL test case
execution remains acceptable, with respect to the industrial quality levels Six
Sigma. Our experimental evaluation with eight publicly available LLMs, ranging
from 3B to 70B parameters, demonstrates both the potential and current
limitations of current LLM agents for GUI testing. Our experiments show that
Meta Llama 3.1 70B demonstrates acceptable capabilities in NL test case
execution with high execution consistency (above the level 3-sigma). We provide
prototype tools, test suites, and results.

</details>


### [434] [An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications](https://arxiv.org/abs/2509.19185)
*Mohammed Mehedi Hasan,Hao Li,Emad Fallahzadeh,Gopi Krishnan Rajbahadur,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: This paper studies testing practices in FM-based AI agents, finding a focus on deterministic components and a neglect of prompts, urging developers to enhance testing support for robust AI agents.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the gaps in understanding how developers verify the internal correctness of FM-based AI agents during development due to non-determinism and non-reproducibility challenges.

Method: Conducted a large-scale empirical analysis of 39 open-source agent frameworks and 439 agentic applications to identify testing patterns and map them to AI agents’ architectural components.

Result: Identified ten testing patterns, noting that novel methods like DeepEval are rarely used (<1%), and deterministic components receive most testing effort while prompts are neglected (1% of tests).

Conclusion: The study highlights the need for improved testing approaches in FM-based AI agents by developing novel methods for non-deterministic components, adopting prompt regression testing, and exploring barriers to adoption.

Abstract: Foundation model (FM)-based AI agents are rapidly gaining adoption across
diverse domains, but their inherent non-determinism and non-reproducibility
pose testing and quality assurance challenges. While recent benchmarks provide
task-level evaluations, there is limited understanding of how developers verify
the internal correctness of these agents during development.
  To address this gap, we conduct the first large-scale empirical study of
testing practices in the AI agent ecosystem, analyzing 39 open-source agent
frameworks and 439 agentic applications. We identify ten distinct testing
patterns and find that novel, agent-specific methods like DeepEval are seldom
used (around 1%), while traditional patterns like negative and membership
testing are widely adapted to manage FM uncertainty. By mapping these patterns
to canonical architectural components of agent frameworks and agentic
applications, we uncover a fundamental inversion of testing effort:
deterministic components like Resource Artifacts (tools) and Coordination
Artifacts (workflows) consume over 70% of testing effort, while the FM-based
Plan Body receives less than 5%. Crucially, this reveals a critical blind spot,
as the Trigger component (prompts) remains neglected, appearing in around 1% of
all tests.
  Our findings offer the first empirical testing baseline in FM-based agent
frameworks and agentic applications, revealing a rational but incomplete
adaptation to non-determinism. To address it, framework developers should
improve support for novel testing methods, application developers must adopt
prompt regression testing, and researchers should explore barriers to adoption.
Strengthening these practices is vital for building more robust and dependable
AI agents.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [435] [Dynamical Modeling of Behaviorally Relevant Spatiotemporal Patterns in Neural Imaging Data](https://arxiv.org/abs/2509.18507)
*Mohammad Hosseini,Maryam M. Shanechi*

Main category: q-bio.NC

TL;DR: The paper introduces SBIND, a deep learning framework for analyzing high-dimensional neural imaging data while disentangling behaviorally relevant dynamics, demonstrating better performance in neural-behavioral prediction.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling high-dimensional neural imaging data, such as calcium and functional ultrasound imaging, which have complex spatiotemporal dependencies and include irrelevant dynamics.

Method: The authors developed SBIND, a deep learning framework designed to model spatiotemporal dependencies in neural images and disentangle behaviorally relevant neural dynamics. The model is validated on multiple imaging datasets.

Result: SBIND successfully identifies local and long-range spatial dependencies in the brain, separates behaviorally relevant from irrelevant dynamics, and outperforms existing models in neural-behavioral prediction.

Conclusion: The study reveals SBIND as an effective and versatile tool for analyzing neural imaging data, aiding in the understanding of neural mechanisms underlying behavior.

Abstract: High-dimensional imaging of neural activity, such as widefield calcium and
functional ultrasound imaging, provide a rich source of information for
understanding the relationship between brain activity and behavior. Accurately
modeling neural dynamics in these modalities is crucial for understanding this
relationship but is hindered by the high-dimensionality, complex spatiotemporal
dependencies, and prevalent behaviorally irrelevant dynamics in these
modalities. Existing dynamical models often employ preprocessing steps to
obtain low-dimensional representations from neural image modalities. However,
this process can discard behaviorally relevant information and miss
spatiotemporal structure. We propose SBIND, a novel data-driven deep learning
framework to model spatiotemporal dependencies in neural images and disentangle
their behaviorally relevant dynamics from other neural dynamics. We validate
SBIND on widefield imaging datasets, and show its extension to functional
ultrasound imaging, a recent modality whose dynamical modeling has largely
remained unexplored. We find that our model effectively identifies both local
and long-range spatial dependencies across the brain while also dissociating
behaviorally relevant neural dynamics. Doing so, SBIND outperforms existing
models in neural-behavioral prediction. Overall, SBIND provides a versatile
tool for investigating the neural mechanisms underlying behavior using imaging
modalities.

</details>


### [436] [From Noise to Insight: Visualizing Neural Dynamics with Segmented SNR Topographies for Improved EEG-BCI Performance](https://arxiv.org/abs/2509.18599)
*Eva Guttmann-Flury,Shan Zhao,Jian Zhao,Mohamad Sawan*

Main category: q-bio.NC

TL;DR: This paper proposes a mathematical framework to improve signal quality in EEG-based BCIs by using data-driven noise interval evaluation and SNR visualization and achieves notable results in recovering P300 EEG characteristics.


<details>
  <summary>Details</summary>
Motivation: Wearable EEG-based BCIs face challenges like low signal-to-noise ratio and non-stationary neural activity, necessitating improved methods for interpreting signals effectively.

Method: A rigorous framework combining data-driven noise interval evaluation with advanced SNR visualization was applied to the Eye-BCI dataset to analyze and enhance EEG signal quality.

Result: The method successfully captured canonical P300 EEG characteristics across multiple frequency bands and identified spatiotemporal patterns for P3a and P3b subcomponents, while cross-session correlations highlighted the impact of alertness on noise interval sensitivity.

Conclusion: The approach provides a foundation for improving BCI interpretability and optimization, as well as potential clinical applications in detecting neurophysiological abnormalities.

Abstract: Electroencephalography (EEG)-based wearable brain-computer interfaces (BCIs)
face challenges due to low signal-to-noise ratio (SNR) and non-stationary
neural activity. We introduce in this manuscript a mathematically rigorous
framework that combines data-driven noise interval evaluation with advanced SNR
visualization to address these limitations. Analysis of the publicly available
Eye-BCI multimodal dataset demonstrates the method's ability to recover
canonical P300 characteristics across frequency bands (delta: 0.5-4 Hz, theta:
4-7.5 Hz, broadband: 1-15 Hz), with precise spatiotemporal localization of both
P3a (frontocentral) and P3b (parietal) subcomponents. To the best of our
knowledge, this is the first study to systematically assess the impact of noise
interval selection on EEG signal quality. Cross-session correlations for four
different choices of noise intervals spanning from early to late pre-stimulus
phases also indicate that alertness and task engagement states modulate noise
interval sensitivity, suggesting broader applications for adaptive BCI systems.
While validated in healthy participants, our results represent a first step
towards providing clinicians with an interpretable tool for detecting
neurophysiological abnormalities and provides quantifiable metrics for system
optimization.

</details>


### [437] [BRAID: Input-Driven Nonlinear Dynamical Modeling of Neural-Behavioral Data](https://arxiv.org/abs/2509.18627)
*Parsa Vahidi,Omid G. Sani,Maryam M. Shanechi*

Main category: q-bio.NC

TL;DR: The paper introduces BRAID, a deep learning model that disentangles neural dynamics from external inputs, improving the understanding and predictions of neural and behavioral data.


<details>
  <summary>Details</summary>
Motivation: To address the common shortcoming of modeling neural populations as autonomous systems, neglecting the influence of external inputs on neural activity and behavioral outcomes.

Method: The proposed BRAID framework incorporates a deep learning model with an input-driven recurrent neural network and a forecasting objective. It disentangles recurrent neural dynamics from external input effects while optimizing for behavior-relevant dynamics through a multi-stage process.

Result: BRAID was validated on simulated nonlinear data and real motor cortex activity, showing improved accuracy in capturing neural-behavioral dynamics and outperforming baseline methods in forecasting.

Conclusion: BRAID effectively models the interplay between external inputs and intrinsic neural dynamics, offering a more accurate characterization of neural population activity and its relationship to behavior.

Abstract: Neural populations exhibit complex recurrent structures that drive behavior,
while continuously receiving and integrating external inputs from sensory
stimuli, upstream regions, and neurostimulation. However, neural populations
are often modeled as autonomous dynamical systems, with little consideration
given to the influence of external inputs that shape the population activity
and behavioral outcomes. Here, we introduce BRAID, a deep learning framework
that models nonlinear neural dynamics underlying behavior while explicitly
incorporating any measured external inputs. Our method disentangles intrinsic
recurrent neural population dynamics from the effects of inputs by including a
forecasting objective within input-driven recurrent neural networks. BRAID
further prioritizes the learning of intrinsic dynamics that are related to a
behavior of interest by using a multi-stage optimization scheme. We validate
BRAID with nonlinear simulations, showing that it can accurately learn the
intrinsic dynamics shared between neural and behavioral modalities. We then
apply BRAID to motor cortical activity recorded during a motor task and
demonstrate that our method more accurately fits the neural-behavioral data by
incorporating measured sensory stimuli into the model and improves the
forecasting of neural-behavioral data compared with various baseline methods,
whether input-driven or not.

</details>


### [438] [Apport de l'imagerie c{é}r{é}brale pour comprendre les d{é}ficits linguistiques](https://arxiv.org/abs/2509.18688)
*Charlotte Jacquemot,Marine Lunven*

Main category: q-bio.NC

TL;DR: The paper discusses how combining psycholinguistic and neurolinguistic approaches yields deeper insights into language processing and supports better rehabilitation strategies.


<details>
  <summary>Details</summary>
Motivation: To understand language production and comprehension by integrating cognitive and brain-based perspectives.

Method: Combining psycholinguistics with brain imaging techniques to study cognitive processes and identifying neural networks associated with language.

Result: Enhanced understanding of language mechanisms, impacts of brain lesions, and connections between psycholinguistic processes and neurolinguistics.

Conclusion: Integration of these approaches deepens knowledge of language processing and aids in developing effective rehabilitation methods.

Abstract: Psycholinguistics and neurolinguistics are two complementary disciplines that
study language from different perspectives. Psycholinguistics focuses on the
cognitive processes involved in language production and comprehension, while
neurolinguistics investigates the brain bases of these mechanisms. Brain
imaging techniques make it possible to identify the regions and networks
involved in various psycholinguistic processes, and to better understand the
effects of brain lesions on language abilities. By integrating psycholinguistic
and neurolinguistic approaches, research provides a deeper understanding of
language processing, whether intact or impaired. This knowledge paves the way
for more targeted and effective rehabilitation strategies.

</details>


### [439] [Complexity of Activity Patterns in a Bio-Inspired Hopfield-Type Network in Different Topologies](https://arxiv.org/abs/2509.18758)
*Marco Cafiso,Paolo Paradisi*

Main category: q-bio.NC

TL;DR: The study compares scale-free and random network topologies in a Hopfield-type neural network model, focusing on their activation dynamics and temporal complexity.


<details>
  <summary>Details</summary>
Motivation: The research aims to analyze neural networks' temporal dynamics, especially their metastable and self-organizing states, using temporal complexity theory, bridging the gap between biological and artificial networks.

Method: The study applied a temporal complexity analysis to Hopfield-type models and conducted a comparative assessment of scale-free and random topologies, examining their activation patterns and complexity behaviors.

Result: The scale-free topology exhibited reduced noise and consistently demonstrated complex dynamical profiles. Despite differing dynamical patterns, both topologies showed similar power-law decays and temporal complexity characteristics.

Conclusion: Scale-free networks are better suited for representing complex dynamics due to their hub structures, indicating their pivotal role in neural network behavior.

Abstract: Neural network models capable of storing memory have been extensively studied
in computer science and computational neuroscience. The Hopfield network is a
prototypical example of a model designed for associative, or
content-addressable, memory and has been analyzed in many forms. Further, ideas
and methods from complex network theory have been incorporated into artificial
neural networks and learning, emphasizing their structural properties.
Nevertheless, the temporal dynamics also play a vital role in biological neural
networks, whose temporal structure is a crucial feature to examine. Biological
neural networks display complex intermittency and, thus, can be studied through
the lens of the temporal complexity (TC) theory. The TC approach look at the
metastability of self-organized states, characterized by a power-law decay in
the inter-event time distribution and in the total activity distribution or a
scaling behavior in the corresponding event-driven diffusion processes. In this
study, we present a temporal complexity (TC) analysis of a
biologically-inspired Hopfield-type neural network model. We conducted a
comparative assessment between scale-free and random network topologies, with
particular emphasis on their global activation patterns. Our parametric
analysis revealed comparable dynamical behaviors across both neural network
architectures. Furthermore, our investigation into temporal complexity
characteristics uncovered that seemingly distinct dynamical patterns exhibit
similar temporal complexity behaviors. In particular, similar power-law decay
in the activity distribution and similar complexity levels are observed in both
topologies, but with a much reduced noise in the scale-free topology. Notably,
most of the complex dynamical profiles were consistently observed in scale-free
network configurations, thus confirming the crucial role of hubs in neural
network dynamics.

</details>


### [440] [Exploring aperiodic, complexity and entropic brain changes during non-ordinary states of consciousness](https://arxiv.org/abs/2509.19254)
*Victor Oswald,Karim Jerbi,Corine Sombrun,Hamza Abdelhedi,Annen Jitka,Charlotte Martial,Audrey Vanhaudenhuyse,Olivia Gosseries*

Main category: q-bio.NC

TL;DR: This study investigates the brain activity underlying Auto-Induced Cognitive Trance (AICT) using EEG analysis, exploring features like spectral exponents, complexity, and entropy during this non-ordinary state of consciousness.


<details>
  <summary>Details</summary>
Motivation: To understand the poorly explored neural mechanisms behind unique and perceptually rich subjective experiences like AICT, which occurs without substances.

Method: The study employed high-density EEG on 27 trained participants during rest and AICT, analyzed the power spectrum's aperiodic component, complexity, and entropy, and utilized machine learning to classify states and localize brain activity.

Result: Machine learning distinguished rest from AICT based on spectral exponents, complexity, and entropy values, with frontal, posterior cingulate, and left parietal cortex identified as key regions. Baseline activity in frontal and parietal areas predicted individual variability during the transition to AICT.

Conclusion: AICT engages specific brain regions associated with rich subjective experiences, offering insights into the neural architecture of self-induced trance states.

Abstract: Non-ordinary states of consciousness (NOC) provide an opportunity to
experience highly intense, unique, and perceptually rich subjective states. The
neural mechanisms supporting these experiences remain poorly understood. This
study examined brain activity associated with a self-induced, substance-free
NOC known as Auto-Induced Cognitive Trance (AICT). Twenty-seven trained
participants underwent high-density electroencephalography (EEG) recordings
during rest and AICT. We analyzed the aperiodic component of the power spectrum
(1/f), Lempel-Ziv complexity, and sample entropy from five-minute signal
segments. A machine learning approach was used to classify rest and AICT,
identify discriminative features, and localize their sources. We also compared
EEG metrics across conditions and assessed whether baseline activity predicted
the magnitude of change during AICT. Classification analyses revealed
condition-specific differences in spectral exponents, complexity, and entropy.
The aperiodic component showed the strongest discriminative power, followed by
entropy and complexity. Source localization highlighted frontal regions, the
posterior cingulate cortex, and the left parietal cortex as key contributors to
the AICT state. Baseline neural activity in frontal and parietal regions
predicted individual variability in the transition from rest to AICT. These
findings indicate that AICT engages brain regions implicated in rich subjective
experiences and provide mechanistic insights into how self-induced trance
states influence neural functioning.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [441] [Surrogate Modelling of Proton Dose with Monte Carlo Dropout Uncertainty Quantification](https://arxiv.org/abs/2509.18155)
*Aaron Pim,Tristan Pryer*

Main category: stat.ML

TL;DR: The paper proposes a neural surrogate model for faster, differentiable dose calculations in proton therapy, retaining uncertainty information when Monte Carlo simulations are computationally demanding.


<details>
  <summary>Details</summary>
Motivation: Monte Carlo methods for proton dose computation are accurate but too slow for repeated evaluations in adaptive workflows, robust planning, and uncertainty-aware optimization.

Method: A neural surrogate model with integrated Monte Carlo dropout is developed to predict dose distributions rapidly and provide voxelwise uncertainty estimates, validated through 1D, 2D, and 3D phantom experiments.

Result: The method provides significant speedup over Monte Carlo simulations while being scalable and accurate under different conditions, effectively separating epistemic and parametric uncertainties.

Conclusion: This approach makes fast and uncertainty-aware dose calculations feasible for practical workflows in proton therapy, addressing the computational inefficiencies of traditional methods.

Abstract: Accurate proton dose calculation using Monte Carlo (MC) is computationally
demanding in workflows like robust optimisation, adaptive replanning, and
probabilistic inference, which require repeated evaluations. To address this,
we develop a neural surrogate that integrates Monte Carlo dropout to provide
fast, differentiable dose predictions along with voxelwise predictive
uncertainty. The method is validated through a series of experiments, starting
with a one-dimensional analytic benchmark that establishes accuracy,
convergence, and variance decomposition. Two-dimensional bone-water phantoms,
generated using TOPAS Geant4, demonstrate the method's behavior under domain
heterogeneity and beam uncertainty, while a three-dimensional water phantom
confirms scalability for volumetric dose prediction. Across these settings, we
separate epistemic (model) from parametric (input) contributions, showing that
epistemic variance increases under distribution shift, while parametric
variance dominates at material boundaries. The approach achieves significant
speedups over MC while retaining uncertainty information, making it suitable
for integration into robust planning, adaptive workflows, and uncertainty-aware
optimisation in proton therapy.

</details>


### [442] [Statistical Insight into Meta-Learning via Predictor Subspace Characterization and Quantification of Task Diversity](https://arxiv.org/abs/2509.18349)
*Saptati Datta,Nicolas W. Hengartner,Yulia Pimonova,Natalie E. Klein,Nicholas Lubbers*

Main category: stat.ML

TL;DR: The paper introduces a statistical framework for analyzing meta-learning, focusing on latent subspaces and task diversity.


<details>
  <summary>Details</summary>
Motivation: Meta-learning relies on shared information across tasks to improve predictive accuracy, but understanding the factors influencing its success is underexplored.

Method: The authors model shared structures using a latent subspace and define a diversity measure based on heterogeneity in task-specific predictors.

Result: They show, through simulations and theory, that prediction accuracy depends on the proportion of variance in alignment with the shared subspace and the precision of subspace estimation.

Conclusion: The alignment of predictor variance with shared structures and accurate subspace estimation are crucial for effective meta-learning.

Abstract: Meta-learning has emerged as a powerful paradigm for leveraging information
across related tasks to improve predictive performance on new tasks. In this
paper, we propose a statistical framework for analyzing meta-learning through
the lens of predictor subspace characterization and quantification of task
diversity. Specifically, we model the shared structure across tasks using a
latent subspace and introduce a measure of diversity that captures
heterogeneity across task-specific predictors. We provide both simulation-based
and theoretical evidence indicating that achieving the desired prediction
accuracy in meta-learning depends on the proportion of predictor variance
aligned with the shared subspace, as well as on the accuracy of subspace
estimation.

</details>


### [443] [End-Cut Preference in Survival Trees](https://arxiv.org/abs/2509.18477)
*Xiaogang Su*

Main category: stat.ML

TL;DR: The paper addresses the end-cut preference (ECP) issue in CART decision trees, proposing a smooth sigmoid surrogate (SSS) method as an effective solution for mitigating ECP.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the well-documented issue of end-cut preference (ECP), which leads to biased and unstable splits in decision trees, especially in survival analysis.

Method: The proposed method replaces the hard-threshold indicator function in CART with a smooth sigmoid function to avoid end-cut preference during greedy search for split points.

Result: The paper demonstrates, both theoretically and numerically, that the smooth sigmoid surrogate (SSS) effectively mitigates or avoids ECP.

Conclusion: The proposed SSS approach is a robust solution for handling ECP and improving the stability and interpretability of survival trees.

Abstract: The end-cut preference (ECP) problem, referring to the tendency to favor
split points near the boundaries of a feature's range, is a well-known issue in
CART (Breiman et al., 1984). ECP may induce highly imbalanced and biased
splits, obscure weak signals, and lead to tree structures that are both
unstable and difficult to interpret. For survival trees, we show that ECP also
arises when using greedy search to select the optimal cutoff point by
maximizing the log-rank test statistic. To address this issue, we propose a
smooth sigmoid surrogate (SSS) approach, in which the hard-threshold indicator
function is replaced by a smooth sigmoid function. We further demonstrate, both
theoretically and through numerical illustrations, that SSS provides an
effective remedy for mitigating or avoiding ECP.

</details>


### [444] [Estimating Heterogeneous Causal Effect on Networks via Orthogonal Learning](https://arxiv.org/abs/2509.18484)
*Yuanchen Wu,Yubai Yuan*

Main category: stat.ML

TL;DR: The paper introduces a two-stage method for estimating causal effects on networks, addressing challenges from heterogeneous effects and network confounding.


<details>
  <summary>Details</summary>
Motivation: Traditional causal effect estimation methods struggle with network settings due to interference and the heterogeneous impact of an intervention. This paper aims to refine such estimations by addressing these challenges.

Method: The study proposes a two-stage approach. Stage one uses graph neural networks to estimate network-dependent components. The second stage utilizes an attention-based interference model for causal inference, integrating both stages with Neyman orthogonalization and cross-fitting.

Result: The method effectively estimates heterogeneous direct and spillover effects on networks while minimizing bias and misspecification issues.

Conclusion: This method makes causal effect estimation on networks more robust and interpretable, facilitating tasks like identifying influential areas and understanding spillover impact.

Abstract: Estimating causal effects on networks is important for both scientific
research and practical applications. Unlike traditional settings that assume
the Stable Unit Treatment Value Assumption (SUTVA), interference allows an
intervention/treatment on one unit to affect the outcomes of others.
Understanding both direct and spillover effects is critical in fields such as
epidemiology, political science, and economics. Causal inference on networks
faces two main challenges. First, causal effects are typically heterogeneous,
varying with unit features and local network structure. Second, connected units
often exhibit dependence due to network homophily, creating confounding between
structural correlations and causal effects. In this paper, we propose a
two-stage method to estimate heterogeneous direct and spillover effects on
networks. The first stage uses graph neural networks to estimate nuisance
components that depend on the complex network topology. In the second stage, we
adjust for network confounding using these estimates and infer causal effects
through a novel attention-based interference model. Our approach balances
expressiveness and interpretability, enabling downstream tasks such as
identifying influential neighborhoods and recovering the sign of spillover
effects. We integrate the two stages using Neyman orthogonalization and
cross-fitting, which ensures that errors from nuisance estimation contribute
only at higher order. As a result, our causal effect estimates are robust to
bias and misspecification in modeling causal effects under network
dependencies.

</details>


### [445] [Consistency of Selection Strategies for Fraud Detection](https://arxiv.org/abs/2509.18739)
*Christos Revelas,Otilia Boldea,Bas J. M. Werker*

Main category: stat.ML

TL;DR: The paper critiques traditional insurer fraud detection methods, which focus only on claims likely to be fraudulent, for being inconsistent, and proposes a randomized alternative to ensure more reliable learning by borrowing insights from the multi-arm bandit problem.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to enhance the consistency and reliability of methods insurers use to detect fraud, which often rely on biased data by only investigating claims with the highest predicted fraud probabilities.

Method: The approach involves formalizing fraud claim selection in a binary regression framework, proposing randomization to counteract bias, and drawing parallels with the multi-arm bandit problem to develop a consistent model for updating and estimating parameters.

Result: Simulations indicate that the standard strategy of investigating highly likely fraudulent claims can result in inconsistent learning, whereas the proposed randomized approach is consistent. Thompson sampling, a common heuristic, is shown to be inefficient for detecting low fraud probabilities.

Conclusion: Relying solely on high-probability fraud claims may lead to biased and inconsistent models. The authors recommend randomized selection strategies to ensure consistent learning and more effective fraud detection.

Abstract: This paper studies how insurers can chose which claims to investigate for
fraud. Given a prediction model, typically only claims with the highest
predicted propability of being fraudulent are investigated. We argue that this
can lead to inconsistent learning and propose a randomized alternative. More
generally, we draw a parallel with the multi-arm bandit literature and argue
that, in the presence of selection, the obtained observations are not iid.
Hence, dependence on past observations should be accounted for when updating
parameter estimates. We formalize selection in a binary regression framework
and show that model updating and maximum-likelihood estimation can be
implemented as if claims were investigated at random. Then, we define
consistency of selection strategies and conjecture sufficient conditions for
consistency. Our simulations suggest that the often-used selection strategy can
be inconsistent while the proposed randomized alternative is consistent.
Finally, we compare our randomized selection strategy with Thompson sampling, a
standard multi-arm bandit heuristic. Our simulations suggest that the latter
can be inefficient in learning low fraud probabilities.

</details>


### [446] [Neighbor Embeddings Using Unbalanced Optimal Transport Metrics](https://arxiv.org/abs/2509.19226)
*Muhammad Rana,Keaton Hamm*

Main category: stat.ML

TL;DR: The paper introduces the Hellinger-Kantorovich metric from unbalanced optimal transport (UOT) into dimensionality reduction and learning pipelines, demonstrating its superior performance over alternatives like regular optimal transport (OT) and Euclidean methods.


<details>
  <summary>Details</summary>
Motivation: Explore the potential of UOT metrics to address challenges in dimensionality reduction and learning tasks, overcoming limitations of existing methods like OT and Euclidean metrics.

Method: Employ the Hellinger-Kantorovich metric for dimensionality reduction, and evaluate its efficacy through performance comparison on datasets including MedMNIST using supervised and unsupervised learning approaches.

Result: Experimental results show UOT generally outperforms OT and Euclidean methods, with notable success on MedMNIST classification tasks (81%) and clustering tasks (83%) statistically validated.

Conclusion: The Hellinger-Kantorovich metric enhances dimensionality reduction and learning across various settings, providing a robust alternative to the current OT and Euclidean approaches.

Abstract: This paper proposes the use of the Hellinger--Kantorovich metric from
unbalanced optimal transport (UOT) in a dimensionality reduction and learning
(supervised and unsupervised) pipeline. The performance of UOT is compared to
that of regular OT and Euclidean-based dimensionality reduction methods on
several benchmark datasets including MedMNIST. The experimental results
demonstrate that, on average, UOT shows improvement over both Euclidean and
OT-based methods as verified by statistical hypothesis tests. In particular, on
the MedMNIST datasets, UOT outperforms OT in classification 81\% of the time.
For clustering MedMNIST, UOT outperforms OT 83\% of the time and outperforms
both other metrics 58\% of the time.

</details>


### [447] [Recovering Wasserstein Distance Matrices from Few Measurements](https://arxiv.org/abs/2509.19250)
*Muhammad Rana,Abiy Tasissa,HanQin Cai,Yakov Gavriyelov,Keaton Hamm*

Main category: stat.ML

TL;DR: The paper presents two algorithms for estimating square Wasserstein distance matrices efficiently, enabling manifold embeddings such as MDS or Isomap with fewer computations.


<details>
  <summary>Details</summary>
Motivation: The computation of square Wasserstein distance matrices is computationally expensive, yet crucial for manifold learning techniques like MDS and Isomap.

Method: The paper analyzes matrix completion using upper triangular samples and introduces a Nyström completion approach that computes only a fraction of the matrix columns, integrating theoretical stability analysis.

Result: The Nyström approach was demonstrated to be stable, capable of outperforming matrix completion approaches under fixed sampling constraints, and effective in a classification task with reduced data.

Conclusion: The Nyström completion method is an effective, stable, and computationally efficient approach for estimating Wasserstein distance matrices, validated through theoretical and experimental benchmarks.

Abstract: This paper proposes two algorithms for estimating square Wasserstein distance
matrices from a small number of entries. These matrices are used to compute
manifold learning embeddings like multidimensional scaling (MDS) or Isomap, but
contrary to Euclidean distance matrices, are extremely costly to compute. We
analyze matrix completion from upper triangular samples and Nystr\"{o}m
completion in which $\mathcal{O}(d\log(d))$ columns of the distance matrices
are computed where $d$ is the desired embedding dimension, prove stability of
MDS under Nystr\"{o}m completion, and show that it can outperform matrix
completion for a fixed budget of sample distances. Finally, we show that
classification of the OrganCMNIST dataset from the MedMNIST benchmark is stable
on data embedded from the Nystr\"{o}m estimation of the distance matrix even
when only 10\% of the columns are computed.

</details>


### [448] [A Gradient Flow Approach to Solving Inverse Problems with Latent Diffusion Models](https://arxiv.org/abs/2509.19276)
*Tim Y. J. Wang,O. Deniz Akyildiz*

Main category: stat.ML

TL;DR: This study introduces Diffusion-regularized Wasserstein Gradient Flow (DWGF), a novel approach utilizing pretrained latent diffusion models for solving ill-posed inverse problems.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of effectively solving ill-posed inverse problems by leveraging powerful and flexible priors from pretrained models.

Method: It formulates the posterior sampling problem as a regularized Wasserstein gradient flow applied to the Kullback-Leibler divergence in the latent space, utilizing StableDiffusion as the prior.

Result: Experiments on standard benchmarks demonstrate the successful performance of DWGF using pretrained latent diffusion models.

Conclusion: The approach shows promise in efficiently solving ill-posed inverse problems while leveraging the flexibility of pretrained latent diffusion models without the need for additional training.

Abstract: Solving ill-posed inverse problems requires powerful and flexible priors. We
propose leveraging pretrained latent diffusion models for this task through a
new training-free approach, termed Diffusion-regularized Wasserstein Gradient
Flow (DWGF). Specifically, we formulate the posterior sampling problem as a
regularized Wasserstein gradient flow of the Kullback-Leibler divergence in the
latent space. We demonstrate the performance of our method on standard
benchmarks using StableDiffusion (Rombach et al., 2022) as the prior.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [449] [CPCLDETECTOR: Knowledge Enhancement and Alignment Selection for Chinese Patronizing and Condescending Language Detection](https://arxiv.org/abs/2509.18562)
*Jiaxun Yang,Yifei Han,Long Zhang,Liu Yujie,Bin Li,Bo Gao,Yangfan He,Kejia Zhan*

Main category: cs.MM

TL;DR: The paper introduces the CPCLDetector model and a new dataset (PCLMMPLUS) to improve the detection of discriminatory toxic speech in Chinese video platforms.


<details>
  <summary>Details</summary>
Motivation: Existing CPCL detection methods fail to account for user comments, leading to lower accuracy in identifying CPLC videos.

Method: The researchers created a larger dataset with 103k comments (PCLMMPLUS) and developed the CPCLDetector model utilizing alignment selection and knowledge-enhanced modules.

Result: CPCLDetector outperformed state-of-the-art models on both the original dataset (PCLMM) and the new dataset (PCLMMPLUS).

Conclusion: The proposed solutions significantly improve CPLC video detection, aiding in content governance and protection of vulnerable groups.

Abstract: Chinese Patronizing and Condescending Language (CPCL) is an implicitly
discriminatory toxic speech targeting vulnerable groups on Chinese video
platforms. The existing dataset lacks user comments, which are a direct
reflection of video content. This undermines the model's understanding of video
content and results in the failure to detect some CPLC videos. To make up for
this loss, this research reconstructs a new dataset PCLMMPLUS that includes
103k comment entries and expands the dataset size. We also propose the
CPCLDetector model with alignment selection and knowledge-enhanced comment
content modules. Extensive experiments show the proposed CPCLDetector
outperforms the SOTA on PCLMM and achieves higher performance on PCLMMPLUS .
CPLC videos are detected more accurately, supporting content governance and
protecting vulnerable groups. Code and dataset are available at
https://github.com/jiaxunyang256/PCLD.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [450] [CayleyPy Growth: Efficient growth computations and hundreds of new conjectures on Cayley graphs (Brief version)](https://arxiv.org/abs/2509.19162)
*A. Chervov,D. Fedoriaka,E. Konstantinova,A. Naumov,I. Kiselev,A. Sheveleva,I. Koltsov,S. Lytkin,A. Smolensky,A. Soibelman,F. Levkovich-Maslyuk,R. Grimov,D. Volovich,A. Isakov,A. Kostin,M. Litvinov,N. Vilkin-Krom,A. Bidzhiev,A. Krasnyi,M. Evseev,E. Geraseva,L. Grunwald,S. Galkin,E. Koldunov,S. Diner,A. Chevychelov,E. Kudasheva,A. Sychev,A. Kravchenko,Z. Kogan,A. Natyrova,L. Shishina,L. Cheldieva,V. Zamkovoy,D. Kovalenko,O. Papulov,S. Kudashev,D. Shiltsov,R. Turtayev,O. Nikitina,D. Mamayeva,S. Nikolenko,M. Obozov,A. Titarenko,A. Dolgorukova,A. Aparnev,O. Debeaupuis,S. Alami C.,H. Isambert*

Main category: math.CO

TL;DR: The third paper of CayleyPy introduces an open-source Python library for analyzing Cayley and Schreier graphs, outperforming GAP and Sage in computational speed and scalability.


<details>
  <summary>Details</summary>
Motivation: This paper aims to enhance computational methods in group theory, solving challenges related to Cayley and Schreier graphs through improved efficiency and scalability.

Method: The authors develop CayleyPy, a Python library for handling group theory computations, focusing on speed, scalability, and generating new conjectures through experiments.

Result: CayleyPy reveals 200 new conjectures related to Cayley and Schreier graphs, provides specific diameter bounds, and creates datasets for benchmarking pathfinding algorithms.

Conclusion: CayleyPy significantly advances computational group theory, offering insights into graph diameters and growth, refining older conjectures, and promoting efficiency in solving graph-related problems.

Abstract: This is the third paper of the CayleyPy project applying artificial
intelligence to problems in group theory. We announce the first public release
of CayleyPy, an open source Python library for computations with Cayley and
Schreier graphs. Compared with systems such as GAP and Sage, CayleyPy handles
much larger graphs and performs several orders of magnitude faster.
  Using CayleyPy we obtained about 200 new conjectures on Cayley and Schreier
graphs, focused on diameters and growth. For many Cayley graphs of symmetric
groups Sn we observe quasi polynomial diameter formulas: a small set of
quadratic or linear polynomials indexed by n mod s. We conjecture that this is
a general phenomenon, giving efficient diameter computation despite the problem
being NP hard. We propose a refinement of the Babai type conjecture on
diameters of Sn: n^2/2 + 4n upper bounds in the undirected case, compared to
previous O(n^2) bounds. We also provide explicit generator families, related to
involutions in a square with whiskers pattern, conjectured to maximize the
diameter; search confirms this for all n up to 15. We further conjecture an
answer to a question posed by V M Glushkov in 1968 on directed Cayley graphs
generated by a cyclic shift and a transposition.
  For nilpotent groups we conjecture an improvement of J S Ellenberg's results
on upper unitriangular matrices over Z/pZ, showing linear dependence of
diameter on p. Moreover.
  Some conjectures are LLM friendly, naturally stated as sorting problems
verifiable by algorithms or Python code. To benchmark path finding we created
more than 10 Kaggle datasets. CayleyPy works with arbitrary permutation or
matrix groups and includes over 100 predefined generators. Our growth
computation code outperforms GAP and Sage up to 1000 times in speed and size.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [451] [Machine learning approach to single-shot multiparameter estimation for the non-linear Schrödinger equation](https://arxiv.org/abs/2509.18479)
*Louis Rossignol,Tangui Aladjidi,Myrann Baker-Rasooli,Quentin Glorieux*

Main category: quant-ph

TL;DR: The study develops a neural network-based approach to accurately estimate parameters of the nonlinear Schrödinger equation (NLSE) from single-shot field measurements.


<details>
  <summary>Details</summary>
Motivation: Parameter estimation in the NLSE, a key model for wave dynamics in nonlinear media, is challenging due to parameter correlations and a lack of reliable methods for single-shot measurement analysis.

Method: The authors train a neural network, using the ConvNeXt architecture and multivariate Gaussian negative log-likelihood loss, to invert the NLSE mapping. The training involves 100,000 simulated images of field data.

Result: The model successfully estimates three key parameters ($n_2$, $I_{sat}$, and $α$) with a mean absolute error of 3.22% on 12,500 unseen test samples, showing strong generalization and accuracy.

Conclusion: This machine learning framework offers an efficient and accurate method for NLSE parameter estimation, bridging theoretical models and experimental data while showing promise for real-world noise inclusion.

Abstract: The nonlinear Schr\"odinger equation (NLSE) is a fundamental model for wave
dynamics in nonlinear media ranging from optical fibers to Bose-Einstein
condensates. Accurately estimating its parameters, which are often strongly
correlated, from a single measurement remains a significant challenge. We
address this problem by treating parameter estimation as an inverse problem and
training a neural network to invert the NLSE mapping. We combine a fast
numerical solver with a machine learning approach based on the ConvNeXt
architecture and a multivariate Gaussian negative log-likelihood loss function.
From single-shot field (density and phase) images, our model estimates three
key parameters: the nonlinear coefficient $n_2$, the saturation intensity
$I_{sat}$, and the linear absorption coefficient $\alpha$. Trained on 100,000
simulated images, the model achieves a mean absolute error of $3.22\%$ on
12,500 unseen test samples, demonstrating strong generalization and close
agreement with ground-truth values. This approach provides an efficient route
for characterizing nonlinear systems and has the potential to bridge
theoretical modeling and experimental data when realistic noise is
incorporated.

</details>


### [452] [Quantum Random Synthetic Skyrmion Texture Generation, a Qiskit Simulation](https://arxiv.org/abs/2509.18947)
*Hillol Biswas*

Main category: quant-ph

TL;DR: The paper investigates the synthetic generation of skyrmion textures using quantum computing and explores their use as qubits due to their topological protection and quantized helicity.


<details>
  <summary>Details</summary>
Motivation: To explore skyrmions' potential applications as qubits and study their synthetic generation through quantum computing, which could push forward skyrmion-based research and applications.

Method: The study utilizes quantum computing to synthetically generate and analyze hundreds of distinct skyrmion textures, comparing different types based on quantum randomness and other criteria.

Result: The generation of diverse skyrmion textures through quantum computing suggests potential avenues for topological spin structures in research and quantum-based applications.

Conclusion: Skyrmion textures show promising potential for quantum information purposes, and synthetic generation via quantum computing provides a novel direction for further exploration of their applications.

Abstract: An integer winding, i.e., topological charge, is a characteristic of
skyrmions, which are topologically nontrivial spin patterns in magnets. They
emerge when smooth two-dimensional spin configurations are stabilized by
conflicting interactions such as exchange, anisotropy, the
Dzyaloshinskii-Moriya interaction, or geometric frustration. These nanoscale
textures, which are typically a few to tens of nanometers in size, are strong
'particle-like' excitations because they are shielded by energy barriers
connected to their topology. By exploiting their helicity, i.e., spin rotation
angle or associated internal modes, as a two-level system, skyrmions can
function as quantum bits or qubits. Two quantized helicity states of a
nanometer-scale skyrmion encode the logical value states in a 'skyrmion qubit.'
Interestingly, skyrmion qubits are topologically protected and macroscopic,
i.e., they involve a large number of spins; however, external influences can
still affect them. When the texture is tiny and disconnected, the helicity
angle of the skyrmion becomes quantized. A qubit basis is made up of the lowest
two energy eigenstates, i.e., symmetric or antisymmetric superpositions of
opposite helicity, for example. Therefore, Skyrmion textures can provide
valuable insights for different purposes. However, is it possible to
synthetically generate skyrmion textures using quantum computing? This paper
investigates the possibility and generates a few hundred different textures,
producing sample comparisons from various types, which indicate a novel
direction for skyrmion-based research based on quantum randomness and other
criteria.

</details>


### [453] [Re-uploading quantum data: A universal function approximator for quantum inputs](https://arxiv.org/abs/2509.18530)
*Hyunho Cha,Daniel K. Park,Jungwoo Lee*

Main category: quant-ph

TL;DR: The paper explores quantum data re-uploading for quantum inputs using qubit interactions to approximate functions, enabling efficient quantum machine learning architectures.


<details>
  <summary>Details</summary>
Motivation: To extend the concept of data re-uploading, successful in classical inputs, to quantum inputs where information isn't directly accessible in classical form.

Method: Developed a quantum data re-uploading circuit where a single qubit sequentially interacts with fresh copies of quantum states, utilizing entangling unitaries and mid-circuit resets.

Result: The proposed architecture can approximate any bounded continuous function using minimal resources: one ancilla qubit and single-qubit measurements.

Conclusion: The framework offers a resource-efficient and expressive approach for creating quantum machine learning models to operate directly on quantum data.

Abstract: Quantum data re-uploading has proved powerful for classical inputs, where
repeatedly encoding features into a small circuit yields universal function
approximation. Extending this idea to quantum inputs remains underexplored, as
the information contained in a quantum state is not directly accessible in
classical form. We propose and analyze a quantum data re-uploading architecture
in which a qubit interacts sequentially with fresh copies of an arbitrary input
state. The circuit can approximate any bounded continuous function using only
one ancilla qubit and single-qubit measurements. By alternating entangling
unitaries with mid-circuit resets of the input register, the architecture
realizes a discrete cascade of completely positive and trace-preserving maps,
analogous to collision models in open quantum system dynamics. Our framework
provides a qubit-efficient and expressive approach to designing quantum machine
learning models that operate directly on quantum data.

</details>


### [454] [Scalable bayesian shadow tomography for quantum property estimation with set transformers](https://arxiv.org/abs/2509.18674)
*Hyunho Cha,Wonjung Kim,Jungwoo Lee*

Main category: quant-ph

TL;DR: This paper introduces a scalable Bayesian framework integrating classical shadows and transformers for estimating quantum state properties, improving accuracy over existing estimators.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to estimate scalar properties of quantum states efficiently, without requiring full density matrix reconstruction, and to improve accuracy and scalability in such estimations.

Method: The authors propose combining classical shadows protocol with a permutation-invariant transformer network that corrects bias in estimators while encoding measurement outcomes into feature vectors for improved accuracy.

Result: For tasks like fidelity and entropy estimation using quantum measurements, the approach consistently outperformed classical shadows, achieving over 99% error reduction in limited sample scenarios.

Conclusion: The proposed Bayesian framework is a robust and scalable method for quantum state property estimation that significantly improves accuracy in challenging measurement regimes.

Abstract: A scalable Bayesian machine learning framework is introduced for estimating
scalar properties of an unknown quantum state from measurement data, which
bypasses full density matrix reconstruction. This work is the first to
integrate the classical shadows protocol with a permutation-invariant set
transformer architecture, enabling the approach to predict and correct bias in
existing estimators to approximate the true Bayesian posterior mean.
Measurement outcomes are encoded as fixed-dimensional feature vectors, and the
network outputs a residual correction to a baseline estimator. Scalability to
large quantum systems is ensured by the polynomial dependence of input size on
system size and number of measurements. On Greenberger-Horne-Zeilinger state
fidelity and second-order R\'enyi entropy estimation tasks -- using random
Pauli and random Clifford measurements -- this Bayesian estimator always
achieves lower mean squared error than classical shadows alone, with more than
a 99\% reduction in the few copy regime.

</details>


### [455] [Quantum Annealing for Minimum Bisection Problem: A Machine Learning-based Approach for Penalty Parameter Tuning](https://arxiv.org/abs/2509.19005)
*Renáta Rusnáková,Martin Chovanec,Juraj Gazda*

Main category: quant-ph

TL;DR: This paper explores using D-Wave's quantum annealing for the Minimum Bisection Problem, introducing a machine learning-based method to adaptively tune penalty parameters for better outcomes.


<details>
  <summary>Details</summary>
Motivation: The Minimum Bisection Problem is NP-hard and has applications in areas like parallel computing, but current optimization methods face challenges in quality and constraint satisfaction.

Method: A Quadratic Unconstrained Binary Optimization model was employed, with an adaptive penalty parameter tuning approach using a Gradient Boosting Regressor trained on graph features.

Result: The proposed method tested on Erdős-Rényi graphs (up to 4,000 nodes) outperformed classical methods like Metis and Kernighan-Lin, improving the quantum solver's performance.

Conclusion: The adaptive tuning strategy enhances quantum annealing results, offering a promising alternative for graph partitioning compared to classical methods.

Abstract: The Minimum Bisection Problem is a well-known NP-hard problem in
combinatorial optimization, with practical applications in areas such as
parallel computing, network design, and machine learning. In this paper, we
examine the potential of using D-Wave Systems' quantum annealing solvers to
solve the Minimum Bisection Problem, which we formulate as a Quadratic
Unconstrained Binary Optimization model. A key challenge in this formulation
lies in choosing an appropriate penalty parameter, as it plays a crucial role
in ensuring both the quality of the solution and the satisfaction of the
problem's constraints. To address this, we introduce a novel machine
learning-based approach for adaptive tuning of the penalty parameter.
Specifically, we use a Gradient Boosting Regressor model trained to predict
suitable penalty parameter values based on structural properties of the input
graph, the number of nodes and the graph's density. This method enables the
penalty parameter to be adjusted dynamically for each specific problem
instance, improving the solver's ability to balance the competing goals of
minimizing the cut size and maintaining equally sized partitions. We test our
approach on a large dataset of randomly generated Erd\H{o}s-R\'enyi graphs with
up to 4,000 nodes, and we compare the results with classical partitioning
algorithms, Metis and Kernighan-Lin. Experimental findings demonstrate that our
adaptive tuning strategy significantly improves the performance of the quantum
annealing hybrid solver and consistently outperforms the classical methods
used, indicating its potential as an alternative for the graph partitioning
problem.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [456] [Weight Mapping Properties of a Dual Tree Single Clock Adiabatic Capacitive Neuron](https://arxiv.org/abs/2509.18143)
*Mike Smart,Sachin Maheshwari,Himadri Singh Raghav,Alexander Serb*

Main category: cs.ET

TL;DR: This paper proposes an optimal methodology for mapping artificial neuron weights to energy-efficient capacitance values in dual tree single clock adiabatic circuits.


<details>
  <summary>Details</summary>
Motivation: Improving energy efficiency in analog IC designs for artificial neural networks.

Method: Explores weight mapping complexities using TensorFlow and Larq frameworks while presenting optimal mapping methodology.

Result: Demonstrated functional equivalency for trained networks and assessed impact on practical IC factors like floor space and decision efficacy.

Conclusion: Optimal AN to ACN mappings enhance chip sizes and classification accuracy for practical deployment.

Abstract: Dual Tree Single Clock (DTSC) Adiabatic Capacitive Neuron (ACN) circuits
offer the potential for highly energy-efficient Artificial Neural Network (ANN)
computation in full custom analog IC designs. The efficient mapping of
Artificial Neuron (AN) abstract weights, extracted from the software-trained
ANNs, onto physical ACN capacitance values has, however, yet to be fully
researched. In this paper, we explore the unexpected hidden complexities,
challenges and properties of the mapping, as well as, the ramifications for IC
designers in terms accuracy, design and implementation. We propose an optimal,
AN to ACN methodology, that promotes smaller chip sizes and improved overall
classification accuracy, necessary for successful practical deployment. Using
TensorFlow and Larq software frameworks, we train three different ANN networks
and map their weights into the energy-efficient DTSC ACN capacitance value
domain to demonstrate 100% functional equivalency. Finally, we delve into the
impact of weight quantization on ACN performance using novel metrics related to
practical IC considerations, such as IC floor space and comparator
decision-making efficacy.

</details>


### [457] [Energy-convergence trade off for the training of neural networks on bio-inspired hardware](https://arxiv.org/abs/2509.18121)
*Nikhil Garg,Paul Uriarte Vicandi,Yanming Zhang,Alexandre Baigol,Donato Francesco Falcone,Saketh Ram Mamidala,Bert Jan Offrein,Laura Bégon-Lours*

Main category: cs.ET

TL;DR: This paper explores ferroelectric synaptic devices for improving ultra-low power, on-chip neural network learning, studying energy and accuracy trade-offs.


<details>
  <summary>Details</summary>
Motivation: To address the growing need for energy-efficient AI processing at the extreme edge due to wearable and implantable device growth.

Method: The study uses ferroelectric devices, runs hardware-aware simulations, and develops a 'symmetry point shifting' technique to counter accuracy losses.

Result: Shorter pulse programming reduces energy usage while maintaining accuracy, though it requires more epochs. Mixed-precision SGD enhances overall accuracy.

Conclusion: Optimized programming and tailored training enable significant gains in energy efficiency without compromising learning accuracy on edge devices.

Abstract: The increasing deployment of wearable sensors and implantable devices is
shifting AI processing demands to the extreme edge, necessitating ultra-low
power for continuous operation. Inspired by the brain, emerging memristive
devices promise to accelerate neural network training by eliminating costly
data transfers between compute and memory. Though, balancing performance and
energy efficiency remains a challenge. We investigate ferroelectric synaptic
devices based on HfO2/ZrO2 superlattices and feed their experimentally measured
weight updates into hardware-aware neural network simulations. Across pulse
widths from 20 ns to 0.2 ms, shorter pulses lower per-update energy but require
more training epochs while still reducing total energy without sacrificing
accuracy. Classification accuracy using plain stochastic gradient descent (SGD)
is diminished compared to mixed-precision SGD. We analyze the causes and
propose a ``symmetry point shifting'' technique, addressing asymmetric updates
and restoring accuracy. These results highlight a trade-off among accuracy,
convergence speed, and energy use, showing that short-pulse programming with
tailored training significantly enhances on-chip learning efficiency.

</details>


### [458] [Integrating Stacked Intelligent Metasurfaces and Power Control for Dynamic Edge Inference via Over-The-Air Neural Networks](https://arxiv.org/abs/2509.18906)
*Kyriakos Stylianopoulos,George C. Alexandropoulos*

Main category: cs.ET

TL;DR: This paper proposes an innovative framework using Stacked Intelligent Metasurfaces to enable the wireless channel itself to perform computation, thereby minimizing computational and communication overhead.


<details>
  <summary>Details</summary>
Motivation: Current Edge Inference systems treat wireless channels as noise, leading to inefficiencies in computation and communication.

Method: The study models the transmitter-channel-receiver system as an end-to-end Deep Neural Network (DNN), where SIM responses are trainable parameters, and employs a dedicated DNN to adjust transmission power using user location data.

Result: Performance evaluations demonstrate improved energy efficiency and a balance between classification accuracy and power consumption using metasurfaces-integrated DNN architecture.

Conclusion: Integrating Stacked Intelligent Metasurfaces with DNN provides a transformative solution to Edge Inference, enhancing both efficiency and accuracy in wireless systems.

Abstract: This paper introduces a novel framework for Edge Inference (EI) that bypasses
the conventional practice of treating the wireless channel as noise. We utilize
Stacked Intelligent Metasurfaces (SIMs) to control wireless propagation,
enabling the channel itself to perform over-the-air computation. This
eliminates the need for symbol estimation at the receiver, significantly
reducing computational and communication overhead. Our approach models the
transmitter-channel-receiver system as an end-to-end Deep Neural Network (DNN)
where the response of the SIM elements are trainable parameters. To address
channel variability, we incorporate a dedicated DNN module responsible for
dynamically adjusting transmission power leveraging user location information.
Our performance evaluations showcase that the proposed metasurfaces-integrated
DNN framework with deep SIM architectures are capable of balancing
classification accuracy and power consumption under diverse scenarios, offering
significant energy efficiency improvements.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [459] [Pay More Attention To Audio: Mitigating Imbalance of Cross-Modal Attention in Large Audio Language Models](https://arxiv.org/abs/2509.18816)
*Junyu Wang,Ziyang Ma,Zhengding Luo,Tianrui Wang,Meng Ge,Xiaobao Wang,Longbiao Wang*

Main category: cs.SD

TL;DR: The paper introduces MATA, a method to reduce audio-textual attention imbalance in large audio-language models, improving their audio reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Large audio-language models often prioritize text over audio signals, leading to suboptimal performance in audio reasoning tasks.

Method: MATA dynamically adjusts self-attention to focus more on audio tokens, targeting the last token without adding parameters or computational cost.

Result: Experiments on MMAU and MMAR benchmarks show consistent performance improvements, outperforming a proprietary model on MMAR for the first time.

Conclusion: MATA effectively mitigates attention bias, enhancing audio reasoning capabilities and paving the way for further research in multi-modal model optimization.

Abstract: Large Audio-Language Models (LALMs) often suffer from audio-textual attention
imbalance, prioritizing text over acoustic information, particularly in the
multi-modal fusion layers of the Transformer architecture. This bias hinders
their ability to fully utilize acoustic cues, causing suboptimal performance on
audio reasoning tasks. To mitigate this, we propose \textbf{MATA}, a novel
training-free method that dynamically pushes LALMs to pay \textbf{M}ore
\textbf{A}ttention \textbf{T}o \textbf{A}udio tokens within the self-attention
mechanism. Specifically, MATA intervenes post raw attention scoring, targeting
only the last token in intermediate layers without introducing additional
parameters or computational overhead. Experiments on the MMAU and MMAR
benchmarks confirm MATA's effectiveness, with consistent performance gains.
Notably, on MMAR, MATA enables an open-source model to surpass the proprietary
Gemini 2.0 Flash for the first time. Our work provides an efficient solution to
mitigate attention bias and opens a new research direction for enhancing the
audio-processing capabilities of multi-modal models.

</details>


### [460] [Finding My Voice: Generative Reconstruction of Disordered Speech for Automated Clinical Evaluation](https://arxiv.org/abs/2509.19231)
*Karen Rosero,Eunjung Yeo,David R. Mortensen,Cortney Van't Slot,Rami R. Hallac,Carlos Busso*

Main category: cs.SD

TL;DR: ChiReSSD is introduced as a framework that reconstructs children’s speech, improves mispronunciation correction, and preserves speaker identity. It generalizes well to adult dysarthric speech.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for speech reconstruction methods that specifically focus on children with speech sound disorders (SSD), an area largely underexplored due to prior research being centered on healthy adults.

Method: ChiReSSD adapts a text-to-speech (TTS) framework emphasizing disentangled, style-based reconstruction with a focus on pitch and prosody. Evaluation is performed via lexical accuracy metrics and clinical speech assessments like PCC.

Result: The method achieves marked improvements in lexical accuracy and speaker identity preservation when tested on the STAR dataset. Its consonant correction aligns closely with clinical benchmarks, and automatic annotations correlate well (0.63) with human expert evaluations. It further generalizes well to adult dysarthric speech via the TORGO dataset.

Conclusion: ChiReSSD offers a clinically valuable tool for producing identity-preserving reconstructed speech, reducing transcription effort, and potentially extending to varied clinical populations such as children and dysarthric adults.

Abstract: We present ChiReSSD, a speech reconstruction framework that preserves
children speaker's identity while suppressing mispronunciations. Unlike prior
approaches trained on healthy adult speech, ChiReSSD adapts to the voices of
children with speech sound disorders (SSD), with particular emphasis on pitch
and prosody. We evaluate our method on the STAR dataset and report substantial
improvements in lexical accuracy and speaker identity preservation.
Furthermore, we automatically predict the phonetic content in the original and
reconstructed pairs, where the proportion of corrected consonants is comparable
to the percentage of correct consonants (PCC), a clinical speech assessment
metric. Our experiments show Pearson correlation of 0.63 between automatic and
human expert annotations, highlighting the potential to reduce the manual
transcription burden. In addition, experiments on the TORGO dataset demonstrate
effective generalization for reconstructing adult dysarthric speech. Our
results indicate that disentangled, style-based TTS reconstruction can provide
identity-preserving speech across diverse clinical populations.

</details>


### [461] [MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech](https://arxiv.org/abs/2509.18196)
*Jialong Mai,Jinxin Ji,Xiaofen Xing,Chen Yang,Weidong Chen,Jingyuan Xing,Xiangmin Xu*

Main category: cs.SD

TL;DR: This paper introduces MNV-17, a comprehensive performative Mandarin speech dataset specifically designed for recognizing 17 distinct nonverbal vocalization categories like sighs, laughs, and coughs.


<details>
  <summary>Details</summary>
Motivation: Current ASR systems excel at lexical transcription but fail to recognize nonverbal vocalizations, which are crucial for understanding emotional and intentional communication cues.

Method: The researchers created MNV-17, a 7.55-hour performative Mandarin dataset with clearly articulated NV instances and benchmarked its use across four mainstream ASR architectures for effective semantic transcription and NV classification.

Result: MNV-17 showcased superior quality and variety in NV categories, enabling effective evaluation of ASR systems' ability to jointly handle text and NV recognition.

Conclusion: The study highlights the importance of comprehensive NV recognition for expressive ASR, providing MNV-17 as a publicly available resource to advance research in this domain.

Abstract: Mainstream Automatic Speech Recognition (ASR) systems excel at transcribing
lexical content, but largely fail to recognize nonverbal vocalizations (NVs)
embedded in speech, such as sighs, laughs, and coughs. This capability is
important for a comprehensive understanding of human communication, as NVs
convey crucial emotional and intentional cues. Progress in NV-aware ASR has
been hindered by the lack of high-quality, well-annotated datasets. To address
this gap, we introduce MNV-17, a 7.55-hour performative Mandarin speech
dataset. Unlike most existing corpora that rely on model-based detection,
MNV-17's performative nature ensures high-fidelity, clearly articulated NV
instances. To the best of our knowledge, MNV-17 provides the most extensive set
of nonverbal vocalization categories, comprising 17 distinct and well-balanced
classes of common NVs. We benchmarked MNV-17 on four mainstream ASR
architectures, evaluating their joint performance on semantic transcription and
NV classification. The dataset and the pretrained model checkpoints will be
made publicly available to facilitate future research in expressive ASR.

</details>


### [462] [Scattering Transformer: A Training-Free Transformer Architecture for Heart Murmur Detection](https://arxiv.org/abs/2509.18424)
*Rami Zewail*

Main category: cs.SD

TL;DR: This study introduces the Scattering Transformer, a lightweight and training-free transformer architecture for heart murmur detection, as an alternative to computationally heavy foundational audio models. It achieves competitive accuracy on a public dataset.


<details>
  <summary>Details</summary>
Motivation: The growing demand for skilled clinicians in interpreting heart sounds necessitates automated solutions, especially in resource-constrained setups. Existing approaches leveraging deep learning are often limited by their dependency on large annotated datasets and computational resources.

Method: The paper proposes the Scattering Transformer, which combines wavelet scattering networks and transformer-like architectures. The Scattering Transformer introduces contextual dependencies without requiring backpropagation and avoids the computational intensity of pre-trained, self-supervised audio models.

Result: The Scattering Transformer was evaluated on the CirCor DigiScope dataset. It achieved a Weighted Accuracy (WAR) of 0.786 and an Unweighted Average Recall (UAR) of 0.697, demonstrating performance competitive with state-of-the-art audio foundation models.

Conclusion: The Scattering Transformer offers a promising solution for heart murmur detection in resource-constrained environments. Its competitive performance and lightweight nature make it a feasible alternative to computationally demanding models.

Abstract: In an attempt to address the need for skilled clinicians in heart sound
interpretation, recent research efforts on automating cardiac auscultation have
explored deep learning approaches. The majority of these approaches have been
based on supervised learning that is always challenged in occasions where
training data is limited. More recently, there has been a growing interest in
potentials of pre-trained self-supervised audio foundation models for
biomedical end tasks. Despite exhibiting promising results, these foundational
models are typically computationally intensive. Within the context of automatic
cardiac auscultation, this study explores a lightweight alternative to these
general-purpose audio foundation models by introducing the Scattering
Transformer, a novel, training-free transformer architecture for heart murmur
detection. The proposed method leverages standard wavelet scattering networks
by introducing contextual dependencies in a transformer-like architecture
without any backpropagation. We evaluate our approach on the public CirCor
DigiScope dataset, directly comparing it against leading general-purpose
foundational models. The Scattering Transformer achieves a Weighted
Accuracy(WAR) of 0.786 and an Unweighted Average Recall(UAR) of 0.697,
demonstrating performance highly competitive with contemporary state of the art
methods. This study establishes the Scattering Transformer as a viable and
promising alternative in resource-constrained setups.

</details>


### [463] [Explore the Reinforcement Learning for the LLM based ASR and TTS system](https://arxiv.org/abs/2509.18569)
*Changfeng Gao,Yabin Li,Keyu An,Zhifu Gao,Zhihao Du,Han Zhao,Xiangang Li*

Main category: cs.SD

TL;DR: The study introduces a lightweight reinforcement learning (RL) framework for enhancing large language models (LLMs) in audio tasks, focusing on automatic speech recognition (ASR) and text-to-speech (TTS). Experiments show RL improves performance notably, even with constrained resources.


<details>
  <summary>Details</summary>
Motivation: Current applications of reinforcement learning significantly enhance text-based LLM tasks, but its potential for audio-based tasks like ASR and TTS remains largely unexplored due to complexity.

Method: The research designs a lightweight reinforcement learning framework for audio-focused LLMs, evaluates rule-based rewards within GRPO for ASR, and compares GRPO and DiffRO approaches for TTS tasks, alongside a combined technique.

Result: Experiments reveal RL significantly boosts ASR and TTS performance, demonstrating effective results despite limited datasets and minimal optimization steps.

Conclusion: Reinforcement learning holds substantial potential to improve audio-based language model applications, offering effective enhancements for ASR and TTS tasks even under limited training conditions.

Abstract: In recent years, large language models (LLMs) have played an important role
in automatic speech recognition (ASR) and text-to-speech (TTS) systems. While
reinforcement learning (RL) has significantly enhanced LLM performance in
text-based tasks, its application to ASR and TTS remains underexplored due to
the complexity of training audio-based models. In this study, we propose a
lightweight RL framework tailored for audio-based LLMs that can process audio
inputs and generate audio outputs. Based on this framework, we evaluate the
effectiveness of reinforcement learning on both ASR and TTS tasks. For the ASR
task, we experiment with different rule-based reward functions within the Group
Relative Policy Optimization (GRPO) framework and investigate the impact of RL
data construction. For the TTS task, we compare GRPO with Differentiable Reward
Optimization (DiffRO) and further combine the two approaches to achieve
improved performance. Our experiments demonstrate that RL can significantly
enhance the performance of both ASR and TTS systems, even with limited training
data and a small number of optimization steps.

</details>


### [464] [An overview of neural architectures for self-supervised audio representation learning from masked spectrograms](https://arxiv.org/abs/2509.18691)
*Sarthak Yadav,Sergios Theodoridis,Zheng-Hua Tan*

Main category: cs.SD

TL;DR: The paper explores masked spectrogram modeling in self-supervised learning for audio, compares Transformer-based methods with Mamba and xLSTM architectures, and evaluates their performance on diverse audio classification tasks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a comprehensive overview of masked spectrogram modeling and novel sequence modeling architectures like Mamba and xLSTM, and to help researchers understand their utility and performance in audio tasks.

Method: Conduct a thorough comparison of Transformer, Mamba, and xLSTM-based masked spectrogram models within a unified, reproducible framework across ten audio classification tasks.

Result: Detailed evaluation shows the strengths and limitations of different architectures on multiple downstream audio classification tasks, aiding researchers in decision-making.

Conclusion: The study provides insights into the effectiveness of Transformers, Mamba, and xLSTM for masked spectrogram modeling, offering guidance for their application in various audio-related tasks.

Abstract: In recent years, self-supervised learning has amassed significant interest
for training deep neural representations without labeled data. One such
self-supervised learning approach is masked spectrogram modeling, where the
objective is to learn semantically rich contextual representations by
predicting removed or hidden portions of the input audio spectrogram. With the
Transformer neural architecture at its core, masked spectrogram modeling has
emerged as the prominent approach for learning general purpose audio
representations, a.k.a. audio foundation models. Meanwhile, addressing the
issues of the Transformer architecture, in particular the underlying Scaled
Dot-product Attention operation, which scales quadratically with input sequence
length, has led to renewed interest in recurrent sequence modeling approaches.
Among them, Selective structured state space models (such as Mamba) and
extended Long Short-Term Memory (xLSTM) are the two most promising approaches
which have experienced widespread adoption. While the body of work on these two
topics continues to grow, there is currently a lack of an adequate overview
encompassing the intersection of these topics. In this paper, we present a
comprehensive overview of the aforementioned research domains, covering masked
spectrogram modeling and the previously mentioned neural sequence modeling
architectures, Mamba and xLSTM. Further, we compare Transformers, Mamba and
xLSTM based masked spectrogram models in a unified, reproducible framework on
ten diverse downstream audio classification tasks, which will help interested
readers to make informed decisions regarding suitability of the evaluated
approaches to adjacent applications.

</details>


### [465] [Identifying birdsong syllables without labelled data](https://arxiv.org/abs/2509.18412)
*Mélisande Teng,Julien Boussard,David Rolnick,Hugo Larochelle*

Main category: cs.SD

TL;DR: The study presents an unsupervised algorithm to identify and decompose birdsong recordings into syllable sequences, achieving high performance without labeled data.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of identifying syllable sequences in birdsongs for applications such as bird individual identification and understanding animal communication and learning processes, without requiring labeled data.

Method: The method involves a three-step unsupervised algorithm: detecting syllable events, clustering them to create syllable representations (templates), and using matching pursuit to sequence the syllables in recordings.

Result: The unsupervised method achieved high performance when validated against human-labelled Bengalese finch songs. Additionally, it distinguished individual birds of the Bengalese finch and great tit species based on their vocal signatures.

Conclusion: The algorithm provides a robust solution for unsupervised birdsong analysis, making it applicable across species while reducing reliance on labeled data and manual annotation.

Abstract: Identifying sequences of syllables within birdsongs is key to tackling a wide
array of challenges, including bird individual identification and better
understanding of animal communication and sensory-motor learning. Recently,
machine learning approaches have demonstrated great potential to alleviate the
need for experts to label long audio recordings by hand. However, they still
typically rely on the availability of labelled data for model training,
restricting applicability to a few species and datasets. In this work, we build
the first fully unsupervised algorithm to decompose birdsong recordings into
sequences of syllables. We first detect syllable events, then cluster them to
extract templates --syllable representations-- before performing matching
pursuit to decompose the recording as a sequence of syllables. We evaluate our
automatic annotations against human labels on a dataset of Bengalese finch
songs and find that our unsupervised method achieves high performance. We also
demonstrate that our approach can distinguish individual birds within a species
through their unique vocal signatures, for both Bengalese finches and another
species, the great tit.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [466] [Circuit Complexity From Physical Constraints: Scaling Limitations of Attention](https://arxiv.org/abs/2509.19161)
*Benjamin Prada,Ankur Mali*

Main category: cs.CC

TL;DR: The paper introduces new circuit complexity measures and classes to address limitations in existing metrics, offering insights into the scalability of attention mechanisms in transformers.


<details>
  <summary>Details</summary>
Motivation: Existing circuit complexity measures like NC, AC, and TC are inadequate for differentiating model expressivity in practical scenarios.

Method: The authors introduce the concept of local uniformity and propose new circuit complexity classes, RC(·), focusing on physical circuit scaling constraints.

Result: Using RC(·), the paper demonstrates that attention mechanisms with runtime exceeding ω(n^(3/2)) fail to scale effectively for complex datasets.

Conclusion: The findings highlight a method to define transformer expressivity bounds and reveal inherent limitations in attention mechanisms.

Abstract: We argue that the standard circuit complexity measures derived from $NC, AC,
TC$ provide limited practical information and are now insufficient to further
differentiate model expressivity. To address these new limitations, we define a
novel notion of local uniformity and a family of circuit complexity classes
$RC(\cdot)$ that capture the fundamental constraints of scaling physical
circuits. Through the lens of $RC(\cdot)$, we show that attention mechanisms
with $\omega(n^{3/2})$ runtime cannot scale to accommodate the entropy of
increasingly complex datasets. Our results simultaneously provide a methodology
for defining meaningful bounds on transformer expressivity and naturally expose
the restricted viability of attention.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [467] [Augmenting Limited and Biased RCTs through Pseudo-Sample Matching-Based Observational Data Fusion Method](https://arxiv.org/abs/2509.18148)
*Kairong Han,Weidong Huang,Taiyang Zhou,Peng Zhen,Kun Kuang*

Main category: stat.ME

TL;DR: The paper proposes a pseudo-sample matching method to integrate randomized controlled trial (RCT) data with observational data for better uplift modeling in ride-hailing pricing, addressing issues of cost, data quality, and implementation challenges in industrial settings.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenges of high costs, low availability, and poor quality of RCT data in online ride-hailing pricing, as well as the difficulty of implementing existing data fusion methods in complex industrial scenarios.

Method: The proposed method generates pseudo-samples from biased RCT data, matches them with similar observational data samples, and combines them to expand the effective RCT dataset while mitigating heterogeneity.

Result: The method showed significant performance improvements during simulations and real-world experiments, with a 0.41% profit gain observed over a week-long online test, which is impactful in large-scale industrial settings.

Conclusion: The proposed pseudo-sample matching method effectively addresses the data quality and generalization issues in RCT data for uplift modeling, emphasizing the need for higher-quality RCT data in industrial environments.

Abstract: In the online ride-hailing pricing context, companies often conduct
randomized controlled trials (RCTs) and utilize uplift models to assess the
effect of discounts on customer orders, which substantially influences
competitive market outcomes. However, due to the high cost of RCTs, the
proportion of trial data relative to observational data is small, which only
accounts for 0.65\% of total traffic in our context, resulting in significant
bias when generalizing to the broader user base. Additionally, the complexity
of industrial processes reduces the quality of RCT data, which is often subject
to heterogeneity from potential interference and selection bias, making it
difficult to correct. Moreover, existing data fusion methods are challenging to
implement effectively in complex industrial settings due to the high
dimensionality of features and the strict assumptions that are hard to verify
with real-world data. To address these issues, we propose an empirical data
fusion method called pseudo-sample matching. By generating pseudo-samples from
biased, low-quality RCT data and matching them with the most similar samples
from large-scale observational data, the method expands the RCT dataset while
mitigating its heterogeneity. We validated the method through simulation
experiments, conducted offline and online tests using real-world data. In a
week-long online experiment, we achieved a 0.41\% improvement in profit, which
is a considerable gain when scaled to industrial scenarios with hundreds of
millions in revenue. In addition, we discuss the harm to model training,
offline evaluation, and online economic benefits when the RCT data quality is
not high, and emphasize the importance of improving RCT data quality in
industrial scenarios. Further details of the simulation experiments can be
found in the GitHub repository https://github.com/Kairong-Han/Pseudo-Matching.

</details>


### [468] [Enhanced Survival Trees](https://arxiv.org/abs/2509.18494)
*Ruiwen Zhou,Ke Xie,Lei Liu,Zhichen Xu,Jimin Ding,Xiaogang Su*

Main category: stat.ME

TL;DR: The paper introduces a new survival tree method for censored failure time data with advancements in splitting procedure, tree structure regularization, and inference approaches.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address inefficiencies and biases in traditional survival tree methods for analyzing censored failure time data.

Method: The authors propose improvements in splitting procedures, intersected validation, fused regularization, pruning techniques, and bootstrap-based bias correction for confidence intervals.

Result: The method was validated through simulations and applied to real-world data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study.

Conclusion: The proposed survival tree method enhances computational efficiency, reduces bias, improves interpretability, and provides valid inference compared to traditional approaches.

Abstract: We introduce a new survival tree method for censored failure time data that
incorporates three key advancements over traditional approaches. First, we
develop a more computationally efficient splitting procedure that effectively
mitigates the end-cut preference problem, and we propose an intersected
validation strategy to reduce the variable selection bias inherent in greedy
searches. Second, we present a novel framework for determining tree structures
through fused regularization. In combination with conventional pruning, this
approach enables the merging of non-adjacent terminal nodes, producing more
parsimonious and interpretable models. Third, we address inference by
constructing valid confidence intervals for median survival times within the
subgroups identified by the final tree. To achieve this, we apply
bootstrap-based bias correction to standard errors. The proposed method is
assessed through extensive simulation studies and illustrated with data from
the Alzheimer's Disease Neuroimaging Initiative (ADNI) study.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [469] [Tensor Train Completion from Fiberwise Observations Along a Single Mode](https://arxiv.org/abs/2509.18149)
*Shakir Showkat Sofi,Lieven De Lathauwer*

Main category: math.NA

TL;DR: This paper proposes a fast and deterministic method for tensor completion using linear algebra operations that specifically deals with fiber-wise observations.


<details>
  <summary>Details</summary>
Motivation: Existing tensor completion techniques rely on random uniform observations and probabilistic recovery, but they may lack efficiency if the observation pattern exhibits low-rank structure.

Method: The authors utilize standard linear algebra to compute tensor train decomposition for tensors with fiber-wise observation patterns (fully observed/missing fibers along a specific mode).

Result: The proposed method demonstrates fast computation and deterministic recovery guarantees under reasonable observation conditions.

Conclusion: The method is efficient, showcases promising applications, and deterministic recovery results make it suitable for scenarios with structured observation patterns.

Abstract: Tensor completion is an extension of matrix completion aimed at recovering a
multiway data tensor by leveraging a given subset of its entries (observations)
and the pattern of observation. The low-rank assumption is key in establishing
a relationship between the observed and unobserved entries of the tensor. The
low-rank tensor completion problem is typically solved using numerical
optimization techniques, where the rank information is used either implicitly
(in the rank minimization approach) or explicitly (in the error minimization
approach). Current theories concerning these techniques often study
probabilistic recovery guarantees under conditions such as random uniform
observations and incoherence requirements. However, if an observation pattern
exhibits some low-rank structure that can be exploited, more efficient
algorithms with deterministic recovery guarantees can be designed by leveraging
this structure. This work shows how to use only standard linear algebra
operations to compute the tensor train decomposition of a specific type of
``fiber-wise" observed tensor, where some of the fibers of a tensor (along a
single specific mode) are either fully observed or entirely missing, unlike the
usual entry-wise observations. From an application viewpoint, this setting is
relevant when it is easier to sample or collect a multiway data tensor along a
specific mode (e.g., temporal). The proposed completion method is fast and is
guaranteed to work under reasonable deterministic conditions on the observation
pattern. Through numerical experiments, we showcase interesting applications
and use cases that illustrate the effectiveness of the proposed approach.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [470] [On Multi-entity, Multivariate Quickest Change Point Detection](https://arxiv.org/abs/2509.18310)
*Bahar Kor,Bipin Gaikwad,Abani Patra,Eric L. Miller*

Main category: eess.SP

TL;DR: This paper introduces a framework for detecting changes in multi-entity, multivariate time series data, particularly for crowd monitoring, using an autoencoder-based approach to score anomalies and identify changes.


<details>
  <summary>Details</summary>
Motivation: The study aims to develop a scalable, privacy-preserving method for detecting system-wide behavioral shifts in complex, dynamic environments, such as crowd monitoring, where traditional sensors like video surveillance are not practical.

Method: The framework uses an autoencoder to calculate Individual Deviations from Normality (IDfN), aggregates the deviations into a System-Wide Anomaly Score (SWAS), and applies CUSUM and statistical deviation techniques to detect changes. The approach is unsupervised and works in real-time without requiring labeled data.

Result: The method was tested on synthetic datasets and crowd simulations, showing accurate detection of significant system-level changes and scalability for multi-agent systems. Additionally, novel datasets for CPD in complex systems were introduced.

Conclusion: The proposed framework offers an effective, privacy-focused solution for CPD in complex, dynamic systems and fills a gap by introducing challenging datasets for future research.

Abstract: We propose a framework for online Change Point Detection (CPD) from
multi-entity, multivariate time series data, motivated by applications in crowd
monitoring where traditional sensing methods (e.g., video surveillance) may be
infeasible. Our approach addresses the challenge of detecting system-wide
behavioral shifts in complex, dynamic environments where the number and
behavior of individual entities may be uncertain or evolve. We introduce the
concept of Individual Deviation from Normality (IDfN), computed via a
reconstruction-error-based autoencoder trained on normal behavior. We aggregate
these individual deviations using mean, variance, and Kernel Density Estimates
(KDE) to yield a System-Wide Anomaly Score (SWAS). To detect persistent or
abrupt changes, we apply statistical deviation metrics and the Cumulative Sum
(CUSUM) technique to these scores. Our unsupervised approach eliminates the
need for labeled data or feature extraction, enabling real-time operation on
streaming input. Evaluations on both synthetic datasets and crowd simulations,
explicitly designed for anomaly detection in group behaviors, demonstrate that
our method accurately detects significant system-level changes, offering a
scalable and privacy-preserving solution for monitoring complex multi-agent
systems. In addition to this methodological contribution, we introduce new,
challenging multi-entity multivariate time series datasets generated from crowd
simulations in Unity and coupled nonlinear oscillators. To the best of our
knowledge, there is currently no publicly available dataset of this type
designed explicitly to evaluate CPD in complex collective and interactive
systems, highlighting an essential gap that our work addresses.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [471] [A decentralized future for the open-science databases](https://arxiv.org/abs/2509.19206)
*Gaurav Sharma,Viorel Munteanu,Nika Mansouri Ghiasi,Jineta Banerjee,Susheel Varma,Luca Foschini,Kyle Ellrott,Onur Mutlu,Dumitru Ciorbă,Roel A. Ophoff,Viorel Bostan,Christopher E Mason,Jason H. Moore,Despoina Sousoni,Arunkumar Krishnan,Christopher E. Mason,Mihai Dimian,Gustavo Stolovitzky,Fabio G. Liberante,Taras K. Oleksyk,Serghei Mangul*

Main category: cs.DB

TL;DR: The paper advocates for replacing centralized biological data repositories, vulnerable to failures, with federated and decentralized frameworks to ensure resilient, FAIR, and sustainable data stewardship.


<details>
  <summary>Details</summary>
Motivation: The reliance on centralized biological data repositories faces risks, such as cyberattacks, natural disasters, and political or funding challenges, which can disrupt scientific progress.

Method: The paper examines the limitations of centralized repositories, evaluates federated and decentralized architectures, and proposes a hybrid framework as a robust alternative.

Result: The hybrid framework is positioned to reduce risks related to governance, infrastructure, and funding, ensuring global accessibility and fairness in scientific data stewardship.

Conclusion: A distributed and hybrid model for scientific data infrastructure is essential to ensure long-term data integrity, accessibility, and sustainability, safeguarding it as a public good for open science.

Abstract: Continuous and reliable access to curated biological data repositories is
indispensable for accelerating rigorous scientific inquiry and fostering
reproducible research. Centralized repositories, though widely used, are
vulnerable to single points of failure arising from cyberattacks, technical
faults, natural disasters, or funding and political uncertainties. This can
lead to widespread data unavailability, data loss, integrity compromises, and
substantial delays in critical research, ultimately impeding scientific
progress. Centralizing essential scientific resources in a single geopolitical
or institutional hub is inherently dangerous, as any disruption can paralyze
diverse ongoing research. The rapid acceleration of data generation, combined
with an increasingly volatile global landscape, necessitates a critical
re-evaluation of the sustainability of centralized models. Implementing
federated and decentralized architectures presents a compelling and
future-oriented pathway to substantially strengthen the resilience of
scientific data infrastructures, thereby mitigating vulnerabilities and
ensuring the long-term integrity of data. Here, we examine the structural
limitations of centralized repositories, evaluate federated and decentralized
models, and propose a hybrid framework for resilient, FAIR, and sustainable
scientific data stewardship. Such an approach offers a significant reduction in
exposure to governance instability, infrastructural fragility, and funding
volatility, and also fosters fairness and global accessibility. The future of
open science depends on integrating these complementary approaches to establish
a globally distributed, economically sustainable, and institutionally robust
infrastructure that safeguards scientific data as a public good, further
ensuring continued accessibility, interoperability, and preservation for
generations to come.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [472] [Pareto-optimal Tradeoffs Between Communication and Computation with Flexible Gradient Tracking](https://arxiv.org/abs/2509.18129)
*Yan Huang,Jinming Xu,Li Chai,Jiming Chen,Karl H. Johansson*

Main category: math.OC

TL;DR: This paper introduces FlexGT, a distributed optimization method for non-i.i.d. scenarios that balances communication and computation efficiency, achieving optimal convergence rates.


<details>
  <summary>Details</summary>
Motivation: The trade-off between communication and computation efficiency in distributed optimization in non-i.i.d scenarios.

Method: FlexGT leverages a tunable gradient tracking mechanism, robust convergence analysis, and introduces an improved variant, Acc-FlexGT, for better computational and communication trade-offs.

Result: FlexGT achieves optimal communication and computation complexities, and Acc-FlexGT optimizes efficiency in nonconvex and strongly convex cases.

Conclusion: FlexGT and Acc-FlexGT are effective and robust methods, offering Pareto-optimal solutions for distributed optimization in heterogeneous settings.

Abstract: This paper addresses distributed optimization problems in non-i.i.d.
scenarios, focusing on the interplay between communication and computation
efficiency. To this end, we propose FlexGT, a flexible snapshot gradient
tracking method with tunable numbers of local updates and neighboring
communications in each round. Leveraging a unified convergence analysis
framework, we prove that FlexGT achieves a linear or sublinear convergence rate
depending on objective-specific properties--from (strongly) convex to
nonconvex--and the above-mentioned tunable parameters. FlexGT is provably
robust to the heterogeneity across nodes and attains the best-known
communication and computation complexity among existing results. Moreover, we
introduce an accelerated gossip-based variant, termed Acc-FlexGT, and show that
with prior knowledge of the graph, it achieves a Pareto-optimal trade-off
between communication and computation. Particularly, Acc-FlexGT achieves the
optimal iteration complexity of $\tilde{\mathcal{O}} \left( L/\epsilon +L\sigma
^2/\left( n\epsilon^2 \sqrt{1-\sqrt{\rho _W}} \right) \right) $ for the
nonconvex case, matching the existing lower bound up to a logarithmic factor,
and improves the existing results for the strongly convex case by a factor of
$\tilde{\mathcal{O}} \left( 1/\sqrt{\epsilon} \right)$, where $\epsilon$ is the
targeted accuracy, $n$ the number of nodes, $L$ the Lipschitz constant,
$\rho_W$ the spectrum gap of the graph, and $\sigma$ the stochastic gradient
variance. Numerical examples are provided to demonstrate the effectiveness of
the proposed methods.

</details>


### [473] [Clapping: Removing Per-sample Storage for Pipeline Parallel Distributed Optimization with Communication Compression](https://arxiv.org/abs/2509.19029)
*Boao Kong,Xu Huang,Yuqi Xu,Yixuan Liang,Bin Wang,Kun Yuan*

Main category: math.OC

TL;DR: The paper introduces Clapping, a flexible communication compression algorithm optimized for pipeline-parallel learning, alleviating communication overhead in distributed optimization systems.


<details>
  <summary>Details</summary>
Motivation: To address significant communication overhead in pipeline-parallel optimization caused by transmitting high-dimensional activations and gradients between workers.

Method: The authors propose Clapping with lazy sampling strategies (Clapping-FC and Clapping-FU), reusing data samples across steps to eliminate sample-wise memory barriers and bypass the need for unbiased gradient assumptions.

Result: Numerical experiments show that Clapping achieves effective convergence with reduced communication overhead across various machine learning tasks.

Conclusion: Clapping provides an efficient solution for distributed optimization by enabling convergence, resolving compression error propagation, and sustaining performance in few-epoch or online training scenarios.

Abstract: Pipeline-parallel distributed optimization is essential for large-scale
machine learning but is challenged by significant communication overhead from
transmitting high-dimensional activations and gradients between workers.
Existing approaches often depend on impractical unbiased gradient assumptions
or incur sample-size memory overhead. This paper introduces Clapping, a
Communication compression algorithm with LAzy samPling for Pipeline-parallel
learnING. Clapping adopts a lazy sampling strategy that reuses data samples
across steps, breaking sample-wise memory barrier and supporting convergence in
few-epoch or online training regimes. Clapping comprises two variants including
Clapping-FC and Clapping-FU, both of which achieve convergence without unbiased
gradient assumption, effectively addressing compression error propagation in
multi-worker settings. Numerical experiments validate the performance of
Clapping across different learning tasks.

</details>


### [474] [Guaranteed Robust Nonlinear MPC via Disturbance Feedback](https://arxiv.org/abs/2509.18760)
*Antoine P. Leeman,Johannes Köhler,Melanie N. Zeilinger*

Main category: math.OC

TL;DR: The paper proposes a robust model predictive control (RMPC) for robots to ensure safety and stability under disturbances and model mismatch. The approach is efficient, scalable, real-time capable, and validated on various dynamics.


<details>
  <summary>Details</summary>
Motivation: To ensure robots can handle safety-critical constraints while adapting to disturbances and mismatches in the dynamic model.

Method: The uncertain nonlinear system is decomposed into a nominal dynamic model, disturbance-feedback controllers, and bounds on model errors, which are optimized through sequential convex programming. Convex subproblems are solved with a disturbance-feedback MPC solver.

Result: The proposed RMPC demonstrated robust performance and was validated on applications, including a challenging rocket-landing control scenario with steerable thrust.

Conclusion: This work presents a practical and efficient RMPC method that guarantees robust constraint satisfaction, stability, and feasibility. An open-source implementation is made available for broader use.

Abstract: Robots must satisfy safety-critical state and input constraints despite
disturbances and model mismatch. We introduce a robust model predictive control
(RMPC) formulation that is fast, scalable, and compatible with real-time
implementation. Our formulation guarantees robust constraint satisfaction,
input-to-state stability (ISS) and recursive feasibility. The key idea is to
decompose the uncertain nonlinear system into (i) a nominal nonlinear dynamic
model, (ii) disturbance-feedback controllers, and (iii) bounds on the model
error. These components are optimized jointly using sequential convex
programming. The resulting convex subproblems are solved efficiently using a
recent disturbance-feedback MPC solver. The approach is validated across
multiple dynamics, including a rocket-landing problem with steerable thrust. An
open-source implementation is available at
https://github.com/antoineleeman/robust-nonlinear-mpc.

</details>


### [475] [Joint Cooperative and Non-Cooperative Localization in WSNs with Distributed Scaled Proximal ADMM Algorithms](https://arxiv.org/abs/2509.18213)
*Qiaojia Zhu,Xiaojing Shen,Haiqi Liu,Pramod K. Varshney*

Main category: math.OC

TL;DR: This paper addresses the problem of cooperative and non-cooperative localization in sensor networks by formulating it as a single optimization problem and proposing a novel algorithm (SP-ADMM-JCNL) to solve it efficiently.


<details>
  <summary>Details</summary>
Motivation: The motivation is to unify the challenging tasks of cooperative and non-cooperative localization into a single framework, overcoming issues of delay and variable coupling in existing sequential or joint approaches while ensuring reliable target estimation.

Method: The paper introduces auxiliary variables to decouple the problem's complexities and proposes the SP-ADMM-JCNL algorithm, which leverages structured design for distributed computation and guarantees convergence to optimal solutions.

Result: The SP-ADMM-JCNL algorithm is theoretically proven to converge to critical solutions of the problem with a sublinear rate, and experiments on synthetic and benchmark datasets validate its accurate and reliable localization performance.

Conclusion: The proposed approach successfully addresses the challenges in cooperative and non-cooperative localization, offering a unified, efficient, and robust solution with theoretical and empirical validation.

Abstract: Cooperative and non-cooperative localization frequently arise together in
wireless sensor networks, particularly when sensor positions are uncertain and
targets are unable to communicate with the network. While joint processing can
eliminate the delay in target estimation found in sequential approaches, it
introduces complex variable coupling, posing challenges in both modeling and
optimization. This paper presents a joint modeling approach that formulates
cooperative and non-cooperative localization as a single optimization problem.
To address the resulting coupling, we introduce auxiliary variables that enable
structural decoupling and distributed computation. Building on this
formulation, we develop the Scaled Proximal Alternating Direction Method of
Multipliers for Joint Cooperative and Non-Cooperative Localization
(SP-ADMM-JCNL). Leveraging the problem's structured design, we provide
theoretical guarantees that the algorithm generates a sequence converging
globally to the Karush-Kuhn-Tucker (KKT) point of the reformulated problem and
further to a critical point of the original non-convex objective function, with
a sublinear rate of O(1/T). Experiments on both synthetic and benchmark
datasets demonstrate that SP-ADMM-JCNL achieves accurate and reliable
localization performance.

</details>


### [476] [Zero-Shot Transferable Solution Method for Parametric Optimal Control Problems](https://arxiv.org/abs/2509.18404)
*Xingjian Li,Kelvin Kan,Deepanshu Verma,Krishna Kumar,Stanley Osher,Ján Drgoňa*

Main category: math.OC

TL;DR: The paper proposes a method for solving optimal control problems with changing objectives using pre-trained neural basis functions, enabling efficient adaptation to new tasks.


<details>
  <summary>Details</summary>
Motivation: Address the high computational costs associated with re-solving optimization problems when control objectives change frequently in real-time applications.

Method: A two-stage approach: offline imitation learning to train reusable neural basis functions, and lightweight coefficient estimation for real-time task adaptation.

Result: The method achieves near-optimal control across diverse tasks and dynamic environments, supporting zero-shot adaptation with minimal computational effort.

Conclusion: The approach demonstrates scalable, efficient, and transferable control methods suitable for real-time deployment, reducing the need for problem-specific re-solving.

Abstract: This paper presents a transferable solution method for optimal control
problems with varying objectives using function encoder (FE) policies.
Traditional optimization-based approaches must be re-solved whenever objectives
change, resulting in prohibitive computational costs for applications requiring
frequent evaluation and adaptation. The proposed method learns a reusable set
of neural basis functions that spans the control policy space, enabling
efficient zero-shot adaptation to new tasks through either projection from data
or direct mapping from problem specifications. The key idea is an
offline-online decomposition: basis functions are learned once during offline
imitation learning, while online adaptation requires only lightweight
coefficient estimation. Numerical experiments across diverse dynamics,
dimensions, and cost structures show our method delivers near-optimal
performance with minimal overhead when generalizing across tasks, enabling
semi-global feedback policies suitable for real-time deployment.

</details>


### [477] [Learning When to Restart: Nonstationary Newsvendor from Uncensored to Censored Demand](https://arxiv.org/abs/2509.18709)
*Xin Chen,Jiameng Lyu,Shilin Yuan,Yuan Zhou*

Main category: math.OC

TL;DR: The paper presents a new adaptive framework for addressing the newsvendor problem under uncertain, nonstationary demand using a distributional-detection-and-restart approach. Two efficient algorithms are proposed for handling censored and uncensored demands.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of nonstationary demand in decision-making, particularly under conditions where the degree of nonstationarity is unknown and data may be censored.

Method: Develop a distributional-detection-and-restart framework instantiated through two adaptive algorithms that address both abrupt and gradual changes in demand. The framework is evaluated theoretically and empirically.

Result: The proposed algorithms show optimality in theoretical regret bounds and robust empirical performance on real-world datasets such as nurse staffing and COVID-19 test demand data.

Conclusion: The study provides a practical, adaptive, and theoretically grounded solution for decision-making in nonstationary environments. The framework has broad applicability beyond the newsvendor problem.

Abstract: We study nonstationary newsvendor problems under nonparametric demand models
and general distributional measures of nonstationarity, addressing the
practical challenges of unknown degree of nonstationarity and demand censoring.
We propose a novel distributional-detection-and-restart framework for learning
in nonstationary environments, and instantiate it through two efficient
algorithms for the uncensored and censored demand settings. The algorithms are
fully adaptive, requiring no prior knowledge of the degree and type of
nonstationarity, and offer a flexible yet powerful approach to handling both
abrupt and gradual changes in nonstationary environments. We establish a
comprehensive optimality theory for our algorithms by deriving matching regret
upper and lower bounds under both general and refined structural conditions
with nontrivial proof techniques that are of independent interest. Numerical
experiments using real-world datasets, including nurse staffing data for
emergency departments and COVID-19 test demand data, showcase the algorithms'
superior and robust empirical performance. While motivated by the newsvendor
problem, the distributional-detection-and-restart framework applies broadly to
a wide class of nonstationary stochastic optimization problems. Managerially,
our framework provides a practical, easy-to-deploy, and theoretically grounded
solution for decision-making under nonstationarity.

</details>


### [478] [On the Convergence of Policy Mirror Descent with Temporal Difference Evaluation](https://arxiv.org/abs/2509.18822)
*Jiacai Liu,Wenye Li,Ke Wei*

Main category: math.OC

TL;DR: The paper investigates the extension of Policy Mirror Descent (PMD) in reinforcement learning using Temporal Difference methods (TD-PMD), showcasing convergence rates and providing sample complexity analysis under a generative model.


<details>
  <summary>Details</summary>
Motivation: The aim is to explore more efficient policy optimization techniques by integrating temporal difference evaluation into the PMD framework, addressing limitations in existing methods reliant solely on exact or unbiased evaluations.

Method: The approach integrates TD methods into PMD, establishing sublinear and linear convergence rates using monotonicity and shift invariance arguments, and analyzing sample complexity under generative models.

Result: TD-PMD achieves dimension-free $O(1/T)$ sublinear convergence with constant step sizes and initializations. Adaptive step size selection yields $\u000b ">\gamma\"$-rate linear convergence. It performs well in the policy domain and improves sample complexity over traditional PMD.

Conclusion: TD-PMD offers efficient and scalable reinforcement learning policy optimization, enhancing theoretical guarantees and practical performance over existing PMD frameworks.

Abstract: Policy mirror descent (PMD) is a general policy optimization framework in
reinforcement learning, which can cover a wide range of typical policy
optimization methods by specifying different mirror maps. Existing analysis of
PMD requires exact or approximate evaluation (for example unbiased estimation
via Monte Carlo simulation) of action values solely based on policy. In this
paper, we consider policy mirror descent with temporal difference evaluation
(TD-PMD). It is shown that, given the access to exact policy evaluations, the
dimension-free $O(1/T)$ sublinear convergence still holds for TD-PMD with any
constant step size and any initialization. In order to achieve this result, new
monotonicity and shift invariance arguments have been developed. The dimension
free $\gamma$-rate linear convergence of TD-PMD is also established provided
the step size is selected adaptively. For the two common instances of TD-PMD
(i.e., TD-PQA and TD-NPG), it is further shown that they enjoy the convergence
in the policy domain. Additionally, we investigate TD-PMD in the inexact
setting and give the sample complexity for it to achieve the last iterate
$\varepsilon$-optimality under a generative model, which improves the last
iterate sample complexity for PMD over the dependence on $1/(1-\gamma)$.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [479] [Enhanced Interpretable Knowledge Tracing for Students Performance Prediction with Human understandable Feature Space](https://arxiv.org/abs/2509.18231)
*Sein Minn,Roger Nkambou*

Main category: cs.CY

TL;DR: This paper improves interpretability in Knowledge Tracing models by using human-understandable features related to students' learning abilities, achieving better prediction accuracy while aligning with cognitive theories.


<details>
  <summary>Details</summary>
Motivation: The lack of interpretability in deep learning-based Knowledge Tracing models, which hinders understanding, trust, and applicability in educational settings.

Method: The authors enhance interpretable Knowledge Tracing models by incorporating features derived from student interaction data, especially those reflecting learning abilities, to balance interpretability and predictive accuracy.

Result: The proposed approach achieves improved predictive accuracy and retains alignment with cognitive theory, offering a more interpretable solution.

Conclusion: The paper successfully advances Knowledge Tracing models, making them better suited for adaptive learning systems by enhancing their balance between predictive power and psychological interpretability.

Abstract: Knowledge Tracing (KT) plays a central role in assessing students skill
mastery and predicting their future performance. While deep learning based KT
models achieve superior predictive accuracy compared to traditional methods,
their complexity and opacity hinder their ability to provide psychologically
meaningful explanations. This disconnect between model parameters and cognitive
theory poses challenges for understanding and enhancing the learning process,
limiting their trustworthiness in educational applications. To address these
challenges, we enhance interpretable KT models by exploring
human-understandable features derived from students interaction data. By
incorporating additional features, particularly those reflecting students
learning abilities, our enhanced approach improves predictive accuracy while
maintaining alignment with cognitive theory. Our contributions aim to balance
predictive power with interpretability, advancing the utility of adaptive
learning systems.

</details>


### [480] [Perceptions of AI Across Sectors: A Comparative Review of Public Attitudes](https://arxiv.org/abs/2509.18233)
*Filip Bialy,Mark Elliot,Robert Meckin*

Main category: cs.CY

TL;DR: This paper reviews 251 studies (2011-2025) on public attitudes towards AI, analyzing acceptance and resistance across domains and highlighting patterns in influencing factors.


<details>
  <summary>Details</summary>
Motivation: To understand how public attitudes toward AI are formed and influenced across different domains and use cases.

Method: A systematic comparative review of 251 studies on public attitudes towards AI, analyzing individual, contextual, and technical factors.

Result: The paper identifies patterns in perception influenced by benefits, risks, institutional trust, ethics, and domain-specific factors.

Conclusion: Public perception of AI is multidimensional, influenced by design, domain context, and cultural narratives, offering a basis for tailored AI governance strategies.

Abstract: This paper offers a domain-mediated comparative review of 251 studies on
public attitudes toward AI, published between 2011 and 2025. Drawing on a
systematic literature review, we analyse how different factors including
perceived benefits and concerns (or risks) shape public acceptance of - or
resistance to - artificial intelligence across domains and use-cases, including
healthcare, education, security, public administration, generative AI, and
autonomous vehicles. The analysis highlights recurring patterns in individual,
contextual, and technical factors influencing perception, while also tracing
variations in institutional trust, perceived fairness, and ethical concerns. We
show that the public perception in AI is shaped not only by technical design or
performance but also by sector-specific considerations as well as imaginaries,
cultural narratives, and historical legacies. This comparative approach offers
a foundation for developing more tailored and context-sensitive strategies for
responsible AI governance.

</details>


### [481] [An Artificial Intelligence Value at Risk Approach: Metrics and Models](https://arxiv.org/abs/2509.18394)
*Luis Enriquez Alvarez*

Main category: cs.CY

TL;DR: The paper discusses the multidimensional nature of AI risks, highlights the immaturity of AI risk management practices amidst emerging AI regulations, and provides a holistic overview of AI risk modeling.


<details>
  <summary>Details</summary>
Motivation: To address the immaturity of current AI risk management despite increasing AI regulations and offer a comprehensive understanding of AI risks and their modeling.

Method: The paper proposes a decomposition of AI risk into multiple dimensions (e.g., data protection, fairness, accuracy) and emphasizes the development of customized metrics and models. It provides basic, understandable examples for stakeholders to adapt within their specific environments.

Result: The study highlights the role of developing tailored risk metrics and models to improve decision-making for AI risk management.

Conclusion: A holistic approach to AI risk modeling can help stakeholders better navigate the interdependencies of AI risks and make informed decisions, emphasizing the collaboration of technical and non-technical teams.

Abstract: Artificial intelligence risks are multidimensional in nature, as the same
risk scenarios may have legal, operational, and financial risk dimensions. With
the emergence of new AI regulations, the state of the art of artificial
intelligence risk management seems to be highly immature due to upcoming AI
regulations. Despite the appearance of several methodologies and generic
criteria, it is rare to find guidelines with real implementation value,
considering that the most important issue is customizing artificial
intelligence risk metrics and risk models for specific AI risk scenarios.
Furthermore, the financial departments, legal departments and Government Risk
Compliance teams seem to remain unaware of many technical aspects of AI
systems, in which data scientists and AI engineers emerge as the most
appropriate implementers. It is crucial to decompose the problem of artificial
intelligence risk in several dimensions: data protection, fairness, accuracy,
robustness, and information security. Consequently, the main task is developing
adequate metrics and risk models that manage to reduce uncertainty for
decision-making in order to take informed decisions concerning the risk
management of AI systems.
  The purpose of this paper is to orientate AI stakeholders about the depths of
AI risk management. Although it is not extremely technical, it requires a basic
knowledge of risk management, quantifying uncertainty, the FAIR model, machine
learning, large language models and AI context engineering. The examples
presented pretend to be very basic and understandable, providing simple ideas
that can be developed regarding specific AI customized environments. There are
many issues to solve in AI risk management, and this paper will present a
holistic overview of the inter-dependencies of AI risks, and how to model them
together, within risk scenarios.

</details>


### [482] [Automatic coherence-driven inference on arguments](https://arxiv.org/abs/2509.18523)
*Steve Huntsman*

Main category: cs.CY

TL;DR: The paper proposes using large language models (LLMs) with neurosymbolic architectures to address inconsistencies in law and policy by enabling coherence-driven inference.


<details>
  <summary>Details</summary>
Motivation: Inconsistencies in law and policy are common, creating challenges for legislative and legal reasoning.

Method: The authors propose leveraging large language models to extract propositions, organize them into coherent data structures, and use combinatorial optimization for inference.

Result: The neurosymbolic architecture separates concerns and supports informed judgments about argument coherence for legislative and legal applications.

Conclusion: This approach offers a technological remedy to inconsistencies in legal reasoning by enhancing coherence-based analysis using LLMs.

Abstract: Inconsistencies are ubiquitous in law, administration, and jurisprudence.
Though a cure is too much to hope for, we propose a technological remedy. Large
language models (LLMs) can accurately extract propositions from arguments and
compile them into natural data structures that enable coherence-driven
inference (CDI) via combinatorial optimization. This neurosymbolic architecture
naturally separates concerns and enables meaningful judgments about the
coherence of arguments that can inform legislative and policy analysis and
legal reasoning.

</details>


### [483] [Learning Progression-Guided AI Evaluation of Scientific Models To Support Diverse Multi-Modal Understanding in NGSS Classroom](https://arxiv.org/abs/2509.18157)
*Leonora Kaldaras,Tingting Li,Prudence Djagba,Kevin Haudek,Joseph Krajcik*

Main category: cs.CY

TL;DR: This paper explores using machine learning to provide personalized feedback on multi-modal science assessments, aiming to support diverse student thinking and promote equitable evaluation.


<details>
  <summary>Details</summary>
Motivation: The authors aim to enhance science education by developing methods to support diverse modes of student expression and understanding, addressing the challenge of equitable assessment for culturally and linguistically diverse students.

Method: The study employs machine learning to evaluate student-generated scientific models and short explanations based on a validated learning progression (LP) framework for physical science assessment.

Result: Machine learning successfully evaluates models and explanations, enabling personalized feedback aligned with diverse modes of student thinking.

Conclusion: The approach demonstrates a scalable and equitable method for science assessment, fostering inclusivity and deeper understanding through multi-modal evaluation and personalized feedback.

Abstract: Learning Progressions (LPs) can help adjust instruction to individual
learners needs if the LPs reflect diverse ways of thinking about a construct
being measured, and if the LP-aligned assessments meaningfully measure this
diversity. The process of doing science is inherently multi-modal with
scientists utilizing drawings, writing and other modalities to explain
phenomena. Thus, fostering deep science understanding requires supporting
students in using multiple modalities when explaining phenomena. We build on a
validated NGSS-aligned multi-modal LP reflecting diverse ways of modeling and
explaining electrostatic phenomena and associated assessments. We focus on
students modeling, an essential practice for building a deep science
understanding. Supporting culturally and linguistically diverse students in
building modeling skills provides them with an alternative mode of
communicating their understanding, essential for equitable science assessment.
Machine learning (ML) has been used to score open-ended modeling tasks (e.g.,
drawings), and short text-based constructed scientific explanations, both of
which are time- consuming to score. We use ML to evaluate LP-aligned scientific
models and the accompanying short text-based explanations reflecting
multi-modal understanding of electrical interactions in high school Physical
Science. We show how LP guides the design of personalized ML-driven feedback
grounded in the diversity of student thinking on both assessment modes.

</details>


### [484] [Large-Scale, Longitudinal Study of Large Language Models During the 2024 US Election Season](https://arxiv.org/abs/2509.18446)
*Sarah H. Cen,Andrew Ilyas,Hedi Driss,Charlotte Park,Aspen Hopkins,Chara Podimata,Aleksander Mądry*

Main category: cs.CY

TL;DR: This paper investigates the influence of large language models (LLMs) on the 2024 US presidential election through a longitudinal study of 12 LLMs to evaluate their election-related behavior and impacts.


<details>
  <summary>Details</summary>
Motivation: To explore how the popularization of large language models (LLMs) affects political discourse and the information ecosystem during a major political event, particularly given concerns about their role in shaping opinions and influencing elections.

Method: The study uses a longitudinal design, querying 12 LLMs with over 12,000 structured survey questions on a near-daily basis from July to November 2024. Queries varied systematically in content and format to examine changes in model behavior, sensitivity to steering, responsiveness to instructions, and their knowledge of election-related topics.

Result: Key findings include tracking variations in LLM behavior during the election season, analyzing sensitivity to demographic steering in responses, studying beliefs about political candidates, and uncovering implicit election outcome predictions by LLMs.

Conclusion: The paper provides insights into the potential impact of LLMs on elections and offers a publicly available dataset to support future research and evaluations in similar contexts.

Abstract: The 2024 US presidential election is the first major contest to occur in the
US since the popularization of large language models (LLMs). Building on
lessons from earlier shifts in media (most notably social media's well studied
role in targeted messaging and political polarization) this moment raises
urgent questions about how LLMs may shape the information ecosystem and
influence political discourse. While platforms have announced some election
safeguards, how well they work in practice remains unclear. Against this
backdrop, we conduct a large-scale, longitudinal study of 12 models, queried
using a structured survey with over 12,000 questions on a near-daily cadence
from July through November 2024. Our design systematically varies content and
format, resulting in a rich dataset that enables analyses of the models'
behavior over time (e.g., across model updates), sensitivity to steering,
responsiveness to instructions, and election-related knowledge and "beliefs."
In the latter half of our work, we perform four analyses of the dataset that
(i) study the longitudinal variation of model behavior during election season,
(ii) illustrate the sensitivity of election-related responses to demographic
steering, (iii) interrogate the models' beliefs about candidates' attributes,
and (iv) reveal the models' implicit predictions of the election outcome. To
facilitate future evaluations of LLMs in electoral contexts, we detail our
methodology, from question generation to the querying pipeline and third-party
tooling. We also publicly release our dataset at
https://huggingface.co/datasets/sarahcen/llm-election-data-2024

</details>


### [485] [The AI Literacy Heptagon: A Structured Approach to AI Literacy in Higher Education](https://arxiv.org/abs/2509.18900)
*Veronika Hackl,Alexandra Mueller,Maximilian Sailer*

Main category: cs.CY

TL;DR: This paper provides an integrative review of AI Literacy (AIL) in Higher Education (HE), synthesizing research insights into seven key dimensions and a practical framework for curriculum development.


<details>
  <summary>Details</summary>
Motivation: To address the lack of clarity and practical guidance in defining and implementing AI Literacy in the context of Higher Education.

Method: The authors conducted an integrative literature review of studies published between 2021 and 2024, analyzing current definitions of AI Literacy and synthesizing them into a comprehensive framework.

Result: The analysis identified seven core dimensions of AI Literacy: technical, applicational, critical thinking, ethical, social, integrational, and legal, which were consolidated into the AI Literacy Heptagon model.

Conclusion: The study proposes a structured framework for AI Literacy in HE, providing a bridge between academic conceptualizations and practical curriculum development.

Abstract: The integrative literature review addresses the conceptualization and
implementation of AI Literacy (AIL) in Higher Education (HE) by examining
recent research literature. Through an analysis of publications (2021-2024), we
explore (1) how AIL is defined and conceptualized in current research,
particularly in HE, and how it can be delineated from related concepts such as
Data Literacy, Media Literacy, and Computational Literacy; (2) how various
definitions can be synthesized into a comprehensive working definition, and (3)
how scientific insights can be effectively translated into educational
practice. Our analysis identifies seven central dimensions of AIL: technical,
applicational, critical thinking, ethical, social, integrational, and legal.
These are synthesized in the AI Literacy Heptagon, deepening conceptual
understanding and supporting the structured development of AIL in HE. The study
aims to bridge the gap between theoretical AIL conceptualizations and the
practical implementation in academic curricula.

</details>


### [486] [A Mega-Study of Digital Twins Reveals Strengths, Weaknesses and Opportunities for Further Improvement](https://arxiv.org/abs/2509.19088)
*Tiany Peng,George Gui,Daniel J. Merlau,Grace Jiarui Fan,Malek Ben Sliman,Melanie Brucks,Eric J. Johnson,Vicki Morwitz,Abdullah Althenayyan,Silvia Bellezza,Dante Donati,Hortense Fong,Elizabeth Friedman,Ariana Guevara,Mohamed Hussein,Kinshuk Jerath,Bruce Kogut,Kristen Lane,Hannah Li,Patryk Perkowski,Oded Netzer,Olivier Toubia*

Main category: cs.CY

TL;DR: This study examines whether AI-generated "digital twins" can accurately reproduce individual human responses in surveys and experiments, finding only modest correlation.


<details>
  <summary>Details</summary>
Motivation: To evaluate if AI-generated digital twins, constructed from individual-level data, can reliably predict human responses in surveys and experiments and capture heterogeneity.

Method: The authors conducted 19 pre-registered studies using a U.S. panel and their AI-powered digital twins, analyzing their responses to 164 outcomes, and examining performance across different participant characteristics.

Result: Digital twins exhibited a correlation of ~0.2 with human responses, were less variable, and had mixed performance based on domain, education, income, and ideology. They showed limited reliability for individual or population predictions.

Conclusion: Digital twins can somewhat capture relative differences but are unsuitable for precise individual-level predictions or population estimations. Caution and validation are needed before deployment.

Abstract: Do "digital twins" capture individual responses in surveys and experiments?
We run 19 pre-registered studies on a national U.S. panel and their LLM-powered
digital twins (constructed based on previously-collected extensive
individual-level data) and compare twin and human answers across 164 outcomes.
The correlation between twin and human answers is modest (approximately 0.2 on
average) and twin responses are less variable than human responses. While
constructing digital twins based on rich individual-level data improves our
ability to capture heterogeneity across participants and predict relative
differences between them, it does not substantially improve our ability to
predict the exact answers given by specific participants or enhance predictions
of population means. Twin performance varies by domain and is higher among more
educated, higher-income, and ideologically moderate participants. These results
suggest current digital twins can capture some degree of relative differences
but are unreliable for individual-level predictions and sample mean and
variance estimation, underscoring the need for careful validation before use.
Our data and code are publicly available for researchers and practitioners
interested in optimizing digital twin pipelines.

</details>


### [487] [Generative Propaganda](https://arxiv.org/abs/2509.19147)
*Madeleine I. G. Daepp,Alejandro Cuevas,Robert Osazuwa Ness,Vickie Yu-Ping Wang,Bharat Kumar Nayak,Dibyendu Mishra,Ti-Chung Cheng,Shaily Desai,Joyojeet Pal*

Main category: cs.CY

TL;DR: This study examines the use of generative AI for propaganda, focusing on its application in India and Taiwan. It introduces a taxonomy for categorizing generative propaganda and highlights differences in its use and perception among creators and defenders in these regions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to investigate the real-world application and implications of generative AI in spreading propaganda, as its impact on public opinion and associated defenses are not well understood.

Method: Interviews were conducted with creators (e.g., influencers, political consultants) and defenders (e.g., journalists, fact-checkers) in India and Taiwan to explore their experiences and strategies regarding generative AI-based propaganda.

Result: The study found significant variations in the use and perception of generative AI. In India, creators emphasized persuasive rather than deceptive techniques to minimize risks, while in Taiwan, defenders viewed deception as part of broader narrative distortions. AI was noted for efficiency in translation and detection evasion.

Conclusion: The paper concludes that generative AI’s role in propaganda extends beyond deception, suggesting a need to refine threat models and account for social constraints and efficiency-based tools to combat misuse globally.

Abstract: Generative propaganda is the use of generative artificial intelligence (AI)
to shape public opinion. To characterize its use in real-world settings, we
conducted interviews with defenders (e.g., factcheckers, journalists,
officials) in Taiwan and creators (e.g., influencers, political consultants,
advertisers) as well as defenders in India, centering two places characterized
by high levels of online propaganda. The term "deepfakes", we find, exerts
outsized discursive power in shaping defenders' expectations of misuse and, in
turn, the interventions that are prioritized. To better characterize the space
of generative propaganda, we develop a taxonomy that distinguishes between
obvious versus hidden and promotional versus derogatory use. Deception was
neither the main driver nor the main impact vector of AI's use; instead, Indian
creators sought to persuade rather than to deceive, often making AI's use
obvious in order to reduce legal and reputational risks, while Taiwan's
defenders saw deception as a subset of broader efforts to distort the
prevalence of strategic narratives online. AI was useful and used, however, in
producing efficiency gains in communicating across languages and modes, and in
evading human and algorithmic detection. Security researchers should reconsider
threat models to clearly differentiate deepfakes from promotional and obvious
uses, to complement and bolster the social factors that constrain misuse by
internal actors, and to counter efficiency gains globally.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [488] [No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS](https://arxiv.org/abs/2509.18531)
*Seungyoun Shin,Dongha Ahn,Jiwoo Kim,Sungwook Jeon*

Main category: eess.AS

TL;DR: This paper addresses the limitations of Group Relative Policy Optimization (GRPO) in improving neural text-to-speech prosody, introducing an iterative Direct Preference Optimization (DPO) method guided by human-labeled preferences.


<details>
  <summary>Details</summary>
Motivation: The motivation is to resolve issues in GRPO-based TTS systems which improve errors but at the cost of generating monotone and unnatural speech, especially in task-oriented dialogs.

Method: The proposed iterative DPO method leverages a few hundred human-labeled preference pairs in each round to optimize prosodic naturalness and regularize the model's current state.

Result: The method demonstrates superior human preference (ELO) scores with competitive transcription error rates (CER), outperforming GRPO and commercial baselines on a specialized Korean dialogue dataset (KoCC-TTS).

Conclusion: Human preference optimization is highlighted as a viable and efficient approach to enhance prosody in TTS systems when automated rewards are not feasible.

Abstract: Recent work reports gains in neural text-to-speech (TTS) with Group Relative
Policy Optimization (GRPO). However, in the absence of a verifiable reward for
\textit{prosody}, GRPO trained on transcription-oriented signals (CER/NLL)
lowers error rates yet collapses prosody into monotone, unnatural speech;
adding speaker-similarity further destabilizes training and degrades CER. We
address this with an \textit{iterative Direct Preference Optimization (DPO)}
scheme that uses only a few hundred human-labeled preference pairs per round to
directly optimize prosodic naturalness while regularizing to the current model.
On \textbf{KoCC-TTS}, a curated dataset of authentic Korean call center
interactions capturing task-oriented dialogues, our method attains the highest
human preference (ELO) with competitive CER, outperforming GRPO and strong
commercial baselines. These results suggest that when prosody cannot be
rewarded automatically, \textit{human preference optimization} offers a
practical and data-efficient path to natural and robust TTS. The demo page is
available at \href{https://tts.ch.dev}

</details>


### [489] [HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for Multi-Task Speech Language Modeling](https://arxiv.org/abs/2509.18570)
*Yuke Si,Runyan Yang,Yingying Gao,Junlan Feng,Chao Deng,Shilei Zhang*

Main category: eess.AS

TL;DR: Development of HarmoniFuse, a speech language model using gated encoders and prompt-adaptive fusion for multitask speech processing.


<details>
  <summary>Details</summary>
Motivation: Existing multitask speech language models struggle with task interference and performance issues due to naive parameter sharing or simplistic prompt-based conditioning, especially under limited data.

Method: Proposed HarmoniFuse, which includes a gated speech encoder for task-specific feature extraction, a prompt-adaptive fusion module to integrate transformer layers, and batch-interleaved training for independent datasets.

Result: HarmoniFuse demonstrates improved performance in ASR and SER tasks, showing scalability and robustness in multitask speech under data constraints.

Conclusion: HarmoniFuse effectively addresses challenges in multitask speech understanding by harmonizing task-specific demands, outperforming prior models.

Abstract: Recent advances in large language models have facilitated the development of
unified speech language models (SLMs) capable of supporting multiple speech
tasks within a shared architecture. However, tasks such as automatic speech
recognition (ASR) and speech emotion recognition (SER) rely on distinct types
of information: ASR primarily depends on linguistic content, whereas SER
requires the integration of both linguistic and paralinguistic cues. Existing
multitask SLMs typically adopt naive parameter sharing or prompt-based
conditioning without explicitly modeling the differences in information
composition required by each task. Such designs risk task interference and
performance degradation, especially under limited data conditions. To address
these limitations, we propose HarmoniFuse, a component-selective and
prompt-adaptive framework for multi-task speech language modeling. HarmoniFuse
is designed to harmonize heterogeneous task demands by selecting and fusing
task-relevant components of speech representations. Specifically, it integrates
a gated speech encoder to extract task-specific acoustic features and a
prompt-adaptive dynamic fusion module to aggregate transformer layers based on
task characteristics. In addition, a batch-interleaved training strategy
enables leveraging separate ASR and SER datasets without requiring joint
annotation. Experimental results demonstrate that HarmoniFuse improves both ASR
and SER performance, offering a scalable and robust solution for multitask
speech understanding under realistic data constraints.

</details>


### [490] [Teaching Audio Models to Reason: A Unified Framework for Source- and Layer-wise Distillation](https://arxiv.org/abs/2509.18579)
*Runyan Yang,Yuke Si,Yingying Gao,Junlan Feng,Chao Deng,Shilei Zhang*

Main category: eess.AS

TL;DR: The paper proposes a knowledge distillation framework to enhance reasoning capabilities in audio models by transferring knowledge from text models, improving audio reasoning performance.


<details>
  <summary>Details</summary>
Motivation: To overcome reasoning limitations in audio language models, which stem from modality gaps and lack of structured supervision.

Method: Introduced a unified knowledge distillation approach using source-wise and layer-wise distillation strategies to transfer reasoning from text models to audio models effectively.

Result: Experiments show substantial improvements in audio reasoning capabilities using the proposed framework.

Conclusion: The proposed distillation framework effectively bridges the gap between audio representations and reasoning, advancing audio modeling capabilities.

Abstract: While large audio language models excel at tasks like ASR and emotion
recognition, they still struggle with complex reasoning due to the modality gap
between audio and text as well as the lack of structured intermediate
supervision. To address this, we propose a unified knowledge distillation
framework to transfer reasoning capabilities from a high-capacity textual
teacher model to a student audio models while preserving its acoustic
competence. Our method introduces two key dimensions: source-wise distillation,
which leverages both textual and acoustic teachers to provide complementary
modality-specific supervision; and layer-wise distillation, which aligns
teacher signals with appropriate student layers to improve transfer efficiency.
This dual-dimensional strategy enables fine-grained control over the
distillation process, effectively bridging the gap between symbolic reasoning
and speech representations. Experimental results show significant improvements
in audio reasoning performance, demonstrating the effectiveness of our
framework as a reasoning transfer solution for audio modeling.

</details>


### [491] [SoundCompass: Navigating Target Sound Extraction With Effective Directional Clue Integration In Complex Acoustic Scenes](https://arxiv.org/abs/2509.18561)
*Dayun Choi,Jung-Woo Choi*

Main category: eess.AS

TL;DR: SoundCompass is a novel framework for extracting target sounds utilizing advanced spatial properties and directional clues (DoA) for improved adaptability and precision.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in previous DoA-based TSE methods that relied on hand-crafted or discrete features, resulting in a loss of fine-grained spatial information and compromised adaptability.

Method: SoundCompass incorporates the SPIN module to capture spatial correlations in multichannel signals, utilizes SH encoding for DoA representation, and employs an iterative refinement strategy called CoI for recursive inference.

Result: Experiments show that SoundCompass effectively extracts target sounds across varied signal classes and spatial configurations, demonstrating robustness and high performance.

Conclusion: SoundCompass is a significant advancement in TSE technology, combining spatial precision and iterative inference for versatile and accurate sound extraction.

Abstract: Recent advances in target sound extraction (TSE) utilize directional clues
derived from direction of arrival (DoA), which represent an inherent spatial
property of sound available in any acoustic scene. However, previous DoA-based
methods rely on hand-crafted features or discrete encodings, which lose
fine-grained spatial information and limit adaptability. We propose
SoundCompass, an effective directional clue integration framework centered on a
Spectral Pairwise INteraction (SPIN) module that captures cross-channel spatial
correlations in the complex spectrogram domain to preserve full spatial
information in multichannel signals. The input feature expressed in terms of
spatial correlations is fused with a DoA clue represented as spherical
harmonics (SH) encoding. The fusion is carried out across overlapping frequency
subbands, inheriting the benefits reported in the previous band-split
architectures. We also incorporate the iterative refinement strategy,
chain-of-inference (CoI), in the TSE framework, which recursively fuses DoA
with sound event activation estimated from the previous inference stage.
Experiments demonstrate that SoundCompass, combining SPIN, SH embedding, and
CoI, robustly extracts target sources across diverse signal classes and spatial
configurations.

</details>


### [492] [SynSonic: Augmenting Sound Event Detection through Text-to-Audio Diffusion ControlNet and Effective Sample Filtering](https://arxiv.org/abs/2509.18603)
*Jiarui Hai,Mounya Elhilali*

Main category: eess.AS

TL;DR: The paper introduces SynSonic, a method for sound event detection augmentation using text-to-audio diffusion models with temporal consistency, improving detection scores.


<details>
  <summary>Details</summary>
Motivation: Scarcity of temporally labeled data for Sound Event Detection creates the need for better augmentation techniques.

Method: Utilizing text-to-audio diffusion models with an energy-envelope ControlNet and joint score filtering using dual classifiers.

Result: SynSonic achieved improved PSDS1 and PSDS2 scores, showing enhanced temporal localization and sound class discrimination.

Conclusion: SynSonic demonstrates the potential of generative models in addressing temporal consistency and quality in sound event augmentation tasks.

Abstract: Data synthesis and augmentation are essential for Sound Event Detection (SED)
due to the scarcity of temporally labeled data. While augmentation methods like
SpecAugment and Mix-up can enhance model performance, they remain constrained
by the diversity of existing samples. Recent generative models offer new
opportunities, yet their direct application to SED is challenging due to the
lack of precise temporal annotations and the risk of introducing noise through
unreliable filtering. To address these challenges and enable generative-based
augmentation for SED, we propose SynSonic, a data augmentation method tailored
for this task. SynSonic leverages text-to-audio diffusion models guided by an
energy-envelope ControlNet to generate temporally coherent sound events. A
joint score filtering strategy with dual classifiers ensures sample quality,
and we explore its practical integration into training pipelines. Experimental
results show that SynSonic improves Polyphonic Sound Detection Scores (PSDS1
and PSDS2), enhancing both temporal localization and sound class
discrimination.

</details>


### [493] [FlexSED: Towards Open-Vocabulary Sound Event Detection](https://arxiv.org/abs/2509.18606)
*Jiarui Hai,Helin Wang,Weizhe Guo,Mounya Elhilali*

Main category: eess.AS

TL;DR: FlexSED introduces an open-vocabulary sound event detection framework that supports text-based queries, zero-shot, and few-shot capabilities, aiming to address limitations in traditional multi-class SED systems.


<details>
  <summary>Details</summary>
Motivation: Current multi-class classification frameworks for sound event detection systems lack support for free-text sound queries, zero-shot, and few-shot adaptability, limiting their flexibility and user-friendliness.

Method: The authors developed FlexSED by integrating a pretrained audio self-supervised learning model with the CLAP text encoder and introducing encoder-decoder composition, an adaptive fusion strategy, and LLM-assisted event query selection for training.

Result: FlexSED outperforms standard SED models on AudioSet-Strong and demonstrates strong zero-shot and few-shot capabilities during evaluation.

Conclusion: FlexSED offers a significant leap forward in sound event detection by combining robust pretrained architectures with innovative strategies for open-vocabulary and adaptive learning, making it valuable for academic and practical advancements.

Abstract: Despite recent progress in large-scale sound event detection (SED) systems
capable of handling hundreds of sound classes, existing multi-class
classification frameworks remain fundamentally limited. They cannot process
free-text sound queries, which enable more flexible and user-friendly
interaction, and they lack zero-shot capabilities and offer poor few-shot
adaptability. Although text-query-based separation methods have been explored,
they primarily focus on source separation and are ill-suited for SED tasks that
require precise temporal localization and efficient detection across large and
diverse sound vocabularies. In this paper, we propose FlexSED, an
open-vocabulary sound event detection system. FlexSED builds on a pretrained
audio SSL model and the CLAP text encoder, introducing an encoder-decoder
composition and an adaptive fusion strategy to enable effective continuous
training from pretrained weights. To ensure robust supervision, it also employs
large language models (LLMs) to assist in event query selection during
training, addressing challenges related to missing labels. As a result, FlexSED
achieves superior performance compared to vanilla SED models on
AudioSet-Strong, while demonstrating strong zero-shot and few-shot
capabilities. We release the code and pretrained models to support future
research and applications based on FlexSED.

</details>


### [494] [Training Flow Matching Models with Reliable Labels via Self-Purification](https://arxiv.org/abs/2509.19091)
*Hyeongju Kim,Yechan Yu,June Young Yi,Juheon Lee*

Main category: eess.AS

TL;DR: The paper introduces Self-Purifying Flow Matching (SPFM), a method for filtering unreliable data during training to address issues caused by noisy labels.


<details>
  <summary>Details</summary>
Motivation: To address performance degradation in models caused by mislabeled and noisy data in training datasets.

Method: SPFM identifies suspicious data during training using the model itself without requiring pretrained models or additional modules.

Result: Experiments showed SPFM enhances sample accuracy and conditioning adherence even with noisy data, outperforming existing baselines on the TITW dataset.

Conclusion: SPFM is a robust and effective method for improving model performance on imperfect datasets, especially for in-the-wild speech data.

Abstract: Training datasets are inherently imperfect, often containing mislabeled
samples due to human annotation errors, limitations of tagging models, and
other sources of noise. Such label contamination can significantly degrade the
performance of a trained model. In this work, we introduce Self-Purifying Flow
Matching (SPFM), a principled approach to filtering unreliable data within the
flow-matching framework. SPFM identifies suspicious data using the model itself
during the training process, bypassing the need for pretrained models or
additional modules. Our experiments demonstrate that models trained with SPFM
generate samples that accurately adhere to the specified conditioning, even
when trained on noisy labels. Furthermore, we validate the robustness of SPFM
on the TITW dataset, which consists of in-the-wild speech data, achieving
performance that surpasses existing baselines.

</details>


### [495] [Audio-Based Pedestrian Detection in the Presence of Vehicular Noise](https://arxiv.org/abs/2509.19295)
*Yonghyun Kim,Chaeyeon Han,Akash Sarode,Noah Posner,Subhrajit Guhathakurta,Alexander Lerch*

Main category: eess.AS

TL;DR: The paper investigates audio-based pedestrian detection in noisy vehicular environments, presents a new comprehensive roadside dataset, and conducts three robust analyses on model performance and robustness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance understanding and performance of audio-based pedestrian detection, especially in traffic-rich, noisy environments.

Method: The study uses a new 1321-hour roadside dataset with synchronized audio, pedestrian annotations, and video thumbnails, and evaluates models through cross-dataset testing, performance impact analysis, and robustness checks.

Result: Findings highlight the challenges of noisy environments on detection models and show varying predictive robustness across acoustic and out-of-domain contexts.

Conclusion: Audio-based pedestrian detection faces significant challenges in noisy environments, but the provided dataset and analyses contribute valuable insights into improving model robustness and performance.

Abstract: Audio-based pedestrian detection is a challenging task and has, thus far,
only been explored in noise-limited environments. We present a new dataset,
results, and a detailed analysis of the state-of-the-art in audio-based
pedestrian detection in the presence of vehicular noise. In our study, we
conduct three analyses: (i) cross-dataset evaluation between noisy and
noise-limited environments, (ii) an assessment of the impact of noisy data on
model performance, highlighting the influence of acoustic context, and (iii) an
evaluation of the model's predictive robustness on out-of-domain sounds. The
new dataset is a comprehensive 1321-hour roadside dataset. It incorporates
traffic-rich soundscapes. Each recording includes 16kHz audio synchronized with
frame-level pedestrian annotations and 1fps video thumbnails.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [496] [Agentic AutoSurvey: Let LLMs Survey LLMs](https://arxiv.org/abs/2509.18661)
*Yixin Liu,Yonghui Wu,Denghui Zhang,Lichao Sun*

Main category: cs.IR

TL;DR: Agentic AutoSurvey employs a multi-agent framework to enhance automated literature survey generation, achieving significant synthesis quality improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges researchers face in synthesizing knowledge due to the exponential growth of scientific literature.

Method: The approach involves four specialized agents collaborating: Paper Search Specialist, Topic Mining & Clustering, Academic Survey Writer, and Quality Evaluator.

Result: Through experiments on six LLM research topics, the multi-agent system surpassed existing baselines, scoring 8.18/10 compared to AutoSurvey's 4.77/10, processing up to 443 papers per topic.

Conclusion: Multi-agent frameworks provide marked improvements in generating literature surveys, offering better synthesis, organization, and critical analysis in dynamic scientific domains.

Abstract: The exponential growth of scientific literature poses unprecedented
challenges for researchers attempting to synthesize knowledge across rapidly
evolving fields. We present \textbf{Agentic AutoSurvey}, a multi-agent
framework for automated survey generation that addresses fundamental
limitations in existing approaches. Our system employs four specialized agents
(Paper Search Specialist, Topic Mining \& Clustering, Academic Survey Writer,
and Quality Evaluator) working in concert to generate comprehensive literature
surveys with superior synthesis quality. Through experiments on six
representative LLM research topics from COLM 2024 categories, we demonstrate
that our multi-agent approach achieves significant improvements over existing
baselines, scoring 8.18/10 compared to AutoSurvey's 4.77/10. The multi-agent
architecture processes 75--443 papers per topic (847 total across six topics)
while targeting high citation coverage (often $\geq$80\% on 75--100-paper sets;
lower on very large sets such as RLHF) through specialized agent orchestration.
Our 12-dimension evaluation captures organization, synthesis integration, and
critical analysis beyond basic metrics. These findings demonstrate that
multi-agent architectures represent a meaningful advancement for automated
literature survey generation in rapidly evolving scientific domains.

</details>


### [497] [The Ranking Blind Spot: Decision Hijacking in LLM-based Text Ranking](https://arxiv.org/abs/2509.18575)
*Yaoyao Qian,Yifan Zeng,Yuchao Jiang,Chelsi Jain,Huazheng Wang*

Main category: cs.IR

TL;DR: The paper explores vulnerabilities in Large Language Models (LLMs) during comparative evaluation tasks, focusing on a "Ranking Blind Spot." It demonstrates how malicious actors can exploit this to manipulate ranking systems, showing the effectiveness of such attacks on various LLMs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate how instruction-following capabilities in LLMs perform during multi-document comparison tasks and to identify weaknesses that could be exploited by malicious actors, specifically the "Ranking Blind Spot."

Method: Two attack approaches are proposed: Decision Objective Hijacking, which alters evaluation goals, and Decision Criteria Hijacking, which changes relevance standards. Experiments applied these attacks to various LLMs and ranking schemes, assessing their effectiveness.

Result: The experiments found that the proposed attacks effectively manipulated ranking systems across different LLMs and ranking schemes. Stronger LLMs were observed to be more vulnerable to these attacks.

Conclusion: The study concludes that LLMs' "Ranking Blind Spot" can be exploited by malicious content providers to manipulate rankings for increased exposure. Efforts are needed to strengthen LLM systems against such attacks.

Abstract: Large Language Models (LLMs) have demonstrated strong performance in
information retrieval tasks like passage ranking. Our research examines how
instruction-following capabilities in LLMs interact with multi-document
comparison tasks, identifying what we term the "Ranking Blind Spot", a
characteristic of LLM decision processes during comparative evaluation. We
analyze how this ranking blind spot affects LLM evaluation systems through two
approaches: Decision Objective Hijacking, which alters the evaluation goal in
pairwise ranking systems, and Decision Criteria Hijacking, which modifies
relevance standards across ranking schemes. These approaches demonstrate how
content providers could potentially influence LLM-based ranking systems to
affect document positioning. These attacks aim to force the LLM ranker to
prefer a specific passage and rank it at the top. Malicious content providers
can exploit this weakness, which helps them gain additional exposure by
attacking the ranker. In our experiment, We empirically show that the proposed
attacks are effective in various LLMs and can be generalized to multiple
ranking schemes. We apply these attack to realistic examples to show their
effectiveness. We also found stronger LLMs are more vulnerable to these
attacks. Our code is available at:
https://github.com/blindspotorg/RankingBlindSpot

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [498] [Reconstruction of Optical Coherence Tomography Images from Wavelength-space Using Deep-learning](https://arxiv.org/abs/2509.18783)
*Maryam Viqar,Erdem Sahin,Elena Stoykova,Violeta Madjarova*

Main category: physics.optics

TL;DR: The paper introduces a deep learning-based method for reconstructing speckle-reduced OCT images directly from the wavelength domain, reducing computational complexity and enhancing image quality.


<details>
  <summary>Details</summary>
Motivation: Current FD-OCT systems require significant computational resources or additional hardware for depth profile extraction and suffer from speckle noise.

Method: The study employs two sequential encoder-decoder networks: a Spatial Domain CNN (SD-CNN) for reducing noise and reconstructing structures, followed by a Fourier Domain CNN (FD-CNN) for further image quality enhancement in the Fourier domain.

Result: The proposed method achieves high-quality OCT images, both quantitatively and visually, while reducing computational complexity.

Conclusion: This approach demonstrates the feasibility of a computationally efficient, deep learning-driven framework for OCT image reconstruction, paving the way for future innovations in the field.

Abstract: Conventional Fourier-domain Optical Coherence Tomography (FD-OCT) systems
depend on resampling into wavenumber (k) domain to extract the depth profile.
This either necessitates additional hardware resources or amplifies the
existing computational complexity. Moreover, the OCT images also suffer from
speckle noise, due to systemic reliance on low coherence interferometry. We
propose a streamlined and computationally efficient approach based on
Deep-Learning (DL) which enables reconstructing speckle-reduced OCT images
directly from the wavelength domain. For reconstruction, two encoder-decoder
styled networks namely Spatial Domain Convolution Neural Network (SD-CNN) and
Fourier Domain CNN (FD-CNN) are used sequentially. The SD-CNN exploits the
highly degraded images obtained by Fourier transforming the domain fringes to
reconstruct the deteriorated morphological structures along with suppression of
unwanted noise. The FD-CNN leverages this output to enhance the image quality
further by optimization in Fourier domain (FD). We quantitatively and visually
demonstrate the efficacy of the method in obtaining high-quality OCT images.
Furthermore, we illustrate the computational complexity reduction by harnessing
the power of DL models. We believe that this work lays the framework for
further innovations in the realm of OCT image reconstruction.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [499] [Linear Regression under Missing or Corrupted Coordinates](https://arxiv.org/abs/2509.19242)
*Ilias Diakonikolas,Jelena Diakonikolas,Daniel M. Kane,Jasper C. H. Lee,Thanasis Pittas*

Main category: cs.DS

TL;DR: The paper studies multivariate linear regression with Gaussian covariates under adversarial missing and corrupted data scenarios and derives optimal error bounds for estimation.


<details>
  <summary>Details</summary>
Motivation: Understanding linear regression under adversarially missing or corrupted data, where the estimation error remains a positive function of the problem parameters, is important for addressing real-world challenges in presence of adversarial noise.

Method: The authors develop information-theoretic lower bounds for the achievable error and compare this error with computationally efficient algorithms across nearly the entire parameter range.

Result: The paper establishes that the optimal error for missing data matches that of corrupted data, indicating that corruption location knowledge does not provide an advantage.

Conclusion: This research advances the understanding of linear regression under adversarial data issues by fully characterizing achievable errors and aligning them with feasible algorithmic performance.

Abstract: We study multivariate linear regression under Gaussian covariates in two
settings, where data may be erased or corrupted by an adversary under a
coordinate-wise budget. In the incomplete data setting, an adversary may
inspect the dataset and delete entries in up to an $\eta$-fraction of samples
per coordinate; a strong form of the Missing Not At Random model. In the
corrupted data setting, the adversary instead replaces values arbitrarily, and
the corruption locations are unknown to the learner. Despite substantial work
on missing data, linear regression under such adversarial missingness remains
poorly understood, even information-theoretically. Unlike the clean setting,
where estimation error vanishes with more samples, here the optimal error
remains a positive function of the problem parameters. Our main contribution is
to characterize this error up to constant factors across essentially the entire
parameter range. Specifically, we establish novel information-theoretic lower
bounds on the achievable error that match the error of (computationally
efficient) algorithms. A key implication is that, perhaps surprisingly, the
optimal error in the missing data setting matches that in the corruption
setting-so knowing the corruption locations offers no general advantage.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [500] [Reversible Kalman Filter for state estimation with Manifold](https://arxiv.org/abs/2509.18224)
*Svyatoslav Covanov,Cedric Pradalier*

Main category: eess.SY

TL;DR: An advanced Kalman filter-based algorithm for accurate state estimation on manifolds, addressing prior limitations and enabling high-precision trajectory reconstruction.


<details>
  <summary>Details</summary>
Motivation: To develop a Kalman filter approach that overcomes the lack of precision and divergence issues present in existing methods, especially for synthetic data analysis.

Method: A novel filter was designed that removes the dependency on the small-velocity assumption, instead relying on precise sensor noise modeling and specific heuristic detection algorithms.

Result: The new filter corrects divergences of prior Kalman filters, supports high precision when using various sensor configurations, and demonstrates its utility in underwater trajectory reconstruction.

Conclusion: The proposed approach offers a numerically stable and accurate Kalman filter-based state estimation framework, adaptable for diverse scenarios.

Abstract: This work introduces an algorithm for state estimation on manifolds within
the framework of the Kalman filter. Its primary objective is to provide a
methodology enabling the evaluation of the precision of existing Kalman filter
variants with arbitrary accuracy on synthetic data, something that, to the best
of our knowledge, has not been addressed in prior work. To this end, we develop
a new filter that exhibits favorable numerical properties, thereby correcting
the divergences observed in previous Kalman filter variants. In this
formulation, the achievable precision is no longer constrained by the
small-velocity assumption and is determined solely by sensor noise. In
addition, this new filter assumes high precision on the sensors, which, in real
scenarios require a detection step that we define heuristically, allowing one
to extend this approach to scenarios, using either a 9-axis IMU or a
combination of odometry, accelerometer, and pressure sensors. The latter
configuration is designed for the reconstruction of trajectories in underwater
environments.

</details>


### [501] [Policy Gradient with Self-Attention for Model-Free Distributed Nonlinear Multi-Agent Games](https://arxiv.org/abs/2509.18371)
*Eduardo Sebastián,Maitrayee Keskar,Eeman Iqbal,Eduardo Montijano,Carlos Sagüés,Nikolay Atanasov*

Main category: eess.SY

TL;DR: The paper tackles model-free multi-agent games in dynamic nonlinear contexts, proposing a distributed policy gradient approach inspired by linear quadratic games and leveraging self-attention layers for modeling time-varying communication structures.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in dynamic nonlinear multi-agent games, particularly the time-varying interactions and non-stationarity of Nash equilibria, in settings where transition and cost functions are unknown.

Method: The authors propose a policy gradient method to learn distributed policies inspired by linear quadratic games, using nonlinear feedback gains parameterized by self-attention layers to adapt to varying multi-agent communication topology.

Result: The approach demonstrates strong performance across various settings, including distributed linear and nonlinear regulation tasks, and simulated as well as real-world multi-robot pursuit-and-evasion scenarios.

Conclusion: A robust distributed policy gradient technique for model-free multi-agent games is presented, demonstrating its adaptability and efficacy in dynamic environments with time-varying interaction structures.

Abstract: Multi-agent games in dynamic nonlinear settings are challenging due to the
time-varying interactions among the agents and the non-stationarity of the
(potential) Nash equilibria. In this paper we consider model-free games, where
agent transitions and costs are observed without knowledge of the transition
and cost functions that generate them. We propose a policy gradient approach to
learn distributed policies that follow the communication structure in
multi-team games, with multiple agents per team. Our formulation is inspired by
the structure of distributed policies in linear quadratic games, which take the
form of time-varying linear feedback gains. In the nonlinear case, we model the
policies as nonlinear feedback gains, parameterized by self-attention layers to
account for the time-varying multi-agent communication topology. We demonstrate
that our distributed policy gradient approach achieves strong performance in
several settings, including distributed linear and nonlinear regulation, and
simulated and real multi-robot pursuit-and-evasion games.

</details>


### [502] [Dual Iterative Learning Control for Multiple-Input Multiple-Output Dynamics with Validation in Robotic Systems](https://arxiv.org/abs/2509.18723)
*Jan-Hendrik Ewering,Alessandro Papa,Simon F. G. Ehlers,Thomas Seel,Michael Meindl*

Main category: eess.SY

TL;DR: This paper introduces MIMO Dual Iterative Learning Control (DILC), a data-driven method for autonomous motion control and model learning in MIMO systems, excelling in automation without prior knowledge or manual tuning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address key challenges in autonomous motion control, particularly coping with unknown dynamics and eliminating the need for manual parameter tuning in complex MIMO systems.

Method: MIMO Dual Iterative Learning Control (DILC) is designed to integrate tracking control and model learning, providing monotonic convergence conditions for both tracking error and model error in linear time-invariant systems.

Result: The DILC scheme demonstrated autonomous performance in industrial robot simulations and nonlinear real-world MIMO systems, solving motion tasks rapidly within 10-20 trials for simpler tasks and under 100 trials for complex motions.

Conclusion: DILC's rapid and autonomous learning capabilities position it as an efficient tool for achieving genuine autonomy in intelligent real-world systems, making it suitable for incorporation into broader learning frameworks.

Abstract: Solving motion tasks autonomously and accurately is a core ability for
intelligent real-world systems. To achieve genuine autonomy across multiple
systems and tasks, key challenges include coping with unknown dynamics and
overcoming the need for manual parameter tuning, which is especially crucial in
complex Multiple-Input Multiple-Output (MIMO) systems.
  This paper presents MIMO Dual Iterative Learning Control (DILC), a novel
data-driven iterative learning scheme for simultaneous tracking control and
model learning, without requiring any prior system knowledge or manual
parameter tuning. The method is designed for repetitive MIMO systems and
integrates seamlessly with established iterative learning control methods. We
provide monotonic convergence conditions for both reference tracking error and
model error in linear time-invariant systems.
  The DILC scheme -- rapidly and autonomously -- solves various motion tasks in
high-fidelity simulations of an industrial robot and in multiple nonlinear
real-world MIMO systems, without requiring model knowledge or manually tuning
the algorithm. In our experiments, many reference tracking tasks are solved
within 10-20 trials, and even complex motions are learned in less than 100
iterations. We believe that, because of its rapid and autonomous learning
capabilities, DILC has the potential to serve as an efficient building block
within complex learning frameworks for intelligent real-world systems.

</details>


### [503] [An Extended Kalman Filter for Systems with Infinite-Dimensional Measurements](https://arxiv.org/abs/2509.18749)
*Maxwell M. Varley,Timothy L. Molloy,Girish N. Nair*

Main category: eess.SY

TL;DR: The paper develops an extended Kalman filter (EKF) for state estimation in nonlinear systems with infinite-dimensional measurements, showing its effectiveness in vision-based localization and outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: The work is driven by the need for accurate state estimation in scenarios like vision-based localization and tracking, where measurements often span infinite dimensions, such as image data.

Method: An extended Kalman filter (EKF) is formulated, where infinite-dimensional measurement noise is modeled as a random field. Image gradients are derived as measurement Jacobians, providing theoretical justification for their use in vision-based state estimation.

Result: The EKF demonstrates superior performance compared to the VINS-MONO algorithm in drone localization tasks using a public dataset, achieving up to an order of magnitude in MSE reduction in some cases.

Conclusion: The paper introduces a theoretically sound and practically effective approach to vision-based state estimation, highlighting the importance of image gradients and setting new benchmarks in performance.

Abstract: This article examines state estimation in discrete-time nonlinear stochastic
systems with finite-dimensional states and infinite-dimensional measurements,
motivated by real-world applications such as vision-based localization and
tracking. We develop an extended Kalman filter (EKF) for real-time state
estimation, with the measurement noise modeled as an infinite-dimensional
random field. When applied to vision-based state estimation, the measurement
Jacobians required to implement the EKF are shown to correspond to image
gradients. This result provides a novel system-theoretic justification for the
use of image gradients as features for vision-based state estimation,
contrasting with their (often heuristic) introduction in many computer-vision
pipelines. We demonstrate the practical utility of the EKF on a public
real-world dataset involving the localization of an aerial drone using video
from a downward-facing monocular camera. The EKF is shown to outperform
VINS-MONO, an established visual-inertial odometry algorithm, in some cases
achieving mean squared error reductions of up to an order of magnitude.

</details>


### [504] [A Fast Initialization Method for Neural Network Controllers: A Case Study of Image-based Visual Servoing Control for the multicopter Interception](https://arxiv.org/abs/2509.19110)
*Chenxu Ke,Congling Tian,Kaichen Xu,Ye Li,Lingcong Bao*

Main category: eess.SY

TL;DR: This paper proposes a method for initializing stable neural network controllers using datasets that meet stability conditions, addressing issues like large data requirements and slow convergence in reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in reinforcement learning-based controller design, such as needing substantial data, slow convergence, randomness, and requiring specialized knowledge for stable controllers.

Method: A neural network rapid initialization method is developed by constructing datasets based on system model-derived stability conditions, enabling better training for control policies.

Result: Simulations and experiments, including a multicopter interception case study, validate the method's performance. The trained policy achieves a significant interception speed of 15 m/s.

Conclusion: The method streamlines the initial training of neural network controllers, reducing dependence on traditional control theory expertise and enhancing practical application performance.

Abstract: Reinforcement learning-based controller design methods often require
substantial data in the initial training phase. Moreover, the training process
tends to exhibit strong randomness and slow convergence. It often requires
considerable time or high computational resources. Another class of
learning-based method incorporates Lyapunov stability theory to obtain a
control policy with stability guarantees. However, these methods generally
require an initially stable neural network control policy at the beginning of
training. Evidently, a stable neural network controller can not only serve as
an initial policy for reinforcement learning, allowing the training to focus on
improving controller performance, but also act as an initial state for
learning-based Lyapunov control methods. Although stable controllers can be
designed using traditional control theory, designers still need to have a great
deal of control design knowledge to address increasingly complicated control
problems. The proposed neural network rapid initialization method in this paper
achieves the initial training of the neural network control policy by
constructing datasets that conform to the stability conditions based on the
system model. Furthermore, using the image-based visual servoing control for
multicopter interception as a case study, simulations and experiments were
conducted to validate the effectiveness and practical performance of the
proposed method. In the experiment, the trained control policy attains a final
interception velocity of 15 m/s.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [505] [AlloyInter: Visualising Alloy Mixture Interpolations in t-SNE Representations](https://arxiv.org/abs/2509.19202)
*Benedikt Kantz,Peter Waldert,Stefan Lengauer,Tobias Schreck*

Main category: cs.CE

TL;DR: AlloyInter is a system that combines XAI and interpolation to explore input mixture ratios based on output goals.


<details>
  <summary>Details</summary>
Motivation: To enhance exploration of input-output spaces in the SciVis Contest 2025 by enabling adjustable goals using AI-guided methods.

Method: Developed a system using a learned model ensemble, XAI, and manifold learning to facilitate input-output interpolation.

Result: Provides users with a tool to iteratively refine input mixtures for specified output parameters.

Conclusion: The system builds on robust XAI techniques and achieves an innovative approach for goal-oriented exploration.

Abstract: This entry description proposes AlloyInter, a novel system to enable joint
exploration of input mixtures and output parameters space in the context of the
SciVis Contest 2025. We propose an interpolation approach, guided by
eXplainable Artificial Intelligence (XAI) based on a learned model ensemble
that allows users to discover input mixture ratios by specifying output
parameter goals that can be iteratively adjusted and improved towards a goal.
We strengthen the capabilities of our system by building upon prior research
within the robustness of XAI, as well as combining well-established techniques
like manifold learning with interpolation approaches.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [506] [Zero-Shot Visual Deepfake Detection: Can AI Predict and Prevent Fake Content Before It's Created?](https://arxiv.org/abs/2509.18461)
*Ayan Sar,Sampurna Roy,Tanupriya Choudhury,Ajith Abraham*

Main category: cs.GR

TL;DR: The paper explores zero-shot detection methods and preventive strategies to combat evolving deepfake threats, emphasizing interdisciplinary collaboration.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the threats posed by deepfakes to digital security, media integrity, and public trust, especially as the technology rapidly evolves.

Method: The methods include self-supervised learning, transformer-based zero-shot classifiers, generative model fingerprinting, and meta-learning for detection. Preventive strategies include adversarial perturbations, digital watermarking, AI monitoring, and blockchain frameworks.

Result: Challenges such as adversarial attacks, scalability issues, and ethical dilemmas were identified, with solutions proposed in the form of future research directions like explainable AI, multimodal fusion, quantum AI, and federated learning.

Conclusion: An integrated defense framework combining zero-shot learning and preventive techniques is needed, alongside collaboration among AI researchers, cybersecurity experts, and policymakers.

Abstract: Generative adversarial networks (GANs) and diffusion models have dramatically
advanced deepfake technology, and its threats to digital security, media
integrity, and public trust have increased rapidly. This research explored
zero-shot deepfake detection, an emerging method even when the models have
never seen a particular deepfake variation. In this work, we studied
self-supervised learning, transformer-based zero-shot classifier, generative
model fingerprinting, and meta-learning techniques that better adapt to the
ever-evolving deepfake threat. In addition, we suggested AI-driven prevention
strategies that mitigated the underlying generation pipeline of the deepfakes
before they occurred. They consisted of adversarial perturbations for creating
deepfake generators, digital watermarking for content authenticity
verification, real-time AI monitoring for content creation pipelines, and
blockchain-based content verification frameworks. Despite these advancements,
zero-shot detection and prevention faced critical challenges such as
adversarial attacks, scalability constraints, ethical dilemmas, and the absence
of standardized evaluation benchmarks. These limitations were addressed by
discussing future research directions on explainable AI for deepfake detection,
multimodal fusion based on image, audio, and text analysis, quantum AI for
enhanced security, and federated learning for privacy-preserving deepfake
detection. This further highlighted the need for an integrated defense
framework for digital authenticity that utilized zero-shot learning in
combination with preventive deepfake mechanisms. Finally, we highlighted the
important role of interdisciplinary collaboration between AI researchers,
cybersecurity experts, and policymakers to create resilient defenses against
the rising tide of deepfake attacks.

</details>


### [507] [Differentiable Light Transport with Gaussian Surfels via Adapted Radiosity for Efficient Relighting and Geometry Reconstruction](https://arxiv.org/abs/2509.18497)
*Kaiwen Jiang,Jia-Mu Sun,Zilu Li,Dan Wang,Tzu-Mao Li,Ravi Ramamoorthi*

Main category: cs.GR

TL;DR: The paper proposes an efficient framework for incorporating physically-based rendering into radiance fields, enabling better geometry reconstruction and realistic relighting.


<details>
  <summary>Details</summary>
Motivation: Address the limitations in radiance fields lacking accurate modeling of reflective properties and lighting conditions, hindering relighting and geometry clarity.

Method: Introduce Gaussian surfels combined with radiosity theory for differentiable light transport. Extend radiosity for semi-opaque primitives, solve light transport in spherical harmonics space, and improve optimization efficiency.

Result: Achieves superior geometry reconstruction, view synthesis, and relighting compared to baselines, with rapid inference speeds and global illumination effects.

Conclusion: The framework significantly enhances radiance fields' capabilities in sparse datasets, allowing more accurate and efficient rendering solutions under diverse lighting setups.

Abstract: Radiance fields have gained tremendous success with applications ranging from
novel view synthesis to geometry reconstruction, especially with the advent of
Gaussian splatting. However, they sacrifice modeling of material reflective
properties and lighting conditions, leading to significant geometric
ambiguities and the inability to easily perform relighting. One way to address
these limitations is to incorporate physically-based rendering, but it has been
prohibitively expensive to include full global illumination within the inner
loop of the optimization. Therefore, previous works adopt simplifications that
make the whole optimization with global illumination effects efficient but less
accurate. In this work, we adopt Gaussian surfels as the primitives and build
an efficient framework for differentiable light transport, inspired from the
classic radiosity theory. The whole framework operates in the coefficient space
of spherical harmonics, enabling both diffuse and specular materials. We extend
the classic radiosity into non-binary visibility and semi-opaque primitives,
propose novel solvers to efficiently solve the light transport, and derive the
backward pass for gradient optimizations, which is more efficient than
auto-differentiation. During inference, we achieve view-independent rendering
where light transport need not be recomputed under viewpoint changes, enabling
hundreds of FPS for global illumination effects, including view-dependent
reflections using a spherical harmonics representation. Through extensive
qualitative and quantitative experiments, we demonstrate superior geometry
reconstruction, view synthesis and relighting than previous inverse rendering
baselines, or data-driven baselines given relatively sparse datasets with known
or unknown lighting conditions.

</details>


### [508] [Text Slider: Efficient and Plug-and-Play Continuous Concept Control for Image/Video Synthesis via LoRA Adapters](https://arxiv.org/abs/2509.18831)
*Pin-Yen Chiu,I-Sheng Fang,Jun-Cheng Chen*

Main category: cs.GR

TL;DR: The paper introduces Text Slider, a lightweight and plug-and-play framework for efficient and scalable control in text-to-image and text-to-video synthesis, using pre-trained text encoders.


<details>
  <summary>Details</summary>
Motivation: Existing concept control methods in diffusion models are inefficient, resource-intensive, and lack adaptability to different backbones, necessitating a scalable and lightweight solution.

Method: Text Slider identifies low-rank directions within pre-trained text encoders, enabling continuous control of visual concepts with significantly reduced training time, GPU memory usage, and the number of trainable parameters.

Result: Text Slider achieves 5x faster training compared to Concept Slider, 47x faster than Attribute Control, and reduces GPU memory usage by nearly 2x and 4x, respectively. It allows smooth modulation of attributes while preserving input structure.

Conclusion: Text Slider is an efficient and adaptable framework that significantly improves the efficiency and scalability of fine-grained control in diffusion models without compromising output quality.

Abstract: Recent advances in diffusion models have significantly improved image and
video synthesis. In addition, several concept control methods have been
proposed to enable fine-grained, continuous, and flexible control over
free-form text prompts. However, these methods not only require intensive
training time and GPU memory usage to learn the sliders or embeddings but also
need to be retrained for different diffusion backbones, limiting their
scalability and adaptability. To address these limitations, we introduce Text
Slider, a lightweight, efficient and plug-and-play framework that identifies
low-rank directions within a pre-trained text encoder, enabling continuous
control of visual concepts while significantly reducing training time, GPU
memory consumption, and the number of trainable parameters. Furthermore, Text
Slider supports multi-concept composition and continuous control, enabling
fine-grained and flexible manipulation in both image and video synthesis. We
show that Text Slider enables smooth and continuous modulation of specific
attributes while preserving the original spatial layout and structure of the
input. Text Slider achieves significantly better efficiency: 5$\times$ faster
training than Concept Slider and 47$\times$ faster than Attribute Control,
while reducing GPU memory usage by nearly 2$\times$ and 4$\times$,
respectively.

</details>


### [509] [One-shot Embroidery Customization via Contrastive LoRA Modulation](https://arxiv.org/abs/2509.18948)
*Jun Ma,Qian He,Gaofeng He,Huang Chen,Chen Liu,Xiaogang Jin,Huamin Wang*

Main category: cs.GR

TL;DR: The paper proposes a novel contrastive learning framework for fine-grained visual feature transfer in embroidery customization and other image manipulation tasks, achieving better style-content separation and superior performance over previous methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of transferring fine-grained visual features, particularly in embroidery—a textile art form that involves complex details—which poses limitations for existing style transfer methods.

Method: The framework builds on image analogy using pretrained diffusion models for style-content separation. It integrates a two-stage contrastive LoRA modulation approach: first, to separate style from content and second, to refine this separation further using self-knowledge distillation.

Result: The proposed method outperforms prior approaches in embroidery customization and generalizes effectively to other domains including artistic style transfer, sketch colorization, and appearance transfer.

Conclusion: The paper demonstrates that the proposed approach offers effective fine-grained style transfer, contributing a significant advancement to both embroidery customization and broader image manipulation applications.

Abstract: Diffusion models have significantly advanced image manipulation techniques,
and their ability to generate photorealistic images is beginning to transform
retail workflows, particularly in presale visualization. Beyond artistic style
transfer, the capability to perform fine-grained visual feature transfer is
becoming increasingly important. Embroidery is a textile art form characterized
by intricate interplay of diverse stitch patterns and material properties,
which poses unique challenges for existing style transfer methods. To explore
the customization for such fine-grained features, we propose a novel
contrastive learning framework that disentangles fine-grained style and content
features with a single reference image, building on the classic concept of
image analogy. We first construct an image pair to define the target style, and
then adopt a similarity metric based on the decoupled representations of
pretrained diffusion models for style-content separation. Subsequently, we
propose a two-stage contrastive LoRA modulation technique to capture
fine-grained style features. In the first stage, we iteratively update the
whole LoRA and the selected style blocks to initially separate style from
content. In the second stage, we design a contrastive learning strategy to
further decouple style and content through self-knowledge distillation.
Finally, we build an inference pipeline to handle image or text inputs with
only the style blocks. To evaluate our method on fine-grained style transfer,
we build a benchmark for embroidery customization. Our approach surpasses prior
methods on this task and further demonstrates strong generalization to three
additional domains: artistic style transfer, sketch colorization, and
appearance transfer.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [510] [Bayesian Calibration and Model Assessment of Cell Migration Dynamics with Surrogate Model Integration](https://arxiv.org/abs/2509.18998)
*Christina Schenk,Jacobo Ayensa Jiménez,Ignacio Romero*

Main category: math.AP

TL;DR: The paper uses Bayesian calibration strategies to evaluate parameter uncertainties and discrepancies in cell migration models, focusing on glioblastoma progression.


<details>
  <summary>Details</summary>
Motivation: Computational models in biology, such as those for cancer progression, often face challenges in calibration due to their complexity and parameter richness.

Method: The paper evaluates four Bayesian calibration strategies—parametric and surrogate models, each with and without explicit model discrepancy.

Result: Surrogate models were computationally efficient with high predictive accuracy, while parametric models showed better reliability in parameter estimation. Introducing model discrepancy helped identify structural limitations.

Conclusion: The findings provide practical guidance for refining computational models to improve both their predictive performance and interpretability in biology.

Abstract: Computational models provide crucial insights into complex biological
processes such as cancer evolution, but their mechanistic nature often makes
them nonlinear and parameter-rich, complicating calibration. We systematically
evaluate parameter probability distributions in cell migration models using
Bayesian calibration across four complementary strategies: parametric and
surrogate models, each with and without explicit model discrepancy. This
approach enables joint analysis of parameter uncertainty, predictive
performance, and interpretability. Applied to a real data experiment of
glioblastoma progression in microfluidic devices, surrogate models achieve
higher computational efficiency and predictive accuracy, whereas parametric
models yield more reliable parameter estimates due to their mechanistic
grounding. Incorporating model discrepancy exposes structural limitations,
clarifying where model refinement is necessary. Together, these comparisons
offer practical guidance for calibrating and improving computational models of
complex biological systems.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [511] [Automatic Classification of Magnetic Chirality of Solar Filaments from H-Alpha Observations](https://arxiv.org/abs/2509.18214)
*Alexis Chalmers,Azim Ahmadzadeh*

Main category: astro-ph.SR

TL;DR: This study uses advanced image classification models to classify solar filament chirality with the MAGFiLO dataset, achieving high per-class accuracy.


<details>
  <summary>Details</summary>
Motivation: Previous research faced challenges due to small datasets, limiting the study's reproducibility and generalizability. This paper aims to establish a reliable baseline for filament chirality classification using a large-scale dataset.

Method: The researchers used manually-annotated filaments from the MAGFiLO dataset, fine-tuned pre-trained image classification models like ConvNeXt and applied techniques such as data augmentation and per-class loss weights.

Result: The best-performing model, ConvNeXtBase, achieved 0.69 accuracy for left chirality and 0.73 accuracy for right chirality filaments.

Conclusion: The study greatly improves upon prior efforts by leveraging a larger dataset and advanced methodologies, setting a benchmark for solar filament chirality classification.

Abstract: In this study, we classify the magnetic chirality of solar filaments from
H-Alpha observations using state-of-the-art image classification models. We
establish the first reproducible baseline for solar filament chirality
classification on the MAGFiLO dataset. The MAGFiLO dataset contains over 10,000
manually-annotated filaments from GONG H-Alpha observations, making it the
largest dataset for filament detection and classification to date. Prior
studies relied on much smaller datasets, which limited their generalizability
and comparability. We fine-tuned several pre-trained, image classification
architectures, including ResNet, WideResNet, ResNeXt, and ConvNeXt, and also
applied data augmentation and per-class loss weights to optimize the models.
Our best model, ConvNeXtBase, achieves a per-class accuracy of 0.69 for left
chirality filaments and $0.73$ for right chirality filaments.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [512] [Efficient Breast and Ovarian Cancer Classification via ViT-Based Preprocessing and Transfer Learning](https://arxiv.org/abs/2509.18553)
*Richa Rawat,Faisal Ahmed*

Main category: eess.IV

TL;DR: This paper proposes a Vision Transformer (ViT)-based method for detecting and classifying breast and ovarian cancer, outperforming existing approaches in both binary and multi-class classification tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional cancer detection methods are labor-intensive, requiring trained pathologists to manually examine imaging data, making the process time-consuming and resource-intensive.

Method: The authors fine-tuned a pre-trained ViT-Base-Patch16-224 model for binary and multi-class classification tasks using histopathological image datasets. A preprocessing pipeline was developed to standardize images into PyTorch tensors compatible with the ViT architecture.

Result: The proposed model outperformed existing CNN, ViT, and topological methods in binary and multi-class classification tasks, as tested on the BreakHis and UBC-OCEAN benchmark datasets respectively.

Conclusion: Vision Transformer-based transfer learning combined with efficient preprocessing provides an effective solution for oncological diagnostics in breast and ovarian cancer detection.

Abstract: Cancer is one of the leading health challenges for women, specifically breast
and ovarian cancer. Early detection can help improve the survival rate through
timely intervention and treatment. Traditional methods of detecting cancer
involve manually examining mammograms, CT scans, ultrasounds, and other imaging
types. However, this makes the process labor-intensive and requires the
expertise of trained pathologists. Hence, making it both time-consuming and
resource-intensive. In this paper, we introduce a novel vision transformer
(ViT)-based method for detecting and classifying breast and ovarian cancer. We
use a pre-trained ViT-Base-Patch16-224 model, which is fine-tuned for both
binary and multi-class classification tasks using publicly available
histopathological image datasets. Further, we use a preprocessing pipeline that
converts raw histophological images into standardized PyTorch tensors, which
are compatible with the ViT architecture and also help improve the model
performance. We evaluated the performance of our model on two benchmark
datasets: the BreakHis dataset for binary classification and the UBC-OCEAN
dataset for five-class classification without any data augmentation. Our model
surpasses existing CNN, ViT, and topological data analysis-based approaches in
binary classification. For multi-class classification, it is evaluated against
recent topological methods and demonstrates superior performance. Our study
highlights the effectiveness of Vision Transformer-based transfer learning
combined with efficient preprocessing in oncological diagnostics.

</details>


### [513] [MOIS-SAM2: Exemplar-based Segment Anything Model 2 for multilesion interactive segmentation of neurobromas in whole-body MRI](https://arxiv.org/abs/2509.19277)
*Georgii Kolokolnikov,Marie-Lena Schmalhofer,Sophie Götz,Lennart Well,Said Farschtschi,Victor-Felix Mautner,Inka Ristow,Rene Werner*

Main category: eess.IV

TL;DR: This paper introduces MOIS-SAM2, a new interactive segmentation model designed for efficient neurofibroma detection using WB-MRI, excelling in precision and scalability compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the limitations of current segmentation methods for neurofibromatosis type 1, which struggle with precision and scalability required for handling large volumes of lesions in WB-MRI scans.

Method: The researchers developed MOIS-SAM2, a transformer-based multi-object segmentation model incorporating exemplar-based semantic propagation and evaluated it on WB-MRI datasets containing domain shifts like scanner variation.

Result: MOIS-SAM2 achieved significant improvement in scan-wise DSC (0.60) over baselines like 3D nnU-Net (0.54) and SAM2 (0.35) on the test set, demonstrating strong generalization under various challenging scenarios.

Conclusion: MOIS-SAM2 offers an efficient, scalable, and generalizable interactive segmentation method for neurofibroma detection in WB-MRI, making it well-suited for clinical use.

Abstract: Background and Objectives: Neurofibromatosis type 1 is a genetic disorder
characterized by the development of numerous neurofibromas (NFs) throughout the
body. Whole-body MRI (WB-MRI) is the clinical standard for detection and
longitudinal surveillance of NF tumor growth. Existing interactive segmentation
methods fail to combine high lesion-wise precision with scalability to hundreds
of lesions. This study proposes a novel interactive segmentation model tailored
to this challenge.
  Methods: We introduce MOIS-SAM2, a multi-object interactive segmentation
model that extends the state-of-the-art, transformer-based, promptable Segment
Anything Model 2 (SAM2) with exemplar-based semantic propagation. MOIS-SAM2 was
trained and evaluated on 119 WB-MRI scans from 84 NF1 patients acquired using
T2-weighted fat-suppressed sequences. The dataset was split at the patient
level into a training set and four test sets (one in-domain and three
reflecting different domain shift scenarios, e.g., MRI field strength
variation, low tumor burden, differences in clinical site and scanner vendor).
  Results: On the in-domain test set, MOIS-SAM2 achieved a scan-wise DSC of
0.60 against expert manual annotations, outperforming baseline 3D nnU-Net (DSC:
0.54) and SAM2 (DSC: 0.35). Performance of the proposed model was maintained
under MRI field strength shift (DSC: 0.53) and scanner vendor variation (DSC:
0.50), and improved in low tumor burden cases (DSC: 0.61). Lesion detection F1
scores ranged from 0.62 to 0.78 across test sets. Preliminary inter-reader
variability analysis showed model-to-expert agreement (DSC: 0.62-0.68),
comparable to inter-expert agreement (DSC: 0.57-0.69).
  Conclusions: The proposed MOIS-SAM2 enables efficient and scalable
interactive segmentation of NFs in WB-MRI with minimal user input and strong
generalization, supporting integration into clinical workflows.

</details>


### [514] [Measurement Score-Based MRI Reconstruction with Automatic Coil Sensitivity Estimation](https://arxiv.org/abs/2509.18402)
*Tingjun Liu,Chicago Y. Park,Yuyang Hu,Hongyu An,Ulugbek S. Kamilov*

Main category: eess.IV

TL;DR: The paper introduces C-MSM, a diffusion-based method for MRI reconstruction that eliminates the need for pre-calibrated coil sensitivity maps and ground truth images.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based solvers for MRI reconstruction face practical challenges due to reliance on pre-calibrated sensitivity maps and lack of ground-truth images.

Method: C-MSM combines automatic coil sensitivity map estimation with self-supervised learning of measurement scores, working directly from raw k-space data.

Result: Experiments prove that C-MSM's performance is comparable to state-of-the-art methods, even without clean priors, pre-calibrated maps, or training data.

Conclusion: C-MSM represents a practical and robust advancement for MRI image reconstruction, addressing accuracy challenges under heavy undersampling and limited ground truth.

Abstract: Diffusion-based inverse problem solvers (DIS) have recently shown outstanding
performance in compressed-sensing parallel MRI reconstruction by combining
diffusion priors with physical measurement models. However, they typically rely
on pre-calibrated coil sensitivity maps (CSMs) and ground truth images, making
them often impractical: CSMs are difficult to estimate accurately under heavy
undersampling and ground-truth images are often unavailable. We propose
Calibration-free Measurement Score-based diffusion Model (C-MSM), a new method
that eliminates these dependencies by jointly performing automatic CSM
estimation and self-supervised learning of measurement scores directly from
k-space data. C-MSM reconstructs images by approximating the full posterior
distribution through stochastic sampling over partial measurement posterior
scores, while simultaneously estimating CSMs. Experiments on the multi-coil
brain fastMRI dataset show that C-MSM achieves reconstruction performance close
to DIS with clean diffusion priors -- even without access to clean training
data and pre-calibrated CSMs.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [515] [Neural Network-Driven Direct CBCT-Based Dose Calculation for Head-and-Neck Proton Treatment Planning](https://arxiv.org/abs/2509.18378)
*Muheng Li,Evangelia Choulilitsa,Lisa Fankhauser,Francesca Albertini,Antony Lomax,Ye Zhang*

Main category: physics.med-ph

TL;DR: This paper proposes a deep learning method using xLSTM neural networks for direct proton dose calculation from CBCT images, achieving high accuracy and efficiency without traditional correction workflows.


<details>
  <summary>Details</summary>
Motivation: Traditional proton dose calculations on CBCT images are limited by poor image quality, which necessitates complex correction workflows. This study aims to streamline and improve accuracy in adaptive treatment planning.

Method: The researchers developed an xLSTM-based neural network (CBCT-NN) trained on a dataset of 40 head-and-neck cancer patients, incorporating energy token encoding and beam's-eye-view sequence modeling. The network learned from 82,500 paired beam configurations with Monte Carlo-generated ground truth doses.

Result: The CBCT-NN achieved a gamma pass rate of 95.1% (2mm/2% criteria), mean dose errors of 2.6% in high-dose regions and 5.9% globally, while preserving target coverage and organ-at-risk dose constraints. Computation time was under 3 minutes.

Conclusion: This work demonstrated the feasibility and proof-of-principle of xLSTM-based proton dose calculation, offering a computationally efficient alternative to traditional methods while maintaining accuracy, making it suitable for adaptive treatment protocols.

Abstract: Accurate dose calculation on cone beam computed tomography (CBCT) images is
essential for modern proton treatment planning workflows, particularly when
accounting for inter-fractional anatomical changes in adaptive treatment
scenarios. Traditional CBCT-based dose calculation suffers from image quality
limitations, requiring complex correction workflows. This study develops and
validates a deep learning approach for direct proton dose calculation from CBCT
images using extended Long Short-Term Memory (xLSTM) neural networks. A
retrospective dataset of 40 head-and-neck cancer patients with paired planning
CT and treatment CBCT images was used to train an xLSTM-based neural network
(CBCT-NN). The architecture incorporates energy token encoding and
beam's-eye-view sequence modelling to capture spatial dependencies in proton
dose deposition patterns. Training utilized 82,500 paired beam configurations
with Monte Carlo-generated ground truth doses. Validation was performed on 5
independent patients using gamma analysis, mean percentage dose error
assessment, and dose-volume histogram comparison. The CBCT-NN achieved gamma
pass rates of 95.1 $\pm$ 2.7% using 2mm/2% criteria. Mean percentage dose
errors were 2.6 $\pm$ 1.4% in high-dose regions ($>$90% of max dose) and 5.9
$\pm$ 1.9% globally. Dose-volume histogram analysis showed excellent
preservation of target coverage metrics (Clinical Target Volume V95%
difference: -0.6 $\pm$ 1.1%) and organ-at-risk constraints (parotid mean dose
difference: -0.5 $\pm$ 1.5%). Computation time is under 3 minutes without
sacrificing Monte Carlo-level accuracy. This study demonstrates the
proof-of-principle of direct CBCT-based proton dose calculation using xLSTM
neural networks. The approach eliminates traditional correction workflows while
achieving comparable accuracy and computational efficiency suitable for
adaptive protocols.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [516] [Whack-a-Mole: Deterministic Packet Spraying Across Multiple Network Paths](https://arxiv.org/abs/2509.18519)
*Michael Luby,John Byers*

Main category: cs.NI

TL;DR: Whack-a-Mole is a deterministic packet-spraying algorithm designed for multipath transport, providing tight control over packet distribution imbalance and quick adaptation to congestion, benefiting AI/ML training workloads.


<details>
  <summary>Details</summary>
Motivation: The paper is driven by the need to address transport imbalance and minimize tail latency in large-scale distributed AI/ML workloads, which are highly sensitive to such variations.

Method: The method involves representing network paths using a discrete allocation model and applying a bit-reversal counter to deterministically spray packets across paths, ensuring tight bounds on discrepancy.

Result: The algorithm achieves a discrepancy bound of $O(\log m)$ in packet distribution and shows quick adaptation to congestion by reallocating load to healthier paths.

Conclusion: Whack-a-Mole demonstrates itself as an efficient approach for maintaining tail latency and transport balance in multipath transports, making it an effective tool for AI/ML training optimization.

Abstract: We present Whack-a-Mole, a deterministic packet spraying algorithm for
distributing packets across multiple network paths with provably tight
discrepancy bounds. The algorithm is motivated by large-scale distributed AI/ML
training and inference workloads, where collective completion time (CCT) and
effective training time ratio (ETTR) are highly sensitive to tail latency and
transport imbalance. Whack-a-Mole represents the path profile as a discrete
allocation of $m$ selection units across $n$ paths and uses a bit-reversal
counter to choose a path for each packet. We prove that the discrepancy between
expected and actual packet counts per path is bounded by $O(\log m)$ over any
contiguous packet sequence. The algorithm responds quickly to congestion
feedback by reducing allocations to degraded paths and redistributing load to
healthier ones. This combination of deterministic distribution, low per-packet
overhead, and compatibility with erasure-coded transport makes Whack-a-Mole an
effective building block for multipath transport protocols that aim to minimize
CCT and maximize GPU utilization.

</details>


### [517] [Accurate and Efficient Prediction of Wi-Fi Link Quality Based on Machine Learning](https://arxiv.org/abs/2509.18933)
*Gabriele Formis,Gianluca Cena,Lukasz Wisniewski,Stefano Scanzio*

Main category: cs.NI

TL;DR: The paper analyzes prediction models for Wi-Fi link quality using machine learning, emphasizing low-complexity implementations and testing their accuracy through real-world experiments.


<details>
  <summary>Details</summary>
Motivation: Wireless communication quality is unpredictable, posing challenges for maintaining consistent communication in industrial settings.

Method: The study evaluates machine-learning prediction models using exponential moving averages, focusing on low-complexity designs suitable for constrained hardware.

Result: Experimental data validated that both channel-dependent and channel-independent models perform well; channel-independent models were competitive and adaptable for generalized training.

Conclusion: Machine learning models can enhance Wi-Fi reliability in industrial environments, offering efficient and deployable solutions for real-world applications.

Abstract: Wireless communications are characterized by their unpredictability, posing
challenges for maintaining consistent communication quality. This paper
presents a comprehensive analysis of various prediction models, with a focus on
achieving accurate and efficient Wi-Fi link quality forecasts using machine
learning techniques. Specifically, the paper evaluates the performance of
data-driven models based on the linear combination of exponential moving
averages, which are designed for low-complexity implementations and are then
suitable for hardware platforms with limited processing resources. Accuracy of
the proposed approaches was assessed using experimental data from a real-world
Wi-Fi testbed, considering both channel-dependent and channel-independent
training data. Remarkably, channel-independent models, which allow for
generalized training by equipment manufacturers, demonstrated competitive
performance. Overall, this study provides insights into the practical
deployment of machine learning-based prediction models for enhancing Wi-Fi
dependability in industrial environments.

</details>


### [518] [Online Learning for Optimizing AoI-Energy Tradeoff under Unknown Channel Statistics](https://arxiv.org/abs/2509.18654)
*Mohamed A. Abd-Elmagid,Ming Shi,Eylem Ekici,Ness B. Shroff*

Main category: cs.NI

TL;DR: The paper studies optimizing the tradeoff between energy consumption and information freshness in real-time monitoring systems using learning-based algorithms when channel statistics are unknown.


<details>
  <summary>Details</summary>
Motivation: Optimize the Age of Information (AoI) in real-time systems while addressing energy limitations and unknown channel statistics.

Method: Developed online learning algorithms inspired by threshold-based policies for AoI, ensuring finite-time guarantees and order-optimal regret.

Result: The proposed algorithms achieve $O(1)$ order-optimal regret for AoI optimization in scenarios with unknown channel statistics.

Conclusion: The learning-based approach effectively balances energy efficiency and AoI performance even under uncertain channel statistics.

Abstract: We consider a real-time monitoring system where a source node (with energy
limitations) aims to keep the information status at a destination node as fresh
as possible by scheduling status update transmissions over a set of channels.
The freshness of information at the destination node is measured in terms of
the Age of Information (AoI) metric. In this setting, a natural tradeoff exists
between the transmission cost (or equivalently, energy consumption) of the
source and the achievable AoI performance at the destination. This tradeoff has
been optimized in the existing literature under the assumption of having a
complete knowledge of the channel statistics. In this work, we develop online
learning-based algorithms with finite-time guarantees that optimize this
tradeoff in the practical scenario where the channel statistics are unknown to
the scheduler. In particular, when the channel statistics are known, the
optimal scheduling policy is first proven to have a threshold-based structure
with respect to the value of AoI (i.e., it is optimal to drop updates when the
AoI value is below some threshold). This key insight was then utilized to
develop the proposed learning algorithms that surprisingly achieve an
order-optimal regret (i.e., $O(1)$) with respect to the time horizon length.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [519] [Forest tree species classification and entropy-derived uncertainty mapping using extreme gradient boosting and Sentinel-1/2 data](https://arxiv.org/abs/2509.18228)
*Abdulhakim M. Abdi,Fan Wang*

Main category: q-bio.QM

TL;DR: This paper developed a method to map tree species distribution in Swedish forests at a high resolution using satellite data and field observations, achieving 85% accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve spatial resolution in mapping tree species distributions and quantify uncertainty at pixel-level using remote sensing and statistical approaches.

Method: Utilized Sentinel-1 and Sentinel-2 satellite data alongside field observations, with an extreme gradient boosting model optimized via Bayesian methods. Uncertainty quantified using Shannon's entropy.

Result: Achieved 85% overall accuracy, strong agreement with official statistics (correlation of r = 0.96), and pixel-level uncertainty estimates.

Conclusion: The approach effectively maps tree species distributions in Swedish forests with high accuracy, making it useful for forest management and ecological studies.

Abstract: We present a new 10-meter map of dominant tree species in Swedish forests
accompanied by pixel-level uncertainty estimates. The tree species
classification is based on spatiotemporal metrics derived from Sentinel-1 and
Sentinel-2 satellite data, combined with field observations from the Swedish
National Forest Inventory. We apply an extreme gradient boosting model with
Bayesian optimization to relate field observations to satellite-derived
features and generate the final species map. Classification uncertainty is
quantified using Shannon's entropy of the predicted class probabilities, which
provide a spatially explicit measure of model confidence. The final model
achieved an overall accuracy of 85% (F1 score = 0.82, Matthews correlation
coefficient = 0.81), and mapped species distributions showed strong agreement
with official forest statistics (r = 0.96).

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [520] [Security smells in infrastructure as code: a taxonomy update beyond the seven sins](https://arxiv.org/abs/2509.18761)
*Aicha War,Serge L. B. Nikiema,Jordan Samhi,Jacques Klein,Tegawende F. Bissyande*

Main category: cs.CR

TL;DR: The study revisits IaC security taxonomy, broadening it from a single tool to seven and increasing identified categories from seven to 62 using automation with human validation.


<details>
  <summary>Details</summary>
Motivation: Security flaws in Infrastructure as Code (IaC) scripts can lead to significant vulnerabilities. Developing a comprehensive taxonomy of security smells is critical to enhance IaC security.

Method: The study analyzed IaC tools such as Terraform, Ansible, Chef, and others, leveraging LLMs for initial pattern analysis, followed by human validation. They created new security detection rules implemented in linters.

Result: A detailed taxonomy of 62 security smell types was established. Enhanced linters achieved precision scores of 1.00, and findings showed these issues often linger in code due to inadequate detection tools.

Conclusion: The research provides practical insights and tools for IaC practitioners to identify and mitigate security issues, promoting better DevSecOps practices.

Abstract: Infrastructure as Code (IaC) has become essential for modern software
management, yet security flaws in IaC scripts can have severe consequences, as
exemplified by the recurring exploits of Cloud Web Services. Prior work has
recognized the need to build a precise taxonomy of security smells in IaC
scripts as a first step towards developing approaches to improve IaC security.
This first effort led to the unveiling of seven sins, limited by the focus on a
single IaC tool as well as by the extensive, and potentially biased, manual
effort that was required. We propose, in our work, to revisit this taxonomy:
first, we extend the study of IaC security smells to a more diverse dataset
with scripts associated with seven popular IaC tools, including Terraform,
Ansible, Chef, Puppet, Pulumi, Saltstack, and Vagrant; second, we bring in some
automation for the analysis by relying on an LLM. While we leverage LLMs for
initial pattern processing, all taxonomic decisions underwent systematic human
validation and reconciliation with established security standards. Our study
yields a comprehensive taxonomy of 62 security smell categories, significantly
expanding beyond the previously known seven. We demonstrate actionability by
implementing new security checking rules within linters for seven popular IaC
tools, often achieving 1.00 precision score. Our evolution study of security
smells in GitHub projects reveals that these issues persist for extended
periods, likely due to inadequate detection and mitigation tools. This work
provides IaC practitioners with insights for addressing common security smells
and systematically adopting DevSecOps practices to build safer infrastructure
code.

</details>


### [521] [Detection of security smells in IaC scripts through semantics-aware code and language processing](https://arxiv.org/abs/2509.18790)
*Aicha War,Adnan A. Rawass,Abdoul K. Kabore,Jordan Samhi,Jacques Klein,Tegawende F. Bissyande*

Main category: cs.CR

TL;DR: The paper presents an approach to detect security misconfigurations in IaC scripts by integrating semantic understanding into static analysis.


<details>
  <summary>Details</summary>
Motivation: IaC scripts often contain recurring security misconfigurations, and current detection methods lack robust semantic understanding, limiting their precision and recall.

Method: The approach combines two ML models, CodeBERT for semantic understanding and LongFormer for handling long IaC scripts, enhancing static analysis with semantic enrichment.

Result: The method achieved significant improvements in precision and recall for detecting misconfigurations in Ansible (from 0.46/0.79 to 0.92/0.88) and Puppet (from 0.55/0.97 to 0.87/0.75).

Conclusion: Using natural language and code representations with semantic enrichment substantially enhances the effectiveness of static analysis for IaC misconfiguration detection, outperforming existing methods and LLMs.

Abstract: Infrastructure as Code (IaC) automates the provisioning and management of IT
infrastructure through scripts and tools, streamlining software deployment.
Prior studies have shown that IaC scripts often contain recurring security
misconfigurations, and several detection and mitigation approaches have been
proposed. Most of these rely on static analysis, using statistical code
representations or Machine Learning (ML) classifiers to distinguish insecure
configurations from safe code.
  In this work, we introduce a novel approach that enhances static analysis
with semantic understanding by jointly leveraging natural language and code
representations. Our method builds on two complementary ML models: CodeBERT, to
capture semantics across code and text, and LongFormer, to represent long IaC
scripts without losing contextual information. We evaluate our approach on
misconfiguration datasets from two widely used IaC tools, Ansible and Puppet.
To validate its effectiveness, we conduct two ablation studies (removing code
text from the natural language input and truncating scripts to reduce context)
and compare against four large language models (LLMs) and prior work. Results
show that semantic enrichment substantially improves detection, raising
precision and recall from 0.46 and 0.79 to 0.92 and 0.88 on Ansible, and from
0.55 and 0.97 to 0.87 and 0.75 on Puppet, respectively.

</details>


### [522] [Security Evaluation of Android apps in budget African Mobile Devices](https://arxiv.org/abs/2509.18800)
*Alioune Diallo,Anta Diop,Abdoul Kader Kabore,Jordan Samhi,Aleksandr Pilgun,Tegawendé F. Bissyande,Jacque Klein*

Main category: cs.CR

TL;DR: The paper investigates pre-installed apps on budget Android devices and finds significant privacy and security risks, including data disclosure, exposed components, and suspicious operations.


<details>
  <summary>Details</summary>
Motivation: Low-cost Android devices often come with pre-installed apps operating with elevated privileges, but these apps' security risks are under-studied, especially in price-sensitive markets.

Method: The researchers built a framework to extract APKs from devices and used static analysis to inspect privacy and security issues in pre-installed apps.

Result: From 1,544 APKs on seven African smartphones, findings revealed 9% disclose sensitive data, 16% expose critical components unsafely, 226 execute dangerous commands, 79 interact with SMS, and 33 silently install apps. A vendor app also transmitted identifiers and location externally.

Conclusion: The study highlights pre-installed apps on budget smartphones as a significant and neglected security and privacy threat that warrants further exploration and mitigation.

Abstract: Android's open-source nature facilitates widespread smartphone accessibility,
particularly in price-sensitive markets. System and vendor applications that
come pre-installed on budget Android devices frequently operate with elevated
privileges, yet they receive limited independent examination. To address this
gap, we developed a framework that extracts APKs from physical devices and
applies static analysis to identify privacy and security issues in embedded
software. Our study examined 1,544 APKs collected from seven African
smartphones. The analysis revealed that 145 applications (9%) disclose
sensitive data, 249 (16%) expose critical components without sufficient
safeguards, and many present additional risks: 226 execute privileged or
dangerous commands, 79 interact with SMS messages (read, send, or delete), and
33 perform silent installation operations. We also uncovered a vendor-supplied
package that appears to transmit device identifiers and location details to an
external third party. These results demonstrate that pre-installed applications
on widely distributed low-cost devices represent a significant and
underexplored threat to user security and privacy.

</details>


### [523] [LLM-based Vulnerability Discovery through the Lens of Code Metrics](https://arxiv.org/abs/2509.19117)
*Felix Weissberg,Lukas Pirch,Erik Imgrund,Jonas Möller,Thorsten Eisenhofer,Konrad Rieck*

Main category: cs.CR

TL;DR: The paper investigates why Large Language Models (LLMs) are limited in vulnerability discovery and finds that LLMs rely heavily on classic code metrics, lacking deeper pattern recognition.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand why progress in utilizing LLMs for vulnerability discovery has stagnated despite advancements in the field.

Method: The study examines the performance of LLMs against a classifier trained solely on classic code metrics and conducts a root-cause analysis to assess the correlation and causal effect between code metrics and LLM predictions.

Result: The study reveals that LLMs and code metrics are strongly correlated, with LLM predictions shifting consistently with metric changes, suggesting shallow pattern recognition.

Conclusion: LLMs currently operate at a shallow analysis level, similar to classic code metrics, limiting their capability to address complex patterns in vulnerability discovery. Recommendations for targeted research improvements are provided.

Abstract: Large language models (LLMs) excel in many tasks of software engineering, yet
progress in leveraging them for vulnerability discovery has stalled in recent
years. To understand this phenomenon, we investigate LLMs through the lens of
classic code metrics. Surprisingly, we find that a classifier trained solely on
these metrics performs on par with state-of-the-art LLMs for vulnerability
discovery. A root-cause analysis reveals a strong correlation and a causal
effect between LLMs and code metrics: When the value of a metric is changed,
LLM predictions tend to shift by a corresponding magnitude. This dependency
suggests that LLMs operate at a similarly shallow level as code metrics,
limiting their ability to grasp complex patterns and fully realize their
potential in vulnerability discovery. Based on these findings, we derive
recommendations on how research should more effectively address this challenge.

</details>


### [524] [LLMs as verification oracles for Solidity](https://arxiv.org/abs/2509.19153)
*Massimo Bartoletti,Enrico Lipparini,Livio Pompianu*

Main category: cs.CR

TL;DR: This paper evaluates the use of GPT-5 for verifying smart contracts and its effectiveness as a reasoning tool in real-world auditing scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring smart contract correctness, especially errors in contract business logic that lead to significant financial losses.

Method: A systematic evaluation of GPT-5 was conducted with benchmarking on verification tasks, comparisons to existing tools, and real-world auditing assessments.

Result: GPT-5 demonstrated surprising effectiveness as a verification oracle in various scenarios, outperforming expectations.

Conclusion: Reasoning-oriented LLMs, like GPT-5, represent a promising direction for integrating AI with formal methods to enhance the security and auditing of smart contracts.

Abstract: Ensuring the correctness of smart contracts is critical, as even subtle flaws
can lead to severe financial losses. While bug detection tools able to spot
common vulnerability patterns can serve as a first line of defense, most
real-world exploits and losses stem from errors in the contract business logic.
Formal verification tools such as SolCMC and the Certora Prover address this
challenge, but their impact remains limited by steep learning curves and
restricted specification languages. Recent works have begun to explore the use
of large language models (LLMs) for security-related tasks such as
vulnerability detection and test generation. Yet, a fundamental question
remains open: can LLMs serve as verification oracles, capable of reasoning
about arbitrary contract-specific properties? In this paper, we provide the
first systematic evaluation of GPT-5, a state-of-the-art reasoning LLM, in this
role. We benchmark its performance on a large dataset of verification tasks,
compare its outputs against those of established formal verification tools, and
assess its practical effectiveness in real-world auditing scenarios. Our study
combines quantitative metrics with qualitative analysis, and shows that recent
reasoning-oriented LLMs can be surprisingly effective as verification oracles,
suggesting a new frontier in the convergence of AI and formal methods for
secure smart contract development and auditing.

</details>


### [525] [Context Lineage Assurance for Non-Human Identities in Critical Multi-Agent Systems](https://arxiv.org/abs/2509.18415)
*Sumana Malkapuram,Sameera Gangavarapu,Kailashnath Reddy Kavalakuntla,Ananya Gangavarapu*

Main category: cs.CR

TL;DR: Introducing cryptographic lineage verification for non-human identities (NHIs) using Merkle tree structures to ensure secure, auditable agent-to-agent (A2A) interactions.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for secure and verifiable interactions among autonomous software agents, especially those instantiated as non-human identities, to establish credibility and trust.

Method: The authors propose a cryptographic mechanism using Merkle tree structures modeled after Certificate Transparency logs and a federated proof server for ensuring lineage verification and multi-hop provenance validation.

Result: The approach allows external verifiers to authenticate the integrity of the entire call chain and substantiates the legitimacy of agent representations through explicit identity verification primitives.

Conclusion: This advancement integrates identity attestation, lineage verification, and proof auditing, enhancing security in inter-agent interactions and providing a robust framework for governance in regulated settings.

Abstract: The proliferation of autonomous software agents necessitates rigorous
frameworks for establishing secure and verifiable agent-to-agent (A2A)
interactions, particularly when such agents are instantiated as non-human
identities(NHIs). We extend the A2A paradigm [1 , 2] by introducing a
cryptographically grounded mechanism for lineage verification, wherein the
provenance and evolution of NHIs are anchored in append-only Merkle tree
structures modeled after Certificate Transparency (CT) logs. Unlike traditional
A2A models that primarily secure point-to-point interactions, our approach
enables both agents and external verifiers to cryptographically validate
multi-hop provenance, thereby ensuring the integrity of the entire call chain.
  A federated proof server acts as an auditor across one or more Merkle logs,
aggregating inclusion proofs and consistency checks into compact, signed
attestations that external parties can verify without access to the full
execution trace. In parallel, we augment the A2A agent card to incorporate
explicit identity verification primitives, enabling both peer agents and human
approvers to authenticate the legitimacy of NHI representations in a
standardized manner. Together, these contributions establish a cohesive model
that integrates identity attestation, lineage verification, and independent
proof auditing, thereby advancing the security posture of inter-agent
ecosystems and providing a foundation for robust governance of NHIs in
regulated environments such as FedRAMP.

</details>


### [526] [Coherence-driven inference for cybersecurity](https://arxiv.org/abs/2509.18520)
*Steve Huntsman*

Main category: cs.CR

TL;DR: Applying large language models (LLMs) for coherence-driven inference (CDI) in cybersecurity involving automatic decision-making.


<details>
  <summary>Details</summary>
Motivation: To leverage LLMs to enhance decision-making and autonomous operations in cybersecurity through coherence-driven inference (CDI).

Method: Utilizing LLMs to create weighted graphs from natural language data for CDI in cybersecurity scenarios.

Result: Demonstrated early application of CDI using LLMs, showing promise for cybersecurity decision-making and blue team operations.

Conclusion: LLMs can support near- to medium-term advancements in cybersecurity decision-making and automation through CDI.

Abstract: Large language models (LLMs) can compile weighted graphs on natural language
data to enable automatic coherence-driven inference (CDI) relevant to red and
blue team operations in cybersecurity. This represents an early application of
automatic CDI that holds near- to medium-term promise for decision-making in
cybersecurity and eventually also for autonomous blue team operations.

</details>


### [527] [VoxGuard: Evaluating User and Attribute Privacy in Speech via Membership Inference Attacks](https://arxiv.org/abs/2509.18413)
*Efthymios Tsaprazlis,Thanathai Lertpetchpun,Tiantian Feng,Sai Praneeth Karimireddy,Shrikanth Narayanan*

Main category: cs.CR

TL;DR: Current voice anonymization evaluation methods, primarily relying on Equal Error Rate (EER), fail to address strong adversary attacks at low False-Positive Rates (FPR). Introducing VoxGuard as a new framework, the study shows that EER underestimates privacy leakage.


<details>
  <summary>Details</summary>
Motivation: The paper identifies the limitations of EER in effectively evaluating privacy in voice anonymization against sophisticated adversaries, leading to potential security risks.

Method: The authors propose VoxGuard, which leverages concepts from differential privacy and membership inference to evaluate voice anonymization in terms of User Privacy and Attribute Privacy at low-FPR settings.

Result: The study reveals that adversaries employing advanced techniques can breach privacy significantly at low-FPR, achieving strong re-identification and attribute recovery after anonymization.

Conclusion: EER fails to provide an accurate assessment of privacy risks in voice anonymization. The paper recommends transitioning to low-FPR evaluations using VoxGuard to better measure and mitigate privacy leakage.

Abstract: Voice anonymization aims to conceal speaker identity and attributes while
preserving intelligibility, but current evaluations rely almost exclusively on
Equal Error Rate (EER) that obscures whether adversaries can mount
high-precision attacks. We argue that privacy should instead be evaluated in
the low false-positive rate (FPR) regime, where even a small number of
successful identifications constitutes a meaningful breach. To this end, we
introduce VoxGuard, a framework grounded in differential privacy and membership
inference that formalizes two complementary notions: User Privacy, preventing
speaker re-identification, and Attribute Privacy, protecting sensitive traits
such as gender and accent. Across synthetic and real datasets, we find that
informed adversaries, especially those using fine-tuned models and
max-similarity scoring, achieve orders-of-magnitude stronger attacks at low-FPR
despite similar EER. For attributes, we show that simple transparent attacks
recover gender and accent with near-perfect accuracy even after anonymization.
Our results demonstrate that EER substantially underestimates leakage,
highlighting the need for low-FPR evaluation, and recommend VoxGuard as a
benchmark for evaluating privacy leakage.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [528] [Discovering strategies for coastal resilience with AI-based prediction and optimization](https://arxiv.org/abs/2509.19263)
*Jared Markowitz,Alexander New,Jennifer Sleeman,Chace Ashcraft,Jay Brett,Gary Collins,Stella In,Nathaniel Winstead*

Main category: physics.ao-ph

TL;DR: The study uses AI to optimize interventions like sea walls and oyster reefs to reduce flooding damage from tropical storms.


<details>
  <summary>Details</summary>
Motivation: Tropical storms cause severe property damage and loss of life, prompting the need for optimized interventions to mitigate their impacts.

Method: The researchers combined AI models to guide intervention types, locations, and scales, using storm surge modeling, surrogate impact assessment, and solving a continuous-armed bandit problem. The study applied this to coastal defenses near Florida's Tyndall AFB.

Result: The optimized intervention strategy demonstrated potential to save billions of dollars in flooding damages by outperforming less optimal solutions.

Conclusion: AI-driven optimization offers significant potential to enhance resilience against coastal flooding and reduce the economic burden of tropical storms.

Abstract: Tropical storms cause extensive property damage and loss of life, making them
one of the most destructive types of natural hazards. The development of
predictive models that identify interventions effective at mitigating storm
impacts has considerable potential to reduce these adverse outcomes. In this
study, we use an artificial intelligence (AI)-driven approach for optimizing
intervention schemes that improve resilience to coastal flooding. We combine
three different AI models to optimize the selection of intervention types,
sites, and scales in order to minimize the expected cost of flooding damage in
a given region, including the cost of installing and maintaining interventions.
Our approach combines data-driven generation of storm surge fields, surrogate
modeling of intervention impacts, and the solving of a continuous-armed bandit
problem. We applied this methodology to optimize the selection of sea wall and
oyster reef interventions near Tyndall Air Force Base (AFB) in Florida, an area
that was catastrophically impacted by Hurricane Michael. Our analysis predicts
that intervention optimization could be used to potentially save billions of
dollars in storm damage, far outpacing greedy or non-optimal solutions.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [529] [Does Embodiment Matter to Biomechanics and Function? A Comparative Analysis of Head-Mounted and Hand-Held Assistive Devices for Individuals with Blindness and Low Vision](https://arxiv.org/abs/2509.18391)
*Gaurav Seth,Hoa Pham,Giles Hamilton-Fletcher,Charles Leclercq,John-Ross Rizzo*

Main category: cs.HC

TL;DR: The study compares hand-held and head-mounted assistive technologies for persons with blindness or low vision, focusing on task performance and physical effort during everyday activities.


<details>
  <summary>Details</summary>
Motivation: Assistive devices like Microsoft Seeing AI can significantly benefit persons with blindness or low vision, but the impact of design choices (e.g., hand-held vs. head-mounted) on functionality and physical effort remains unclear.

Method: 11 participants used Seeing AI on two device types (hand-held smartphone and head-mounted ARx Vision) while performing six daily activities. Researchers measured functional outcomes (e.g., task time, success rate) and biomechanical metrics (e.g., movement smoothness, range of motion) using motion capture.

Result: Head-mounted devices generally reduced upper-body movement and task times, especially for document scanning, while hand-held devices had higher success rates for tasks requiring precise text reading.

Conclusion: Both device types are viable for assistive technology but differ in physical demands and usability. Biomechanical analysis can inform the design of more user-friendly and efficient assistive tools.

Abstract: Visual assistive technologies, such as Microsoft Seeing AI, can improve
access to environmental information for persons with blindness or low vision
(pBLV). Yet, the physical and functional implications of different device
embodiments remain unclear. In this study, 11 pBLV participants used Seeing AI
on a hand-held smartphone and on a head-mounted ARx Vision system to perform
six activities of daily living, while their movements were captured with Xsens
motion capture. Functional outcomes included task time, success rate, and
number of attempts, and biomechanical measures included joint range of motion,
angular path length, working volume, and movement smoothness. The head-mounted
system generally reduced upper-body movement and task time, especially for
document-scanning style tasks, whereas the hand-held system yielded higher
success rates for tasks involving small or curved text. These findings indicate
that both embodiments are viable, but they differ in terms of physical demands
and ease of use. Incorporating biomechanical measures into assistive technology
evaluations can inform designs that optimise user experience by balancing
functional efficiency, physical sustainability, and intuitive interaction.

</details>


### [530] [NaviSense: A Multimodal Assistive Mobile application for Object Retrieval by Persons with Visual Impairment](https://arxiv.org/abs/2509.18672)
*Ajay Narayanan Sridhar,Fuli Qiao,Nelson Daniel Troncoso Aldas,Yanpei Shi,Mehrdad Mahdavi,Laurent Itti,Vijaykrishnan Narayanan*

Main category: cs.HC

TL;DR: NaviSense is an assistive system designed for people with visual impairments to locate objects using conversational AI, vision-language models, AR, and LiDAR, providing audio-haptic guidance and open-world detection.


<details>
  <summary>Details</summary>
Motivation: Visually impaired people face challenges in locating and retrieving objects, as existing technologies either require preset object categories or lack real-time spatial feedback.

Method: The system uses conversational AI to accept natural language queries, integrates vision-language models for object detection, and provides real-time spatial audio-haptic feedback using AR and LiDAR.

Result: NaviSense significantly reduced object retrieval time and was preferred by visually impaired users over existing tools in evaluations with 12 participants.

Conclusion: Integrating open-world object detection with spatially precise, accessible guidance can substantially improve object retrieval for visually impaired users.

Abstract: People with visual impairments often face significant challenges in locating
and retrieving objects in their surroundings. Existing assistive technologies
present a trade-off: systems that offer precise guidance typically require
pre-scanning or support only fixed object categories, while those with
open-world object recognition lack spatial feedback for reaching the object. To
address this gap, we introduce 'NaviSense', a mobile assistive system that
combines conversational AI, vision-language models, augmented reality (AR), and
LiDAR to support open-world object detection with real-time audio-haptic
guidance. Users specify objects via natural language and receive continuous
spatial feedback to navigate toward the target without needing prior setup.
Designed with insights from a formative study and evaluated with 12 blind and
low-vision participants, NaviSense significantly reduced object retrieval time
and was preferred over existing tools, demonstrating the value of integrating
open-world perception with precise, accessible guidance.

</details>


### [531] [When Ads Become Profiles: Large-Scale Audit of Algorithmic Biases and LLM Profiling Risks](https://arxiv.org/abs/2509.18874)
*Baiyu Chen,Benjamin Tag,Hao Xue,Daniel Angus,Flora Salim*

Main category: cs.HC

TL;DR: This study audits social media ad targeting, uncovering biases and risks using ads to infer sensitive user data through AI. It calls for privacy-minded reforms.


<details>
  <summary>Details</summary>
Motivation: The opacity of automated ad targeting on social media poses harm to users while preventing external auditing. The concern is exacerbated by LLMs, which may infer sensitive user details from ad exposure.

Method: A multi-stage auditing framework: (1) large-scale audit of 435,000 Facebook ad impressions across 891 users; (2) using a multimodal LLM to reconstruct demographics from ad streams.

Result: The audit found biases in ad targeting, such as vulnerable groups receiving disproportionate Gambling and Politics ads. LLMs inferred user demographics with high accuracy.

Conclusion: Ad streams are rich data sources that raise privacy risks, emphasizing the need for auditing and governance to protect user information.

Abstract: Automated ad targeting on social media is opaque, creating risks of
exploitation and invisibility to external scrutiny. Users may be steered toward
harmful content while independent auditing of these processes remains blocked.
Large Language Models (LLMs) raise a new concern: the potential to
reverse-engineer sensitive user attributes from exposure alone. We introduce a
multi-stage auditing framework to investigate these risks. First, a large-scale
audit of over 435,000 ad impressions delivered to 891 Australian Facebook users
reveals algorithmic biases, including disproportionate Gambling and Politics
ads shown to socioeconomically vulnerable and politically aligned groups.
Second, a multimodal LLM can reconstruct users' demographic profiles from ad
streams, outperforming census-based baselines and matching or exceeding human
performance. Our results provide the first empirical evidence that ad streams
constitute rich digital footprints for public AI inference, highlighting urgent
privacy risks and the need for content-level auditing and governance.

</details>


### [532] [YAC: Bridging Natural Language and Interactive Visual Exploration with Generative AI for Biomedical Data Discovery](https://arxiv.org/abs/2509.19182)
*Devin Lange,Shanghua Gao,Pengwei Sui,Austen Money,Priya Misner,Marinka Zitnik,Nils Gehlenborg*

Main category: cs.HC

TL;DR: This paper introduces YAC, a chatbot prototype that combines natural language input with interactive visualizations for biomedical data exploration.


<details>
  <summary>Details</summary>
Motivation: To enhance biomedical data discovery by integrating natural language input with user interface elements and visualizations.

Method: The authors developed YAC, a prototype system employing a multi-agent approach to translate natural language inputs into structured outputs, which are then used to create interactive visualizations and apply filters. Widgets enable user adjustments.

Result: YAC successfully bridges natural language understanding with interactive visualization for data exploration, demonstrated through four usage scenarios.

Conclusion: The system exemplifies how combining natural language with interactive visualizations can enhance data discovery interfaces, with technical reflections and illustrative use cases.

Abstract: Incorporating natural language input has the potential to improve the
capabilities of biomedical data discovery interfaces. However, user interface
elements and visualizations are still powerful tools for interacting with data,
even in the new world of generative AI. In our prototype system, YAC, Yet
Another Chatbot, we bridge the gap between natural language and interactive
visualizations by generating structured declarative output with a multi-agent
system and interpreting that output to render linked interactive visualizations
and apply data filters. Furthermore, we include widgets, which allow users to
adjust the values of that structured output through user interface elements. We
reflect on the capabilities and design of this system with an analysis of its
technical dimensions and illustrate the capabilities through four usage
scenarios.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [533] [Hierarchical Semi-Markov Models with Duration-Aware Dynamics for Activity Sequences](https://arxiv.org/abs/2509.18414)
*Rohit Dube,Natarajan Gautam,Amarnath Banerjee,Harsha Nagarajan*

Main category: stat.AP

TL;DR: The paper focuses on accurately forecasting residential electricity demand by modeling human daily activities, emphasizing timing and duration of behaviors using a hierarchical semi-Markov framework.


<details>
  <summary>Details</summary>
Motivation: To fulfill the need for precise forecasts of residential electricity demand, essential for microgrid management and demand response, via realistic modeling of human daily activity sequences.

Method: A hierarchical semi-Markov model combines a time-inhomogeneous Markov router for activity transitions and a semi-Markov hazard component for activity durations, ensuring broad statistical stability through pooling data over demographics and time blocks.

Result: Modeling activity durations significantly improves predictive accuracy, identifying key demographic factors like Sex, Day-Type, and Household Size as influential, while Region and Season have minimal impact.

Conclusion: The developed generative model effectively creates synthetic daily activity traces, providing a reliable foundation for energy system applications and offering insights into critical demographic determinants.

Abstract: Residential electricity demand at granular scales is driven by what people do
and for how long. Accurately forecasting this demand for applications like
microgrid management and demand response therefore requires generative models
that can produce realistic daily activity sequences, capturing both the timing
and duration of human behavior. This paper develops a generative model of human
activity sequences using nationally representative time-use diaries at a
10-minute resolution. We use this model to quantify which demographic factors
are most critical for improving predictive performance.
  We propose a hierarchical semi-Markov framework that addresses two key
modeling challenges. First, a time-inhomogeneous Markov \emph{router} learns
the patterns of ``which activity comes next." Second, a semi-Markov
\emph{hazard} component explicitly models activity durations, capturing ``how
long" activities realistically last. To ensure statistical stability when data
are sparse, the model pools information across related demographic groups and
time blocks. The entire framework is trained and evaluated using survey design
weights to ensure our findings are representative of the U.S. population.
  On a held-out test set, we demonstrate that explicitly modeling durations
with the hazard component provides a substantial and statistically significant
improvement over purely Markovian models. Furthermore, our analysis reveals a
clear hierarchy of demographic factors: Sex, Day-Type, and Household Size
provide the largest predictive gains, while Region and Season, though important
for energy calculations, contribute little to predicting the activity sequence
itself. The result is an interpretable and robust generator of synthetic
activity traces, providing a high-fidelity foundation for downstream energy
systems modeling.

</details>
