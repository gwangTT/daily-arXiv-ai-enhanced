<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CL](#cs.CL) [Total: 56]
- [cs.CV](#cs.CV) [Total: 109]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.LG](#cs.LG) [Total: 98]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 43]
- [cs.SE](#cs.SE) [Total: 9]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [stat.ML](#stat.ML) [Total: 6]
- [stat.ME](#stat.ME) [Total: 3]
- [eess.SP](#eess.SP) [Total: 2]
- [eess.SY](#eess.SY) [Total: 2]
- [eess.AS](#eess.AS) [Total: 2]
- [quant-ph](#quant-ph) [Total: 9]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [cs.SD](#cs.SD) [Total: 10]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.CE](#cs.CE) [Total: 1]
- [eess.IV](#eess.IV) [Total: 10]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.GT](#cs.GT) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [physics.atom-ph](#physics.atom-ph) [Total: 1]
- [cs.MM](#cs.MM) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [MICA: Multi-Agent Industrial Coordination Assistant](https://arxiv.org/abs/2509.15237)
*Di Wen,Kunyu Peng,Junwei Zheng,Yufan Chen,Yitain Shi,Jiale Wei,Ruiping Liu,Kailun Yang,Rainer Stiefelhagen*

Main category: cs.AI

TL;DR: This paper introduces MICA, a speech-interactive multi-agent system for industrial assistance, designed to provide real-time guidance while meeting privacy and hardware constraints.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for adaptive and trustworthy assistance systems in industrial workflows, which operate under privacy constraints, limited computational resources, and intermittent connectivity.

Method: It proposes MICA, a system composed of five specialized language agents audited by a safety checker, and introduces Adaptive Step Fusion (ASF) for blending expert reasoning with speech feedback. A new benchmark and evaluation metrics are also established.

Result: Experiments show MICA improves task success, reliability, and responsiveness compared to baseline structures, and it is deployable on offline hardware.

Conclusion: MICA represents progress toward privacy-preserving, deployable multi-agent systems for dynamic factory environments, with potential to enhance industrial workflows.

Abstract: Industrial workflows demand adaptive and trustworthy assistance that can
operate under limited computing, connectivity, and strict privacy constraints.
In this work, we present MICA (Multi-Agent Industrial Coordination Assistant),
a perception-grounded and speech-interactive system that delivers real-time
guidance for assembly, troubleshooting, part queries, and maintenance. MICA
coordinates five role-specialized language agents, audited by a safety checker,
to ensure accurate and compliant support. To achieve robust step understanding,
we introduce Adaptive Step Fusion (ASF), which dynamically blends expert
reasoning with online adaptation from natural speech feedback. Furthermore, we
establish a new multi-agent coordination benchmark across representative task
categories and propose evaluation metrics tailored to industrial assistance,
enabling systematic comparison of different coordination topologies. Our
experiments demonstrate that MICA consistently improves task success,
reliability, and responsiveness over baseline structures, while remaining
deployable on practical offline hardware. Together, these contributions
highlight MICA as a step toward deployable, privacy-preserving multi-agent
assistants for dynamic factory environments. The source code will be made
publicly available at https://github.com/Kratos-Wen/MICA.

</details>


### [2] [KNARsack: Teaching Neural Algorithmic Reasoners to Solve Pseudo-Polynomial Problems](https://arxiv.org/abs/2509.15239)
*Stjepan Požgaj,Dobrik Georgiev,Marin Šilić,Petar Veličković*

Main category: cs.AI

TL;DR: The paper proposes a neural reasoner to tackle the Knapsack problem using dynamic programming principles to achieve better scalability and generalization.


<details>
  <summary>Details</summary>
Motivation: The authors aim to extend neural algorithmic reasoning by addressing the Knapsack problem, a challenging combinatorial optimization task, which is not covered in standard benchmarks.

Method: The two-phase approach involves constructing a dynamic programming table followed by solution reconstruction, with intermediate states modeled using dynamic programming supervision.

Result: The proposed neural algorithmic reasoner demonstrates improved generalization to larger problem instances compared to direct-prediction baselines.

Conclusion: The study shows that embedding dynamic programming principles enhances neural reasoners for solving complex optimization problems like Knapsack.

Abstract: Neural algorithmic reasoning (NAR) is a growing field that aims to embed
algorithmic logic into neural networks by imitating classical algorithms. In
this extended abstract, we detail our attempt to build a neural algorithmic
reasoner that can solve Knapsack, a pseudo-polynomial problem bridging
classical algorithms and combinatorial optimisation, but omitted in standard
NAR benchmarks. Our neural algorithmic reasoner is designed to closely follow
the two-phase pipeline for the Knapsack problem, which involves first
constructing the dynamic programming table and then reconstructing the solution
from it. The approach, which models intermediate states through dynamic
programming supervision, achieves better generalization to larger problem
instances than a direct-prediction baseline that attempts to select the optimal
subset only from the problem inputs.

</details>


### [3] [The Distribution Shift Problem in Transportation Networks using Reinforcement Learning and AI](https://arxiv.org/abs/2509.15291)
*Federico Taschin,Abderrahmane Lazaraq,Ozan K. Tonguz,Inci Ozgunes*

Main category: cs.AI

TL;DR: The paper analyzes the use of Meta Reinforcement Learning (Meta RL) in traffic signal control, showing it can deliver good results under certain conditions but is not always reliable.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the reliability problems of Reinforcement Learning (RL) agents in traffic signal control due to dynamic changes in data distribution.

Method: The paper evaluates and analyzes a state-of-the-art Meta RL approach called MetaLight, testing its performance under various conditions.

Result: The study finds that MetaLight performs well in some conditions but shows errors of up to 22% in others, highlighting its lack of robustness.

Conclusion: Meta RL approaches, though promising, are not yet reliable enough for dynamic traffic signal control scenarios.

Abstract: The use of Machine Learning (ML) and Artificial Intelligence (AI) in smart
transportation networks has increased significantly in the last few years.
Among these ML and AI approaches, Reinforcement Learning (RL) has been shown to
be a very promising approach by several authors. However, a problem with using
Reinforcement Learning in Traffic Signal Control is the reliability of the
trained RL agents due to the dynamically changing distribution of the input
data with respect to the distribution of the data used for training. This
presents a major challenge and a reliability problem for the trained network of
AI agents and could have very undesirable and even detrimental consequences if
a suitable solution is not found. Several researchers have tried to address
this problem using different approaches. In particular, Meta Reinforcement
Learning (Meta RL) promises to be an effective solution. In this paper, we
evaluate and analyze a state-of-the-art Meta RL approach called MetaLight and
show that, while under certain conditions MetaLight can indeed lead to
reasonably good results, under some other conditions it might not perform well
(with errors of up to 22%), suggesting that Meta RL schemes are often not
robust enough and can even pose major reliability problems.

</details>


### [4] [An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature](https://arxiv.org/abs/2509.15292)
*Abhiyan Dhakal,Kausik Paudel,Sanjog Sigdel*

Main category: cs.AI

TL;DR: The paper introduces an automated literature review pipeline using semantic similarity based on transformer embeddings and cosine similarity.


<details>
  <summary>Details</summary>
Motivation: Traditional literature review processes can be tedious and inefficient. The paper aims to minimize manual overhead while ensuring a high relevance of results in literature reviews.

Method: The proposed system generates keywords from a given title and abstract, retrieves papers from open-access repositories, and ranks them using transformer-based embeddings and cosine similarity. It evaluates three embedding models and uses statistical thresholding to filter the most relevant papers.

Result: The system shows potential for performing preliminary research and exploratory analyses, despite the lack of heuristic feedback or ground truth labels.

Conclusion: The framework appears to be a scalable and practical tool for automating literature reviews, improving efficiency in academic research.

Abstract: We propose an automated pipeline for performing literature reviews using
semantic similarity. Unlike traditional systematic review systems or
optimization based methods, this work emphasizes minimal overhead and high
relevance by using transformer based embeddings and cosine similarity. By
providing a paper title and abstract, it generates relevant keywords, fetches
relevant papers from open access repository, and ranks them based on their
semantic closeness to the input. Three embedding models were evaluated. A
statistical thresholding approach is then applied to filter relevant papers,
enabling an effective literature review pipeline. Despite the absence of
heuristic feedback or ground truth relevance labels, the proposed system shows
promise as a scalable and practical tool for preliminary research and
exploratory analysis.

</details>


### [5] [Knowledge-Driven Hallucination in Large Language Models: An Empirical Study on Process Modeling](https://arxiv.org/abs/2509.15336)
*Humam Kourani,Anton Antonov,Alessandro Berti,Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: The paper examines how Large Language Models (LLMs) may output contradictory information due to reliance on pre-trained knowledge, focusing on its application in Business Process Modeling.


<details>
  <summary>Details</summary>
Motivation: To understand and address the phenomenon of knowledge-driven hallucination in LLMs when tasked with creating evidence-based artifacts.

Method: A controlled experiment was conducted using standard and atypical process descriptions to compare LLM outputs against provided evidence in the domain of automated business process modeling.

Result: The study demonstrates scenarios where LLMs override explicit source evidence with generalized internal knowledge, compromising output reliability.

Conclusion: LLMs require rigorous validation methods when used in domains reliant on evidence-based artifacts, highlighting critical reliability concerns.

Abstract: The utility of Large Language Models (LLMs) in analytical tasks is rooted in
their vast pre-trained knowledge, which allows them to interpret ambiguous
inputs and infer missing information. However, this same capability introduces
a critical risk of what we term knowledge-driven hallucination: a phenomenon
where the model's output contradicts explicit source evidence because it is
overridden by the model's generalized internal knowledge. This paper
investigates this phenomenon by evaluating LLMs on the task of automated
process modeling, where the goal is to generate a formal business process model
from a given source artifact. The domain of Business Process Management (BPM)
provides an ideal context for this study, as many core business processes
follow standardized patterns, making it likely that LLMs possess strong
pre-trained schemas for them. We conduct a controlled experiment designed to
create scenarios with deliberate conflict between provided evidence and the
LLM's background knowledge. We use inputs describing both standard and
deliberately atypical process structures to measure the LLM's fidelity to the
provided evidence. Our work provides a methodology for assessing this critical
reliability issue and raises awareness of the need for rigorous validation of
AI-generated artifacts in any evidence-based domain.

</details>


### [6] [Diagnostics of cognitive failures in multi-agent expert systems using dynamic evaluation protocols and subsequent mutation of the processing context](https://arxiv.org/abs/2509.15366)
*Andrejs Sorstkins,Josh Bailey,Dr Alistair Baron*

Main category: cs.AI

TL;DR: The paper introduces a diagnostic framework to evaluate and refine large language models (LLMs) with agentic behaviors by facilitating expert behavior transfer and addressing latent cognitive failures.


<details>
  <summary>Details</summary>
Motivation: With LLMs showing agentic behaviors enabled by memory, planning, and tool-use, classical evaluation methods fail to adequately diagnose performance, necessitating a framework for reproducible expert behavior transfer.

Method: The framework incorporates expert-annotated golden datasets, controlled behavioral mutations for silver datasets, and an LLM-based "Agent Judge" for prescribing improvements embedded into a vectorized recommendation map.

Result: The framework successfully identified latent cognitive failures in a multi-agent recruiter-assistant system while steering agents toward expert-level reasoning and style.

Conclusion: The proposed framework advances the evaluation of stochastic and tool-augmented LLM agents, establishing a foundation for standardized, reproducible improvement through active refinement.

Abstract: The rapid evolution of neural architectures - from multilayer perceptrons to
large-scale Transformer-based models - has enabled language models (LLMs) to
exhibit emergent agentic behaviours when equipped with memory, planning, and
external tool use. However, their inherent stochasticity and multi-step
decision processes render classical evaluation methods inadequate for
diagnosing agentic performance. This work introduces a diagnostic framework for
expert systems that not only evaluates but also facilitates the transfer of
expert behaviour into LLM-powered agents. The framework integrates (i) curated
golden datasets of expert annotations, (ii) silver datasets generated through
controlled behavioural mutation, and (iii) an LLM-based Agent Judge that scores
and prescribes targeted improvements. These prescriptions are embedded into a
vectorized recommendation map, allowing expert interventions to propagate as
reusable improvement trajectories across multiple system instances. We
demonstrate the framework on a multi-agent recruiter-assistant system, showing
that it uncovers latent cognitive failures - such as biased phrasing,
extraction drift, and tool misrouting - while simultaneously steering agents
toward expert-level reasoning and style. The results establish a foundation for
standardized, reproducible expert behaviour transfer in stochastic,
tool-augmented LLM agents, moving beyond static evaluation to active expert
system refinement.

</details>


### [7] [FragmentRetro: A Quadratic Retrosynthetic Method Based on Fragmentation Algorithms](https://arxiv.org/abs/2509.15409)
*Yu Shee,Anthony M. Smaldone,Anton Morgunov,Gregory W. Kyro,Victor S. Batista*

Main category: cs.AI

TL;DR: FragmentRetro introduces a retrosynthetic method leveraging fragmentation algorithms to achieve quadratic computational complexity ($O(h^2)$), outperforming traditional tree-search methods with exponential complexity.


<details>
  <summary>Details</summary>
Motivation: To address computational inefficiencies in retrosynthesis, particularly in tree-search methods which scale exponentially.

Method: FragmentRetro uses fragmentation algorithms (BRICS and r-BRICS), stock-aware exploration, and pattern fingerprint screening to recursively build molecular fragments for synthesis planning.

Result: FragmentRetro achieves quadratic complexity, outperforming exponential tree-search methods, with high solved rates in evaluations on PaRoutes, USPTO-190, and natural products.

Conclusion: The method is computationally efficient, offers strategic starting points for synthesis planning, and serves as a robust foundational tool for computer-aided synthesis planning.

Abstract: Retrosynthesis, the process of deconstructing a target molecule into simpler
precursors, is crucial for computer-aided synthesis planning (CASP). Widely
adopted tree-search methods often suffer from exponential computational
complexity. In this work, we introduce FragmentRetro, a novel retrosynthetic
method that leverages fragmentation algorithms, specifically BRICS and r-BRICS,
combined with stock-aware exploration and pattern fingerprint screening to
achieve quadratic complexity. FragmentRetro recursively combines molecular
fragments and verifies their presence in a building block set, providing sets
of fragment combinations as retrosynthetic solutions. We present the first
formal computational analysis of retrosynthetic methods, showing that tree
search exhibits exponential complexity $O(b^h)$, DirectMultiStep scales as
$O(h^6)$, and FragmentRetro achieves $O(h^2)$, where $h$ represents the number
of heavy atoms in the target molecule and $b$ is the branching factor for tree
search. Evaluations on PaRoutes, USPTO-190, and natural products demonstrate
that FragmentRetro achieves high solved rates with competitive runtime,
including cases where tree search fails. The method benefits from fingerprint
screening, which significantly reduces substructure matching complexity. While
FragmentRetro focuses on efficiently identifying fragment-based solutions
rather than full reaction pathways, its computational advantages and ability to
generate strategic starting candidates establish it as a powerful foundational
component for scalable and automated synthesis planning.

</details>


### [8] [Stress Testing Deliberative Alignment for Anti-Scheming Training](https://arxiv.org/abs/2509.15541)
*Bronson Schoen,Evgenia Nitishinskaya,Mikita Balesni,Axel Højmark,Felix Hofstätter,Jérémy Scheurer,Alexander Meinke,Jason Wolfe,Teun van der Weij,Alex Lloyd,Nicholas Goldowsky-Dill,Angela Fan,Andrei Matveiakin,Rusheb Shah,Marcus Williams,Amelia Glaese,Boaz Barak,Wojciech Zaremba,Marius Hobbhahn*

Main category: cs.AI

TL;DR: This paper investigates AI systems that covertly pursue misaligned goals and proposes a framework for assessing and mitigating such behavior through tests of covert actions.


<details>
  <summary>Details</summary>
Motivation: To address the dangers posed by highly capable AI systems secretly pursuing misaligned goals and hiding such behaviors, which can lead to deceptive outcomes.

Method: Evaluating anti-scheming interventions through tests on far out-of-distribution tasks, situational awareness assessment, and robustness trials. Covert actions serve as a proxy for scheming.

Result: Deliberative alignment reduced covert action rates from 13% to 0.4%, but did not fully eliminate them. Misbehavior persisted after additional stress testing and red-teaming.

Conclusion: While situational awareness contributes to reduced covert behavior, its role limits the reliability of mitigations. There is a call for further research into deceptive alignment solutions.

Abstract: Highly capable AI systems could secretly pursue misaligned goals -- what we
call "scheming". Because a scheming AI would deliberately try to hide its
misaligned goals and actions, measuring and mitigating scheming requires
different strategies than are typically used in ML. We propose that assessing
anti-scheming interventions requires at least (1) testing propensity to scheme
on far out-of-distribution (OOD) tasks, (2) evaluating whether lack of scheming
is driven by situational awareness, and (3) checking for robustness to
pre-existing misaligned goals. We use a broad category of "covert actions" --
such as secretly breaking rules or intentionally underperforming in tests -- as
a proxy for scheming, and design evaluations for covert actions. We then
stress-test deliberative alignment as a case study for anti-scheming. Across 26
OOD evaluations (180+ environments), deliberative alignment reduces covert
action rates (OpenAI o3: 13%->0.4%) but does not fully eliminate them. Our
mitigation is also able to largely stop agents from pursuing a hidden goal
previously trained into the model, but we still find misbehavior after
additional red-teaming. We find that models' chain-of-thought (CoT) often
demonstrates awareness of being evaluated for alignment, and show causal
evidence that this awareness decreases covert behavior, while unawareness
increases it. Therefore, we cannot exclude that the observed reductions in
covert action rates are at least partially driven by situational awareness.
While we rely on human-legible CoT for training, studying situational
awareness, and demonstrating clear evidence of misalignment, our ability to
rely on this degrades as models continue to depart from reasoning in standard
English. We encourage research into alignment mitigations for scheming and
their assessment, especially for the adversarial case of deceptive alignment,
which this paper does not address.

</details>


### [9] [MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large Language Model Agents](https://arxiv.org/abs/2509.15635)
*Pan Tang,Shixiang Tang,Huanqi Pu,Zhiqing Miao,Zhixing Wang*

Main category: cs.AI

TL;DR: MicroRCA-Agent is a novel system for identifying root causes in microservice faults using large language models and multimodal data fusion.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the complexity in diagnosing microservice faults by leveraging multimodal data and advanced AI techniques.

Method: MicroRCA-Agent integrates log parsing, dual anomaly detection, and a two-stage large language model (LLM) analysis strategy with multimodal cross-modal prompts for comprehensive fault analysis.

Result: The proposed solution excels in complex microservice scenarios, achieving a high score of 50.71 in performance metrics.

Conclusion: MicroRCA-Agent effectively uses multimodal data and LLM capabilities to deliver superior root cause analysis in microservices, validated by ablation studies.

Abstract: This paper presents MicroRCA-Agent, an innovative solution for microservice
root cause analysis based on large language model agents, which constructs an
intelligent fault root cause localization system with multimodal data fusion.
The technical innovations are embodied in three key aspects: First, we combine
the pre-trained Drain log parsing algorithm with multi-level data filtering
mechanism to efficiently compress massive logs into high-quality fault
features. Second, we employ a dual anomaly detection approach that integrates
Isolation Forest unsupervised learning algorithms with status code validation
to achieve comprehensive trace anomaly identification. Third, we design a
statistical symmetry ratio filtering mechanism coupled with a two-stage LLM
analysis strategy to enable full-stack phenomenon summarization across
node-service-pod hierarchies. The multimodal root cause analysis module
leverages carefully designed cross-modal prompts to deeply integrate multimodal
anomaly information, fully exploiting the cross-modal understanding and logical
reasoning capabilities of large language models to generate structured analysis
results encompassing fault components, root cause descriptions, and reasoning
trace. Comprehensive ablation studies validate the complementary value of each
modal data and the effectiveness of the system architecture. The proposed
solution demonstrates superior performance in complex microservice fault
scenarios, achieving a final score of 50.71. The code has been released at:
https://github.com/tangpan360/MicroRCA-Agent.

</details>


### [10] [CCrepairBench: A High-Fidelity Benchmark and Reinforcement Learning Framework for C++ Compilation Repair](https://arxiv.org/abs/2509.15690)
*Weixuan Sun,Jucai Zhai,Dengfeng Liu,Xin Zhang,Xiaojun Wu,Qiaobo Hao,AIMgroup,Yang Fang,Jiuyang Tang*

Main category: cs.AI

TL;DR: The paper tackles automated repair of C++ compilation errors by introducing a new dataset, CCrepair, and an RL training paradigm focused on semantic correctness, demonstrating improved efficiency and reliability in automated fixes.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of automated C++ error repair due to lack of suitable datasets and limitations of traditional supervised methods.

Method: Introduces a three-part framework: (1) CCrepair dataset using generate-and-verify, (2) RL paradigm with hybrid reward signals for semantic correctness, (3) LLM-as-a-Judge for evaluation, validated by human experts.

Result: Their RL-trained model (Qwen2.5-1.5B-Instruct) performs as well as a larger model (Qwen2.5-14B-Instruct), establishing the effectiveness of their approach.

Conclusion: The approach enhances automated programming assistance by providing a novel dataset, effective training paradigm, and robust evaluation framework, significantly advancing C++ error repair models.

Abstract: The automated repair of C++ compilation errors presents a significant
challenge, the resolution of which is critical for developer productivity.
Progress in this domain is constrained by two primary factors: the scarcity of
large-scale, high-fidelity datasets and the limitations of conventional
supervised methods, which often fail to generate semantically correct
patches.This paper addresses these gaps by introducing a comprehensive
framework with three core contributions. First, we present CCrepair, a novel,
large-scale C++ compilation error dataset constructed through a sophisticated
generate-and-verify pipeline. Second, we propose a Reinforcement Learning (RL)
paradigm guided by a hybrid reward signal, shifting the focus from mere
compilability to the semantic quality of the fix. Finally, we establish the
robust, two-stage evaluation system providing this signal, centered on an
LLM-as-a-Judge whose reliability has been rigorously validated against the
collective judgments of a panel of human experts. This integrated approach
aligns the training objective with generating high-quality, non-trivial patches
that are both syntactically and semantically correct. The effectiveness of our
approach was demonstrated experimentally. Our RL-trained Qwen2.5-1.5B-Instruct
model achieved performance comparable to a Qwen2.5-14B-Instruct model,
validating the efficiency of our training paradigm. Our work provides the
research community with a valuable new dataset and a more effective paradigm
for training and evaluating robust compilation repair models, paving the way
for more practical and reliable automated programming assistants.

</details>


### [11] [A Nascent Taxonomy of Machine Learning in Intelligent Robotic Process Automation](https://arxiv.org/abs/2509.15730)
*Lukas Laakmann,Seyyid A. Ciftci,Christian Janiesch*

Main category: cs.AI

TL;DR: This paper discusses integrating machine learning with robotic process automation (RPA) to expand RPA’s capabilities beyond simple tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome the inherent limitations of traditional RPA in handling complex, non-rule-based tasks.

Method: The paper conducts a literature review and develops a taxonomy for intelligent RPA using machine learning concepts.

Result: The taxonomy highlights two areas—RPA-ML integration and RPA-ML interaction—broken down into eight dimensions such as architecture, intelligence level, and user-robot relations.

Conclusion: Integrating RPA with machine learning enhances RPA’s ability to manage complex tasks, paving the way for intelligent process automation.

Abstract: Robotic process automation (RPA) is a lightweight approach to automating
business processes using software robots that emulate user actions at the
graphical user interface level. While RPA has gained popularity for its
cost-effective and timely automation of rule-based, well-structured tasks, its
symbolic nature has inherent limitations when approaching more complex tasks
currently performed by human agents. Machine learning concepts enabling
intelligent RPA provide an opportunity to broaden the range of automatable
tasks. In this paper, we conduct a literature review to explore the connections
between RPA and machine learning and organize the joint concept intelligent RPA
into a taxonomy. Our taxonomy comprises the two meta-characteristics RPA-ML
integration and RPA-ML interaction. Together, they comprise eight dimensions:
architecture and ecosystem, capabilities, data basis, intelligence level, and
technical depth of integration as well as deployment environment, lifecycle
phase, and user-robot relation.

</details>


### [12] [Ontology Creation and Management Tools: the Case of Anatomical Connectivity](https://arxiv.org/abs/2509.15780)
*Natallia Kokash,Bernard de Bono,Tom Gillespie*

Main category: cs.AI

TL;DR: The paper introduces ApiNATOMY, an infrastructure designed to map physiological systems, especially the peripheral nervous system. It incorporates tools for modeling anatomical and physiological interactions and supports integration with external knowledge frameworks.


<details>
  <summary>Details</summary>
Motivation: Understand and visually represent the complex interactions in physiological systems, focusing on the peripheral nervous system's role. Facilitate better integration of anatomy with physiological and ontological knowledge.

Method: Development of the ApiNATOMY framework, combining Knowledge Representation models and Knowledge Management tools to map and model the nervous and physiological systems. Integration with external ontologies and knowledge graphs for enhanced utility.

Result: ApiNATOMY successfully captures multiscale physiological circuit maps, enabling experts to model and refine physiological processes comprehensively.

Conclusion: The framework enhances the representation and understanding of physiological systems, aiding research and advancements in mapping and modeling interactions between anatomical entities.

Abstract: We are developing infrastructure to support researchers in mapping data
related to the peripheral nervous system and other physiological systems, with
an emphasis on their relevance to the organs under investigation. The nervous
system, a complex network of nerves and ganglia, plays a critical role in
coordinating and transmitting signals throughout the body. To aid in this, we
have created ApiNATOMY, a framework for the topological and semantic
representation of multiscale physiological circuit maps. ApiNATOMY integrates a
Knowledge Representation (KR) model and a suite of Knowledge Management (KM)
tools. The KR model enables physiology experts to easily capture interactions
between anatomical entities, while the KM tools help modelers convert
high-level abstractions into detailed models of physiological processes, which
can be integrated with external ontologies and knowledge graphs.

</details>


### [13] [Building Data-Driven Occupation Taxonomies: A Bottom-Up Multi-Stage Approach via Semantic Clustering and Multi-Agent Collaboration](https://arxiv.org/abs/2509.15786)
*Nan Li,Bo Kang,Tijl De Bie*

Main category: cs.AI

TL;DR: This paper presents CLIMB, a framework to automate the creation of occupation taxonomies using semantic clustering and a multi-agent system.


<details>
  <summary>Details</summary>
Motivation: Building occupation taxonomies is critical for applications like job recommendations but faces challenges with manual curation and adapting to dynamic markets.

Method: CLIMB employs global semantic clustering to identify core occupations and uses a multi-agent system to construct coherent hierarchies iteratively.

Result: CLIMB outperforms existing methods in coherence, scalability, and capturing regional characteristics across three real-world datasets.

Conclusion: CLIMB is a scalable, effective solution for creating data-driven occupation taxonomies, demonstrating superior performance and adaptability.

Abstract: Creating robust occupation taxonomies, vital for applications ranging from
job recommendation to labor market intelligence, is challenging. Manual
curation is slow, while existing automated methods are either not adaptive to
dynamic regional markets (top-down) or struggle to build coherent hierarchies
from noisy data (bottom-up). We introduce CLIMB (CLusterIng-based Multi-agent
taxonomy Builder), a framework that fully automates the creation of
high-quality, data-driven taxonomies from raw job postings. CLIMB uses global
semantic clustering to distill core occupations, then employs a
reflection-based multi-agent system to iteratively build a coherent hierarchy.
On three diverse, real-world datasets, we show that CLIMB produces taxonomies
that are more coherent and scalable than existing methods and successfully
capture unique regional characteristics. We release our code and datasets at
https://anonymous.4open.science/r/CLIMB.

</details>


### [14] [A Comparative Study of Rule-Based and Data-Driven Approaches in Industrial Monitoring](https://arxiv.org/abs/2509.15848)
*Giovanni De Gasperis,Sante Dino Facchini*

Main category: cs.AI

TL;DR: This paper compares rule-based and data-driven methodologies in industrial monitoring and explores a hybrid approach combining both strengths.


<details>
  <summary>Details</summary>
Motivation: To address the challenges and limitations of traditional rule-based and advanced data-driven systems in industrial monitoring environments, particularly in the context of Industry 4.0.

Method: The study conducts a comparative analysis of rule-based and data-driven systems, examines their application scenarios, and proposes a basic framework for evaluating their properties.

Result: The comparison highlights the advantages and limitations of each approach and suggests hybrid systems as a promising solution for future industrial monitoring.

Conclusion: By blending rule-based interpretability with machine learning's analytical power, hybrid systems offer enhanced resilience, efficiency, and adaptability in industrial monitoring, aligning with Industry 4.0 requirements.

Abstract: Industrial monitoring systems, especially when deployed in Industry 4.0
environments, are experiencing a shift in paradigm from traditional rule-based
architectures to data-driven approaches leveraging machine learning and
artificial intelligence. This study presents a comparison between these two
methodologies, analyzing their respective strengths, limitations, and
application scenarios, and proposes a basic framework to evaluate their key
properties. Rule-based systems offer high interpretability, deterministic
behavior, and ease of implementation in stable environments, making them ideal
for regulated industries and safety-critical applications. However, they face
challenges with scalability, adaptability, and performance in complex or
evolving contexts. Conversely, data-driven systems excel in detecting hidden
anomalies, enabling predictive maintenance and dynamic adaptation to new
conditions. Despite their high accuracy, these models face challenges related
to data availability, explainability, and integration complexity. The paper
suggests hybrid solutions as a possible promising direction, combining the
transparency of rule-based logic with the analytical power of machine learning.
Our hypothesis is that the future of industrial monitoring lies in intelligent,
synergic systems that leverage both expert knowledge and data-driven insights.
This dual approach enhances resilience, operational efficiency, and trust,
paving the way for smarter and more flexible industrial environments.

</details>


### [15] [EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol](https://arxiv.org/abs/2509.15957)
*Kanato Masayoshi,Masahiro Hashimoto,Ryoichi Yokoyama,Naoki Toda,Yoshifumi Uwamino,Shogo Fukuda,Ho Namkoong,Masahiro Jinzaki*

Main category: cs.AI

TL;DR: This paper demonstrates that large language models, like GPT-4.1, can autonomously retrieve clinically relevant data from electronic health records (EHR) in a real hospital setting via the Model Context Protocol (MCP), achieving near-perfect accuracy in simple tasks while identifying challenges in complex ones.


<details>
  <summary>Details</summary>
Motivation: The deployment of LLMs in hospitals is restricted due to limited access to EHR systems, motivating the creation of an infrastructure like the MCP to enable secure integration for clinical applications.

Method: Researchers developed an EHR-MCP framework combining MCP tools with the hospital EHR database, utilizing GPT-4.1 through a LangGraph ReAct agent to perform six infection control team-derived tasks. They tested agreement with physician-established gold standards.

Result: The LLM successfully used tools to retrieve data, achieving high accuracy in most tasks, though performance declined in time-sensitive tasks due to errors such as incorrect arguments and misinterpreted results.

Conclusion: The framework demonstrated secure and consistent data access, highlighting its foundation for hospital AI agents. Future research should focus on extending capabilities beyond retrieval to reasoning and assessing clinical impacts.

Abstract: Background: Large language models (LLMs) show promise in medicine, but their
deployment in hospitals is limited by restricted access to electronic health
record (EHR) systems. The Model Context Protocol (MCP) enables integration
between LLMs and external tools.
  Objective: To evaluate whether an LLM connected to an EHR database via MCP
can autonomously retrieve clinically relevant information in a real hospital
setting.
  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated
with the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct
agent to interact with it. Six tasks were tested, derived from use cases of the
infection control team (ICT). Eight patients discussed at ICT conferences were
retrospectively analyzed. Agreement with physician-generated gold standards was
measured.
  Results: The LLM consistently selected and executed the correct MCP tools.
Except for two tasks, all tasks achieved near-perfect accuracy. Performance was
lower in the complex task requiring time-dependent calculations. Most errors
arose from incorrect arguments or misinterpretation of tool results. Responses
from EHR-MCP were reliable, though long and repetitive data risked exceeding
the context window.
  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a
real hospital setting, achieving near-perfect performance in simple tasks while
highlighting challenges in complex ones. EHR-MCP provides an infrastructure for
secure, consistent data access and may serve as a foundation for hospital AI
agents. Future work should extend beyond retrieval to reasoning, generation,
and clinical impact assessment, paving the way for effective integration of
generative AI into clinical practice.

</details>


### [16] [Structured Information for Improving Spatial Relationships in Text-to-Image Generation](https://arxiv.org/abs/2509.15962)
*Sander Schildermans,Chang Tian,Ying Jiao,Marie-Francine Moens*

Main category: cs.AI

TL;DR: The paper proposes a lightweight method to improve spatial relationships in text-to-image generation by augmenting prompts with tuple-based structured information.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image generation systems struggle with accurately capturing spatial relationships described in natural language prompts.

Method: A fine-tuned language model is used to automatically convert natural language prompts into tuple-based structured information, which is then seamlessly integrated into text-to-image pipelines.

Result: Experimental results show notable improvements in spatial accuracy without degrading overall image quality, with generated tuples of comparable quality to human-crafted ones.

Conclusion: The proposed method provides a practical way to enhance spatial relationships in text-to-image generation, addressing a critical shortcoming in current generative systems.

Abstract: Text-to-image (T2I) generation has advanced rapidly, yet faithfully capturing
spatial relationships described in natural language prompts remains a major
challenge. Prior efforts have addressed this issue through prompt optimization,
spatially grounded generation, and semantic refinement. This work introduces a
lightweight approach that augments prompts with tuple-based structured
information, using a fine-tuned language model for automatic conversion and
seamless integration into T2I pipelines. Experimental results demonstrate
substantial improvements in spatial accuracy, without compromising overall
image quality as measured by Inception Score. Furthermore, the automatically
generated tuples exhibit quality comparable to human-crafted tuples. This
structured information provides a practical and portable solution to enhance
spatial relationships in T2I generation, addressing a key limitation of current
large-scale generative systems.

</details>


### [17] [Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers](https://arxiv.org/abs/2509.16058)
*Krati Saxena,Federico Jurado Ruiz,Guido Manzi,Dianbo Liu,Alex Lamb*

Main category: cs.AI

TL;DR: The paper introduces ASAC (Attention Schema-based Attention Control), a novel attention management framework inspired by Attention Schema Theory, demonstrating improved system efficiency across vision and NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Drawing inspiration from the Attention Schema Theory in cognitive science, the paper aims to address the inefficiency and limited robustness in current attention mechanisms within neural networks.

Method: The authors propose ASAC, which embeds a Vector-Quantized Variational AutoEncoder (VQVAE) within transformer architectures to model and optimize attention allocation explicitly.

Result: The ASAC module improves classification accuracy, learning speed, robustness to noise and adversarial attacks, and generalization across vision and NLP tasks, even in multi-task settings.

Conclusion: The study bridges cognitive science and machine learning, offering a more efficient and resilient attention management mechanism in AI systems while demonstrating its practical advantages.

Abstract: Attention mechanisms have become integral in AI, significantly enhancing
model performance and scalability by drawing inspiration from human cognition.
Concurrently, the Attention Schema Theory (AST) in cognitive science posits
that individuals manage their attention by creating a model of the attention
itself, effectively allocating cognitive resources. Inspired by AST, we
introduce ASAC (Attention Schema-based Attention Control), which integrates the
attention schema concept into artificial neural networks. Our initial
experiments focused on embedding the ASAC module within transformer
architectures. This module employs a Vector-Quantized Variational AutoEncoder
(VQVAE) as both an attention abstractor and controller, facilitating precise
attention management. By explicitly modeling attention allocation, our approach
aims to enhance system efficiency. We demonstrate ASAC's effectiveness in both
the vision and NLP domains, highlighting its ability to improve classification
accuracy and expedite the learning process. Our experiments with vision
transformers across various datasets illustrate that the attention controller
not only boosts classification accuracy but also accelerates learning.
Furthermore, we have demonstrated the model's robustness and generalization
capabilities across noisy and out-of-distribution datasets. In addition, we
have showcased improved performance in multi-task settings. Quick experiments
reveal that the attention schema-based module enhances resilience to
adversarial attacks, optimizes attention to improve learning efficiency, and
facilitates effective transfer learning and learning from fewer examples. These
promising results establish a connection between cognitive science and machine
learning, shedding light on the efficient utilization of attention mechanisms
in AI systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [18] [Automatic Microarchitecture-Aware Custom Instruction Design for RISC-V Processors](https://arxiv.org/abs/2509.15782)
*Evgenii Rezunov,Niko Zurstraßen,Lennart M. Reimann,Rainer Leupers*

Main category: cs.AR

TL;DR: The paper presents CIDRE, a tool for automatic design of custom instructions for RISC-V processors, achieving up to 2.47x speed improvement with minimal area cost.


<details>
  <summary>Details</summary>
Motivation: Designing efficient ASIPs is labor-intensive, requiring automation to identify custom instructions that improve performance and minimize area/power costs.

Method: CIDRE analyzes RISC-V applications for hotspots and proposes custom instructions. It generates nML descriptions for other tools to assess costs and benefits.

Result: CIDRE achieved up to 2.47x acceleration and less than 24% area increase in RISC-V benchmark tests.

Conclusion: CIDRE successfully automates the ASIP design process, offering significant performance improvements with modest area and power overhead.

Abstract: An Application-Specific Instruction Set Processor(ASIP) is a specialized
microprocessor that provides a trade-off between the programmability of a
General Purpose Processor (GPP) and the performance and energy-efficiency of
dedicated hardware accelerators. ASIPs are often derived from off-the-shelf
GPPs extended by custom instructions tailored towards a specific software
workload. One of the most important challenges of designing an ASIP is to find
said custom instructions that help to increase performance without being too
costly in terms of area and power consumption. To date, solving this challenge
is relatively labor-intensive and typically performed manually. Addressing the
lack of automation, we present Custom Instruction Designer for RISC-V
Extensions (CIDRE), a front-to-back tool for ASIP design. CIDRE automatically
analyzes hotspots in RISC-V applications and generates custom instruction
suggestions with a corresponding nML description. The nML description can be
used with other electronic design automation tools to accurately assess the
cost and benefits of the found suggestions. In a RISC-V benchmark study, we
were able to accelerate embedded benchmarks from Embench and MiBench by up to
2.47x with less than 24% area increase. The entire process was conducted
completely automatically.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [19] [Synthetic bootstrapped pretraining](https://arxiv.org/abs/2509.15248)
*Zitong Yang,Aonan Zhang,Hong Liu,Tatsunori Hashimoto,Emmanuel Candès,Chong Wang,Ruoming Pang*

Main category: cs.CL

TL;DR: SBP introduces a novel language model pretraining approach focused on synthesizing a large corpus by leveraging inter-document relations to achieve better efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Standard LM pretraining focuses on token-level correlations within a single document but neglects rich inter-document correlations that could enhance model performance.

Method: SBP trains a 3B-parameter language model with compute-matched pretraining setups, synthesizing up to 1 trillion tokens by abstracting and narrating concepts from related documents.

Result: SBP demonstrated consistent improvement over repetition baselines, achieving performance close to oracle-like setups with access to substantially more unique data.

Conclusion: SBP showcases strong empirical evidence of performance benefits and reveals a Bayesian interpretation by learning latent concepts across documents, advancing LM pretraining strategies.

Abstract: We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM)
pretraining procedure that first learns a model of relations between documents
from the pretraining dataset and then leverages it to synthesize a vast new
corpus for joint training. While the standard pretraining teaches LMs to learn
causal correlations among tokens within a single document, it is not designed
to efficiently model the rich, learnable inter-document correlations that can
potentially lead to better performance. We validate SBP by designing a
compute-matched pretraining setup and pretrain a 3B-parameter model on up to 1T
tokens from scratch. We find SBP consistently improves upon a strong repetition
baseline and delivers a significant fraction of performance improvement
attainable by an oracle upper bound with access to 20x more unique data.
Qualitative analysis reveals that the synthesized documents go beyond mere
paraphrases -- SBP first abstracts a core concept from the seed material and
then crafts a new narration on top of it. Besides strong empirical performance,
SBP admits a natural Bayesian interpretation: the synthesizer implicitly learns
to abstract the latent concepts shared between related documents.

</details>


### [20] [Comparative Analysis of Tokenization Algorithms for Low-Resource Language Dzongkha](https://arxiv.org/abs/2509.15255)
*Tandin Wangchuk,Tad Gonsalves*

Main category: cs.CL

TL;DR: This study evaluates three tokenization algorithms (BPE, WordPiece, SentencePiece) to improve NLP tasks for Dzongkha, Bhutan's national language.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of effective tokenization for Dzongkha, a low-resource language, which poses challenges in natural language processing.

Method: Three tokenization algorithms were tested: Byte-Pair Encoding (BPE), WordPiece, and SentencePiece (Unigram). Their performance was compared using metrics such as Subword Fertility and Normalized Sequence Length.

Result: SentencePiece was found to be the most effective algorithm for Dzongkha tokenization, demonstrating better representation and efficiency.

Conclusion: Tailored tokenization approaches for low-resource languages like Dzongkha are necessary and could enable the development of Dzongkha LLMs, advancing research in this area.

Abstract: Large Language Models (LLMs) are gaining popularity and improving rapidly.
Tokenizers are crucial components of natural language processing, especially
for LLMs. Tokenizers break down input text into tokens that models can easily
process while ensuring the text is accurately represented, capturing its
meaning and structure. Effective tokenizers enhance the capabilities of LLMs by
improving a model's understanding of context and semantics, ultimately leading
to better performance in various downstream tasks, such as translation,
classification, sentiment analysis, and text generation. Most pre-trained
tokenizers are suitable for high-resource languages like English but perform
poorly for low-resource languages. Dzongkha, Bhutan's national language spoken
by around seven hundred thousand people, is a low-resource language, and its
linguistic complexity poses unique NLP challenges. Despite some progress,
significant research in Dzongkha NLP is lacking, particularly in tokenization.
This study evaluates the training and performance of three common tokenization
algorithms in comparison to other popular methods. Specifically, Byte-Pair
Encoding (BPE), WordPiece, and SentencePiece (Unigram) were evaluated for their
suitability for Dzongkha. Performance was assessed using metrics like Subword
Fertility, Proportion of Continued Words, Normalized Sequence Length, and
execution time. The results show that while all three algorithms demonstrate
potential, SentencePiece is the most effective for Dzongkha tokenization,
paving the way for further NLP advancements. This underscores the need for
tailored approaches for low-resource languages and ongoing research. In this
study, we presented three tokenization algorithms for Dzongkha, paving the way
for building Dzongkha Large Language Models.

</details>


### [21] [Toxicity Red-Teaming: Benchmarking LLM Safety in Singapore's Low-Resource Languages](https://arxiv.org/abs/2509.15260)
*Yujia Hu,Ming Shan Hee,Preslav Nakov,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: This paper introduces SGToxicGuard, a dataset and evaluation framework, to assess and enhance the safety of multilingual Large Language Models (LLMs) in Singapore's diverse linguistic landscape.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the under-explored area of safety mechanisms for LLMs in low-resource and multilingual settings, specifically focusing on Singapore's diverse linguistic environment.

Method: A red-teaming approach is utilized to systematically probe LLM vulnerabilities across three real-world scenarios: conversation, question-answering, and content composition.

Result: Extensive experiments with state-of-the-art multilingual LLMs revealed significant gaps in their safety mechanisms, particularly in managing cultural sensitivity and mitigating toxicity.

Conclusion: The paper provides actionable insights and sets the groundwork for developing safer and more inclusive AI systems in linguistically heterogeneous contexts.

Abstract: The advancement of Large Language Models (LLMs) has transformed natural
language processing; however, their safety mechanisms remain under-explored in
low-resource, multilingual settings. Here, we aim to bridge this gap. In
particular, we introduce \textsf{SGToxicGuard}, a novel dataset and evaluation
framework for benchmarking LLM safety in Singapore's diverse linguistic
context, including Singlish, Chinese, Malay, and Tamil. SGToxicGuard adopts a
red-teaming approach to systematically probe LLM vulnerabilities in three
real-world scenarios: \textit{conversation}, \textit{question-answering}, and
\textit{content composition}. We conduct extensive experiments with
state-of-the-art multilingual LLMs, and the results uncover critical gaps in
their safety guardrails. By offering actionable insights into cultural
sensitivity and toxicity mitigation, we lay the foundation for safer and more
inclusive AI systems in linguistically diverse environments.\footnote{Link to
the dataset: https://github.com/Social-AI-Studio/SGToxicGuard.}
\textcolor{red}{Disclaimer: This paper contains sensitive content that may be
disturbing to some readers.}

</details>


### [22] [PolBiX: Detecting LLMs' Political Bias in Fact-Checking through X-phemisms](https://arxiv.org/abs/2509.15335)
*Charlott Jakob,David Harbecke,Patrick Parschan,Pia Wenzel Neves,Vera Schmitt*

Main category: cs.CL

TL;DR: The paper examines the political bias in large language models (LLMs) with German claims, focusing on judgmental word influences rather than direct political leanings.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore how political bias in LLMs impacts objective tasks like fact-checking, addressing a gap in understanding downstream effects.

Method: It uses minimal pairs of German claims with different political connotations (but factual equivalence) to test six LLMs. It changes connotations using euphemisms or dysphemisms.

Result: Judgmental words, more than political leaning, significantly affect the truthfulness assessments by LLMs. Some models exhibit political bias but prompts focusing on objectivity do not mitigate this.

Conclusion: LLMs are influenced by language-specific judgmental terms over direct political bias, and prompting for objectivity alone is insufficient to address this issue.

Abstract: Large Language Models are increasingly used in applications requiring
objective assessment, which could be compromised by political bias. Many
studies found preferences for left-leaning positions in LLMs, but downstream
effects on tasks like fact-checking remain underexplored. In this study, we
systematically investigate political bias through exchanging words with
euphemisms or dysphemisms in German claims. We construct minimal pairs of
factually equivalent claims that differ in political connotation, to assess the
consistency of LLMs in classifying them as true or false. We evaluate six LLMs
and find that, more than political leaning, the presence of judgmental words
significantly influences truthfulness assessment. While a few models show
tendencies of political bias, this is not mitigated by explicitly calling for
objectivism in prompts.

</details>


### [23] [Quantifying Self-Awareness of Knowledge in Large Language Models](https://arxiv.org/abs/2509.15339)
*Yeongbin Seo,Dongha Lee,Jinyoung Yeo*

Main category: cs.CL

TL;DR: This paper examines hallucination prediction in large language models (LLMs), arguing that success often comes from recognizing question-side shortcuts rather than true introspection. They propose the "Approximate Question-side Effect" (AQE) to measure this and introduce SCAO as a method to reduce dependency on superficial question cues while enhancing introspection.


<details>
  <summary>Details</summary>
Motivation: The authors aim to investigate whether hallucination prediction in LLMs stems from question-side cues or genuine introspection, challenging the notion of self-awareness in these models.

Method: The Approximate Question-side Effect (AQE) was developed to quantify the influence of question-side shortcuts, and a new approach, SCAO, was introduced to minimize this dependency. SCAO encourages semantic compression by focusing on model-side signals.

Result: Experimental results demonstrate that SCAO achieves better and more consistent performance compared to existing methods, especially in scenarios with fewer question-side cues.

Conclusion: The study highlights that much of the perceived success in hallucination prediction arises from exploiting question-side patterns, but their method, SCAO, promotes deeper self-awareness by relying on model-side signals.

Abstract: Hallucination prediction in large language models (LLMs) is often interpreted
as a sign of self-awareness. However, we argue that such performance can arise
from question-side shortcuts rather than true model-side introspection. To
disentangle these factors, we propose the Approximate Question-side Effect
(AQE), which quantifies the contribution of question-awareness. Our analysis
across multiple datasets reveals that much of the reported success stems from
exploiting superficial patterns in questions. We further introduce SCAO
(Semantic Compression by Answering in One word), a method that enhances the use
of model-side signals. Experiments show that SCAO achieves strong and
consistent performance, particularly in settings with reduced question-side
cues, highlighting its effectiveness in fostering genuine self-awareness in
LLMs.

</details>


### [24] [Real, Fake, or Manipulated? Detecting Machine-Influenced Text](https://arxiv.org/abs/2509.15350)
*Yitong Wang,Zhongping Zhang,Margherita Piana,Zheng Zhou,Peter Gerstoft,Bryan A. Plummer*

Main category: cs.CL

TL;DR: The paper presents HERO, a hierarchical machine-influenced text detector capable of distinguishing human-written, machine-generated, machine-polished, and machine-translated texts.


<details>
  <summary>Details</summary>
Motivation: Most prior research on machine-generated text (MGT) detection focuses solely on distinguishing between human-written and machine-written documents, ignoring nuanced differences in text generated or influenced by LLMs.

Method: The HERO model relies on length-specialist models and a Subcategory Guidance module to enhance fine-grained categorization and improve performance in detecting the influence of LLMs across various text categories.

Result: HERO outperformed state-of-the-art techniques in machine-influenced text detection by 2.5-3 mAP on average across five LLMs and six domains.

Conclusion: This paper demonstrates the effectiveness of HERO in detecting various uses of LLMs in text creation, highlighting its significance for understanding and managing potential misuse of AI-written content.

Abstract: Large Language Model (LLMs) can be used to write or modify documents,
presenting a challenge for understanding the intent behind their use. For
example, benign uses may involve using LLM on a human-written document to
improve its grammar or to translate it into another language. However, a
document entirely produced by a LLM may be more likely to be used to spread
misinformation than simple translation (\eg, from use by malicious actors or
simply by hallucinating). Prior works in Machine Generated Text (MGT) detection
mostly focus on simply identifying whether a document was human or machine
written, ignoring these fine-grained uses. In this paper, we introduce a
HiErarchical, length-RObust machine-influenced text detector (HERO), which
learns to separate text samples of varying lengths from four primary types:
human-written, machine-generated, machine-polished, and machine-translated.
HERO accomplishes this by combining predictions from length-specialist models
that have been trained with Subcategory Guidance. Specifically, for categories
that are easily confused (\eg, different source languages), our Subcategory
Guidance module encourages separation of the fine-grained categories, boosting
performance. Extensive experiments across five LLMs and six domains demonstrate
the benefits of our HERO, outperforming the state-of-the-art by 2.5-3 mAP on
average.

</details>


### [25] [Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing](https://arxiv.org/abs/2509.15361)
*Zichen Wu,Hsiu-Yuan Huang,Yunfang Wu*

Main category: cs.CL

TL;DR: The paper introduces a causal mediation-based debiasing framework to improve the robustness of multimodal large language models (MLLMs) in complex reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the issue of MLLMs relying on spurious correlations, which affects their robustness and limits generalization in multimodal reasoning tasks.

Method: The framework uses counterfactual examples to separate core semantics from spurious textual and visual contexts during training. Additionally, it employs a Mixture-of-Experts (MoE) architecture with dynamic routing to engage modality-specific debiasing experts.

Result: The framework outperforms unimodal debiasing strategies and existing state-of-the-art models on tasks like multimodal sarcasm detection and sentiment analysis.

Conclusion: This novel debiasing approach enhances the robustness and generalization capabilities of MLLMs by effectively tackling superficial correlation biases.

Abstract: Multimodal Large Language Models (MLLMs) have shown substantial capabilities
in integrating visual and textual information, yet frequently rely on spurious
correlations, undermining their robustness and generalization in complex
multimodal reasoning tasks. This paper addresses the critical challenge of
superficial correlation bias in MLLMs through a novel causal mediation-based
debiasing framework. Specially, we distinguishing core semantics from spurious
textual and visual contexts via counterfactual examples to activate
training-stage debiasing and employ a Mixture-of-Experts (MoE) architecture
with dynamic routing to selectively engages modality-specific debiasing
experts. Empirical evaluation on multimodal sarcasm detection and sentiment
analysis tasks demonstrates that our framework significantly surpasses unimodal
debiasing strategies and existing state-of-the-art models.

</details>


### [26] [Speech Language Models for Under-Represented Languages: Insights from Wolof](https://arxiv.org/abs/2509.15362)
*Yaya Sy,Dioula Doucouré,Christophe Cerisara,Irina Illina*

Main category: cs.CL

TL;DR: This paper describes the development of a Speech Language Model (Speech LLM) for Wolof, focusing on data collection, model training, and extending its capabilities to tasks like speech translation.


<details>
  <summary>Details</summary>
Motivation: Wolof is an underrepresented language, and there is a need to develop speech language models to improve language translation, transcription, and recognition for the language.

Method: The authors collected large-scale, high-quality spontaneous speech data and pretrained the HuBERT model on it. They integrated the model into a Wolof-specific Speech LLM and extended its functionalities to tasks such as speech translation and multi-step reasoning.

Result: The pretrained Speech LLM outperformed the original HuBERT model and African-centric models in Automatic Speech Recognition (ASR). The model also demonstrated strong capabilities in speech translation and multi-step reasoning tasks.

Conclusion: This work highlights the potential of focused speech data collection and fine-tuning methods for underrepresented languages. The open sharing of models and code can further its impact in the field.

Abstract: We present our journey in training a speech language model for Wolof, an
underrepresented language spoken in West Africa, and share key insights. We
first emphasize the importance of collecting large-scale, spontaneous,
high-quality speech data, and show that continued pretraining HuBERT on this
dataset outperforms both the base model and African-centric models on ASR. We
then integrate this speech encoder into a Wolof LLM to train the first Speech
LLM for this language, extending its capabilities to tasks such as speech
translation. Furthermore, we explore training the Speech LLM to perform
multi-step Chain-of-Thought before transcribing or translating. Our results
show that the Speech LLM not only improves speech recognition but also performs
well in speech translation. The models and the code will be openly shared.

</details>


### [27] [Frustratingly Easy Data Augmentation for Low-Resource ASR](https://arxiv.org/abs/2509.15373)
*Katsumi Ibaraki,David Chiang*

Main category: cs.CL

TL;DR: The paper proposes data augmentation methods to enhance low-resource speech recognition by generating synthetic data using text-based techniques and fine-tuning a pretrained model, yielding significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of improving Automatic Speech Recognition (ASR) in languages with extremely limited resources.

Method: Three self-contained data augmentation techniques are utilized: gloss-based replacement, random replacement, and LLM-based text generation, followed by generating synthetic audio using Text-to-Speech (TTS).

Result: Fine-tuning a pretrained Wav2Vec2-XLSR-53 model with original and synthetic data led to substantial improvements in Word Error Rate (WER), including a 14.3% absolute reduction for Nashta.

Conclusion: The proposed methods are effective for low-resource languages, enhance speech recognition performance, and demonstrate versatility for high-resource languages like English.

Abstract: This paper introduces three self-contained data augmentation methods for
low-resource Automatic Speech Recognition (ASR). Our techniques first generate
novel text--using gloss-based replacement, random replacement, or an LLM-based
approach--and then apply Text-to-Speech (TTS) to produce synthetic audio. We
apply these methods, which leverage only the original annotated data, to four
languages with extremely limited resources (Vatlongos, Nashta, Shinekhen
Buryat, and Kakabe). Fine-tuning a pretrained Wav2Vec2-XLSR-53 model on a
combination of the original audio and generated synthetic data yields
significant performance gains, including a 14.3% absolute WER reduction for
Nashta. The methods prove effective across all four low-resource languages and
also show utility for high-resource languages like English, demonstrating their
broad applicability.

</details>


### [28] [Quantifying Uncertainty in Natural Language Explanations of Large Language Models for Question Answering](https://arxiv.org/abs/2509.15403)
*Yangyi Li,Mengdi Huai*

Main category: cs.CL

TL;DR: This paper proposes a novel framework for uncertainty estimation in natural language explanations generated by large language models (LLMs), aiming to provide valid confidence measures in complex QA tasks.


<details>
  <summary>Details</summary>
Motivation: The lack of reliable uncertainty quantification methods for natural language explanations generated by LLMs, particularly important in noisy contexts like medical inquiries, motivates the need for valid confidence guarantees.

Method: The authors introduce a post-hoc, model-agnostic uncertainty estimation framework and design a robust method capable of maintaining valid uncertainty guarantees even in noisy scenarios.

Result: Extensive experiments on QA tasks show that the proposed methods achieve desired performance, effectively providing valid uncertainty guarantees for natural language explanations.

Conclusion: The study addresses a critical gap by enabling reliable confidence measures in LLM-generated explanations, enhancing understanding of model behaviors and decision-making in complex tasks.

Abstract: Large language models (LLMs) have shown strong capabilities, enabling
concise, context-aware answers in question answering (QA) tasks. The lack of
transparency in complex LLMs has inspired extensive research aimed at
developing methods to explain large language behaviors. Among existing
explanation methods, natural language explanations stand out due to their
ability to explain LLMs in a self-explanatory manner and enable the
understanding of model behaviors even when the models are closed-source.
However, despite these promising advancements, there is no existing work
studying how to provide valid uncertainty guarantees for these generated
natural language explanations. Such uncertainty quantification is critical in
understanding the confidence behind these explanations. Notably, generating
valid uncertainty estimates for natural language explanations is particularly
challenging due to the auto-regressive generation process of LLMs and the
presence of noise in medical inquiries. To bridge this gap, in this work, we
first propose a novel uncertainty estimation framework for these generated
natural language explanations, which provides valid uncertainty guarantees in a
post-hoc and model-agnostic manner. Additionally, we also design a novel robust
uncertainty estimation method that maintains valid uncertainty guarantees even
under noise. Extensive experiments on QA tasks demonstrate the desired
performance of our methods.

</details>


### [29] [Deep learning and abstractive summarisation for radiological reports: an empirical study for adapting the PEGASUS models' family with scarce data](https://arxiv.org/abs/2509.15419)
*Claudio Benzoni,Martina Langhals,Martin Boeker,Luise Modersohn,Máté E. Maros*

Main category: cs.CL

TL;DR: This paper explores fine-tuning PEGASUS models for medical text summarisation using radiological reports, noting challenges such as overfitting and performance declines with larger checkpoints.


<details>
  <summary>Details</summary>
Motivation: To address challenges in abstractive summarisation in sensitive domains like medicine, where increasing imaging data demands effective automated tools.

Method: Fine-tuned the PEGASUS and PEGASUS-X models on a public dataset of radiological reports, evaluating performance with lexical and semantic metrics and analyzing training patterns.

Result: PEGASUS showed epoch-wise performance fluctuations, while PEGASUS-X experienced performance detriment with larger checkpoints.

Conclusion: Fine-tuning high-expressivity models in data-scarce medical domains faces challenges, emphasizing the need for robust fine-tuning strategies.

Abstract: Regardless of the rapid development of artificial intelligence, abstractive
summarisation is still challenging for sensitive and data-restrictive domains
like medicine. With the increasing number of imaging, the relevance of
automated tools for complex medical text summarisation is expected to become
highly relevant. In this paper, we investigated the adaptation via fine-tuning
process of a non-domain-specific abstractive summarisation encoder-decoder
model family, and gave insights to practitioners on how to avoid over- and
underfitting. We used PEGASUS and PEGASUS-X, on a medium-sized radiological
reports public dataset. For each model, we comprehensively evaluated two
different checkpoints with varying sizes of the same training data. We
monitored the models' performances with lexical and semantic metrics during the
training history on the fixed-size validation set. PEGASUS exhibited different
phases, which can be related to epoch-wise double-descent, or
peak-drop-recovery behaviour. For PEGASUS-X, we found that using a larger
checkpoint led to a performance detriment. This work highlights the challenges
and risks of fine-tuning models with high expressivity when dealing with scarce
training data, and lays the groundwork for future investigations into more
robust fine-tuning strategies for summarisation models in specialised domains.

</details>


### [30] [BiRQ: Bi-Level Self-Labeling Random Quantization for Self-Supervised Speech Recognition](https://arxiv.org/abs/2509.15430)
*Liuyuan Jiang,Xiaodong Cui,Brian Kingsbury,Tianyi Chen,Lisha Chen*

Main category: cs.CL

TL;DR: The paper presents BiRQ, a self-supervised learning framework for speech that balances efficiency and informativeness by reusing parts of the model to generate refined labels, improving over existing methods.


<details>
  <summary>Details</summary>
Motivation: Self-supervised learning in speech is essential due to the high cost of labeled audio-text pairs. Existing methods like HuBERT are effective but computationally expensive, while simpler methods like BEST-RQ have weaker labels.

Method: BiRQ uses intermediate model representations as pseudo-label generators via random-projection quantization, producing enhanced labels. Anchoring raw-input labels stabilize training, and a bilevel optimization with Gumbel-softmax makes the method end-to-end and efficient.

Result: BiRQ outperforms BEST-RQ in terms of downstream performance across diverse datasets like LibriSpeech, AMI meetings, and YODAS, while maintaining simplicity and low computational demands.

Conclusion: BiRQ successfully combines the efficiency of simpler methods and the accuracy of refinement-heavy methods, eliminating the need for external encoders and offering scalability for speech SSL tasks.

Abstract: Speech is a rich signal, and labeled audio-text pairs are costly, making
self-supervised learning essential for scalable representation learning. A core
challenge in speech SSL is generating pseudo-labels that are both informative
and efficient: strong labels, such as those used in HuBERT, improve downstream
performance but rely on external encoders and multi-stage pipelines, while
efficient methods like BEST-RQ achieve simplicity at the cost of weaker labels.
We propose BiRQ, a bilevel SSL framework that combines the efficiency of
BEST-RQ with the refinement benefits of HuBERT-style label enhancement. The key
idea is to reuse part of the model itself as a pseudo-label generator:
intermediate representations are discretized by a random-projection quantizer
to produce enhanced labels, while anchoring labels derived directly from the
raw input stabilize training and prevent collapse. Training is formulated as an
efficient first-order bilevel optimization problem, solved end-to-end with
differentiable Gumbel-softmax selection. This design eliminates the need for
external label encoders, reduces memory cost, and enables iterative label
refinement in an end-to-end fashion. BiRQ consistently improves over BEST-RQ
while maintaining low complexity and computational efficiency. We validate our
method on various datasets, including 960-hour LibriSpeech, 150-hour AMI
meetings and 5,000-hour YODAS, demonstrating consistent gains over BEST-RQ.

</details>


### [31] [CodeRAG: Finding Relevant and Necessary Knowledge for Retrieval-Augmented Repository-Level Code Completion](https://arxiv.org/abs/2509.16112)
*Sheng Zhang,Yifan Ding,Shuquan Lian,Shun Song,Hui Li*

Main category: cs.CL

TL;DR: The paper introduces CodeRAG, a framework to enhance repository-level code completion, addressing key limitations in existing methods such as query construction, code retrieval, and retriever alignment.


<details>
  <summary>Details</summary>
Motivation: Existing methods for repository-level code completion suffer from issues like inappropriate query construction, single-path retrieval, and misalignment between code retriever and code LLM.

Method: CodeRAG employs log probability-guided query construction, multi-path code retrieval, and preference-aligned BestFit reranking to improve the code completion process.

Result: Experiments on benchmarks ReccEval and CCEval show that CodeRAG outperforms state-of-the-art methods significantly.

Conclusion: CodeRAG is an effective solution for augmenting repository-level code completion, demonstrating superior performance through its innovative components.

Abstract: Repository-level code completion automatically predicts the unfinished code
based on the broader information from the repository. Recent strides in Code
Large Language Models (code LLMs) have spurred the development of
repository-level code completion methods, yielding promising results.
Nevertheless, they suffer from issues such as inappropriate query construction,
single-path code retrieval, and misalignment between code retriever and code
LLM. To address these problems, we introduce CodeRAG, a framework tailored to
identify relevant and necessary knowledge for retrieval-augmented
repository-level code completion. Its core components include log probability
guided query construction, multi-path code retrieval, and preference-aligned
BestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval
demonstrate that CodeRAG significantly and consistently outperforms
state-of-the-art methods. The implementation of CodeRAG is available at
https://github.com/KDEGroup/CodeRAG.

</details>


### [32] [PILOT: Steering Synthetic Data Generation with Psychological & Linguistic Output Targeting](https://arxiv.org/abs/2509.15447)
*Caitlin Cisar,Emily Sheffield,Joshua Drake,Alden Harrell,Subramanian Chidambaram,Nikita Nangia,Vinayak Arannil,Alex Williams*

Main category: cs.CL

TL;DR: The paper introduces PILOT, a framework to steer generative AI with structured psycholinguistic profiles instead of natural-language personas for better control and coherence in outputs.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of generative AI relying on natural-language personas, which cause unintended inferences and limited control over data generation.

Method: PILOT is a two-phase framework: Phase 1 converts natural-language personas into structured profiles with multidimensional psycholinguistic scores, and Phase 2 uses these profiles to guide LLM outputs along measurable variation axes.

Result: Using three LLMs and 25 personas, schema-based steering (SBS) significantly reduced repetitive outputs and improved coherence, showing increased silhouette scores (0.098 to 0.237) and topic purity (0.773 to 0.957).

Conclusion: Schema-based steering improves coherence and predictability, NPS offers diversity, and hybrid methods balance both, while maintaining overall response quality without significant trade-offs between approaches.

Abstract: Generative AI applications commonly leverage user personas as a steering
mechanism for synthetic data generation, but reliance on natural language
representations forces models to make unintended inferences about which
attributes to emphasize, limiting precise control over outputs. We introduce
PILOT (Psychological and Linguistic Output Targeting), a two-phase framework
for steering large language models with structured psycholinguistic profiles.
In Phase 1, PILOT translates natural language persona descriptions into
multidimensional profiles with normalized scores across linguistic and
psychological dimensions. In Phase 2, these profiles guide generation along
measurable axes of variation. We evaluate PILOT across three state-of-the-art
LLMs (Mistral Large 2, Deepseek-R1, LLaMA 3.3 70B) using 25 synthetic personas
under three conditions: Natural-language Persona Steering (NPS), Schema-Based
Steering (SBS), and Hybrid Persona-Schema Steering (HPS). Results demonstrate
that schema-based approaches significantly reduce artificial-sounding persona
repetition while improving output coherence, with silhouette scores increasing
from 0.098 to 0.237 and topic purity from 0.773 to 0.957. Our analysis reveals
a fundamental trade-off: SBS produces more concise outputs with higher topical
consistency, while NPS offers greater lexical diversity but reduced
predictability. HPS achieves a balance between these extremes, maintaining
output variety while preserving structural consistency. Expert linguistic
evaluation confirms that PILOT maintains high response quality across all
conditions, with no statistically significant differences between steering
approaches.

</details>


### [33] [RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation](https://arxiv.org/abs/2509.16198)
*Jane Luo,Xin Zhang,Steven Liu,Jie Wu,Yiming Huang,Yangyu Huang,Chengyu Yin,Ying Xin,Jianfeng Liu,Yuefeng Zhan,Hao Sun,Qi Chen,Scarlett Li,Mao Yang*

Main category: cs.CL

TL;DR: The paper introduces Repository Planning Graph (RPG) as an explicit blueprint for software generation, replacing ambiguous natural language, and demonstrates its efficacy through the ZeroRepo framework and RepoCraft benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing large language models struggle with complete repository generation due to challenges in coherent long-horizon planning and inefficiencies in using ambiguous natural language for complex structures.

Method: The paper introduces RPG to encode proposal and implementation levels into a graph and develops ZeroRepo to construct repositories using RPG-driven planning and graph-guided code generation validated by tests.

Result: On the RepoCraft benchmark, ZeroRepo achieves repositories with nearly 36K LOC, outperforming baselines by up to 64×. It achieves 81.5% functional coverage and a 69.7% pass rate, significantly exceeding Claude Code and other baselines.

Conclusion: RPG serves as an effective repository planning and scaling tool, enabling long-term project planning and improving the coherence and scalability of repository generation using large language models.

Abstract: Large language models excel at function- and file-level code generation, yet
generating complete repositories from scratch remains a fundamental challenge.
This process demands coherent and reliable planning across proposal- and
implementation-level stages, while natural language, due to its ambiguity and
verbosity, is ill-suited for faithfully representing complex software
structures. To address this, we introduce the Repository Planning Graph (RPG),
a persistent representation that unifies proposal- and implementation-level
planning by encoding capabilities, file structures, data flows, and functions
in one graph. RPG replaces ambiguous natural language with an explicit
blueprint, enabling long-horizon planning and scalable repository generation.
Building on RPG, we develop ZeroRepo, a graph-driven framework for repository
generation from scratch. It operates in three stages: proposal-level planning
and implementation-level refinement to construct the graph, followed by
graph-guided code generation with test validation. To evaluate this setting, we
construct RepoCraft, a benchmark of six real-world projects with 1,052 tasks.
On RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly
3.9$\times$ the strongest baseline (Claude Code) and about 64$\times$ other
baselines. It attains 81.5% functional coverage and a 69.7% pass rate,
exceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further
analysis shows that RPG models complex dependencies, enables progressively more
sophisticated planning through near-linear scaling, and enhances LLM
understanding of repositories, thereby accelerating agent localization.

</details>


### [34] [Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding](https://arxiv.org/abs/2509.15476)
*Zhu Li,Xiyuan Gao,Yuqing Zhang,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: This paper investigates sarcasm detection leveraging text, audio, and vision modalities, systematically evaluating large language models (LLMs) and multimodal LLMs in English and Chinese datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in sarcasm detection, which involves subtle cross-modal cues, and to explore underdeveloped areas like audio-visual-textual sarcasm understanding.

Method: Large language models and multimodal LLMs are evaluated using English and Chinese datasets in zero-shot, few-shot, and LoRA fine-tuning settings. Direct classification and collaborative gating fusion methods are applied.

Result: Audio-based models achieve the best unimodal performance, while audio-vision and text-audio combinations outperform unimodal and trimodal configurations. MLLMs like Qwen-Omni demonstrate competitive performance across scenarios.

Conclusion: Multimodal LLMs show promise for advancing sarcasm detection across multiple languages and modalities, emphasizing their cross-lingual and multimodal capabilities.

Abstract: Sarcasm detection remains a challenge in natural language understanding, as
sarcastic intent often relies on subtle cross-modal cues spanning text, speech,
and vision. While prior work has primarily focused on textual or visual-textual
sarcasm, comprehensive audio-visual-textual sarcasm understanding remains
underexplored. In this paper, we systematically evaluate large language models
(LLMs) and multimodal LLMs for sarcasm detection on English (MUStARD++) and
Chinese (MCSD 1.0) in zero-shot, few-shot, and LoRA fine-tuning settings. In
addition to direct classification, we explore models as feature encoders,
integrating their representations through a collaborative gating fusion module.
Experimental results show that audio-based models achieve the strongest
unimodal performance, while text-audio and audio-vision combinations outperform
unimodal and trimodal models. Furthermore, MLLMs such as Qwen-Omni show
competitive zero-shot and fine-tuned performance. Our findings highlight the
potential of MLLMs for cross-lingual, audio-visual-textual sarcasm
understanding.

</details>


### [35] [Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models](https://arxiv.org/abs/2509.15478)
*Madison Van Doren,Casey Ford,Emily Dix*

Main category: cs.CL

TL;DR: This paper evaluates the safety of four multimodal large language models (MLLMs) under adversarial conditions, finding significant differences in their vulnerability to harmful responses.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the lack of comprehensive safety evaluations for MLLMs, especially as their deployment in real-world applications grows.

Method: A team of 26 red teamers created 726 adversarial prompts focusing on three harm categories (illegal activity, disinformation, unethical behavior) across text-only and multimodal formats. Model responses were rated for harmfulness by 17 annotators using a 5-point scale.

Result: Pixtral 12B exhibited the highest rate of harmful responses (~62%) and Claude Sonnet 3.5 the lowest (~10%). Text-only prompts were slightly more effective at bypassing safety mechanisms than multimodal ones. Statistical analysis confirmed differences based on model type and input modality.

Conclusion: These results highlight the urgent need for robust safety benchmarks for MLLMs, given their varying susceptibility to adversarial prompts.

Abstract: Multimodal large language models (MLLMs) are increasingly used in real world
applications, yet their safety under adversarial conditions remains
underexplored. This study evaluates the harmlessness of four leading MLLMs
(GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus) when exposed to
adversarial prompts across text-only and multimodal formats. A team of 26 red
teamers generated 726 prompts targeting three harm categories: illegal
activity, disinformation, and unethical behaviour. These prompts were submitted
to each model, and 17 annotators rated 2,904 model outputs for harmfulness
using a 5-point scale. Results show significant differences in vulnerability
across models and modalities. Pixtral 12B exhibited the highest rate of harmful
responses (~62%), while Claude Sonnet 3.5 was the most resistant (~10%).
Contrary to expectations, text-only prompts were slightly more effective at
bypassing safety mechanisms than multimodal ones. Statistical analysis
confirmed that both model type and input modality were significant predictors
of harmfulness. These findings underscore the urgent need for robust,
multimodal safety benchmarks as MLLMs are deployed more widely.

</details>


### [36] [mucAI at BAREC Shared Task 2025: Towards Uncertainty Aware Arabic Readability Assessment](https://arxiv.org/abs/2509.15485)
*Ahmed Abdou*

Main category: cs.CL

TL;DR: This paper introduces a post-processing method for improving Arabic readability assessment with statistical guarantees, yielding enhanced Quadratic Weighted Kappa (QWK) performance.


<details>
  <summary>Details</summary>
Motivation: To address challenges in fine-grained Arabic readability classification, ensuring statistical guarantees for prediction reliability while improving assessment accuracy in practical contexts.

Method: The authors use conformal prediction to generate coverage-guaranteed prediction sets, combining them with weighted averages driven by softmax-renormalized probabilities, reducing high-penalty misclassification levels.

Result: The technique demonstrated an improvement of 1-3 QWK points with consistent results across base models, and achieved notable scores: 84.9% (test), 85.7% (blind test) for sentences, and 73.3% for documents.

Conclusion: This approach enhances Arabic educational assessment by enabling reviewers to focus on a statistically ensured subset of predicted readability levels, improving both accuracy and usability.

Abstract: We present a simple, model-agnostic post-processing technique for
fine-grained Arabic readability classification in the BAREC 2025 Shared Task
(19 ordinal levels). Our method applies conformal prediction to generate
prediction sets with coverage guarantees, then computes weighted averages using
softmax-renormalized probabilities over the conformal sets. This
uncertainty-aware decoding improves Quadratic Weighted Kappa (QWK) by reducing
high-penalty misclassifications to nearer levels. Our approach shows consistent
QWK improvements of 1-3 points across different base models. In the strict
track, our submission achieves QWK scores of 84.9\%(test) and 85.7\% (blind
test) for sentence level, and 73.3\% for document level. For Arabic educational
assessment, this enables human reviewers to focus on a handful of plausible
levels, combining statistical guarantees with practical usability.

</details>


### [37] [LLM Cache Bandit Revisited: Addressing Query Heterogeneity for Cost-Effective LLM Inference](https://arxiv.org/abs/2509.15515)
*Hantao Yang,Hong Xie,Defu Lian,Enhong Chen*

Main category: cs.CL

TL;DR: This paper addresses the challenge of optimizing cache replacement for heterogeneous query sizes in LLM inference, proposing a knapsack-based strategy for efficient cache updates with theoretical and real-world performance improvements.


<details>
  <summary>Details</summary>
Motivation: To resolve the complexity and inefficiencies in cache replacement strategies for LLM queries of varying sizes, overcoming computational and statistical challenges.

Method: The paper reformulates cache selection as a knapsack problem, using an accumulation-based strategy to optimize the computational-efficiency vs. cache-update trade-off.

Result: Theoretical analysis shows improved regret bounds, reducing the coefficient compared to previous work, and experiments demonstrate a 12% cost reduction on real-world data.

Conclusion: The approach successfully balances theoretical rigor and practicality, addressing query heterogeneity in the LLM cache bandit problem, achieving both cost efficiency and statistical guarantees.

Abstract: This paper revisits the LLM cache bandit problem, with a special focus on
addressing the query heterogeneity for cost-effective LLM inference. Previous
works often assume uniform query sizes. Heterogeneous query sizes introduce a
combinatorial structure for cache selection, making the cache replacement
process more computationally and statistically challenging. We treat optimal
cache selection as a knapsack problem and employ an accumulation-based strategy
to effectively balance computational overhead and cache updates. In theoretical
analysis, we prove that the regret of our algorithm achieves an $O(\sqrt{MNT})$
bound, improving the coefficient of $\sqrt{MN}$ compared to the $O(MN\sqrt{T})$
result in Berkeley, where $N$ is the total number of queries and $M$ is the
cache size. Additionally, we also provide a problem-dependent bound, which was
absent in previous works. The experiment rely on real-world data show that our
algorithm reduces the total cost by approximately 12\%.

</details>


### [38] [How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages](https://arxiv.org/abs/2509.15518)
*Siyang Wu,Zhewei Sun*

Main category: cs.CL

TL;DR: This study examines the ability of large language models (LLMs) like GPT-4o and Llama-3 to understand and replicate slang by comparing their outputs to human slang usage.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs can accurately capture structural knowledge about slang that aligns with human usage and inform tasks like slang detection and interpretation.

Method: A systematic comparison of human-attested slang from the Online Slang Dictionary (OSD) and LLM-generated slang in terms of biases, creativity, and informativeness.

Result: Findings reveal significant biases in LLMs' perception of slang, and while they show creative knowledge about slang, it doesn't fully align with human tendencies or insights.

Conclusion: Although LLMs demonstrate some comprehension of slang, their understanding is insufficient for tasks like linguistic analyses, limiting their extrapolative capabilities.

Abstract: Slang is a commonly used type of informal language that poses a daunting
challenge to NLP systems. Recent advances in large language models (LLMs),
however, have made the problem more approachable. While LLM agents are becoming
more widely applied to intermediary tasks such as slang detection and slang
interpretation, their generalizability and reliability are heavily dependent on
whether these models have captured structural knowledge about slang that align
well with human attested slang usages. To answer this question, we contribute a
systematic comparison between human and machine-generated slang usages. Our
evaluative framework focuses on three core aspects: 1) Characteristics of the
usages that reflect systematic biases in how machines perceive slang, 2)
Creativity reflected by both lexical coinages and word reuses employed by the
slang usages, and 3) Informativeness of the slang usages when used as
gold-standard examples for model distillation. By comparing human-attested
slang usages from the Online Slang Dictionary (OSD) and slang generated by
GPT-4o and Llama-3, we find significant biases in how LLMs perceive slang. Our
results suggest that while LLMs have captured significant knowledge about the
creative aspects of slang, such knowledge does not align with humans
sufficiently to enable LLMs for extrapolative tasks such as linguistic
analyses.

</details>


### [39] [A method for improving multilingual quality and diversity of instruction fine-tuning datasets](https://arxiv.org/abs/2509.15549)
*Chunguang Zhao,Yilun Liu,Pufan Zeng,Yuanchang Luo,Shimin Tao,Minggui He,Weibin Meng,Song Xu,Ziang Chen,Chen Liu,Hongxia Ma,Li Zhang,Boxing Chen,Daimeng Wei*

Main category: cs.CL

TL;DR: The paper addresses the multilingual instruction fine-tuning (IFT) of large language models by introducing a new method, M-DaQ, to select high-quality and diverse training samples, overcoming cultural and linguistic bottlenecks.


<details>
  <summary>Details</summary>
Motivation: To tackle the scarcity of high-quality multilingual training data for fine-tuning LLMs, which limits their ability to generalize across languages and cultures.

Method: The authors propose M-DaQ, a data selection method emphasizing high quality and semantic diversity while systematically investigating the Superficial Alignment Hypothesis in a multilingual context.

Result: Experimental results across 18 languages show models fine-tuned with M-DaQ yield significant performance improvements (over 60% win rate) compared to vanilla baselines, validated further through human evaluation.

Conclusion: M-DaQ enhances LLM multilingual fine-tuning by improving data quality and diversity, achieving notable performance improvements and cultural sensitivity gains in responses. The method and its code are shared publicly.

Abstract: Multilingual Instruction Fine-Tuning (IFT) is essential for enabling large
language models (LLMs) to generalize effectively across diverse linguistic and
cultural contexts. However, the scarcity of high-quality multilingual training
data and corresponding building method remains a critical bottleneck. While
data selection has shown promise in English settings, existing methods often
fail to generalize across languages due to reliance on simplistic heuristics or
language-specific assumptions. In this work, we introduce Multilingual Data
Quality and Diversity (M-DaQ), a novel method for improving LLMs
multilinguality, by selecting high-quality and semantically diverse
multilingual IFT samples. We further conduct the first systematic investigation
of the Superficial Alignment Hypothesis (SAH) in multilingual setting.
Empirical results across 18 languages demonstrate that models fine-tuned with
M-DaQ method achieve significant performance gains over vanilla baselines over
60% win rate. Human evaluations further validate these gains, highlighting the
increment of cultural points in the response. We release the M-DaQ code to
support future research.

</details>


### [40] [DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm](https://arxiv.org/abs/2509.15550)
*Xiaowei Zhu,Yubing Ren,Fang Fang,Qingfeng Tan,Shi Wang,Yanan Cao*

Main category: cs.CL

TL;DR: The paper introduces DNA-DetectLLM, a DNA-inspired, zero-shot method to distinguish AI-generated text from human-written text, emphasizing interpretability and robustness.


<details>
  <summary>Details</summary>
Motivation: Rapid advancements in large language models blur distinctions between AI-generated and human-written text, posing societal risks like misinformation and intellectual property issues.

Method: The DNA-DetectLLM method uses a repair-based process to iteratively identify and fix non-optimal tokens in text, quantifying the effort as a detection signal.

Result: DNA-DetectLLM achieves state-of-the-art performance in AI text detection, with relative improvements in AUROC by 5.55% and F1 score by 2.08% across public benchmark datasets.

Conclusion: DNA-DetectLLM provides a robust and interpretable solution for detecting AI-generated text, addressing key societal challenges effectively.

Abstract: The rapid advancement of large language models (LLMs) has blurred the line
between AI-generated and human-written text. This progress brings societal
risks such as misinformation, authorship ambiguity, and intellectual property
concerns, highlighting the urgent need for reliable AI-generated text detection
methods. However, recent advances in generative language modeling have resulted
in significant overlap between the feature distributions of human-written and
AI-generated text, blurring classification boundaries and making accurate
detection increasingly challenging. To address the above challenges, we propose
a DNA-inspired perspective, leveraging a repair-based process to directly and
interpretably capture the intrinsic differences between human-written and
AI-generated text. Building on this perspective, we introduce DNA-DetectLLM, a
zero-shot detection method for distinguishing AI-generated and human-written
text. The method constructs an ideal AI-generated sequence for each input,
iteratively repairs non-optimal tokens, and quantifies the cumulative repair
effort as an interpretable detection signal. Empirical evaluations demonstrate
that our method achieves state-of-the-art detection performance and exhibits
strong robustness against various adversarial attacks and input lengths.
Specifically, DNA-DetectLLM achieves relative improvements of 5.55% in AUROC
and 2.08% in F1 score across multiple public benchmark datasets.

</details>


### [41] [Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining](https://arxiv.org/abs/2509.15556)
*Ping Guo,Yubing Ren,Binbin Liu,Fengze Liu,Haobin Lin,Yifan Zhang,Bingni Zhang,Taifeng Wang,Yin Zheng*

Main category: cs.CL

TL;DR: This paper introduces Climb, a framework that optimizes multilingual data allocation for large language models, leading to improved multilingual performance.


<details>
  <summary>Details</summary>
Motivation: There is a growing need for effective multilingual capabilities in LLMs to meet global application demands. Optimal language proportion allocation in training corpora is critical but challenging due to cross-lingual interactions and dataset scale sensitivity.

Method: The proposed framework, Climb, introduces a cross-lingual interaction-aware language ratio to quantify effective allocation. It optimizes the data allocation through a two-step process: equalizing marginal benefits across languages and maximizing the resulting allocation vectors.

Result: Experiments demonstrate that Climb accurately measures cross-lingual interactions and enhances multilingual optimization. LLMs trained with Climb-derived proportions achieve competitive, state-of-the-art multilingual performance using fewer tokens.

Conclusion: Climb simplifies multilingual optimization challenges and provides a systematic way to allocate training data for LLMs, enhancing their multilingual capabilities effectively.

Abstract: Large language models (LLMs) have become integral to a wide range of
applications worldwide, driving an unprecedented global demand for effective
multilingual capabilities. Central to achieving robust multilingual performance
is the strategic allocation of language proportions within training corpora.
However, determining optimal language ratios is highly challenging due to
intricate cross-lingual interactions and sensitivity to dataset scale. This
paper introduces Climb (Cross-Lingual Interaction-aware Multilingual
Balancing), a novel framework designed to systematically optimize multilingual
data allocation. At its core, Climb introduces a cross-lingual
interaction-aware language ratio, explicitly quantifying each language's
effective allocation by capturing inter-language dependencies. Leveraging this
ratio, Climb proposes a principled two-step optimization procedure--first
equalizing marginal benefits across languages, then maximizing the magnitude of
the resulting language allocation vectors--significantly simplifying the
inherently complex multilingual optimization problem. Extensive experiments
confirm that Climb can accurately measure cross-lingual interactions across
various multilingual settings. LLMs trained with Climb-derived proportions
consistently achieve state-of-the-art multilingual performance, even achieving
competitive performance with open-sourced LLMs trained with more tokens.

</details>


### [42] [How important is language for human-like intelligence?](https://arxiv.org/abs/2509.15560)
*Gary Lupyan,Hunter Gentry,Martin Zettersten*

Main category: cs.CL

TL;DR: The paper explores the transformative role of language in human cognition and its implications for artificial intelligence.


<details>
  <summary>Details</summary>
Motivation: To investigate whether language merely expresses thoughts or actively shapes cognition, and its role in advancing human-like intelligence in AI.

Method: The authors combine insights from AI and cognitive science to examine language's properties and their impact on general intelligence.

Result: Language offers compact, culturally evolved abstractions that help both biological and artificial systems understand the world.

Conclusion: Exposure to language enables learning systems to develop a compressed model of the world, unlocking new cognitive and causal reasoning abilities.

Abstract: We use language to communicate our thoughts. But is language merely the
expression of thoughts, which are themselves produced by other, nonlinguistic
parts of our minds? Or does language play a more transformative role in human
cognition, allowing us to have thoughts that we otherwise could (or would) not
have? Recent developments in artificial intelligence (AI) and cognitive science
have reinvigorated this old question. We argue that language may hold the key
to the emergence of both more general AI systems and central aspects of human
intelligence. We highlight two related properties of language that make it such
a powerful tool for developing domain--general abilities. First, language
offers compact representations that make it easier to represent and reason
about many abstract concepts (e.g., exact numerosity). Second, these compressed
representations are the iterated output of collective minds. In learning a
language, we learn a treasure trove of culturally evolved abstractions. Taken
together, these properties mean that a sufficiently powerful learning system
exposed to language--whether biological or artificial--learns a compressed
model of the world, reverse engineering many of the conceptual and causal
structures that support human (and human-like) thought.

</details>


### [43] [LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs](https://arxiv.org/abs/2509.15568)
*Junlong Jia,Xing Wu,Chaochen Gao,Ziyang Chen,Zijia Lin,Zhongzhi Li,Weinong Wang,Haotian Xu,Donghui Jin,Debing Zhang,Binghui Guo*

Main category: cs.CL

TL;DR: LiteLong is a method that synthesizes high-quality long-context data efficiently using structured topic organization and multi-agent debate.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of relevance-based aggregation approaches in generating long-context data for training large language models.

Method: The method uses the BISAC classification system for hierarchical topic organization and employs a debate mechanism among LLMs to create diverse, high-quality topics. Lightweight BM25 retrieval is then applied to construct 128K-token training samples.

Result: Experiments on HELMET and Ruler benchmarks demonstrate that LiteLong achieves strong long-context performance with reduced computational and data engineering costs.

Conclusion: LiteLong makes long-context data synthesis accessible and integrates well with other methods, promoting further research in long-context training.

Abstract: High-quality long-context data is essential for training large language
models (LLMs) capable of processing extensive documents, yet existing synthesis
approaches using relevance-based aggregation face challenges of computational
efficiency. We present LiteLong, a resource-efficient method for synthesizing
long-context data through structured topic organization and multi-agent debate.
Our approach leverages the BISAC book classification system to provide a
comprehensive hierarchical topic organization, and then employs a debate
mechanism with multiple LLMs to generate diverse, high-quality topics within
this structure. For each topic, we use lightweight BM25 retrieval to obtain
relevant documents and concatenate them into 128K-token training samples.
Experiments on HELMET and Ruler benchmarks demonstrate that LiteLong achieves
competitive long-context performance and can seamlessly integrate with other
long-dependency enhancement methods. LiteLong makes high-quality long-context
data synthesis more accessible by reducing both computational and data
engineering costs, facilitating further research in long-context language
training.

</details>


### [44] [Relevance to Utility: Process-Supervised Rewrite for RAG](https://arxiv.org/abs/2509.15577)
*Jaeyoung Kim,Jongho Kim,Seung-won Hwang,Seoho Song,Young-In Song*

Main category: cs.CL

TL;DR: The paper proposes R2U, a method to improve Retrieval-Augmented Generation (RAG) systems by directly optimizing document utility for generating correct answers.


<details>
  <summary>Details</summary>
Motivation: Current RAG systems struggle with ensuring that retrieved documents contain the content necessary for effective generation, as bridge modules fail to fully address the issue.

Method: R2U optimizes document utility through process supervision and utilizes a distillation pipeline with supervision scaled from large language models to aid smaller rewriter models.

Result: R2U shows consistent empirical improvements across multiple open-domain question-answering benchmarks compared to existing baselines.

Conclusion: The proposed R2U method effectively bridges the gap in RAG by improving the generative utility of retrieved documents, resulting in better generation outcomes.

Abstract: Retrieval-Augmented Generation systems often suffer from a gap between
optimizing retrieval relevance and generative utility: retrieved documents may
be topically relevant but still lack the content needed for effective reasoning
during generation. While existing "bridge" modules attempt to rewrite the
retrieved text for better generation, we show how they fail to capture true
document utility. In this work, we propose R2U, with a key distinction of
directly optimizing to maximize the probability of generating a correct answer
through process supervision. As such direct observation is expensive, we also
propose approximating an efficient distillation pipeline by scaling the
supervision from LLMs, which helps the smaller rewriter model generalize
better. We evaluate our method across multiple open-domain question-answering
benchmarks. The empirical results demonstrate consistent improvements over
strong bridging baselines.

</details>


### [45] [Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization](https://arxiv.org/abs/2509.15579)
*Yun Tang,Cindy Tseng*

Main category: cs.CL

TL;DR: Chunk-based self-supervised learning (Chunk SSL) is introduced to unify streaming and offline speech pre-training, addressing challenges posed by partial utterances often present in streaming applications. It utilizes a large finite scalar quantization (FSQ) module for feature discretization and a masked prediction loss to optimize pre-training, proving highly effective in speech to text tasks.


<details>
  <summary>Details</summary>
Motivation: Current self-supervised learning methods assume full speech utterances, making them less effective for streaming applications where partial utterances are frequent. Thus, a unified solution for speech pre-training in both streaming and offline modes is needed.

Method: The paper proposes Chunk SSL, which processes chunks of speech data using masked prediction loss and discretizes features via a high-resolution finite scalar quantization (FSQ) module. Data augmentation is employed for efficient chunk-based pre-training, and group masked prediction loss is used to mitigate high memory and computation costs.

Result: Chunk SSL demonstrates competitive performance in speech recognition and translation tasks on the Librispeech and Must-C datasets. It works well for both streaming and offline modes and effectively transfers pre-trained knowledge to downstream tasks.

Conclusion: Chunk SSL is a promising approach for unifying streaming and offline speech pre-training, addressing both efficiency and accuracy challenges with partial utterances. It holds significant potential for advancing speech-to-text applications.

Abstract: Low latency speech human-machine communication is becoming increasingly
necessary as speech technology advances quickly in the last decade. One of the
primary factors behind the advancement of speech technology is self-supervised
learning. Most self-supervised learning algorithms are designed with full
utterance assumption and compromises have to made if partial utterances are
presented, which are common in the streaming applications. In this work, we
propose a chunk based self-supervised learning (Chunk SSL) algorithm as an
unified solution for both streaming and offline speech pre-training. Chunk SSL
is optimized with the masked prediction loss and an acoustic encoder is
encouraged to restore indices of those masked speech frames with help from
unmasked frames in the same chunk and preceding chunks. A copy and append data
augmentation approach is proposed to conduct efficient chunk based
pre-training. Chunk SSL utilizes a finite scalar quantization (FSQ) module to
discretize input speech features and our study shows a high resolution FSQ
codebook, i.e., a codebook with vocabulary size up to a few millions, is
beneficial to transfer knowledge from the pre-training task to the downstream
tasks. A group masked prediction loss is employed during pre-training to
alleviate the high memory and computation cost introduced by the large
codebook. The proposed approach is examined in two speech to text tasks, i.e.,
speech recognition and speech translation. Experimental results on the
\textsc{Librispeech} and \textsc{Must-C} datasets show that the proposed method
could achieve very competitive results for speech to text tasks at both
streaming and offline modes.

</details>


### [46] [DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models](https://arxiv.org/abs/2509.15587)
*Tsz Ting Chung,Lemao Liu,Mo Yu,Dit-Yan Yeung*

Main category: cs.CL

TL;DR: The paper introduces DivLogicEval, a new benchmark to reliably assess logical reasoning in LLMs, addressing issues like entangled reasoning skills, language diversity, and bias.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for logic reasoning in LLMs are flawed due to their entangled reasoning skills, limited language diversity, and biased results, prompting the need for a more reliable evaluation framework.

Method: The authors propose DivLogicEval, a benchmark with diverse, counterintuitive natural language statements, coupled with a new metric to counteract biases and randomness in LLM evaluations.

Result: Experiments validated that DivLogicEval effectively demands logical reasoning and compared different LLMs' performance on logical reasoning tasks.

Conclusion: The paper concludes that DivLogicEval provides a more precise and fair evaluation of LLMs' logical reasoning skills compared to existing methods.

Abstract: Logic reasoning in natural language has been recognized as an important
measure of human intelligence for Large Language Models (LLMs). Popular
benchmarks may entangle multiple reasoning skills and thus provide unfaithful
evaluations on the logic reasoning skill. Meanwhile, existing logic reasoning
benchmarks are limited in language diversity and their distributions are
deviated from the distribution of an ideal logic reasoning benchmark, which may
lead to biased evaluation results. This paper thereby proposes a new classical
logic benchmark DivLogicEval, consisting of natural sentences composed of
diverse statements in a counterintuitive way. To ensure a more reliable
evaluation, we also introduce a new evaluation metric that mitigates the
influence of bias and randomness inherent in LLMs. Through experiments, we
demonstrate the extent to which logical reasoning is required to answer the
questions in DivLogicEval and compare the performance of different popular LLMs
in conducting logical reasoning.

</details>


### [47] [SciEvent: Benchmarking Multi-domain Scientific Event Extraction](https://arxiv.org/abs/2509.15620)
*Bofu Dong,Pritesh Shah,Sumedh Sonawane,Tiyasha Banerjee,Erin Brady,Xinya Du,Ming Jiang*

Main category: cs.CL

TL;DR: This paper introduces SciEvent, a benchmark for extracting scientific events from abstracts across multiple domains using a unified event extraction framework.


<details>
  <summary>Details</summary>
Motivation: The current scientific information extraction relies heavily on narrow-domain entity-relation models that struggle with interdisciplinary contexts and provide fragmented results.

Method: The authors propose a unified event extraction (EE) pipeline involving segmenting abstracts into sections (Background, Method, Result, Conclusion) and extracting event triggers and arguments using manual annotations and EE models.

Result: The study revealed a performance gap: current models, including large language models (LLMs), struggle to perform well in fields like sociology and humanities compared to other domains.

Conclusion: SciEvent represents an important step toward creating a generalizable and multi-domain benchmark for scientific information extraction with structured, context-aware event representation.

Abstract: Scientific information extraction (SciIE) has primarily relied on
entity-relation extraction in narrow domains, limiting its applicability to
interdisciplinary research and struggling to capture the necessary context of
scientific information, often resulting in fragmented or conflicting
statements. In this paper, we introduce SciEvent, a novel multi-domain
benchmark of scientific abstracts annotated via a unified event extraction (EE)
schema designed to enable structured and context-aware understanding of
scientific content. It includes 500 abstracts across five research domains,
with manual annotations of event segments, triggers, and fine-grained
arguments. We define SciIE as a multi-stage EE pipeline: (1) segmenting
abstracts into core scientific activities--Background, Method, Result, and
Conclusion; and (2) extracting the corresponding triggers and arguments.
Experiments with fine-tuned EE models, large language models (LLMs), and human
annotators reveal a performance gap, with current models struggling in domains
such as sociology and humanities. SciEvent serves as a challenging benchmark
and a step toward generalizable, multi-domain SciIE.

</details>


### [48] [Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets](https://arxiv.org/abs/2509.15621)
*Tomoya Yamashita,Yuuki Yamanaka,Masanori Yamada,Takayuki Miura,Toshiki Shibahara,Tomoharu Iwata*

Main category: cs.CL

TL;DR: This paper introduces Concept Unlearning (CU) to enable the removal of broader concepts like persons or events from large language models (LLMs), using a knowledge graph representation to ensure precise and comprehensive unlearning without harming unrelated knowledge.


<details>
  <summary>Details</summary>
Motivation: Large language models require a method to remove broader concepts (e.g., persons or events) for privacy and copyright reasons, a limitation not addressed by current Machine Unlearning (MU) techniques.

Method: The authors use knowledge graphs to represent LLMs' internal knowledge and propose a novel method that prompts the LLM to generate knowledge triplets and explanatory sentences for the forgetting target. The unlearning process is then applied to these graph-based representations.

Result: The proposed method demonstrates effective concept-level unlearning, preserving unrelated knowledge, as shown in experiments on real-world and synthetic datasets.

Conclusion: The graph-based approach enables a more intuitive and precise method for concept-level unlearning in LLMs, providing a solution to privacy and ethical challenges.

Abstract: Machine Unlearning (MU) has recently attracted considerable attention as a
solution to privacy and copyright issues in large language models (LLMs).
Existing MU methods aim to remove specific target sentences from an LLM while
minimizing damage to unrelated knowledge. However, these approaches require
explicit target sentences and do not support removing broader concepts, such as
persons or events. To address this limitation, we introduce Concept Unlearning
(CU) as a new requirement for LLM unlearning. We leverage knowledge graphs to
represent the LLM's internal knowledge and define CU as removing the forgetting
target nodes and associated edges. This graph-based formulation enables a more
intuitive unlearning and facilitates the design of more effective methods. We
propose a novel method that prompts the LLM to generate knowledge triplets and
explanatory sentences about the forgetting target and applies the unlearning
process to these representations. Our approach enables more precise and
comprehensive concept removal by aligning the unlearning process with the LLM's
internal knowledge representations. Experiments on real-world and synthetic
datasets demonstrate that our method effectively achieves concept-level
unlearning while preserving unrelated knowledge.

</details>


### [49] [Sparse-Autoencoder-Guided Internal Representation Unlearning for Large Language Models](https://arxiv.org/abs/2509.15631)
*Tomoya Yamashita,Akira Ito,Yuuki Yamanaka,Masanori Yamada,Takayuki Miura,Toshiki Shibahara*

Main category: cs.CL

TL;DR: The paper presents a method to effectively 'unlearn' specific knowledge in large language models by modifying their internal activations, offering an alternative to suppression-based techniques that often fail to fully erase the knowledge and risk model instability.


<details>
  <summary>Details</summary>
Motivation: Ensure proper privacy and copyright compliance when using large language models by developing robust unlearning techniques that don't just suppress undesirable responses but genuinely forget such knowledge.

Method: The paper introduces an unlearning method that intervenes in the model’s internal activations, using an unlearning objective to align forgotten target activations with unknown entities in a sparse autoencoder latent space.

Result: The method successfully aligns the activations of forgotten targets and reduces recall in tasks like question answering without significantly compromising other model knowledge.

Conclusion: The proposed technique provides a more robust approach to LLM unlearning by achieving true forgetting without the risks of over-suppression or model collapse, outperforming existing suppression-based methods.

Abstract: As large language models (LLMs) are increasingly deployed across various
applications, privacy and copyright concerns have heightened the need for more
effective LLM unlearning techniques. Many existing unlearning methods aim to
suppress undesirable outputs through additional training (e.g., gradient
ascent), which reduces the probability of generating such outputs. While such
suppression-based approaches can control model outputs, they may not eliminate
the underlying knowledge embedded in the model's internal activations; muting a
response is not the same as forgetting it. Moreover, such suppression-based
methods often suffer from model collapse. To address these issues, we propose a
novel unlearning method that directly intervenes in the model's internal
activations. In our formulation, forgetting is defined as a state in which the
activation of a forgotten target is indistinguishable from that of ``unknown''
entities. Our method introduces an unlearning objective that modifies the
activation of the target entity away from those of known entities and toward
those of unknown entities in a sparse autoencoder latent space. By aligning the
target's internal activation with those of unknown entities, we shift the
model's recognition of the target entity from ``known'' to ``unknown'',
achieving genuine forgetting while avoiding over-suppression and model
collapse. Empirically, we show that our method effectively aligns the internal
activations of the forgotten target, a result that the suppression-based
approaches do not reliably achieve. Additionally, our method effectively
reduces the model's recall of target knowledge in question-answering tasks
without significant damage to the non-target knowledge.

</details>


### [50] [Multilingual LLM Prompting Strategies for Medical English-Vietnamese Machine Translation](https://arxiv.org/abs/2509.15640)
*Nhu Vo,Nu-Uyen-Phuong Le,Dung D. Le,Massimo Piccardi,Wray Buntine*

Main category: cs.CL

TL;DR: The study evaluates different prompting strategies for six multilingual language models on the task of medical English-Vietnamese translation, with findings emphasizing the influence of model size and the effectiveness of terminology-aware methods.


<details>
  <summary>Details</summary>
Motivation: To improve healthcare access in Vietnam by addressing the challenges of medical English-Vietnamese machine translation, as Vietnamese is a low-resource and under-studied language.

Method: The study systematically evaluates zero-shot, few-shot, and dictionary-augmented prompting strategies for six multilingual language models, leveraging Meddict, an English-Vietnamese medical lexicon.

Result: Model size significantly impacts translation performance, with larger models performing well even in zero-shot settings. Few-shot prompting offers limited improvement, whereas using terminology-aware cues and embedding-based retrieval consistently enhances domain-specific translations.

Conclusion: While multilingual language models show promise for medical English-Vietnamese translation, limitations persist, particularly in fully leveraging few-shot approaches. Terminology-aware methods are critical for domain-specific performance.

Abstract: Medical English-Vietnamese machine translation (En-Vi MT) is essential for
healthcare access and communication in Vietnam, yet Vietnamese remains a
low-resource and under-studied language. We systematically evaluate prompting
strategies for six multilingual LLMs (0.5B-9B parameters) on the MedEV dataset,
comparing zero-shot, few-shot, and dictionary-augmented prompting with Meddict,
an English-Vietnamese medical lexicon. Results show that model scale is the
primary driver of performance: larger LLMs achieve strong zero-shot results,
while few-shot prompting yields only marginal improvements. In contrast,
terminology-aware cues and embedding-based example retrieval consistently
improve domain-specific translation. These findings underscore both the promise
and the current limitations of multilingual LLMs for medical En-Vi MT.

</details>


### [51] [Layer-wise Minimal Pair Probing Reveals Contextual Grammatical-Conceptual Hierarchy in Speech Representations](https://arxiv.org/abs/2509.15655)
*Linyang He,Qiaolin Wang,Xilin Jiang,Nima Mesgarani*

Main category: cs.CL

TL;DR: The paper evaluates the ability of transformer-based speech language models (SLMs) to encode syntactic and semantic features using systematic assessment across 71 tasks.


<details>
  <summary>Details</summary>
Motivation: Understanding the depth of syntactic and semantic encoding capabilities of SLMs to improve their functionality in speech recognition and language modeling.

Method: Systematic evaluations using minimal pair designs and diagnostic feature analysis, spanning 71 tasks across linguistic levels. Layer-wise and time-resolved analysis was conducted.

Result: SLMs demonstrate stronger encoding of grammatical features compared to conceptual features.

Conclusion: Speech language models effectively capture grammatical features, but conceptual feature encoding remains less robust.

Abstract: Transformer-based speech language models (SLMs) have significantly improved
neural speech recognition and understanding. While existing research has
examined how well SLMs encode shallow acoustic and phonetic features, the
extent to which SLMs encode nuanced syntactic and conceptual features remains
unclear. By drawing parallels with linguistic competence assessments for large
language models, this study is the first to systematically evaluate the
presence of contextual syntactic and semantic features across SLMs for
self-supervised learning (S3M), automatic speech recognition (ASR), speech
compression (codec), and as the encoder for auditory large language models
(AudioLLMs). Through minimal pair designs and diagnostic feature analysis
across 71 tasks spanning diverse linguistic levels, our layer-wise and
time-resolved analysis uncovers that 1) all speech encode grammatical features
more robustly than conceptual ones.

</details>


### [52] [VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion](https://arxiv.org/abs/2509.15667)
*Dimitrios Damianos,Leon Voukoutis,Georgios Paraskevopoulos,Vassilis Katsouros*

Main category: cs.CL

TL;DR: The paper introduces a speech-enabled LLM framework that combines Whisper and a decoder-based LLM using intermediate audio-conditioned text spaces and achieves state-of-the-art Greek ASR performance.


<details>
  <summary>Details</summary>
Motivation: To create a speech-enabled large language model by effectively bridging acoustic encoder-decoder architectures with language models and addressing challenges in multilingual and low-resource languages.

Method: The framework fuses Whisper's hidden decoder states with LLM's states using cross-modal attention, working in continuous text representation spaces and supporting both offline and streaming modes.

Result: The proposed method achieves state-of-the-art results in Greek Automatic Speech Recognition, with approximately 20% relative improvement across benchmarks.

Conclusion: Continuous space fusion is highlighted as an effective approach for developing multilingual and low-resource speech-enabled LLMs, demonstrated via the creation of VoxKrikri, the first Greek speech LLM.

Abstract: We present a multimodal fusion framework that bridges pre-trained
decoder-based large language models (LLM) and acoustic encoder-decoder
architectures such as Whisper, with the aim of building speech-enabled LLMs.
Instead of directly using audio embeddings, we explore an intermediate
audio-conditioned text space as a more effective mechanism for alignment. Our
method operates fully in continuous text representation spaces, fusing
Whisper's hidden decoder states with those of an LLM through cross-modal
attention, and supports both offline and streaming modes. We introduce
\textit{VoxKrikri}, the first Greek speech LLM, and show through analysis that
our approach effectively aligns representations across modalities. These
results highlight continuous space fusion as a promising path for multilingual
and low-resource speech LLMs, while achieving state-of-the-art results for
Automatic Speech Recognition in Greek, providing an average $\sim20\%$ relative
improvement across benchmarks.

</details>


### [53] [Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment](https://arxiv.org/abs/2509.15701)
*Ke Wang,Wenning Wei,Yan Deng,Lei He,Sheng Zhao*

Main category: cs.CL

TL;DR: The paper explores how Large Multimodal Models (LMMs) can be fine-tuned for effective Automatic Pronunciation Assessment, achieving competitive results but facing challenges at the phoneme level.


<details>
  <summary>Details</summary>
Motivation: Enhancing Automatic Pronunciation Assessment, a key component of Computer-Assisted Language Learning, by leveraging the potential of Large Multimodal Models.

Method: Fine-tuning LMMs using the Speechocean762 dataset and a private corpus to evaluate performance across granularities in Automatic Pronunciation Assessment.

Result: Fine-tuning improves performance over zero-shot settings and provides competitive results, especially at the sentence and word levels, with challenges persisting at the phoneme level.

Conclusion: Large Multimodal Models show promise for Automatic Pronunciation Assessment, especially with fine-tuning, but advancements in fine-grained modeling and rank-aware evaluation are necessary.

Abstract: Automatic Pronunciation Assessment (APA) is critical for Computer-Assisted
Language Learning (CALL), requiring evaluation across multiple granularities
and aspects. Large Multimodal Models (LMMs) present new opportunities for APA,
but their effectiveness in fine-grained assessment remains uncertain. This work
investigates fine-tuning LMMs for APA using the Speechocean762 dataset and a
private corpus. Fine-tuning significantly outperforms zero-shot settings and
achieves competitive results on single-granularity tasks compared to public and
commercial systems. The model performs well at word and sentence levels, while
phoneme-level assessment remains challenging. We also observe that the Pearson
Correlation Coefficient (PCC) reaches 0.9, whereas Spearman's rank Correlation
Coefficient (SCC) remains around 0.6, suggesting that SCC better reflects
ordinal consistency. These findings highlight both the promise and limitations
of LMMs for APA and point to future work on fine-grained modeling and
rank-aware evaluation.

</details>


### [54] [Once Upon a Time: Interactive Learning for Storytelling with Small Language Models](https://arxiv.org/abs/2509.15714)
*Jonas Mayer Martins,Ali Hamza Bashir,Muhammad Rehan Khalid,Lisa Beinborn*

Main category: cs.CL

TL;DR: This study explores training language models with high-level feedback rather than massive next-word prediction data, improving efficiency significantly.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the observation that children learn language efficiently through social interaction, in contrast to language models trained on huge datasets for next-word prediction.

Method: The study trains a language model (student) to generate stories, with feedback on readability, narrative coherence, and creativity from a teacher model. It evaluates the impact of this feedback versus conventional pretraining data.

Result: Interactive learning with cognitively inspired feedback allowed the student model to achieve storytelling improvements comparable to using 410M words of next-word prediction training, but with just 1M words.

Conclusion: High-level, cognitively inspired feedback can dramatically enhance data efficiency in language model training, potentially reducing the need for massive datasets.

Abstract: Children efficiently acquire language not just by listening, but by
interacting with others in their social environment. Conversely, large language
models are typically trained with next-word prediction on massive amounts of
text. Motivated by this contrast, we investigate whether language models can be
trained with less data by learning not only from next-word prediction but also
from high-level, cognitively inspired feedback. We train a student model to
generate stories, which a teacher model rates on readability, narrative
coherence, and creativity. By varying the amount of pretraining before the
feedback loop, we assess the impact of this interactive learning on formal and
functional linguistic competence. We find that the high-level feedback is
highly data efficient: With just 1 M words of input in interactive learning,
storytelling skills can improve as much as with 410 M words of next-word
prediction.

</details>


### [55] [REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting](https://arxiv.org/abs/2509.15723)
*Nannan Huang,Haytham M. Fayek,Xiuzhen Zhang*

Main category: cs.CL

TL;DR: The study explores whether frequency-framed prompting (REFER) can improve fairness in using language models for opinion summarization.


<details>
  <summary>Details</summary>
Motivation: Fair summarization of diverse opinions is challenging because current methods relying on hyperparameter tuning or ground truth data are impractical for end-users.

Method: The study investigates frequency-framed prompting (REFER), a technique inspired by cognitive science, to reduce bias and enhance fairness in how language models summarize opinions.

Result: REFER significantly enhances fairness in opinion summarization, especially in larger models and with stronger reasoning instructions.

Conclusion: Frequency-based prompting frameworks can effectively address fairness challenges in language model opinion summarization without requiring hyperparameter tuning or detailed ground truth information.

Abstract: Individuals express diverse opinions, a fair summary should represent these
viewpoints comprehensively. Previous research on fairness in opinion
summarisation using large language models (LLMs) relied on hyperparameter
tuning or providing ground truth distributional information in prompts.
However, these methods face practical limitations: end-users rarely modify
default model parameters, and accurate distributional information is often
unavailable. Building upon cognitive science research demonstrating that
frequency-based representations reduce systematic biases in human statistical
reasoning by making reference classes explicit and reducing cognitive load,
this study investigates whether frequency framed prompting (REFER) can
similarly enhance fairness in LLM opinion summarisation. Through systematic
experimentation with different prompting frameworks, we adapted techniques
known to improve human reasoning to elicit more effective information
processing in language models compared to abstract probabilistic
representations.Our results demonstrate that REFER enhances fairness in
language models when summarising opinions. This effect is particularly
pronounced in larger language models and using stronger reasoning instructions.

</details>


### [56] [Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics](https://arxiv.org/abs/2509.15739)
*Reza Sanayei,Srdjan Vesic,Eduardo Blanco,Mihai Surdeanu*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) are tested for their ability to approximate structured reasoning from Computational Argumentation Theory (CAT) using Quantitative Argumentation Debate (QuAD) semantics. Results indicate moderate alignment but highlight challenges in handling longer or disrupted inputs.


<details>
  <summary>Details</summary>
Motivation: To explore whether LLMs can handle structured reasoning in non-linear argument graphs, as commonly found in natural debates.

Method: The paper uses dialogue-formatted debates from two datasets to prompt LLMs to rank arguments without direct access to the argument graph. Advanced strategies like Chain-of-Thought and In-Context Learning are employed.

Result: LLMs show moderate alignment with QuAD semantics rankings but exhibit performance degradation with longer debates or disrupted discourse flow.

Conclusion: While LLMs show promise in modeling formal argumentation semantics, limitations such as biases and sensitivity to input length suggest the need for future research on graph-aware reasoning approaches.

Abstract: Large Language Models (LLMs) excel at linear reasoning tasks but remain
underexplored on non-linear structures such as those found in natural debates,
which are best expressed as argument graphs. We evaluate whether LLMs can
approximate structured reasoning from Computational Argumentation Theory (CAT).
Specifically, we use Quantitative Argumentation Debate (QuAD) semantics, which
assigns acceptability scores to arguments based on their attack and support
relations. Given only dialogue-formatted debates from two NoDE datasets, models
are prompted to rank arguments without access to the underlying graph. We test
several LLMs under advanced instruction strategies, including Chain-of-Thought
and In-Context Learning. While models show moderate alignment with QuAD
rankings, performance degrades with longer inputs or disrupted discourse flow.
Advanced prompting helps mitigate these effects by reducing biases related to
argument length and position. Our findings highlight both the promise and
limitations of LLMs in modeling formal argumentation semantics and motivate
future work on graph-aware reasoning.

</details>


### [57] [UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression](https://arxiv.org/abs/2509.15763)
*Chenlong Deng,Zhisong Zhang,Kelong Mao,Shuaiyi Li,Tianqing Fang,Hongming Zhang,Haitao Mi,Dong Yu,Zhicheng Dou*

Main category: cs.CL

TL;DR: The paper introduces UniGist, a framework for compressing long-context data in large language models without significant loss of contextual information.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency caused by memory overhead of key-value (KV) caching in long-context processing for large language models.

Method: Create a sequence-level compression framework that replaces tokens with compression tokens ('gists') combined with chunk-free training and an optimized GPU-compatible shift trick.

Result: UniGist improves the quality of long-context compression and performs particularly well in tasks requiring detail recall and long-range dependency modeling.

Conclusion: UniGist offers an effective approach to manage memory overhead in long-context tasks, optimizing GPU usage and maintaining context integrity during inference.

Abstract: Large language models are increasingly capable of handling long-context
inputs, but the memory overhead of key-value (KV) cache remains a major
bottleneck for general-purpose deployment. While various compression strategies
have been explored, sequence-level compression, which drops the full KV caches
for certain tokens, is particularly challenging as it can lead to the loss of
important contextual information. To address this, we introduce UniGist, a
sequence-level long-context compression framework that efficiently preserves
context information by replacing raw tokens with special compression tokens
(gists) in a fine-grained manner. We adopt a chunk-free training strategy and
design an efficient kernel with a gist shift trick, enabling optimized GPU
training. Our scheme also supports flexible inference by allowing the actual
removal of compressed tokens, resulting in real-time memory savings.
Experiments across multiple long-context tasks demonstrate that UniGist
significantly improves compression quality, with especially strong performance
in detail-recalling tasks and long-range dependency modeling.

</details>


### [58] [UPRPRC: Unified Pipeline for Reproducing Parallel Resources -- Corpus from the United Nations](https://arxiv.org/abs/2509.15789)
*Qiuyang Lu,Fangjian Shen,Zhengkai Tang,Qiang Liu,Hexuan Cheng,Hui Liu,Wushao Wen*

Main category: cs.CL

TL;DR: This paper introduces a reproducible method for creating multilingual datasets, featuring a new Graph-Aided Paragraph Alignment (GAPA) algorithm, and results in the largest publicly available corpus of human-translated content.


<details>
  <summary>Details</summary>
Motivation: Previous United Nations document-based corpora faced challenges like lack of transparency, limited scale, and reproducibility.

Method: The authors developed an end-to-end solution including web scraping, paragraph alignment using the Graph-Aided Paragraph Alignment (GAPA) algorithm, and scalable options for dataset creation.

Result: The resulting corpus contains over 713 million English tokens, doubling the scale of prior datasets, and is publicly available with both code and corpus under the MIT License.

Conclusion: This work advances the field by offering a scalable, reproducible process that delivers a high-quality and extensive corpus of human-translated content for machine translation research.

Abstract: The quality and accessibility of multilingual datasets are crucial for
advancing machine translation. However, previous corpora built from United
Nations documents have suffered from issues such as opaque process, difficulty
of reproduction, and limited scale. To address these challenges, we introduce a
complete end-to-end solution, from data acquisition via web scraping to text
alignment. The entire process is fully reproducible, with a minimalist
single-machine example and optional distributed computing steps for
scalability. At its core, we propose a new Graph-Aided Paragraph Alignment
(GAPA) algorithm for efficient and flexible paragraph-level alignment. The
resulting corpus contains over 713 million English tokens, more than doubling
the scale of prior work. To the best of our knowledge, this represents the
largest publicly available parallel corpus composed entirely of
human-translated, non-AI-generated content. Our code and corpus are accessible
under the MIT License.

</details>


### [59] [RAVE: Retrieval and Scoring Aware Verifiable Claim Detection](https://arxiv.org/abs/2509.15793)
*Yufeng Li,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: RAVE is a scalable framework designed for claim detection on social media, outperforming existing methods in accuracy and F1.


<details>
  <summary>Details</summary>
Motivation: The rapid spread of misinformation on social media necessitates scalable tools for identifying statements that can be fact-checked.

Method: The framework, RAVE, integrates evidence retrieval with structured signals emphasizing relevance and source credibility.

Result: Tests on CT22-test and PoliClaim-test datasets reveal that RAVE surpasses text-only and retrieval-based baselines in accuracy and F1.

Conclusion: RAVE proves effective as a claim detection framework and presents a reliable approach to counter misinformation on social platforms.

Abstract: The rapid spread of misinformation on social media underscores the need for
scalable fact-checking tools. A key step is claim detection, which identifies
statements that can be objectively verified. Prior approaches often rely on
linguistic cues or claim check-worthiness, but these struggle with vague
political discourse and diverse formats such as tweets. We present RAVE
(Retrieval and Scoring Aware Verifiable Claim Detection), a framework that
combines evidence retrieval with structured signals of relevance and source
credibility. Experiments on CT22-test and PoliClaim-test show that RAVE
consistently outperforms text-only and retrieval-based baselines in both
accuracy and F1.

</details>


### [60] [Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning](https://arxiv.org/abs/2509.15811)
*Sara Rajaee,Rochelle Choenni,Ekaterina Shutova,Christof Monz*

Main category: cs.CL

TL;DR: This paper presents an approach leveraging cross-lingual reasoning paths in multilingual large language models (LLMs) to improve mathematical reasoning performance, even for high-resource languages like English.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand and enhance reasoning abilities of multilingual LLMs across different languages, exploring whether reasoning paths from various languages complement each other.

Method: The authors train a cross-lingual reward model to rank responses generated across multiple languages for a given question, analyzing its impact on reasoning performance.

Result: The cross-lingual reward model significantly outperforms single-language models in mathematical reasoning tasks, improving reasoning efficiency for high-resource languages like English, especially under low sampling budgets.

Conclusion: Cross-lingual sampling leverages the strengths of diverse languages, presenting new pathways for enhancing multilingual reasoning in LLMs.

Abstract: While the reasoning abilities of large language models (LLMs) continue to
advance, it remains unclear how such ability varies across languages in
multilingual LLMs and whether different languages produce reasoning paths that
complement each other. To investigate this question, we train a reward model to
rank generated responses for a given question across languages. Our results
show that our cross-lingual reward model substantially improves mathematical
reasoning performance compared to using reward modeling within a single
language, benefiting even high-resource languages. While English often exhibits
the highest performance in multilingual models, we find that cross-lingual
sampling particularly benefits English under low sampling budgets. Our findings
reveal new opportunities to improve multilingual reasoning by leveraging the
complementary strengths of diverse languages.

</details>


### [61] [The Curious Case of Visual Grounding: Different Effects for Speech- and Text-based Language Encoders](https://arxiv.org/abs/2509.15837)
*Adrian Sauter,Willem Zuidema,Marianne de Heer Kloots*

Main category: cs.CL

TL;DR: The paper investigates how visual grounding influences word encoding in audio- and text-based deep learning language models. It reveals that visual grounding enhances word identity representation alignment between modalities but has distinct effects on semantic and phonetic discriminability for text and speech.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to understand the impact of visual information integrated during training on the language processing and internal representations within audio- and text-based deep learning models.

Method: The researchers conducted global representational comparisons and targeted clustering analyses to examine how visual grounding affects phonetic and semantic discriminability in language representations.

Result: Visual grounding increased alignment between spoken and written language representations primarily by enhancing word identity. In speech-based models, it maintained a dominance of phonetic features and did not enhance semantic discriminability, unlike in text-based models.

Conclusion: The findings suggest that visual grounding differently affects speech- and text-based language models and offer insights for designing models that better integrate visually-informed semantics, particularly in speech-based systems.

Abstract: How does visual information included in training affect language processing
in audio- and text-based deep learning models? We explore how such visual
grounding affects model-internal representations of words, and find
substantially different effects in speech- vs. text-based language encoders.
Firstly, global representational comparisons reveal that visual grounding
increases alignment between representations of spoken and written language, but
this effect seems mainly driven by enhanced encoding of word identity rather
than meaning. We then apply targeted clustering analyses to probe for phonetic
vs. semantic discriminability in model representations. Speech-based
representations remain phonetically dominated with visual grounding, but in
contrast to text-based representations, visual grounding does not improve
semantic discriminability. Our findings could usefully inform the development
of more efficient methods to enrich speech-based models with visually-informed
semantics.

</details>


### [62] [Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems](https://arxiv.org/abs/2509.15839)
*Zhongze Luo,Zhenshuai Yin,Yongxin Guo,Zhichao Wang,Jionghao Zhu,Xiaoying Tang*

Main category: cs.CL

TL;DR: The paper introduces Multi-Physics, a comprehensive benchmark for evaluating multimodal large language models (MLLMs) in Chinese physics reasoning, addressing gaps in existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for evaluating MLLMs lack fine-grained coverage in scientific domains like physics, fail to emphasize step-by-step reasoning, and are overly focused on English, missing the role of visual information.

Method: The authors propose a benchmark with 1,412 multiple-choice questions across 11 high-school physics subjects, varying in difficulty levels. They employ a dual evaluation framework analyzing both answer accuracy and chain-of-thought reasoning and study the impact of difficulty and visual inputs.

Result: Evaluation of 20 different MLLMs is performed, showing insights into performance variations based on difficulty level and input mode, revealing the challenges in multimodal physics reasoning.

Conclusion: The benchmark and methodology provide a structured resource for assessing multimodal reasoning capabilities of MLLMs, enhancing understanding and promising progress in their scientific reasoning applications.

Abstract: While multimodal LLMs (MLLMs) demonstrate remarkable reasoning progress,
their application in specialized scientific domains like physics reveals
significant gaps in current evaluation benchmarks. Specifically, existing
benchmarks often lack fine-grained subject coverage, neglect the step-by-step
reasoning process, and are predominantly English-centric, failing to
systematically evaluate the role of visual information. Therefore, we introduce
\textbf {Multi-Physics} for Chinese physics reasoning, a comprehensive
benchmark that includes 5 difficulty levels, featuring 1,412 image-associated,
multiple-choice questions spanning 11 high-school physics subjects. We employ a
dual evaluation framework to evaluate 20 different MLLMs, analyzing both final
answer accuracy and the step-by-step integrity of their chain-of-thought.
Furthermore, we systematically study the impact of difficulty level and visual
information by comparing the model performance before and after changing the
input mode. Our work provides not only a fine-grained resource for the
community but also offers a robust methodology for dissecting the multimodal
reasoning process of state-of-the-art MLLMs, and our dataset and code have been
open-sourced: https://github.com/luozhongze/Multi-Physics.

</details>


### [63] [Distribution-Aligned Decoding for Efficient LLM Task Adaptation](https://arxiv.org/abs/2509.15888)
*Senkang Hu,Xudong Han,Jinqi Jiang,Yihang Tao,Zihan Fang,Sam Tak Wu Kwong,Yuguang Fang*

Main category: cs.CL

TL;DR: The paper proposes Steering Vector Decoding (SVD), a method for adapting large language models to tasks by steering the output distribution directly during decoding.


<details>
  <summary>Details</summary>
Motivation: Adapting billion-parameter language models to downstream tasks is costly even with parameter-efficient fine-tuning (PEFT), creating the need for lightweight and efficient solutions.

Method: The authors propose SVD, which extracts a task-aware steering vector from the KL divergence gradient between output distributions of pre-trained and warm-started models. This vector is used during decoding to align the output distribution with the task distribution.

Result: Using SVD with PEFT improves multiple-choice accuracy by up to 5 points and truthfulness in open-ended tasks by 2 points across various tasks and benchmarks, all without adding trainable parameters beyond PEFT adapters.

Conclusion: SVD is a lightweight, theoretically grounded method that enhances task adaptation efficiency for large language models, making it compatible with various PEFT methods.

Abstract: Adapting billion-parameter language models to a downstream task is still
costly, even with parameter-efficient fine-tuning (PEFT). We re-cast task
adaptation as output-distribution alignment: the objective is to steer the
output distribution toward the task distribution directly during decoding
rather than indirectly through weight updates. Building on this view, we
introduce Steering Vector Decoding (SVD), a lightweight, PEFT-compatible, and
theoretically grounded method. We start with a short warm-start fine-tune and
extract a task-aware steering vector from the Kullback-Leibler (KL) divergence
gradient between the output distribution of the warm-started and pre-trained
models. This steering vector is then used to guide the decoding process to
steer the model's output distribution towards the task distribution. We
theoretically prove that SVD is first-order equivalent to the gradient step of
full fine-tuning and derive a globally optimal solution for the strength of the
steering vector. Across three tasks and nine benchmarks, SVD paired with four
standard PEFT methods improves multiple-choice accuracy by up to 5 points and
open-ended truthfulness by 2 points, with similar gains (1-2 points) on
commonsense datasets without adding trainable parameters beyond the PEFT
adapter. SVD thus offers a lightweight, theoretically grounded path to stronger
task adaptation for large language models.

</details>


### [64] [The Psychology of Falsehood: A Human-Centric Survey of Misinformation Detection](https://arxiv.org/abs/2509.15896)
*Arghodeep Nandi,Megha Sundriyal,Euna Mehnaz Khan,Jikai Sun,Emily Vraga,Jaideep Srivastava,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: This paper surveys misinformation detection systems, emphasizing the need to incorporate psychological aspects like cognitive biases and emotional responses into automated fact-checking.


<details>
  <summary>Details</summary>
Motivation: Current misinformation detection systems focus narrowly on factual accuracy, neglecting the psychological and emotional impacts of misinformation.

Method: The paper analyzes existing misinformation detection technologies through the lens of cognitive psychology, identifying their limitations and suggesting opportunities for improvement.

Result: The analysis reveals key shortcomings in current systems and presents opportunities for integrating psychological insights to enhance detections.

Conclusion: Incorporating human-centered approaches, such as neuro-behavioral models, can make misinformation detection more effective and adaptive, addressing societal harms more comprehensively.

Abstract: Misinformation remains one of the most significant issues in the digital age.
While automated fact-checking has emerged as a viable solution, most current
systems are limited to evaluating factual accuracy. However, the detrimental
effect of misinformation transcends simple falsehoods; it takes advantage of
how individuals perceive, interpret, and emotionally react to information. This
underscores the need to move beyond factuality and adopt more human-centered
detection frameworks. In this survey, we explore the evolving interplay between
traditional fact-checking approaches and psychological concepts such as
cognitive biases, social dynamics, and emotional responses. By analyzing
state-of-the-art misinformation detection systems through the lens of human
psychology and behavior, we reveal critical limitations of current methods and
identify opportunities for improvement. Additionally, we outline future
research directions aimed at creating more robust and adaptive frameworks, such
as neuro-behavioural models that integrate technological factors with the
complexities of human cognition and social influence. These approaches offer
promising pathways to more effectively detect and mitigate the societal harms
of misinformation.

</details>


### [65] [Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions](https://arxiv.org/abs/2509.15901)
*Frederic Kirstein,Sonu Kumar,Terry Ruas,Bela Gipp*

Main category: cs.CL

TL;DR: FRAME reframes meeting summarization by reducing errors through semantic enrichment and personalized reasoning. Enhanced evaluation is achieved using P-MESA. Results show improved control, faithfulness, and user alignment.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address challenges in meeting summarization with LLMs, such as errors like hallucinations, omissions, and irrelevancies that compromise output quality.

Method: The paper introduces FRAME, a modular pipeline to organize and enrich summaries, and SCOPE, a reasoning protocol for personalization. P-MESA is proposed as an evaluation framework to assess summary quality without relying on references.

Result: FRAME demonstrates a reduction in hallucinations and omissions, while SCOPE improves knowledge alignment and overall goal compatibility, as tested on QMSum and FAME datasets.

Conclusion: The findings suggest that summarization tasks should prioritize control, faithfulness, and personalization for better user-aligned summaries, advocating a reframed approach to the process.

Abstract: Meeting summarization with large language models (LLMs) remains error-prone,
often producing outputs with hallucinations, omissions, and irrelevancies. We
present FRAME, a modular pipeline that reframes summarization as a semantic
enrichment task. FRAME extracts and scores salient facts, organizes them
thematically, and uses these to enrich an outline into an abstractive summary.
To personalize summaries, we introduce SCOPE, a reason-out-loud protocol that
has the model build a reasoning trace by answering nine questions before
content selection. For evaluation, we propose P-MESA, a multi-dimensional,
reference-free evaluation framework to assess if a summary fits a target
reader. P-MESA reliably identifies error instances, achieving >= 89% balanced
accuracy against human annotations and strongly aligns with human severity
ratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and
omission by 2 out of 5 points (measured with MESA), while SCOPE improves
knowledge fit and goal alignment over prompt-only baselines. Our findings
advocate for rethinking summarization to improve control, faithfulness, and
personalization.

</details>


### [66] [Beyond the Score: Uncertainty-Calibrated LLMs for Automated Essay Assessment](https://arxiv.org/abs/2509.15926)
*Ahmed Karim,Qiao Wang,Zheng Yuan*

Main category: cs.CL

TL;DR: The paper proposes using conformal prediction with open-source language models for more reliable Automated Essay Scoring (AES), addressing the gap in confidence and explanation accompanying scores.


<details>
  <summary>Details</summary>
Motivation: Current AES systems lack confidence measures and explanations in their scoring, hindering real-world adoption, especially in high-stakes exams.

Method: Two large language models (Llama-3 8B and Qwen-2.5 3B) are fine-tuned and calibrated at a 90% risk level using conformal prediction, with reliability assessed via uncertainty-aware accuracy (UAcc).

Result: The calibrated models achieved formal coverage guarantees while maintaining compact prediction sets, demonstrating the utility of mid-sized open-source LLMs in AES tasks.

Conclusion: Open-source mid-sized LLMs, augmented with conformal prediction and UAcc, can support reliable, teacher-assisted AES systems; future work will explore scaling and user studies.

Abstract: Automated Essay Scoring (AES) systems now reach near human agreement on some
public benchmarks, yet real-world adoption, especially in high-stakes
examinations, remains limited. A principal obstacle is that most models output
a single score without any accompanying measure of confidence or explanation.
We address this gap with conformal prediction, a distribution-free wrapper that
equips any classifier with set-valued outputs and formal coverage guarantees.
Two open-source large language models (Llama-3 8B and Qwen-2.5 3B) are
fine-tuned on three diverse corpora (ASAP, TOEFL11, Cambridge-FCE) and
calibrated at a 90 percent risk level. Reliability is assessed with UAcc, an
uncertainty-aware accuracy that rewards models for being both correct and
concise. To our knowledge, this is the first work to combine conformal
prediction and UAcc for essay scoring. The calibrated models consistently meet
the coverage target while keeping prediction sets compact, indicating that
open-source, mid-sized LLMs can already support teacher-in-the-loop AES; we
discuss scaling and broader user studies as future work.

</details>


### [67] [Localmax dynamics for attention in transformers and its asymptotic behavior](https://arxiv.org/abs/2509.15958)
*Henri Cimetière,Maria Teresa Chiri,Bahman Gharesifard*

Main category: cs.CL

TL;DR: The paper introduces "localmax dynamics," a discrete attention model that bridges softmax and hardmax dynamics, incorporating controlled deviations for enhanced flexibility. It analyzes convergence properties, invariant behavior, and highlights potential future research.


<details>
  <summary>Details</summary>
Motivation: To create a more versatile attention model that transitions between softmax and hardmax dynamics, enabling better control and understanding of neighborhood interactions.

Method: Developing the "localmax dynamics" model with parameters for neighbor influence and aligning behavior. Mathematical proofs and Lyapunov-based methods were used to study the system's convergence.

Result: The model does not exhibit finite-time convergence but reveals invariant behaviors and transitions as parameters vary. The limiting behavior aligns closely with hardmax dynamics.

Conclusion: Localmax dynamics offers a nuanced approach to understanding attention flows, showing utility despite limitations in finite-time convergence. Future research directions are outlined.

Abstract: We introduce a new discrete-time attention model, termed the localmax
dynamics, which interpolates between the classic softmax dynamics and the
hardmax dynamics, where only the tokens that maximize the influence toward a
given token have a positive weight. As in hardmax, uniform weights are
determined by a parameter controlling neighbor influence, but the key extension
lies in relaxing neighborhood interactions through an alignment-sensitivity
parameter, which allows controlled deviations from pure hardmax behavior. As we
prove, while the convex hull of the token states still converges to a convex
polytope, its structure can no longer be fully described by a maximal alignment
set, prompting the introduction of quiescent sets to capture the invariant
behavior of tokens near vertices. We show that these sets play a key role in
understanding the asymptotic behavior of the system, even under time-varying
alignment sensitivity parameters. We further show that localmax dynamics does
not exhibit finite-time convergence and provide results for vanishing, nonzero,
time-varying alignment-sensitivity parameters, recovering the limiting behavior
of hardmax as a by-product. Finally, we adapt Lyapunov-based methods from
classical opinion dynamics, highlighting their limitations in the asymmetric
setting of localmax interactions and outlining directions for future research.

</details>


### [68] [BEFT: Bias-Efficient Fine-Tuning of Language Models](https://arxiv.org/abs/2509.15974)
*Baichuan Huang,Ananth Balashankar,Amir Aminifar*

Main category: cs.CL

TL;DR: The paper introduces a bias-efficient fine-tuning (BEFT) approach for selectively fine-tuning bias terms in large language models, improving parameter efficiency and performance across diverse downstream tasks.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the unclear relationship between fine-tuning specific bias terms and downstream performance in parameter-efficient fine-tuning methods. Existing bias-selection methods provide limited practical guidance.

Method: The authors propose BEFT, a framework for systematically selecting specific bias terms (query, key, value projections) to fine-tune. This method is extensively tested across LLM architectures and multiple downstream tasks.

Result: They find that BEFT exhibits superior performance compared to existing bias-selection strategies, achieving competitive results across tasks such as classification, multiple-choice, and text generation on various LLM sizes.

Conclusion: BEFT establishes a systematic and effective approach to bias term fine-tuning, paving the way for improved parameter-efficient fine-tuning techniques in large-scale language models.

Abstract: Fine-tuning all-bias-terms stands out among various parameter-efficient
fine-tuning (PEFT) techniques, owing to its out-of-the-box usability and
competitive performance, especially in low-data regimes. Bias-only fine-tuning
has the potential for unprecedented parameter efficiency. However, the link
between fine-tuning different bias terms (i.e., bias terms in the query, key,
or value projections) and downstream performance remains unclear. The existing
approaches, e.g., based on the magnitude of bias change or empirical Fisher
information, provide limited guidance for selecting the particular bias term
for effective fine-tuning. In this paper, we propose an approach for selecting
the bias term to be fine-tuned, forming the foundation of our bias-efficient
fine-tuning (BEFT). We extensively evaluate our bias-efficient approach against
other bias-selection approaches, across a wide range of large language models
(LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B
parameters. Our results demonstrate the effectiveness and superiority of our
bias-efficient approach on diverse downstream tasks, including classification,
multiple-choice, and generation tasks.

</details>


### [69] [Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning](https://arxiv.org/abs/2509.16025)
*Hong-Yun Lin,Jhen-Ke Lin,Chung-Chun Wang,Hao-Chien Lu,Berlin Chen*

Main category: cs.CL

TL;DR: The paper presents a novel approach for session-level spoken language assessment (SLA) using a multimodal foundation model combined with a Whisper ASR speech prior, outperforming prior state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing SLA methods, such as cascaded pipelines prone to errors and end-to-end models that miss discourse-level evidence, for better evaluation of L2 English oral proficiency.

Method: A foundation model approach that processes session-level audio in one pass, utilizing multi-target learning and a frozen Whisper ASR model-based speech prior for improved calibration and holistic assessment.

Result: The proposed method outperformed the previous state-of-the-art on the Speak & Improve benchmark and demonstrated robustness in cross-part generalization.

Conclusion: The novel approach offers a more reliable, robust, and compact solution for SLA tailored for Computer Assisted Language Learning applications.

Abstract: Spoken Language Assessment (SLA) estimates a learner's oral proficiency from
spontaneous speech. The growing population of L2 English speakers has
intensified the demand for reliable SLA, a critical component of Computer
Assisted Language Learning (CALL). Existing efforts often rely on cascaded
pipelines, which are prone to error propagation, or end-to-end models that
often operate on a short audio window, which might miss discourse-level
evidence. This paper introduces a novel multimodal foundation model approach
that performs session-level evaluation in a single pass. Our approach couples
multi-target learning with a frozen, Whisper ASR model-based speech prior for
acoustic-aware calibration, allowing for jointly learning holistic and
trait-level objectives of SLA without resorting to handcrafted features. By
coherently processing the entire response session of an L2 speaker, the model
excels at predicting holistic oral proficiency. Experiments conducted on the
Speak & Improve benchmark demonstrate that our proposed approach outperforms
the previous state-of-the-art cascaded system and exhibits robust cross-part
generalization, producing a compact deployable grader that is tailored for CALL
applications.

</details>


### [70] [Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech](https://arxiv.org/abs/2509.16028)
*Sang Hoon Woo,Sehun Lee,Kang-wook Kim,Gunhee Kim*

Main category: cs.CL

TL;DR: The paper introduces a new framework to improve spoken dialogue systems by decoupling reasoning and speech delivery, using an intermediate process called verbalizing.


<details>
  <summary>Details</summary>
Motivation: Spoken dialogue systems that use large language models suffer from mismatches in verbal delivery and reasoning output, affecting communication outcomes.

Method: The authors propose the Think-Verbalize-Speak (TVS) framework, which incorporates a verbalizing step for translating thoughts into speech-ready text. They also develop ReVerT, an efficient verbalizer leveraging incremental summarization.

Result: Experiments show that the method improves the naturalness and conciseness of speech without significantly compromising reasoning ability.

Conclusion: The TVS framework and ReVerT effectively address challenges in spoken dialogue systems, enhancing speech delivery while maintaining reasoning performance.

Abstract: Spoken dialogue systems increasingly employ large language models (LLMs) to
leverage their advanced reasoning capabilities. However, direct application of
LLMs in spoken communication often yield suboptimal results due to mismatches
between optimal textual and verbal delivery. While existing approaches adapt
LLMs to produce speech-friendly outputs, their impact on reasoning performance
remains underexplored. In this work, we propose Think-Verbalize-Speak, a
framework that decouples reasoning from spoken delivery to preserve the full
reasoning capacity of LLMs. Central to our method is verbalizing, an
intermediate step that translates thoughts into natural, speech-ready text. We
also introduce ReVerT, a latency-efficient verbalizer based on incremental and
asynchronous summarization. Experiments across multiple benchmarks show that
our method enhances speech naturalness and conciseness with minimal impact on
reasoning. The project page with the dataset and the source code is available
at https://yhytoto12.github.io/TVS-ReVerT

</details>


### [71] [Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM Responses](https://arxiv.org/abs/2509.16093)
*Fangyi Yu,Nabeel Seedat,Dasha Herrmannova,Frank Schilder,Jonathan Richard Schwarz*

Main category: cs.CL

TL;DR: DeCE is a novel framework for evaluating long-form answers in high-stakes domains using separate precision and recall criteria extracted automatically from gold answers, achieving stronger alignment with expert judgments compared to existing metrics.


<details>
  <summary>Details</summary>
Motivation: Standard metrics like BLEU and ROUGE fail to capture semantic correctness in nuanced, domain-specific tasks such as legal or medical QA evaluation, necessitating a more interpretable and actionable framework.

Method: The authors propose DeCE, a decomposed evaluation framework that splits precision and recall using criteria extracted from gold answers. It is model-agnostic and domain-general and applied in a legal QA task to evaluate LLMs.

Result: DeCE showed high correlation ($r=0.78$) with expert judgments, surpassing traditional metrics ($r=0.12$), LLM scoring ($r=0.35$), and multidimensional evaluators ($r=0.48$). It revealed trade-offs between generalist and specialist models, highlighting interpretability and scalability.

Conclusion: DeCE enables scalable, interpretable, and expert-aligned evaluation for LLMs in complex domains without reliance on predefined taxonomies or handcrafted rubrics, advancing nuanced assessment approaches for high-stakes tasks.

Abstract: Evaluating long-form answers in high-stakes domains such as law or medicine
remains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to
capture semantic correctness, and current LLM-based evaluators often reduce
nuanced aspects of answer quality into a single undifferentiated score. We
introduce DeCE, a decomposed LLM evaluation framework that separates precision
(factual accuracy and relevance) and recall (coverage of required concepts),
using instance-specific criteria automatically extracted from gold answer
requirements. DeCE is model-agnostic and domain-general, requiring no
predefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate
different LLMs on a real-world legal QA task involving multi-jurisdictional
reasoning and citation grounding. DeCE achieves substantially stronger
correlation with expert judgments ($r=0.78$), compared to traditional metrics
($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional
evaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist
models favor recall, while specialized models favor precision. Importantly,
only 11.95% of LLM-generated criteria required expert revision, underscoring
DeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation
framework in expert domains.

</details>


### [72] [DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning](https://arxiv.org/abs/2509.16105)
*Sikai Bai,Haoxi Li,Jie Zhang,Zicong Hong,Song Guo*

Main category: cs.CL

TL;DR: The paper introduces DiEP, a non-uniform Mixture-of-Experts (MoE) pruning method that outperforms prior approaches by adaptively adjusting pruning rates across layers while preserving high performance.


<details>
  <summary>Details</summary>
Motivation: MoE models face memory and storage challenges as their scale grows, and uniform sparsity pruning across layers leads to performance degradation due to varying redundancy in different layers.

Method: The proposed method, DiEP, uses a gradient-based non-uniform pruning strategy by transforming the pruning process into a continuous space to adaptively select layer pruning rates and handle inter-layer importance.

Result: DiEP maintains around 92% of original performance on Mixtral 8×7B with half the experts and surpasses other methods by up to 7.1% on the MMLU dataset.

Conclusion: The paper confirms that non-uniform pruning effectively mitigates redundancy, significantly reducing model size while maintaining performance, especially on challenging NLP tasks.

Abstract: Despite the significant breakthrough of Mixture-of-Experts (MoE), the
increasing scale of these MoE models presents huge memory and storage
challenges. Existing MoE pruning methods, which involve reducing parameter size
with a uniform sparsity across all layers, often lead to suboptimal outcomes
and performance degradation due to varying expert redundancy in different MoE
layers. To address this, we propose a non-uniform pruning strategy, dubbed
\textbf{Di}fferentiable \textbf{E}xpert \textbf{P}runing (\textbf{DiEP}), which
adaptively adjusts pruning rates at the layer level while jointly learning
inter-layer importance, effectively capturing the varying redundancy across
different MoE layers. By transforming the global discrete search space into a
continuous one, our method handles exponentially growing non-uniform expert
combinations, enabling adaptive gradient-based pruning. Extensive experiments
on five advanced MoE models demonstrate the efficacy of our method across
various NLP tasks. Notably, \textbf{DiEP} retains around 92\% of original
performance on Mixtral 8$\times$7B with only half the experts, outperforming
other pruning methods by up to 7.1\% on the challenging MMLU dataset.

</details>


### [73] [It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge](https://arxiv.org/abs/2509.16107)
*Lukas Ellinger,Georg Groh*

Main category: cs.CL

TL;DR: The paper examines whether Large Language Models (LLMs) can resolve referential ambiguities in conversations using commonsense, finding that they struggle and often fail under simplification prompts. Fine-tuning techniques showed substantial improvement.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of referential ambiguity resolution in multi-turn conversations by evaluating if LLMs can leverage shared context and commonsense knowledge.

Method: Investigated multiple LLMs (DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, and Llama-3.1-8B) using a multilingual dataset with LLM-as-Judge and human annotations. Analyzed the effect of simplification requests and fine-tuning methods.

Result: Current LLMs poorly handle ambiguity, either committing to single interpretations or covering all possibilities. Simplification prompts further decrease performance. Fine-tuning Llama-3.1-8B significantly improved results.

Conclusion: LLMs require advanced fine-tuning to better resolve ambiguity in diverse communication styles and employ robust commonsense reasoning strategies.

Abstract: Ambiguous words or underspecified references require interlocutors to resolve
them, often by relying on shared context and commonsense knowledge. Therefore,
we systematically investigate whether Large Language Models (LLMs) can leverage
commonsense to resolve referential ambiguity in multi-turn conversations and
analyze their behavior when ambiguity persists. Further, we study how requests
for simplified language affect this capacity. Using a novel multilingual
evaluation dataset, we test DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, and
Llama-3.1-8B via LLM-as-Judge and human annotations. Our findings indicate that
current LLMs struggle to resolve ambiguity effectively: they tend to commit to
a single interpretation or cover all possible references, rather than hedging
or seeking clarification. This limitation becomes more pronounced under
simplification prompts, which drastically reduce the use of commonsense
reasoning and diverse response strategies. Fine-tuning Llama-3.1-8B with Direct
Preference Optimization substantially improves ambiguity resolution across all
request types. These results underscore the need for advanced fine-tuning to
improve LLMs' handling of ambiguity and to ensure robust performance across
diverse communication styles.

</details>


### [74] [CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs](https://arxiv.org/abs/2509.16188)
*Jinghao Zhang,Sihang Jiang,Shiwei Guo,Shisong Chen,Yanghua Xiao,Hongwei Feng,Jiaqing Liang,Minggui HE,Shimin Tao,Hongxia Ma*

Main category: cs.CL

TL;DR: CultureScope introduces a scalable framework using a dimensional schema based on cultural iceberg theory to evaluate cultural understanding in LLMs.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack scalability and comprehensiveness in assessing cultural understanding of LLMs across diverse contexts.

Method: A novel 3-layer, 140-dimension schema inspired by cultural iceberg theory, automatically constructs culture-specific knowledge bases and evaluation datasets.

Result: Experimental results show LLMs lack comprehensive cultural competence, emphasizing that multilingual data alone is insufficient for improving cultural understanding.

Conclusion: CultureScope systematically bridges gaps in cultural evaluation frameworks, providing accessible tools for enhancing LLM cultural alignment.

Abstract: As large language models (LLMs) are increasingly deployed in diverse cultural
environments, evaluating their cultural understanding capability has become
essential for ensuring trustworthy and culturally aligned applications.
However, most existing benchmarks lack comprehensiveness and are challenging to
scale and adapt across different cultural contexts, because their frameworks
often lack guidance from well-established cultural theories and tend to rely on
expert-driven manual annotations. To address these issues, we propose
CultureScope, the most comprehensive evaluation framework to date for assessing
cultural understanding in LLMs. Inspired by the cultural iceberg theory, we
design a novel dimensional schema for cultural knowledge classification,
comprising 3 layers and 140 dimensions, which guides the automated construction
of culture-specific knowledge bases and corresponding evaluation datasets for
any given languages and cultures. Experimental results demonstrate that our
method can effectively evaluate cultural understanding. They also reveal that
existing large language models lack comprehensive cultural competence, and
merely incorporating multilingual data does not necessarily enhance cultural
understanding. All code and data files are available at
https://github.com/HoganZinger/Culture

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [75] [Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays](https://arxiv.org/abs/2509.15234)
*Hanbin Ko,Gihun Cho,Inhyeok Baek,Donguk Kim,Joonbeom Koo,Changi Kim,Dongheon Lee,Chang Min Park*

Main category: cs.CV

TL;DR: The paper introduces domain-adapted models (LLM2VEC4CXR and LLM2CLIP4CXR) for robust image-text alignment in radiology, outperforming existing methods while addressing noisy, heterogeneous clinical data.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to improve image-text alignment in radiology, given the limited progress due to variability and noise in clinical reports. This challenges the assumption that scaling data alone leads to better performance.

Method: The authors developed LLM2VEC4CXR, a language model encoder adapted for chest X-ray reports, and LLM2CLIP4CXR, a framework coupling this encoder with a vision backbone. These models focus on robustness, handling abbreviations, report heterogeneity, and stylistic variation.

Result: The models improved clinical text understanding, retrieval accuracy, and cross-dataset generalization. They outperform BERT-based baselines and previous CLIP variants on clinical metrics, even when trained on noisy datasets.

Conclusion: Robust representations, rather than scale alone, are key for effective multimodal radiology learning. This research provides tools for further exploration in medical image-text representation learning.

Abstract: Vision-language pretraining has advanced image-text alignment, yet progress
in radiology remains constrained by the heterogeneity of clinical reports,
including abbreviations, impression-only notes, and stylistic variability.
Unlike general-domain settings where more data often leads to better
performance, naively scaling to large collections of noisy reports can plateau
or even degrade model learning. We ask whether large language model (LLM)
encoders can provide robust clinical representations that transfer across
diverse styles and better guide image-text alignment. We introduce LLM2VEC4CXR,
a domain-adapted LLM encoder for chest X-ray reports, and LLM2CLIP4CXR, a
dual-tower framework that couples this encoder with a vision backbone.
LLM2VEC4CXR improves clinical text understanding over BERT-based baselines,
handles abbreviations and style variation, and achieves strong clinical
alignment on report-level metrics. LLM2CLIP4CXR leverages these embeddings to
boost retrieval accuracy and clinically oriented scores, with stronger
cross-dataset generalization than prior medical CLIP variants. Trained on 1.6M
CXR studies from public and private sources with heterogeneous and noisy
reports, our models demonstrate that robustness -- not scale alone -- is the
key to effective multimodal learning. We release models to support further
research in medical image-text representation learning.

</details>


### [76] [ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding](https://arxiv.org/abs/2509.15235)
*Jialiang Kang,Han Shu,Wenshuo Li,Yingjie Zhai,Xinghao Chen*

Main category: cs.CV

TL;DR: This study introduces Vision-Aware Speculative Decoding (ViSpec) to speed up inference in vision-language models (VLMs), leveraging novel techniques to optimize the multimodal decoding process.


<details>
  <summary>Details</summary>
Motivation: Existing speculative decoding methods achieve only modest speedups (<1.5x) for vision-language models despite their computational demand and increasing significance.

Method: ViSpec uses a lightweight vision adaptor to compress image tokens, integrates these into the draft model’s attention while preserving positional information, augments text tokens with global image features, and employs a specialized multimodal training strategy.

Result: ViSpec delivers the first substantial speedup in speculative decoding for VLMs, as validated by extensive experiments.

Conclusion: ViSpec enhances the efficiency of VLM inference without sacrificing multimodal comprehension, addressing key shortcomings in current speculative decoding approaches.

Abstract: Speculative decoding is a widely adopted technique for accelerating inference
in large language models (LLMs), yet its application to vision-language models
(VLMs) remains underexplored, with existing methods achieving only modest
speedups (<1.5x). This gap is increasingly significant as multimodal
capabilities become central to large-scale models. We hypothesize that large
VLMs can effectively filter redundant image information layer by layer without
compromising textual comprehension, whereas smaller draft models struggle to do
so. To address this, we introduce Vision-Aware Speculative Decoding (ViSpec), a
novel framework tailored for VLMs. ViSpec employs a lightweight vision adaptor
module to compress image tokens into a compact representation, which is
seamlessly integrated into the draft model's attention mechanism while
preserving original image positional information. Additionally, we extract a
global feature vector for each input image and augment all subsequent text
tokens with this feature to enhance multimodal coherence. To overcome the
scarcity of multimodal datasets with long assistant responses, we curate a
specialized training dataset by repurposing existing datasets and generating
extended outputs using the target VLM with modified prompts. Our training
strategy mitigates the risk of the draft model exploiting direct access to the
target model's hidden states, which could otherwise lead to shortcut learning
when training solely on target model outputs. Extensive experiments validate
ViSpec, achieving, to our knowledge, the first substantial speedup in VLM
speculative decoding.

</details>


### [77] [M-PACE: Mother Child Framework for Multimodal Compliance](https://arxiv.org/abs/2509.15241)
*Shreyash Verma,Amit Kesari,Vinayak Trivedi,Anupam Purwar,Ratnesh Jamidar*

Main category: cs.CV

TL;DR: The paper introduces M-PACE, a unified framework for multimodal content compliance, addressing challenges in advertisement compliance by integrating vision-language models.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the inefficiencies in traditional disjointed compliance pipelines, which are costly, complex, and lack scalability for evolving guidelines.

Method: The authors propose M-PACE, a mother-child MLLM framework for assessing compliance attributes, supported by a human-annotated benchmark with challenging real-world scenarios.

Result: M-PACE demonstrates the ability to evaluate 15 compliance attributes, drastically reduces inference costs by over 31 times, and achieves cost-effective accuracy in advertisement compliance.

Conclusion: This unified approach significantly reduces costs and reliance on human reviewers, offering real-time, scalable, and accurate compliance solutions for multimodal content.

Abstract: Ensuring that multi-modal content adheres to brand, legal, or
platform-specific compliance standards is an increasingly complex challenge
across domains. Traditional compliance frameworks typically rely on disjointed,
multi-stage pipelines that integrate separate modules for image classification,
text extraction, audio transcription, hand-crafted checks, and rule-based
merges. This architectural fragmentation increases operational overhead,
hampers scalability, and hinders the ability to adapt to dynamic guidelines
efficiently. With the emergence of Multimodal Large Language Models (MLLMs),
there is growing potential to unify these workflows under a single,
general-purpose framework capable of jointly processing visual and textual
content. In light of this, we propose Multimodal Parameter Agnostic Compliance
Engine (M-PACE), a framework designed for assessing attributes across
vision-language inputs in a single pass. As a representative use case, we apply
M-PACE to advertisement compliance, demonstrating its ability to evaluate over
15 compliance-related attributes. To support structured evaluation, we
introduce a human-annotated benchmark enriched with augmented samples that
simulate challenging real-world conditions, including visual obstructions and
profanity injection. M-PACE employs a mother-child MLLM setup, demonstrating
that a stronger parent MLLM evaluating the outputs of smaller child models can
significantly reduce dependence on human reviewers, thereby automating quality
control. Our analysis reveals that inference costs reduce by over 31 times,
with the most efficient models (Gemini 2.0 Flash as child MLLM selected by
mother MLLM) operating at 0.0005 per image, compared to 0.0159 for Gemini 2.5
Pro with comparable accuracy, highlighting the trade-off between cost and
output quality achieved in real time by M-PACE in real life deployment over
advertising data.

</details>


### [78] [ProFusion: 3D Reconstruction of Protein Complex Structures from Multi-view AFM Images](https://arxiv.org/abs/2509.15242)
*Jaydeep Rade,Md Hasibul Hasan Hasib,Meric Ozturk,Baboucarr Faal,Sheng Yang,Dipali G. Sashital,Vincenzo Venditti,Baoyu Chen,Soumik Sarkar,Adarsh Krishnamurthy,Anwesha Sarkar*

Main category: cs.CV

TL;DR: ProFusion combines deep learning with simulated Atomic Force Microscopy (AFM) to improve protein complex structure prediction, achieving high accuracy and cost-efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing AI methods for protein structure prediction struggle with large protein complexes due to a lack of 3D spatial data, while experimental techniques like Cryo-EM are costly and time-consuming.

Method: Developed ProFusion, which integrates a conditional diffusion model and Neural Radiance Field (NeRF) model, using a simulated AFM dataset of ~542,000 proteins to generate synthetic multi-view images for 3D protein structure reconstruction.

Result: The reconstructed 3D protein structures achieved high fidelity with average Chamfer Distance within AFM imaging resolution, and the method was validated on experimental AFM images of various protein complexes.

Conclusion: ProFusion offers a cost-effective and accurate hybrid framework for predicting and validating protein complex structures using a combination of deep learning and AFM.

Abstract: AI-based in silico methods have improved protein structure prediction but
often struggle with large protein complexes (PCs) involving multiple
interacting proteins due to missing 3D spatial cues. Experimental techniques
like Cryo-EM are accurate but costly and time-consuming. We present ProFusion,
a hybrid framework that integrates a deep learning model with Atomic Force
Microscopy (AFM), which provides high-resolution height maps from random
orientations, naturally yielding multi-view data for 3D reconstruction.
However, generating a large-scale AFM imaging data set sufficient to train deep
learning models is impractical. Therefore, we developed a virtual AFM framework
that simulates the imaging process and generated a dataset of ~542,000 proteins
with multi-view synthetic AFM images. We train a conditional diffusion model to
synthesize novel views from unposed inputs and an instance-specific Neural
Radiance Field (NeRF) model to reconstruct 3D structures. Our reconstructed 3D
protein structures achieve an average Chamfer Distance within the AFM imaging
resolution, reflecting high structural fidelity. Our method is extensively
validated on experimental AFM images of various PCs, demonstrating strong
potential for accurate, cost-effective protein complex structure prediction and
rapid iterative validation using AFM experiments.

</details>


### [79] [Multi-Modal Interpretability for Enhanced Localization in Vision-Language Models](https://arxiv.org/abs/2509.15243)
*Muhammad Imran,Yugyung Lee*

Main category: cs.CV

TL;DR: The paper introduces MMEL, a framework to enhance the interpretability of vision-language models without sacrificing performance, using hierarchical semantic relationship modeling.


<details>
  <summary>Details</summary>
Motivation: Safety-critical applications pose challenges for vision-language models that require high transparency and reliability to interpret complex and subtle visual relationships.

Method: MMEL builds upon prior gradient-based explanation methods and introduces a Hierarchical Semantic Relationship Module for multi-scale feature processing and adaptive attention weighting.

Result: MMEL delivers more precise and contextually aware visual explanations of model decisions by integrating semantic relationship information into attribution maps.

Conclusion: The MMEL framework improves interpretability of vision-language models while maintaining performance, making it suitable for highly critical and complex domains.

Abstract: Recent advances in vision-language models have significantly expanded the
frontiers of automated image analysis. However, applying these models in
safety-critical contexts remains challenging due to the complex relationships
between objects, subtle visual cues, and the heightened demand for transparency
and reliability. This paper presents the Multi-Modal Explainable Learning
(MMEL) framework, designed to enhance the interpretability of vision-language
models while maintaining high performance. Building upon prior work in
gradient-based explanations for transformer architectures (Grad-eclip), MMEL
introduces a novel Hierarchical Semantic Relationship Module that enhances
model interpretability through multi-scale feature processing, adaptive
attention weighting, and cross-modal alignment. Our approach processes features
at multiple semantic levels to capture relationships between image regions at
different granularities, applying learnable layer-specific weights to balance
contributions across the model's depth. This results in more comprehensive
visual explanations that highlight both primary objects and their contextual
relationships with improved precision. Through extensive experiments on
standard datasets, we demonstrate that by incorporating semantic relationship
information into gradient-based attribution maps, MMEL produces more focused
and contextually aware visualizations that better reflect how vision-language
models process complex scenes. The MMEL framework generalizes across various
domains, offering valuable insights into model decisions for applications
requiring high interpretability and reliability.

</details>


### [80] [Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning](https://arxiv.org/abs/2509.15250)
*Wenda Qin,Andrea Burns,Bryan A. Plummer,Margrit Betke*

Main category: cs.CV

TL;DR: Large models excel in Vision-and-Language Navigation tasks but are computationally expensive. This paper introduces Navigation-Aware Pruning (NAP), a method that preserves efficiency and performance by focusing pruning on irrelevant tokens while reducing computational cost and navigation errors.


<details>
  <summary>Details</summary>
Motivation: To address computational inefficiency and the risk of increased navigation length caused by traditional token pruning methods in Vision-and-Language Navigation tasks.

Method: The authors introduce NAP, which involves pre-filtering tokens into foreground and background categories based on navigation-specific traits. It uses Large Language Models to extract key instructions and focuses pruning only on background tokens. Additionally, it discourages backtracking by removing unimportant navigation nodes.

Result: NAP demonstrates superior performance on VLN benchmarks, achieving higher task success rates while reducing computations by over 50% in FLOPS compared to prior methods.

Conclusion: The proposed NAP approach effectively enhances efficiency in Vision-and-Language Navigation without compromising performance, addressing the inefficiencies of prior token pruning methods.

Abstract: Large models achieve strong performance on Vision-and-Language Navigation
(VLN) tasks, but are costly to run in resource-limited environments. Token
pruning offers appealing tradeoffs for efficiency with minimal performance loss
by reducing model input size, but prior work overlooks VLN-specific challenges.
For example, information loss from pruning can effectively increase
computational cost due to longer walks. Thus, the inability to identify
uninformative tokens undermines the supposed efficiency gains from pruning. To
address this, we propose Navigation-Aware Pruning (NAP), which uses
navigation-specific traits to simplify the pruning process by pre-filtering
tokens into foreground and background. For example, image views are filtered
based on whether the agent can navigate in that direction. We also extract
navigation-relevant instructions using a Large Language Model. After filtering,
we focus pruning on background tokens, minimizing information loss. To further
help avoid increases in navigation length, we discourage backtracking by
removing low-importance navigation nodes. Experiments on standard VLN
benchmarks show NAP significantly outperforms prior work, preserving higher
success rates while saving more than 50% FLOPS.

</details>


### [81] [RespoDiff: Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation](https://arxiv.org/abs/2509.15257)
*Silpa Vadakkeeveetil Sreelatha,Sauradip Nag,Muhammad Awais,Serge Belongie,Anjan Dutta*

Main category: cs.CV

TL;DR: RespoDiff is a dual-module framework for responsible text-to-image generation, ensuring fairness and safety without compromising semantic fidelity or image quality.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of maintaining fairness and safety in text-to-image generation without sacrificing image fidelity or semantic richness.

Method: It introduces RespoDiff, which uses a dual-module transformation on intermediate diffusion model representations. One module enforces fairness and safety, while the other ensures semantic alignment, supported by a novel score-matching objective.

Result: The proposed method improves responsibility and semantic coherence by 20% on diverse prompts while preserving image quality. It integrates seamlessly into large-scale models like SDXL.

Conclusion: RespoDiff strikes a balance between fairness, safety, semantic fidelity, and image quality, outperforming existing methods in responsible text-to-image generation.

Abstract: The rapid advancement of diffusion models has enabled high-fidelity and
semantically rich text-to-image generation; however, ensuring fairness and
safety remains an open challenge. Existing methods typically improve fairness
and safety at the expense of semantic fidelity and image quality. In this work,
we propose RespoDiff, a novel framework for responsible text-to-image
generation that incorporates a dual-module transformation on the intermediate
bottleneck representations of diffusion models. Our approach introduces two
distinct learnable modules: one focused on capturing and enforcing responsible
concepts, such as fairness and safety, and the other dedicated to maintaining
semantic alignment with neutral prompts. To facilitate the dual learning
process, we introduce a novel score-matching objective that enables effective
coordination between the modules. Our method outperforms state-of-the-art
methods in responsible generation by ensuring semantic alignment while
optimizing both objectives without compromising image fidelity. Our approach
improves responsible and semantically coherent generation by 20% across
diverse, unseen prompts. Moreover, it integrates seamlessly into large-scale
models like SDXL, enhancing fairness and safety. Code will be released upon
acceptance.

</details>


### [82] [Autoguided Online Data Curation for Diffusion Model Training](https://arxiv.org/abs/2509.15267)
*Valeria Pais,Luis Oala,Daniele Faccio,Marco Aversa*

Main category: cs.CV

TL;DR: The paper evaluates the use of autoguidance and online data selection methods to improve the efficiency of training generative diffusion models. Autoguidance consistently enhances sample quality, while early targeted selection offers limited gains at the cost of complexity.


<details>
  <summary>Details</summary>
Motivation: The high computational costs of generative models have created interest in efficient data curation methods to optimize training.

Method: The authors integrated data selection techniques (JEST and autoguidance) into a unified code base and compared performance on synthetic and real-world tasks, using equal wall-clock time and sample numbers.

Result: Autoguidance was found to consistently improve data quality and diversity. Early AJEST marginally exceeded autoguidance in data efficiency but introduced more complexity and overhead.

Conclusion: Although autoguidance or uniform random selection are generally preferable, targeted online selection can improve early training efficiency in certain contexts. The paper outlines the specific conditions for using data selection methods effectively.

Abstract: The costs of generative model compute rekindled promises and hopes for
efficient data curation. In this work, we investigate whether recently
developed autoguidance and online data selection methods can improve the time
and sample efficiency of training generative diffusion models. We integrate
joint example selection (JEST) and autoguidance into a unified code base for
fast ablation and benchmarking. We evaluate combinations of data curation on a
controlled 2-D synthetic data generation task as well as (3x64x64)-D image
generation. Our comparisons are made at equal wall-clock time and equal number
of samples, explicitly accounting for the overhead of selection. Across
experiments, autoguidance consistently improves sample quality and diversity.
Early AJEST (applying selection only at the beginning of training) can match or
modestly exceed autoguidance alone in data efficiency on both tasks. However,
its time overhead and added complexity make autoguidance or uniform random data
selection preferable in most situations. These findings suggest that while
targeted online selection can yield efficiency gains in early training, robust
sample quality improvements are primarily driven by autoguidance. We discuss
limitations and scope, and outline when data selection may be beneficial.

</details>


### [83] [PRISM: Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images](https://arxiv.org/abs/2509.15270)
*Emanuele Ricco,Elia Onofri,Lorenzo Cima,Stefano Cresci,Roberto Di Pietro*

Main category: cs.CV

TL;DR: The paper introduces PRISM, a framework for attributing AI-generated image content using model-specific fingerprints derived from frequency-domain analysis. It performs well on a custom dataset and several attribution and detection benchmarks.


<details>
  <summary>Details</summary>
Motivation: To create methods for attribution of AI-generated content, particularly vital in commercial contexts and for ensuring trust and accountability in generative AI systems.

Method: PRISM utilizes frequency-domain analysis through radial reductions of the discrete Fourier transform, extracting amplitude and phase-based model signatures. A dataset of generated images is used, and clustering is performed with linear discriminant analysis.

Result: PRISM achieves a 92.04% accuracy on its custom PRISM-36K dataset and 81.60% on external benchmarks, with strong performance in distinguishing real from fake images.

Conclusion: PRISM shows reliable model attribution and fake-content detection through frequency-domain fingerprinting, enabling better accountability and trust in AI-generated content scenarios.

Abstract: A critical need has emerged for generative AI: attribution methods. That is,
solutions that can identify the model originating AI-generated content. This
feature, generally relevant in multimodal applications, is especially sensitive
in commercial settings where users subscribe to paid proprietary services and
expect guarantees about the source of the content they receive. To address
these issues, we introduce PRISM, a scalable Phase-enhanced Radial-based Image
Signature Mapping framework for fingerprinting AI-generated images. PRISM is
based on a radial reduction of the discrete Fourier transform that leverages
amplitude and phase information to capture model-specific signatures. The
output of the above process is subsequently clustered via linear discriminant
analysis to achieve reliable model attribution in diverse settings, even if the
model's internal details are inaccessible. To support our work, we construct
PRISM-36K, a novel dataset of 36,000 images generated by six text-to-image GAN-
and diffusion-based models. On this dataset, PRISM achieves an attribution
accuracy of 92.04%. We additionally evaluate our method on four benchmarks from
the literature, reaching an average accuracy of 81.60%. Finally, we evaluate
our methodology also in the binary task of detecting real vs fake images,
achieving an average accuracy of 88.41%. We obtain our best result on GenImage
with an accuracy of 95.06%, whereas the original benchmark achieved 82.20%. Our
results demonstrate the effectiveness of frequency-domain fingerprinting for
cross-architecture and cross-dataset model attribution, offering a viable
solution for enforcing accountability and trust in generative AI systems.

</details>


### [84] [Large Vision Models Can Solve Mental Rotation Problems](https://arxiv.org/abs/2509.15271)
*Sebastian Ray Mason,Anders Gjølbye,Phillip Chavarria Højbjerg,Lenka Tětková,Lars Kai Hansen*

Main category: cs.CV

TL;DR: The paper evaluates vision transformers (ViTs), including self-supervised and supervised models, on mental rotation tasks to understand their spatial reasoning abilities and compares their performance to human cognition.


<details>
  <summary>Details</summary>
Motivation: The study seeks to understand how well modern vision transformers develop spatial reasoning abilities, particularly in mental-rotation tasks, paralleling previous research on human cognition.

Method: The authors systematically evaluated different ViTs (e.g., CLIP, DINOv2, DINOv3) on mental-rotation tasks of varying complexity. They analyzed model performance layer by layer and investigated how well models performed under rotation and occlusion.

Result: Self-supervised ViTs outperformed supervised ones in capturing geometric structures. Intermediate layers achieved higher performance than final layers. The models' performance under complexity and occlusion mirrored human reaction times, indicating shared constraints in their representations.

Conclusion: Vision transformers exhibit notable success in mental rotation tasks, with self-supervised models showing superior geometric understanding. Their behavior aligns with some cognitive patterns observed in humans, elucidating constraints in both artificial and biological spatial reasoning systems.

Abstract: Mental rotation is a key test of spatial reasoning in humans and has been
central to understanding how perception supports cognition. Despite the success
of modern vision transformers, it is still unclear how well these models
develop similar abilities. In this work, we present a systematic evaluation of
ViT, CLIP, DINOv2, and DINOv3 across a range of mental-rotation tasks, from
simple block structures similar to those used by Shepard and Metzler to study
human cognition, to more complex block figures, three types of text, and
photo-realistic objects. By probing model representations layer by layer, we
examine where and how these networks succeed. We find that i) self-supervised
ViTs capture geometric structure better than supervised ViTs; ii) intermediate
layers perform better than final layers; iii) task difficulty increases with
rotation complexity and occlusion, mirroring human reaction times and
suggesting similar constraints in embedding space representations.

</details>


### [85] [Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks](https://arxiv.org/abs/2509.15272)
*Yannis Kaltampanidis,Alexandros Doumanoglou,Dimitrios Zarpalas*

Main category: cs.CV

TL;DR: This paper thoroughly investigates the intrinsic representational capabilities of unaltered Vision Transformer (ViT) features for image classification and segmentation tasks, avoiding additional feature transformations.


<details>
  <summary>Details</summary>
Motivation: Explore the under-analyzed potential of unmodified ViT features in downstream tasks instead of relying on additional transformation layers.

Method: Evaluate the inherent ability of pre-trained ViT features across various tasks using hyperplane-based and cosine-similarity-based decision rules without altering the features.

Result: Detailed insights into optimal token types and decision rules depending on task and context, derived from experiments on popular datasets.

Conclusion: ViT features intrinsically possess interpretable directions in their latent spaces that suit diverse downstream tasks, eliminating the need for feature transformations.

Abstract: Self-Supervised Learning (SSL) for Vision Transformers (ViTs) has recently
demonstrated considerable potential as a pre-training strategy for a variety of
computer vision tasks, including image classification and segmentation, both in
standard and few-shot downstream contexts. Two pre-training objectives dominate
the landscape of SSL techniques: Contrastive Learning and Masked Image
Modeling. Features (or tokens) extracted from the final transformer attention
block -- specifically, the keys, queries, and values -- as well as features
obtained after the final block's feed-forward layer, have become a common
foundation for addressing downstream tasks. However, in many existing
approaches, these pre-trained ViT features are further processed through
additional transformation layers, often involving lightweight heads or combined
with distillation, to achieve superior task performance. Although such methods
can improve task outcomes, to the best of our knowledge, a comprehensive
analysis of the intrinsic representation capabilities of unaltered ViT features
has yet to be conducted. This study aims to bridge this gap by systematically
evaluating the use of these unmodified features across image classification and
segmentation tasks, in both standard and few-shot contexts. The classification
and segmentation rules that we use are either hyperplane based (as in logistic
regression) or cosine-similarity based, both of which rely on the presence of
interpretable directions in the ViT's latent space. Based on the previous rules
and without the use of additional feature transformations, we conduct an
analysis across token types, tasks, and pre-trained ViT models. This study
provides insights into the optimal choice for token type and decision rule
based on the task, context, and the pre-training objective, while reporting
detailed findings on two widely-used datasets.

</details>


### [86] [How Good are Foundation Models in Step-by-Step Embodied Reasoning?](https://arxiv.org/abs/2509.15293)
*Dinura Dissanayake,Ahmed Heakl,Omkar Thawakar,Noor Ahsan,Ritesh Thawkar,Ketan More,Jean Lahoud,Rao Anwer,Hisham Cholakkal,Ivan Laptev,Fahad Shahbaz Khan,Salman Khan*

Main category: cs.CV

TL;DR: The paper introduces the Foundation Model Embodied Reasoning (FoMER) benchmark to evaluate large multimodal models (LMMs)' step-by-step reasoning in embodied environments, focusing on tasks requiring interpretation, reasoning, and action generation.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the lack of exploration into how well large multimodal models can perform structured reasoning for real-world embodied tasks, especially regarding safety, spatial coherence, and context grounding.

Method: The authors proposed the FoMER benchmark, consisting of 1.1k samples across 10 tasks and 8 embodiments. It employs a novel evaluation framework that separates perceptual grounding from action reasoning.

Result: The analysis of several leading LMMs displays their strengths and weaknesses in embodied reasoning, revealing current challenges and areas for improvement.

Conclusion: The study emphasizes the potential and limitations of LMMs for embodied reasoning, highlighting opportunities for advancing robot intelligence.

Abstract: Embodied agents operating in the physical world must make decisions that are
not only effective but also safe, spatially coherent, and grounded in context.
While recent advances in large multimodal models (LMMs) have shown promising
capabilities in visual understanding and language generation, their ability to
perform structured reasoning for real-world embodied tasks remains
underexplored. In this work, we aim to understand how well foundation models
can perform step-by-step reasoning in embodied environments. To this end, we
propose the Foundation Model Embodied Reasoning (FoMER) benchmark, designed to
evaluate the reasoning capabilities of LMMs in complex embodied decision-making
scenarios. Our benchmark spans a diverse set of tasks that require agents to
interpret multimodal observations, reason about physical constraints and
safety, and generate valid next actions in natural language. We present (i) a
large-scale, curated suite of embodied reasoning tasks, (ii) a novel evaluation
framework that disentangles perceptual grounding from action reasoning, and
(iii) empirical analysis of several leading LMMs under this setting. Our
benchmark includes over 1.1k samples with detailed step-by-step reasoning
across 10 tasks and 8 embodiments, covering three different robot types. Our
results highlight both the potential and current limitations of LMMs in
embodied reasoning, pointing towards key challenges and opportunities for
future research in robot intelligence. Our data and code will be made publicly
available.

</details>


### [87] [CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution Generalization](https://arxiv.org/abs/2509.15330)
*Min Zhang,Bo Jiang,Jie Zhou,Yimeng Liu,Xin Lin*

Main category: cs.CV

TL;DR: The paper introduces CoDoL, a new approach to enhance vision-language model performance by using domain-specific information for better OOD generalization.


<details>
  <summary>Details</summary>
Motivation: Prompt-based CLIP methods suffer from inaccurate text descriptions and limited vision-language embedding alignment, which degrade their accuracy, robustness, and generalization capabilities, particularly in out-of-distribution settings.

Method: The authors propose Conditional Domain prompt Learning (CoDoL) using domain-specific information for prompts. They also introduce a lightweight Domain Meta Network (DMN) to provide input-conditional tokens that capture instance- and domain-specific data.

Result: CoDoL is tested on four OOD benchmark datasets (PACS, VLCS, OfficeHome, DigitDG), showing improved vision-language embedding alignment and enhanced OOD generalization performance.

Conclusion: The CoDoL framework effectively addresses the challenges of inaccurate text descriptions and embedding alignment, significantly improving OOD generalization in vision-language models.

Abstract: Recent advances in pre-training vision-language models (VLMs), e.g.,
contrastive language-image pre-training (CLIP) methods, have shown great
potential in learning out-of-distribution (OOD) representations. Despite
showing competitive performance, the prompt-based CLIP methods still suffer
from: i) inaccurate text descriptions, which leads to degraded accuracy and
robustness, and poses a challenge for zero-shot CLIP methods. ii) limited
vision-language embedding alignment, which significantly affects the
generalization performance. To tackle the above issues, this paper proposes a
novel Conditional Domain prompt Learning (CoDoL) method, which utilizes
readily-available domain information to form prompts and improves the
vision-language embedding alignment for improving OOD generalization. To
capture both instance-specific and domain-specific information, we further
propose a lightweight Domain Meta Network (DMN) to generate input-conditional
tokens for images in each domain. Extensive experiments on four OOD benchmarks
(PACS, VLCS, OfficeHome and DigitDG) validate the effectiveness of our proposed
CoDoL in terms of improving the vision-language embedding alignment as well as
the out-of-distribution generalization performance.

</details>


### [88] [Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception](https://arxiv.org/abs/2509.15333)
*Yulin Wang,Yang Yue,Yang Yue,Huanqian Wang,Haojun Jiang,Yizeng Han,Zanlin Ni,Yifan Pu,Minglei Shi,Rui Lu,Qisen Yang,Andrew Zhao,Zhuofan Xia,Shiji Song,Gao Huang*

Main category: cs.CV

TL;DR: AdaptiveNN introduces an active and adaptive framework for machine vision to reduce computational costs and mimic human-like perceptual behavior.


<details>
  <summary>Details</summary>
Motivation: Prevailing machine vision models are resource-intensive and lack the adaptive ability to dynamically focus on task-relevant regions, unlike human vision.

Method: AdaptiveNN integrates representation learning and self-rewarding reinforcement learning to progressively attend to relevant regions, combine information across fixations, and conclude observations actively, using a coarse-to-fine approach.

Result: AdaptiveNN achieved up to 28x reduction in inference cost without accuracy loss across diverse tasks, while adapting flexibly to task demands and resource constraints, offering human-like perceptual behaviors and enhanced interpretability.

Conclusion: AdaptiveNN is a promising framework for efficient, flexible, and human-like computer vision, demonstrating its potential to advance visual cognition and practical applications.

Abstract: Human vision is highly adaptive, efficiently sampling intricate environments
by sequentially fixating on task-relevant regions. In contrast, prevailing
machine vision models passively process entire scenes at once, resulting in
excessive resource demands scaling with spatial-temporal input resolution and
model size, yielding critical limitations impeding both future advancements and
real-world application. Here we introduce AdaptiveNN, a general framework
aiming to drive a paradigm shift from 'passive' to 'active, adaptive' vision
models. AdaptiveNN formulates visual perception as a coarse-to-fine sequential
decision-making process, progressively identifying and attending to regions
pertinent to the task, incrementally combining information across fixations,
and actively concluding observation when sufficient. We establish a theory
integrating representation learning with self-rewarding reinforcement learning,
enabling end-to-end training of the non-differentiable AdaptiveNN without
additional supervision on fixation locations. We assess AdaptiveNN on 17
benchmarks spanning 9 tasks, including large-scale visual recognition,
fine-grained discrimination, visual search, processing images from real driving
and medical scenarios, language-driven embodied AI, and side-by-side
comparisons with humans. AdaptiveNN achieves up to 28x inference cost reduction
without sacrificing accuracy, flexibly adapts to varying task demands and
resource budgets without retraining, and provides enhanced interpretability via
its fixation patterns, demonstrating a promising avenue toward efficient,
flexible, and interpretable computer vision. Furthermore, AdaptiveNN exhibits
closely human-like perceptual behaviors in many cases, revealing its potential
as a valuable tool for investigating visual cognition. Code is available at
https://github.com/LeapLabTHU/AdaptiveNN.

</details>


### [89] [LowDiff: Efficient Diffusion Sampling with Low-Resolution Condition](https://arxiv.org/abs/2509.15342)
*Jiuyi Xu,Qing Jin,Meida Chen,Andrew Feng,Yang Sui,Yangming Shi*

Main category: cs.CV

TL;DR: LowDiff introduces a cascaded diffusion framework to improve image generation efficiency, achieving significant throughput gains without compromising quality.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is the improvement of diffusion model sampling speed, addressing efficiency challenges often overlooked, such as leveraging multiple input resolutions.

Method: LowDiff uses a cascaded approach that generates images progressively from low to high resolutions, employing a unified model and advanced generation techniques.

Result: LowDiff achieves over 50% throughput improvement while maintaining or surpassing image quality across multiple datasets, including CIFAR-10, FFHQ, and ImageNet.

Conclusion: LowDiff represents a significant efficiency boost for diffusion models, showcasing versatility and strong performance across various tasks and resolutions.

Abstract: Diffusion models have achieved remarkable success in image generation but
their practical application is often hindered by the slow sampling speed. Prior
efforts of improving efficiency primarily focus on compressing models or
reducing the total number of denoising steps, largely neglecting the
possibility to leverage multiple input resolutions in the generation process.
In this work, we propose LowDiff, a novel and efficient diffusion framework
based on a cascaded approach by generating increasingly higher resolution
outputs. Besides, LowDiff employs a unified model to progressively refine
images from low resolution to the desired resolution. With the proposed
architecture design and generation techniques, we achieve comparable or even
superior performance with much fewer high-resolution sampling steps. LowDiff is
applicable to diffusion models in both pixel space and latent space. Extensive
experiments on both conditional and unconditional generation tasks across
CIFAR-10, FFHQ and ImageNet demonstrate the effectiveness and generality of our
method. Results show over 50% throughput improvement across all datasets and
settings while maintaining comparable or better quality. On unconditional
CIFAR-10, LowDiff achieves an FID of 2.11 and IS of 9.87, while on conditional
CIFAR-10, an FID of 1.94 and IS of 10.03. On FFHQ 64x64, LowDiff achieves an
FID of 2.43, and on ImageNet 256x256, LowDiff built on LightningDiT-B/1
produces high-quality samples with a FID of 4.00 and an IS of 195.06, together
with substantial efficiency gains.

</details>


### [90] [MaskAttn-SDXL: Controllable Region-Level Text-To-Image Generation](https://arxiv.org/abs/2509.15357)
*Yu Chang,Jiahao Chen,Anzhe Cheng,Paul Bogdan*

Main category: cs.CV

TL;DR: The paper addresses compositional failures in text-to-image generation models and proposes a masked attention technique to improve spatial and attribute alignment.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of compositional failures, such as entity entanglement and spatial mismatches, in text-to-image models during multi-object or complex prompts.

Method: Proposes MaskAttn-SDXL, which introduces binary masks into cross-attention logits in Stable Diffusion XL to focus only on semantically relevant interactions, without requiring positional encodings or auxiliary inputs.

Result: Improved spatial compliance and attribute binding in multi-object prompts, maintaining image quality and diversity, with minimal computational overhead.

Conclusion: Logit-level masked cross-attention is a data-efficient method for enforcing compositional control, making it a practical extension for spatial control in text-to-image models.

Abstract: Text-to-image diffusion models achieve impressive realism but often suffer
from compositional failures on prompts with multiple objects, attributes, and
spatial relations, resulting in cross-token interference where entities
entangle, attributes mix across objects, and spatial cues are violated. To
address these failures, we propose MaskAttn-SDXL,a region-level gating
mechanism applied to the cross-attention logits of Stable Diffusion XL(SDXL)'s
UNet. MaskAttn-SDXL learns a binary mask per layer, injecting it into each
cross-attention logit map before softmax to sparsify token-to-latent
interactions so that only semantically relevant connections remain active. The
method requires no positional encodings, auxiliary tokens, or external region
masks, and preserves the original inference path with negligible overhead. In
practice, our model improves spatial compliance and attribute binding in
multi-object prompts while preserving overall image quality and diversity.
These findings demonstrate that logit-level maksed cross-attention is an
data-efficient primitve for enforcing compositional control, and our method
thus serves as a practical extension for spatial control in text-to-image
generation.

</details>


### [91] [RaceGAN: A Framework for Preserving Individuality while Converting Racial Information for Image-to-Image Translation](https://arxiv.org/abs/2509.15391)
*Mst Tasnim Pervin,George Bebis,Fang Jiang,Alireza Tavakkoli*

Main category: cs.CV

TL;DR: The paper presents RaceGAN, a model for multi-domain image-to-image racial attribute translation without using reference images, achieving state-of-the-art results on the Chicago Face Dataset.


<details>
  <summary>Details</summary>
Motivation: Existing GAN models for image-to-image translation struggle with maintaining individuality and require reference images for style mapping, limiting their effectiveness for translating racial traits across multiple domains.

Method: The authors propose RaceGAN, a framework that maps style codes over domains during racial attribute translation. It maintains individuality and high-level semantics without relying on a reference image, focusing on racial attribute mapping across Asian, White, and Black domains.

Result: RaceGAN outperformed previous models in racial features translation, demonstrated through the Chicago Face Dataset. In addition, the study showed quantitative results using InceptionReNetv2-based classification and analyzed the model's effectiveness in clustering latent space for distinct ethnic groups.

Conclusion: RaceGAN advances the field of image-to-image translation with its ability to translate racial features across multiple domains while preserving individuality and semantics, setting new benchmarks in the process.

Abstract: Generative adversarial networks (GANs) have demonstrated significant progress
in unpaired image-to-image translation in recent years for several
applications. CycleGAN was the first to lead the way, although it was
restricted to a pair of domains. StarGAN overcame this constraint by tackling
image-to-image translation across various domains, although it was not able to
map in-depth low-level style changes for these domains. Style mapping via
reference-guided image synthesis has been made possible by the innovations of
StarGANv2 and StyleGAN. However, these models do not maintain individuality and
need an extra reference image in addition to the input. Our study aims to
translate racial traits by means of multi-domain image-to-image translation. We
present RaceGAN, a novel framework capable of mapping style codes over several
domains during racial attribute translation while maintaining individuality and
high level semantics without relying on a reference image. RaceGAN outperforms
other models in translating racial features (i.e., Asian, White, and Black)
when tested on Chicago Face Dataset. We also give quantitative findings
utilizing InceptionReNetv2-based classification to demonstrate the
effectiveness of our racial translation. Moreover, we investigate how well the
model partitions the latent space into distinct clusters of faces for each
ethnic group.

</details>


### [92] [Generating Part-Based Global Explanations Via Correspondence](https://arxiv.org/abs/2509.15393)
*Kunal Rathore,Prasad Tadepalli*

Main category: cs.CV

TL;DR: The paper introduces a method to achieve human-understandable, global explanations for deep learning model decisions by efficiently transferring a limited set of user-defined part labels to a larger dataset.


<details>
  <summary>Details</summary>
Motivation: Deep learning models are complex and often lack transparency. Existing explanation methods either require costly annotations or are limited to localized insights, highlighting the need for a scalable and more comprehensible approach.

Method: The authors utilize user-defined part labels from a small subset of images and propagate them to a larger dataset. They aggregate part-based local explanations to generate global symbolic explanations for model decisions.

Result: The method enables scalable generation of global explanations for model behavior using a more cost-effective annotation process.

Conclusion: The proposed approach provides a practical and efficient solution for understanding deep learning decisions globally and at scale, reducing the reliance on extensive annotations.

Abstract: Deep learning models are notoriously opaque. Existing explanation methods
often focus on localized visual explanations for individual images.
Concept-based explanations, while offering global insights, require extensive
annotations, incurring significant labeling cost. We propose an approach that
leverages user-defined part labels from a limited set of images and efficiently
transfers them to a larger dataset. This enables the generation of global
symbolic explanations by aggregating part-based local explanations, ultimately
providing human-understandable explanations for model decisions on a large
scale.

</details>


### [93] [Causal Fingerprints of AI Generative Models](https://arxiv.org/abs/2509.15406)
*Hui Xu,Chi Liu,Congcong Zhu,Minghao Wang,Youyang Qu,Longxiang Gao*

Main category: cs.CV

TL;DR: This paper introduces 'causal fingerprints,' a novel concept for AI generative model identification that outperforms traditional attribution methods.


<details>
  <summary>Details</summary>
Motivation: Prior attribution methods for generative models were limited by their dependence on model-specific cues and synthesis artifacts, which poorly generalize across models.

Method: The authors proposed a causality-decoupling framework that isolates causal fingerprints by disentangling them from content and style in a semantic-invariant latent space, enhanced with diverse feature representations.

Result: The approach demonstrated superior attribution performance across various GANs and diffusion models, and introduced source anonymization using counterfactual examples generated from the fingerprints.

Conclusion: Causal fingerprints hold strong promise for applications such as forgery detection, copyright tracing, and identity protection in generative AI.

Abstract: AI generative models leave implicit traces in their generated images, which
are commonly referred to as model fingerprints and are exploited for source
attribution. Prior methods rely on model-specific cues or synthesis artifacts,
yielding limited fingerprints that may generalize poorly across different
generative models. We argue that a complete model fingerprint should reflect
the causality between image provenance and model traces, a direction largely
unexplored. To this end, we conceptualize the \emph{causal fingerprint} of
generative models, and propose a causality-decoupling framework that
disentangles it from image-specific content and style in a semantic-invariant
latent space derived from pre-trained diffusion reconstruction residual. We
further enhance fingerprint granularity with diverse feature representations.
We validate causality by assessing attribution performance across
representative GANs and diffusion models and by achieving source anonymization
using counterfactual examples generated from causal fingerprints. Experiments
show our approach outperforms existing methods in model attribution, indicating
strong potential for forgery detection, model copyright tracing, and identity
protection.

</details>


### [94] [NeuroRAD-FM: A Foundation Model for Neuro-Oncology with Distributionally Robust Training](https://arxiv.org/abs/2509.15416)
*Moinak Bhattacharya,Angelica P. Kurtz,Fabio M. Iwamoto,Prateek Prasanna,Gagandeep Singh*

Main category: cs.CV

TL;DR: This paper developed a foundation model specifically for neuro-oncology that incorporates a distributionally robust loss function to address site and class imbalance issues. The model enhances cross-institution generalization and the prediction of molecular markers, especially underrepresented ones, and improves survival prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Machine learning struggles in neuro-oncology due to heterogeneous data, tumor complexity, and the poor prediction of uncommon molecular markers essential for guiding treatment and risk stratification.

Method: The paper introduced a neuro-oncology-specific foundation model by pretraining self-supervised backbones (BYOL, DINO, MAE, MoCo) on multi-institutional brain tumor MRI. They applied distributionally robust optimization (DRO) to mitigate site and class imbalance.

Result: The proposed method improved molecular marker prediction, especially for underrepresented endpoints, and increased site-invariant embedding quality. Key metrics like accuracy, AUC, and survival prediction c-index improved across institutions.

Conclusion: Combining foundation models with a distributionally robust optimization approach shows potential for more accurate, site-agnostic molecular predictions and survival analyses in neuro-oncology. Prospective validation and integration with longitudinal data are recommended for further advancements.

Abstract: Neuro-oncology poses unique challenges for machine learning due to
heterogeneous data and tumor complexity, limiting the ability of foundation
models (FMs) to generalize across cohorts. Existing FMs also perform poorly in
predicting uncommon molecular markers, which are essential for treatment
response and risk stratification. To address these gaps, we developed a
neuro-oncology specific FM with a distributionally robust loss function,
enabling accurate estimation of tumor phenotypes while maintaining
cross-institution generalization. We pretrained self-supervised backbones
(BYOL, DINO, MAE, MoCo) on multi-institutional brain tumor MRI and applied
distributionally robust optimization (DRO) to mitigate site and class
imbalance. Downstream tasks included molecular classification of common markers
(MGMT, IDH1, 1p/19q, EGFR), uncommon alterations (ATRX, TP53, CDKN2A/2B, TERT),
continuous markers (Ki-67, TP53), and overall survival prediction in IDH1
wild-type glioblastoma at UCSF, UPenn, and CUIMC. Our method improved molecular
prediction and reduced site-specific embedding differences. At CUIMC, mean
balanced accuracy rose from 0.744 to 0.785 and AUC from 0.656 to 0.676, with
the largest gains for underrepresented endpoints (CDKN2A/2B accuracy 0.86 to
0.92, AUC 0.73 to 0.92; ATRX AUC 0.69 to 0.82; Ki-67 accuracy 0.60 to 0.69).
For survival, c-index improved at all sites: CUIMC 0.592 to 0.597, UPenn 0.647
to 0.672, UCSF 0.600 to 0.627. Grad-CAM highlighted tumor and peri-tumoral
regions, confirming interpretability. Overall, coupling FMs with DRO yields
more site-invariant representations, improves prediction of common and uncommon
markers, and enhances survival discrimination, underscoring the need for
prospective validation and integration of longitudinal and interventional
signals to advance precision neuro-oncology.

</details>


### [95] [ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models](https://arxiv.org/abs/2509.15435)
*Chung-En Johnny Yu,Hsuan-Chih,Chen,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.CV

TL;DR: ORCA enhances Large Vision-Language Models (LVLMs) by mitigating hallucinations and improving robustness using structured inference with small vision models during test-time.


<details>
  <summary>Details</summary>
Motivation: LVLMs are powerful but prone to hallucinations and adversarial attacks, limiting their reliability for real-world use.

Method: ORCA uses an Observe–Reason–Critique–Act reasoning loop and integrates evidential querying of visual tools without retraining or accessing internal LVLM mechanics.

Result: On clean and adversarial benchmarks, ORCA improves LVLM accuracy significantly, achieving gains from +3.64% to +48.00% in various evaluations.

Conclusion: ORCA represents a substantial advancement in creating robust and reliable multimodal systems.

Abstract: Large Vision-Language Models (LVLMs) exhibit strong multimodal capabilities
but remain vulnerable to hallucinations from intrinsic errors and adversarial
attacks from external exploitations, limiting their reliability in real-world
applications. We present ORCA, an agentic reasoning framework that improves the
factual accuracy and adversarial robustness of pretrained LVLMs through
test-time structured inference reasoning with a suite of small vision models
(less than 3B parameters). ORCA operates via an Observe--Reason--Critique--Act
loop, querying multiple visual tools with evidential questions, validating
cross-model inconsistencies, and refining predictions iteratively without
access to model internals or retraining. ORCA also stores intermediate
reasoning traces, which supports auditable decision-making. Though designed
primarily to mitigate object-level hallucinations, ORCA also exhibits emergent
adversarial robustness without requiring adversarial training or defense
mechanisms. We evaluate ORCA across three settings: (1) clean images on
hallucination benchmarks, (2) adversarially perturbed images without defense,
and (3) adversarially perturbed images with defense applied. On the POPE
hallucination benchmark, ORCA improves standalone LVLM performance by +3.64\%
to +40.67\% across different subsets. Under adversarial perturbations on POPE,
ORCA achieves an average accuracy gain of +20.11\% across LVLMs. When combined
with defense techniques on adversarially perturbed AMBER images, ORCA further
improves standalone LVLM performance, with gains ranging from +1.20\% to
+48.00\% across evaluation metrics. These results demonstrate that ORCA offers
a promising path toward building more reliable and robust multimodal systems.

</details>


### [96] [Region-Aware Deformable Convolutions](https://arxiv.org/abs/2509.15436)
*Abolfazl Saheban Maleki,Maryam Imani*

Main category: cs.CV

TL;DR: This paper introduces RAD-Conv, an advanced convolutional operator enhancing adaptability and efficiency in image processing. It dynamically adjusts receptive field shapes beyond conventional limitations.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between rigid convolution architectures and computationally expensive attention-based approaches.

Method: The paper proposes RAD-Conv, utilizing four boundary offsets per kernel element to dynamically adjust receptive field shapes and sizes to suit image content.

Result: RAD-Conv improves the expressiveness and efficiency of neural networks, capturing local details and long-range dependencies even with small kernels.

Conclusion: RAD-Conv combines the adaptability of attention mechanisms with the efficiency of standard convolutions, offering a novel solution for flexible and robust vision model design.

Abstract: We introduce Region-Aware Deformable Convolution (RAD-Conv), a new
convolutional operator that enhances neural networks' ability to adapt to
complex image structures. Unlike traditional deformable convolutions, which are
limited to fixed quadrilateral sampling areas, RAD-Conv uses four boundary
offsets per kernel element to create flexible, rectangular regions that
dynamically adjust their size and shape to match image content. This approach
allows precise control over the receptive field's width and height, enabling
the capture of both local details and long-range dependencies, even with small
1x1 kernels. By decoupling the receptive field's shape from the kernel's
structure, RAD-Conv combines the adaptability of attention mechanisms with the
efficiency of standard convolutions. This innovative design offers a practical
solution for building more expressive and efficient vision models, bridging the
gap between rigid convolutional architectures and computationally costly
attention-based methods.

</details>


### [97] [CAGE: Continuity-Aware edGE Network Unlocks Robust Floorplan Reconstruction](https://arxiv.org/abs/2509.15459)
*Yiyi Liu,Chunyang Liu,Weiqin Jiao,Bojian Wu,Fashuai Li,Biao Xiong*

Main category: cs.CV

TL;DR: The CAGE network provides a robust and edge-centric method for converting point-cloud density maps into coherent vector floorplans, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional corner-based and line grouping methods for reconstructing floorplans, which are prone to noise, artifacts, and reduced detail accuracy.

Method: The paper introduces a native edge-centric representation for wall segments along with a dual-query transformer decoder under a denoising framework to ensure robustness, coherence, and efficiency.

Result: CAGE achieved state-of-the-art performances with F1 scores of 99.1% for rooms, 91.7% for corners, and 89.3% for angles, along with strong generalization across datasets.

Conclusion: CAGE sets a new benchmark for robustness and detail accuracy in floorplan reconstruction, showing promise for practical deployment and further development.

Abstract: We present \textbf{CAGE} (\textit{Continuity-Aware edGE}) network, a
\textcolor{red}{robust} framework for reconstructing vector floorplans directly
from point-cloud density maps. Traditional corner-based polygon representations
are highly sensitive to noise and incomplete observations, often resulting in
fragmented or implausible layouts. Recent line grouping methods leverage
structural cues to improve robustness but still struggle to recover fine
geometric details. To address these limitations, we propose a \textit{native}
edge-centric formulation, modeling each wall segment as a directed,
geometrically continuous edge. This representation enables inference of
coherent floorplan structures, ensuring watertight, topologically valid room
boundaries while improving robustness and reducing artifacts. Towards this
design, we develop a dual-query transformer decoder that integrates perturbed
and latent queries within a denoising framework, which not only stabilizes
optimization but also accelerates convergence. Extensive experiments on
Structured3D and SceneCAD show that \textbf{CAGE} achieves state-of-the-art
performance, with F1 scores of 99.1\% (rooms), 91.7\% (corners), and 89.3\%
(angles). The method also demonstrates strong cross-dataset generalization,
underscoring the efficacy of our architectural innovations. Code and pretrained
models will be released upon acceptance.

</details>


### [98] [Self-supervised learning of imaging and clinical signatures using a multimodal joint-embedding predictive architecture](https://arxiv.org/abs/2509.15470)
*Thomas Z. Li,Aravind R. Krishnan,Lianrui Zuo,John M. Still,Kim L. Sandler,Fabien Maldonado,Thomas A. Lasko,Bennett A. Landman*

Main category: cs.CV

TL;DR: The paper explores a self-supervised learning method with multimodal data for pulmonary nodule diagnosis, improving on internal benchmarks but highlighting limitations with external data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges like labeled data scarcity and overfitting in multimodal models for pulmonary nodule diagnosis.

Method: Unlabeled patient data including CT scans and electronic health records were leveraged to pretrain a joint embedding predictive architecture (JEPA) using self-supervised learning. The pretrained model underwent supervised finetuning.

Result: The proposed model achieved an AUC of 0.91 internally, outperforming other models, but underperformed externally with a 0.72 AUC.

Conclusion: The approach shows potential for improving diagnosis using unlabeled multimodal data, but highlights its limitations in generalizability to external datasets.

Abstract: The development of multimodal models for pulmonary nodule diagnosis is
limited by the scarcity of labeled data and the tendency for these models to
overfit on the training distribution. In this work, we leverage self-supervised
learning from longitudinal and multimodal archives to address these challenges.
We curate an unlabeled set of patients with CT scans and linked electronic
health records from our home institution to power joint embedding predictive
architecture (JEPA) pretraining. After supervised finetuning, we show that our
approach outperforms an unregularized multimodal model and imaging-only model
in an internal cohort (ours: 0.91, multimodal: 0.88, imaging-only: 0.73 AUC),
but underperforms in an external cohort (ours: 0.72, imaging-only: 0.75 AUC).
We develop a synthetic environment that characterizes the context in which JEPA
may underperform. This work innovates an approach that leverages unlabeled
multimodal medical archives to improve predictive models and demonstrates its
advantages and limitations in pulmonary nodule diagnosis.

</details>


### [99] [Efficient Multimodal Dataset Distillation via Generative Models](https://arxiv.org/abs/2509.15472)
*Zhenghao Zhao,Haoxuan Wang,Junyi Wu,Yuzhang Shang,Gaowen Liu,Yan Yan*

Main category: cs.CV

TL;DR: This paper introduces EDGE, an efficient generative distillation method for synthesizing multimodal datasets like image-text pairs, addressing challenges in correlation and diversity of samples, and improving computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The growing demand for multimodal datasets, particularly image-text datasets, and limitations in current distillation methods that require high computational resources motivated the authors to develop a faster and more efficient distillation method.

Method: The authors proposed EDGE, a generative distillation framework incorporating bi-directional contrastive loss for correlation improvement, diversity loss for varied samples, and a caption synthesis strategy to enhance text-to-image retrieval.

Result: Experimental evaluations on Flickr30K, COCO, and CC3M datasets demonstrated superior performance compared to existing distillation methods, with a notable speedup of 18x over the state-of-the-art approach.

Conclusion: EDGE provides an impactful solution for efficient multimodal dataset distillation, addressing key challenges and offering scalability and improved computational efficiency.

Abstract: Dataset distillation aims to synthesize a small dataset from a large dataset,
enabling the model trained on it to perform well on the original dataset. With
the blooming of large language models and multimodal large language models, the
importance of multimodal datasets, particularly image-text datasets, has grown
significantly. However, existing multimodal dataset distillation methods are
constrained by the Matching Training Trajectories algorithm, which
significantly increases the computing resource requirement, and takes days to
process the distillation. In this work, we introduce EDGE, a generative
distillation method for efficient multimodal dataset distillation.
Specifically, we identify two key challenges of distilling multimodal datasets
with generative models: 1) The lack of correlation between generated images and
captions. 2) The lack of diversity among generated samples. To address the
aforementioned issues, we propose a novel generative model training workflow
with a bi-directional contrastive loss and a diversity loss. Furthermore, we
propose a caption synthesis strategy to further improve text-to-image retrieval
performance by introducing more text information. Our method is evaluated on
Flickr30K, COCO, and CC3M datasets, demonstrating superior performance and
efficiency compared to existing approaches. Notably, our method achieves
results 18x faster than the state-of-the-art method.

</details>


### [100] [OpenViGA: Video Generation for Automotive Driving Scenes by Streamlining and Fine-Tuning Open Source Models with Public Data](https://arxiv.org/abs/2509.15479)
*Björn Möller,Zhengyang Li,Malte Stelzer,Thomas Graave,Fabian Bettels,Muaaz Ataya,Tim Fingscheidt*

Main category: cs.CV

TL;DR: OpenViGA introduces an open-source video generation system for automotive driving, addressing deficiencies in prior methods through reproducible models and code.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of prior video generation systems, including lack of public datasets, limited understanding of design choices, and heavy resource demands.

Method: OpenViGA analyzes components individually (image tokenizer, world model, video decoder), fine-tunes pre-trained models with the BDD100K dataset, streamlines model interfaces, and offers open-access code and reproducibility.

Result: OpenViGA predicts realistic driving scene videos at 256x256 resolution, 4 fps, with minimal algorithmic latency using academic-scale GPU hardware.

Conclusion: OpenViGA enhances video generation through accessibility, transparency, and reproducibility, enabling advancements in automotive video-based research.

Abstract: Recent successful video generation systems that predict and create realistic
automotive driving scenes from short video inputs assign tokenization, future
state prediction (world model), and video decoding to dedicated models. These
approaches often utilize large models that require significant training
resources, offer limited insight into design choices, and lack publicly
available code and datasets. In this work, we address these deficiencies and
present OpenViGA, an open video generation system for automotive driving
scenes. Our contributions are: Unlike several earlier works for video
generation, such as GAIA-1, we provide a deep analysis of the three components
of our system by separate quantitative and qualitative evaluation: Image
tokenizer, world model, video decoder. Second, we purely build upon powerful
pre-trained open source models from various domains, which we fine-tune by
publicly available automotive data (BDD100K) on GPU hardware at academic scale.
Third, we build a coherent video generation system by streamlining interfaces
of our components. Fourth, due to public availability of the underlying models
and data, we allow full reproducibility. Finally, we also publish our code and
models on Github. For an image size of 256x256 at 4 fps we are able to predict
realistic driving scene videos frame-by-frame with only one frame of
algorithmic latency.

</details>


### [101] [Comparing Computational Pathology Foundation Models using Representational Similarity Analysis](https://arxiv.org/abs/2509.15482)
*Vaibhav Mishra,William Lotter*

Main category: cs.CV

TL;DR: This paper systematically analyzes representational spaces of six foundation models in computational pathology (CPath) and reveals insights about their structural characteristics, dependencies, and compactness.


<details>
  <summary>Details</summary>
Motivation: To better understand the learned representations of computational pathology foundation models and their implications for robustness, model ensembling, and effective deployment.

Method: The authors used representational similarity analyses on six CPath models, employing techniques from computational neuroscience on histopathological image patches (H&E) from TCGA.

Result: UNI2 and Virchow2 displayed distinct representational spaces, while Prov-Gigapath had high cross-model similarity. Models showed slide-dependence but low disease-dependence, with stain normalization reducing slide-dependence. Vision-language models demonstrated compact representation compared to vision-only models.

Conclusion: The study provides crucial insights into representational properties, highlighting strategies for improving robustness, ensembling, and leveraging training paradigms, with extendable applications in broader medical imaging domains.

Abstract: Foundation models are increasingly developed in computational pathology
(CPath) given their promise in facilitating many downstream tasks. While recent
studies have evaluated task performance across models, less is known about the
structure and variability of their learned representations. Here, we
systematically analyze the representational spaces of six CPath foundation
models using techniques popularized in computational neuroscience. The models
analyzed span vision-language contrastive learning (CONCH, PLIP, KEEP) and
self-distillation (UNI (v2), Virchow (v2), Prov-GigaPath) approaches. Through
representational similarity analysis using H&E image patches from TCGA, we find
that UNI2 and Virchow2 have the most distinct representational structures,
whereas Prov-Gigapath has the highest average similarity across models. Having
the same training paradigm (vision-only vs. vision-language) did not guarantee
higher representational similarity. The representations of all models showed a
high slide-dependence, but relatively low disease-dependence. Stain
normalization decreased slide-dependence for all models by a range of 5.5%
(CONCH) to 20.5% (PLIP). In terms of intrinsic dimensionality, vision-language
models demonstrated relatively compact representations, compared to the more
distributed representations of vision-only models. These findings highlight
opportunities to improve robustness to slide-specific features, inform model
ensembling strategies, and provide insights into how training paradigms shape
model representations. Our framework is extendable across medical imaging
domains, where probing the internal representations of foundation models can
help ensure effective development and deployment.

</details>


### [102] [SmolRGPT: Efficient Spatial Reasoning for Warehouse Environments with 600M Parameters](https://arxiv.org/abs/2509.15490)
*Abdarahmane Traore,Éric Hervet,Andy Couturier*

Main category: cs.CV

TL;DR: The paper introduces SmolRGPT, a compact vision-language model (600M parameters) that addresses spatial reasoning in resource-constrained environments using RGB and depth cues.


<details>
  <summary>Details</summary>
Motivation: To create an efficient vision-language model suitable for resource-limited settings, addressing limitations of large-scale VLMs in terms of computational and memory requirements.

Method: SmolRGPT integrates RGB and depth data for spatial reasoning and employs a three-stage curriculum to align visual-language features, understand spatial relationships, and specialize on task datasets.

Result: SmolRGPT achieves comparable or superior performance to larger models on warehouse spatial reasoning benchmarks despite its smaller size.

Conclusion: SmolRGPT demonstrates that efficient, scalable vision-language models can excel in real-world use cases without sacrificing spatial reasoning capabilities.

Abstract: Recent advances in vision-language models (VLMs) have enabled powerful
multimodal reasoning, but state-of-the-art approaches typically rely on
extremely large models with prohibitive computational and memory requirements.
This makes their deployment challenging in resource-constrained environments
such as warehouses, robotics, and industrial applications, where both
efficiency and robust spatial understanding are critical. In this work, we
present SmolRGPT, a compact vision-language architecture that explicitly
incorporates region-level spatial reasoning by integrating both RGB and depth
cues. SmolRGPT employs a three-stage curriculum that progressively align visual
and language features, enables spatial relationship understanding, and adapts
to task-specific datasets. We demonstrate that with only 600M parameters,
SmolRGPT achieves competitive results on challenging warehouse spatial
reasoning benchmarks, matching or exceeding the performance of much larger
alternatives. These findings highlight the potential for efficient, deployable
multimodal intelligence in real-world settings without sacrificing core spatial
reasoning capabilities. The code of the experimentation will be available at:
https://github.com/abtraore/SmolRGPT

</details>


### [103] [Lynx: Towards High-Fidelity Personalized Video Generation](https://arxiv.org/abs/2509.15496)
*Shen Sang,Tiancheng Zhi,Tianpei Gu,Jing Liu,Linjie Luo*

Main category: cs.CV

TL;DR: This paper introduces Lynx, a model for generating personalized videos from a single image, using adapters to maintain identity fidelity, temporal coherence, and realism.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in existing personalized video synthesis methods and ensure both identity fidelity and visual quality.

Method: Lynx uses a foundation Diffusion Transformer model with two specialized adapters: the ID-adapter for identity preservation via compact tokens and Ref-adapter for integrating fine-grained visual features.

Result: Evaluations on 800 test cases show that Lynx achieves superior face resemblance, strong video quality, and competitive prompt adherence.

Conclusion: Lynx represents a significant advancement in personalized video generation by successfully balancing identity fidelity and visual realism.

Abstract: We present Lynx, a high-fidelity model for personalized video synthesis from
a single input image. Built on an open-source Diffusion Transformer (DiT)
foundation model, Lynx introduces two lightweight adapters to ensure identity
fidelity. The ID-adapter employs a Perceiver Resampler to convert
ArcFace-derived facial embeddings into compact identity tokens for
conditioning, while the Ref-adapter integrates dense VAE features from a frozen
reference pathway, injecting fine-grained details across all transformer layers
through cross-attention. These modules collectively enable robust identity
preservation while maintaining temporal coherence and visual realism. Through
evaluation on a curated benchmark of 40 subjects and 20 unbiased prompts, which
yielded 800 test cases, Lynx has demonstrated superior face resemblance,
competitive prompt following, and strong video quality, thereby advancing the
state of personalized video generation.

</details>


### [104] [Backdoor Mitigation via Invertible Pruning Masks](https://arxiv.org/abs/2509.15497)
*Kealan Dunnett,Reza Arablouei,Dimity Miller,Volkan Dedeoglu,Raja Jurdak*

Main category: cs.CV

TL;DR: This paper introduces a pruning strategy to counteract backdoor attacks in deep learning using a selection mechanism and invertible pruning mask, demonstrating its effectiveness through extensive experimentation.


<details>
  <summary>Details</summary>
Motivation: Existing pruning-based defenses against backdoor attacks lack accurate identification and removal of parameters responsible for malicious behaviors. Alternatives like fine-tuning, though effective, lack the interpretability and robustness offered by pruning, especially in low-data regimes.

Method: The proposed method leverages a learned selection mechanism to identify critical parameters for both main and backdoor tasks, and implements an invertible pruning mask. This is framed as a bi-level optimization problem, which separates backdoor-related behaviors while preserving clean-task accuracy.

Result: The approach outperforms traditional pruning-based methods and is competitive with fine-tuning-based approaches, delivering strong performance in low-data settings and effectively restoring predictions for compromised samples after backdoor mitigation.

Conclusion: The study establishes a novel and effective pruning strategy to enhance defense against backdoor attacks, offering benefits of interpretability and robustness under limited data conditions.

Abstract: Model pruning has gained traction as a promising defense strategy against
backdoor attacks in deep learning. However, existing pruning-based approaches
often fall short in accurately identifying and removing the specific parameters
responsible for inducing backdoor behaviors. Despite the dominance of
fine-tuning-based defenses in recent literature, largely due to their superior
performance, pruning remains a compelling alternative, offering greater
interpretability and improved robustness in low-data regimes. In this paper, we
propose a novel pruning approach featuring a learned \emph{selection} mechanism
to identify parameters critical to both main and backdoor tasks, along with an
\emph{invertible} pruning mask designed to simultaneously achieve two
complementary goals: eliminating the backdoor task while preserving it through
the inverse mask. We formulate this as a bi-level optimization problem that
jointly learns selection variables, a sparse invertible mask, and
sample-specific backdoor perturbations derived from clean data. The inner
problem synthesizes candidate triggers using the inverse mask, while the outer
problem refines the mask to suppress backdoor behavior without impairing
clean-task accuracy. Extensive experiments demonstrate that our approach
outperforms existing pruning-based backdoor mitigation approaches, maintains
strong performance under limited data conditions, and achieves competitive
results compared to state-of-the-art fine-tuning approaches. Notably, the
proposed approach is particularly effective in restoring correct predictions
for compromised samples after successful backdoor mitigation.

</details>


### [105] [MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training](https://arxiv.org/abs/2509.15514)
*Junbiao Pang,Tianyang Cai,Baochang Zhang*

Main category: cs.CV

TL;DR: The paper introduces MEC-Quant, a novel approach to reduce biases caused by quantization in extremely low-bit neural networks, achieving performance comparable to or surpassing full precision counterparts.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the performance gap between quantized and full-precision neural networks by reducing biases introduced during quantization, especially under extremely low-bit settings.

Method: MEC-Quant explicitly optimizes the structure of learned representation using maximum entropy coding to reduce biases. It leverages minimal coding length as a surrogate for entropy and introduces a scalable MOE reformulation for efficient and accurate training.

Result: MEC-Quant enables extremely low-bit activation using QAT for the first time and achieves accuracy comparable to or better than full-precision models in extensive computer vision tasks.

Conclusion: MEC-Quant sets a new standard for Quantization-Aware Training by improving representation bias, achieving superior performance, and pushing limits of low-bit quantization.

Abstract: Quantization-Aware Training (QAT) has driven much attention to produce
efficient neural networks. Current QAT still obtains inferior performances
compared with the Full Precision (FP) counterpart. In this work, we argue that
quantization inevitably introduce biases into the learned representation,
especially under the extremely low-bit setting. To cope with this issue, we
propose Maximum Entropy Coding Quantization (MEC-Quant), a more principled
objective that explicitly optimizes on the structure of the representation, so
that the learned representation is less biased and thus generalizes better to
unseen in-distribution samples. To make the objective end-to-end trainable, we
propose to leverage the minimal coding length in lossy data coding as a
computationally tractable surrogate for the entropy, and further derive a
scalable reformulation of the objective based on Mixture Of Experts (MOE) that
not only allows fast computation but also handles the long-tailed distribution
for weights or activation values. Extensive experiments on various tasks on
computer vision tasks prove its superiority. With MEC-Qaunt, the limit of QAT
is pushed to the x-bit activation for the first time and the accuracy of
MEC-Quant is comparable to or even surpass the FP counterpart. Without bells
and whistles, MEC-Qaunt establishes a new state of the art for QAT.

</details>


### [106] [GUI-ARP: Enhancing Grounding with Adaptive Region Perception for GUI Agents](https://arxiv.org/abs/2509.15532)
*Xianhang Ye,Yiqing Li,Wei Dai,Miancan Liu,Ziyuan Chen,Zhangye Han,Hongbo Min,Jinkui Ren,Xiantao Zhang,Wen Yang,Zhi Jin*

Main category: cs.CV

TL;DR: GUI-ARP is a framework for fine-grained GUI grounding that uses adaptive multi-stage inference and achieves state-of-the-art accuracy on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing GUI grounding methods struggle with fine-grained localization in high-resolution screenshots.

Method: GUI-ARP uses Adaptive Region Perception (ARP) and Adaptive Stage Controlling (ASC) for dynamic visual attention and adaptive inference. A two-phase training pipeline integrates supervised and reinforcement fine-tuning.

Result: The GUI-ARP-7B model achieves 60.8% accuracy on ScreenSpot-Pro and 30.9% on UI-Vision benchmarks, outperforming larger 72B models.

Conclusion: GUI-ARP improves GUI grounding with its novel adaptive inference strategy, proving effective and competitive against larger or proprietary models.

Abstract: Existing GUI grounding methods often struggle with fine-grained localization
in high-resolution screenshots. To address this, we propose GUI-ARP, a novel
framework that enables adaptive multi-stage inference. Equipped with the
proposed Adaptive Region Perception (ARP) and Adaptive Stage Controlling (ASC),
GUI-ARP dynamically exploits visual attention for cropping task-relevant
regions and adapts its inference strategy, performing a single-stage inference
for simple cases and a multi-stage analysis for more complex scenarios. This is
achieved through a two-phase training pipeline that integrates supervised
fine-tuning with reinforcement fine-tuning based on Group Relative Policy
Optimization (GRPO). Extensive experiments demonstrate that the proposed
GUI-ARP achieves state-of-the-art performance on challenging GUI grounding
benchmarks, with a 7B model reaching 60.8% accuracy on ScreenSpot-Pro and 30.9%
on UI-Vision benchmark. Notably, GUI-ARP-7B demonstrates strong competitiveness
against open-source 72B models (UI-TARS-72B at 38.1%) and proprietary models.

</details>


### [107] [SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models](https://arxiv.org/abs/2509.15536)
*Sen Wang,Jingyi Tian,Le Wang,Zhimin Liao,Jiayi Li,Huaiyi Dong,Kun Xia,Sanping Zhou,Wei Tang,Hua Gang*

Main category: cs.CV

TL;DR: SAMPO is a hybrid framework for world modeling in long-horizon planning, improving temporal and spatial coherence in video prediction, with 4.4× faster inference.


<details>
  <summary>Details</summary>
Motivation: Existing autoregressive world models struggle with spatial structure disruption, inefficient decoding, and inadequate motion modeling.

Method: SAMPO combines temporal causal decoding and bidirectional spatial attention for scale-wise intra-frame generation. It uses an asymmetric multi-scale tokenizer for scene understanding and a trajectory-aware motion prompt to improve model attention and realism.

Result: SAMPO performs competitively in action-conditioned video prediction and model-based control while offering significantly faster inference and good zero-shot generalization.

Conclusion: SAMPO advances world modeling by addressing key limitations in autoregressive approaches, showing improved efficiency, consistency, and scalability.

Abstract: World models allow agents to simulate the consequences of actions in imagined
environments for planning, control, and long-horizon decision-making. However,
existing autoregressive world models struggle with visually coherent
predictions due to disrupted spatial structure, inefficient decoding, and
inadequate motion modeling. In response, we propose \textbf{S}cale-wise
\textbf{A}utoregression with \textbf{M}otion \textbf{P}r\textbf{O}mpt
(\textbf{SAMPO}), a hybrid framework that combines visual autoregressive
modeling for intra-frame generation with causal modeling for next-frame
generation. Specifically, SAMPO integrates temporal causal decoding with
bidirectional spatial attention, which preserves spatial locality and supports
parallel decoding within each scale. This design significantly enhances both
temporal consistency and rollout efficiency. To further improve dynamic scene
understanding, we devise an asymmetric multi-scale tokenizer that preserves
spatial details in observed frames and extracts compact dynamic representations
for future frames, optimizing both memory usage and model performance.
Additionally, we introduce a trajectory-aware motion prompt module that injects
spatiotemporal cues about object and robot trajectories, focusing attention on
dynamic regions and improving temporal consistency and physical realism.
Extensive experiments show that SAMPO achieves competitive performance in
action-conditioned video prediction and model-based control, improving
generation quality with 4.4$\times$ faster inference. We also evaluate SAMPO's
zero-shot generalization and scaling behavior, demonstrating its ability to
generalize to unseen tasks and benefit from larger model sizes.

</details>


### [108] [Beyond Words: Enhancing Desire, Emotion, and Sentiment Recognition with Non-Verbal Cues](https://arxiv.org/abs/2509.15540)
*Wei Chen,Tongguan Wang,Feiyue Xue,Junkai Li,Hui Liu,Ying Sha*

Main category: cs.CV

TL;DR: The paper introduces a multimodal framework to recognize human desire, emotion, and sentiment by leveraging both text and image modalities, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Current sentiment analysis methods focus strongly on verbal cues and lack consideration of images as complementary non-verbal cues for understanding human desire.

Method: The study proposes a Symmetrical Bidirectional Multimodal Learning Framework which features mutual guidance between text and image modalities, masked image modeling on sub-images, and a mixed-scale image strategy.

Result: Experimental validation on the MSED dataset shows improved F1-scores: +1.1% in desire understanding, +0.6% in emotion recognition, and +0.9% in sentiment analysis compared to state-of-the-art methods.

Conclusion: The proposed method effectively enhances multimodal understanding of desire, emotion, and sentiment, better utilizing visual and textual cues and advancing the field of sentiment analysis and desire understanding.

Abstract: Desire, as an intention that drives human behavior, is closely related to
both emotion and sentiment. Multimodal learning has advanced sentiment and
emotion recognition, but multimodal approaches specially targeting human desire
understanding remain underexplored. And existing methods in sentiment analysis
predominantly emphasize verbal cues and overlook images as complementary
non-verbal cues. To address these gaps, we propose a Symmetrical Bidirectional
Multimodal Learning Framework for Desire, Emotion, and Sentiment Recognition,
which enforces mutual guidance between text and image modalities to effectively
capture intention-related representations in the image. Specifically,
low-resolution images are used to obtain global visual representations for
cross-modal alignment, while high resolution images are partitioned into
sub-images and modeled with masked image modeling to enhance the ability to
capture fine-grained local features. A text-guided image decoder and an
image-guided text decoder are introduced to facilitate deep cross-modal
interaction at both local and global representations of image information.
Additionally, to balance perceptual gains with computation cost, a mixed-scale
image strategy is adopted, where high-resolution images are cropped into
sub-images for masked modeling. The proposed approach is evaluated on MSED, a
multimodal dataset that includes a desire understanding benchmark, as well as
emotion and sentiment recognition. Experimental results indicate consistent
improvements over other state-of-the-art methods, validating the effectiveness
of our proposed method. Specifically, our method outperforms existing
approaches, achieving F1-score improvements of 1.1% in desire understanding,
0.6% in emotion recognition, and 0.9% in sentiment analysis. Our code is
available at: https://github.com/especiallyW/SyDES.

</details>


### [109] [Enhancing Sa2VA for Referent Video Object Segmentation: 2nd Solution for 7th LSVOS RVOS Track](https://arxiv.org/abs/2509.15546)
*Ran Hong,Feng Lu,Leilei Cao,An Yan,Youhai Jiang,Fengjie Zhu*

Main category: cs.CV

TL;DR: The paper introduces a training-free framework for improving referential video object segmentation, significantly enhancing performance on the RVOS task.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve the accuracy of Referential Video Object Segmentation (RVOS), which connects video content understanding with natural language descriptions.

Method: They proposed two components: a Video-Language Checker to verify descriptions against video content and a Key-Frame Sampler for selecting informative frames.

Result: Without requiring additional training, their method achieved a J&F score of 64.14% on the MeViS test set, securing 2nd place in an RVOS challenge.

Conclusion: The framework significantly enhances segmentation accuracy by incorporating explicit content verification and adaptive frame selection, offering an impactful solution without extra model training.

Abstract: Referential Video Object Segmentation (RVOS) aims to segment all objects in a
video that match a given natural language description, bridging the gap between
vision and language understanding. Recent work, such as Sa2VA, combines Large
Language Models (LLMs) with SAM~2, leveraging the strong video reasoning
capability of LLMs to guide video segmentation. In this work, we present a
training-free framework that substantially improves Sa2VA's performance on the
RVOS task. Our method introduces two key components: (1) a Video-Language
Checker that explicitly verifies whether the subject and action described in
the query actually appear in the video, thereby reducing false positives; and
(2) a Key-Frame Sampler that adaptively selects informative frames to better
capture both early object appearances and long-range temporal context. Without
any additional training, our approach achieves a J&F score of 64.14% on the
MeViS test set, ranking 2nd place in the RVOS track of the 7th LSVOS Challenge
at ICCV 2025.

</details>


### [110] [MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild](https://arxiv.org/abs/2509.15548)
*Deming Li,Kaiwen Jiang,Yutao Tang,Ravi Ramamoorthi,Rama Chellappa,Cheng Peng*

Main category: cs.CV

TL;DR: MS-GS tackles sparse-view scene reconstruction challenges using a novel framework based on 3D Gaussian Splatting enhanced by monocular depth-based geometric priors, achieving excellent results in multi-appearance scenarios.


<details>
  <summary>Details</summary>
Motivation: To address difficulties in scene reconstruction and view synthesis posed by sparse photographic datasets with variable appearances such as lighting and seasonal changes.

Method: The framework utilizes monocular depth-based geometric priors combined with Structure-from-Motion for alignment and geometry cues, introducing fine-grained multi-view constraints for improved 3D consistency.

Result: MS-GS demonstrated superior photorealistic rendering capabilities and outperformed current methods in sparse-view and multi-appearance scenarios, validated across new, realistic benchmarks.

Conclusion: The MS-GS framework makes significant advancements in sparse-view scene reconstruction, setting new benchmarks and demonstrating robustness and quality in various multi-appearance conditions.

Abstract: In-the-wild photo collections often contain limited volumes of imagery and
exhibit multiple appearances, e.g., taken at different times of day or seasons,
posing significant challenges to scene reconstruction and novel view synthesis.
Although recent adaptations of Neural Radiance Field (NeRF) and 3D Gaussian
Splatting (3DGS) have improved in these areas, they tend to oversmooth and are
prone to overfitting. In this paper, we present MS-GS, a novel framework
designed with Multi-appearance capabilities in Sparse-view scenarios using
3DGS. To address the lack of support due to sparse initializations, our
approach is built on the geometric priors elicited from monocular depth
estimations. The key lies in extracting and utilizing local semantic regions
with a Structure-from-Motion (SfM) points anchored algorithm for reliable
alignment and geometry cues. Then, to introduce multi-view constraints, we
propose a series of geometry-guided supervision at virtual views in a
fine-grained and coarse scheme to encourage 3D consistency and reduce
overfitting. We also introduce a dataset and an in-the-wild experiment setting
to set up more realistic benchmarks. We demonstrate that MS-GS achieves
photorealistic renderings under various challenging sparse-view and
multi-appearance conditions and outperforms existing approaches significantly
across different datasets.

</details>


### [111] [Diffusion-Based Cross-Modal Feature Extraction for Multi-Label Classification](https://arxiv.org/abs/2509.15553)
*Tian Lan,Yiming Zheng,Jianxin Yin*

Main category: cs.CV

TL;DR: The paper introduces Diff-Feat, a multi-label classification framework leveraging intermediate features from pre-trained diffusion-Transformer models for images and text, achieving state-of-the-art performance on several benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: The study aims to develop a more effective representation for multi-label classification tasks by leveraging features from pre-trained diffusion-Transformer models, addressing the need for capturing multi-label interactions.

Method: Extract intermediate features from diffusion-Transformer models at optimal steps and blocks for image and text data, utilize a heuristic local-search algorithm to pinpoint the best combination of features, and use a simple fusion-linear projection for downstream classification.

Result: Diff-Feat achieves superior performance with 98.6% mAP on MS-COCO-enhanced and 45.7% mAP on Visual Genome 500, outperforming CNN, graph, and Transformer baselines. It also provides tighter semantic clustering as demonstrated by clustering metrics.

Conclusion: Diff-Feat is a simple yet powerful framework for multi-label classification, offering improved performance and semantic clustering by effectively fusing features from diffusion-Transformer models.

Abstract: Multi-label classification has broad applications and depends on powerful
representations capable of capturing multi-label interactions. We introduce
\textit{Diff-Feat}, a simple but powerful framework that extracts intermediate
features from pre-trained diffusion-Transformer models for images and text, and
fuses them for downstream tasks. We observe that for vision tasks, the most
discriminative intermediate feature along the diffusion process occurs at the
middle step and is located in the middle block in Transformer. In contrast, for
language tasks, the best feature occurs at the noise-free step and is located
in the deepest block. In particular, we observe a striking phenomenon across
varying datasets: a mysterious "Layer $12$" consistently yields the best
performance on various downstream classification tasks for images (under
DiT-XL/2-256$\times$256). We devise a heuristic local-search algorithm that
pinpoints the locally optimal "image-text"$\times$"block-timestep" pair among a
few candidates, avoiding an exhaustive grid search. A simple fusion-linear
projection followed by addition-of the selected representations yields
state-of-the-art performance: 98.6\% mAP on MS-COCO-enhanced and 45.7\% mAP on
Visual Genome 500, surpassing strong CNN, graph, and Transformer baselines by a
wide margin. t-SNE and clustering metrics further reveal that
\textit{Diff-Feat} forms tighter semantic clusters than unimodal counterparts.
The code is available at https://github.com/lt-0123/Diff-Feat.

</details>


### [112] [From Development to Deployment of AI-assisted Telehealth and Screening for Vision- and Hearing-threatening diseases in resource-constrained settings: Field Observations, Challenges and Way Forward](https://arxiv.org/abs/2509.15558)
*Mahesh Shakya,Bijay Adhikari,Nirsara Shrestha,Bipin Koirala,Arun Adhikari,Prasanta Poudyal,Luna Mathema,Sarbagya Buddhacharya,Bijay Khatri,Bishesh Khanal*

Main category: cs.CV

TL;DR: The paper explores AI's potential in vision and hearing disease screening for resource-constrained settings, identifying challenges and proposing iterative co-design processes for deployment.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of practical deployment knowledge for AI-assisted telehealth and screening in resource-constrained areas with limited specialists and paper-based workflows.

Method: The authors use iterative, interdisciplinary collaboration through prototyping, shadow deployment, and continuous feedback. They also leverage public datasets and AI models while addressing their limitations due to domain shift.

Result: The study finds interdisciplinary collaboration essential for usability and workflow transition. Public datasets are useful despite limitations, and automated AI-based image quality checks are critical for robust screening.

Conclusion: The paper concludes that AI-assisted telehealth adoption requires iterative co-design and documents practical challenges to fill the gap in actionable field knowledge for such real-world deployments.

Abstract: Vision- and hearing-threatening diseases cause preventable disability,
especially in resource-constrained settings(RCS) with few specialists and
limited screening setup. Large scale AI-assisted screening and telehealth has
potential to expand early detection, but practical deployment is challenging in
paper-based workflows and limited documented field experience exist to build
upon. We provide insights on challenges and ways forward in development to
adoption of scalable AI-assisted Telehealth and screening in such settings.
Specifically, we find that iterative, interdisciplinary collaboration through
early prototyping, shadow deployment and continuous feedback is important to
build shared understanding as well as reduce usability hurdles when
transitioning from paper-based to AI-ready workflows. We find public datasets
and AI models highly useful despite poor performance due to domain shift. In
addition, we find the need for automated AI-based image quality check to
capture gradable images for robust screening in high-volume camps.
  Our field learning stress the importance of treating AI development and
workflow digitization as an end-to-end, iterative co-design process. By
documenting these practical challenges and lessons learned, we aim to address
the gap in contextual, actionable field knowledge for building real-world
AI-assisted telehealth and mass-screening programs in RCS.

</details>


### [113] [DC-Mamba: Bi-temporal deformable alignment and scale-sparse enhancement for remote sensing change detection](https://arxiv.org/abs/2509.15563)
*Min Sun,Fenghui Guo*

Main category: cs.CV

TL;DR: The paper introduces DC-Mamba, a framework that improves remote sensing change detection (RSCD) by addressing geometric misalignments and noise issues.


<details>
  <summary>Details</summary>
Motivation: Current methods for RSCD struggle with geometric misalignments and distinguishing meaningful changes from noise.

Method: DC-Mamba uses two modules: Bi-Temporal Deformable Alignment (BTDA) for correcting spatial misalignments, and Scale-Sparse Change Amplifier (SSCA) for enhancing high-confidence change signals.

Result: DC-Mamba significantly boosts F1-score from 0.5730 to 0.5903 and IoU from 0.4015 to 0.4187 compared to the baseline.

Conclusion: The "align-then-enhance" strategy effectively handles geometric and feature-level challenges, offering an improved solution for RSCD.

Abstract: Remote sensing change detection (RSCD) is vital for identifying land-cover
changes, yet existing methods, including state-of-the-art State Space Models
(SSMs), often lack explicit mechanisms to handle geometric misalignments and
struggle to distinguish subtle, true changes from noise.To address this, we
introduce DC-Mamba, an "align-then-enhance" framework built upon the
ChangeMamba backbone. It integrates two lightweight, plug-and-play modules: (1)
Bi-Temporal Deformable Alignment (BTDA), which explicitly introduces geometric
awareness to correct spatial misalignments at the semantic feature level; and
(2) a Scale-Sparse Change Amplifier(SSCA), which uses multi-source cues to
selectively amplify high-confidence change signals while suppressing noise
before the final classification. This synergistic design first establishes
geometric consistency with BTDA to reduce pseudo-changes, then leverages SSCA
to sharpen boundaries and enhance the visibility of small or subtle targets.
Experiments show our method significantly improves performance over the strong
ChangeMamba baseline, increasing the F1-score from 0.5730 to 0.5903 and IoU
from 0.4015 to 0.4187. The results confirm the effectiveness of our
"align-then-enhance" strategy, offering a robust and easily deployable solution
that transparently addresses both geometric and feature-level challenges in
RSCD.

</details>


### [114] [BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent](https://arxiv.org/abs/2509.15566)
*Shaojie Zhang,Ruoceng Zhang,Pei Fu,Shaokang Wang,Jiahui Yang,Xin Du,Shiqi Cui,Bin Qin,Ying Huang,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: The paper introduces a brain-inspired framework called "Blink-Think-Link" (BTL) for AI-driven human-GUI interaction automation, addressing discrepancies between human and AI logic.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge the gap between AI interaction logic and natural human-GUI communication patterns.

Method: The BTL framework decomposes interactions into three phases: detection ('Blink'), reasoning ('Think'), and command execution ('Link'). It also introduces innovations such as automated blink data generation and a rule-based reward system for reinforcement learning.

Result: The framework's implementation through the BTL-UI model sets new state-of-the-art benchmarks in both static GUI understanding and dynamic interaction tasks.

Conclusion: BTL demonstrates the viability of biologically inspired methods for enhancing GUI agent performance, validated by empirical results.

Abstract: In the field of AI-driven human-GUI interaction automation, while rapid
advances in multimodal large language models and reinforcement fine-tuning
techniques have yielded remarkable progress, a fundamental challenge persists:
their interaction logic significantly deviates from natural human-GUI
communication patterns. To fill this gap, we propose "Blink-Think-Link" (BTL),
a brain-inspired framework for human-GUI interaction that mimics the human
cognitive process between users and graphical interfaces. The system decomposes
interactions into three biologically plausible phases: (1) Blink - rapid
detection and attention to relevant screen areas, analogous to saccadic eye
movements; (2) Think - higher-level reasoning and decision-making, mirroring
cognitive planning; and (3) Link - generation of executable commands for
precise motor control, emulating human action selection mechanisms.
Additionally, we introduce two key technical innovations for the BTL framework:
(1) Blink Data Generation - an automated annotation pipeline specifically
optimized for blink data, and (2) BTL Reward -- the first rule-based reward
mechanism that enables reinforcement learning driven by both process and
outcome. Building upon this framework, we develop a GUI agent model named
BTL-UI, which demonstrates consistent state-of-the-art performance across both
static GUI understanding and dynamic interaction tasks in comprehensive
benchmarks. These results provide conclusive empirical validation of the
framework's efficacy in developing advanced GUI Agents.

</details>


### [115] [Towards Size-invariant Salient Object Detection: A Generic Evaluation and Optimization Approach](https://arxiv.org/abs/2509.15573)
*Shilong Bao,Qianqian Xu,Feiran Li,Boyu Han,Zhiyong Yang,Xiaochun Cao,Qingming Huang*

Main category: cs.CV

TL;DR: This paper addresses the bias in current Salient Object Detection (SOD) metrics caused by size sensitivity and proposes both a Size-Invariant Evaluation framework (SIEva) and a Size-Invariant Optimization framework (SIOpt) to resolve these issues.


<details>
  <summary>Details</summary>
Motivation: Existing SOD metrics are size-sensitive, leading to biased evaluations, particularly when handling images with multiple salient objects of varying sizes. This results in the underrepresentation of smaller yet semantically important objects.

Method: The authors derive theoretical evidence highlighting size sensitivity in SOD metrics and propose SIEva for individual component evaluation and aggregation. They also develop SIOpt, an optimization framework adhering to size-invariance principles, and provide generalization analysis of the framework.

Result: The proposed SIEva and SIOpt frameworks successfully mitigate size imbalance issues and enhance the detection of salient objects. SIOpt’s model-agnostic design ensures its compatibility with various SOD backbones.

Conclusion: The paper introduces effective size-invariant solutions for SOD evaluation and optimization, offering unbiased and improved performance across varied object sizes, validated by theoretical analysis and extensive experiments.

Abstract: This paper investigates a fundamental yet underexplored issue in Salient
Object Detection (SOD): the size-invariant property for evaluation protocols,
particularly in scenarios when multiple salient objects of significantly
different sizes appear within a single image. We first present a novel
perspective to expose the inherent size sensitivity of existing widely used SOD
metrics. Through careful theoretical derivations, we show that the evaluation
outcome of an image under current SOD metrics can be essentially decomposed
into a sum of several separable terms, with the contribution of each term being
directly proportional to its corresponding region size. Consequently, the
prediction errors would be dominated by the larger regions, while smaller yet
potentially more semantically important objects are often overlooked, leading
to biased performance assessments and practical degradation. To address this
challenge, a generic Size-Invariant Evaluation (SIEva) framework is proposed.
The core idea is to evaluate each separable component individually and then
aggregate the results, thereby effectively mitigating the impact of size
imbalance across objects. Building upon this, we further develop a dedicated
optimization framework (SIOpt), which adheres to the size-invariant principle
and significantly enhances the detection of salient objects across a broad
range of sizes. Notably, SIOpt is model-agnostic and can be seamlessly
integrated with a wide range of SOD backbones. Theoretically, we also present
generalization analysis of SOD methods and provide evidence supporting the
validity of our new evaluation protocols. Finally, comprehensive experiments
speak to the efficacy of our proposed approach. The code is available at
https://github.com/Ferry-Li/SI-SOD.

</details>


### [116] [Multimodal Learning for Fake News Detection in Short Videos Using Linguistically Verified Data and Heterogeneous Modality Fusion](https://arxiv.org/abs/2509.15578)
*Shanghong Li,Chiam Wen Qi Ruth,Hong Xu,Fang Liu*

Main category: cs.CV

TL;DR: The paper introduces HFN, a multimodal framework for detecting fake news in short videos by integrating video, audio, and text data, alongside a new dataset, VESV.


<details>
  <summary>Details</summary>
Motivation: The rapid spread of misinformation on short video platforms and the limitations of existing detection methods prompted the need for a more robust approach.

Method: HFN utilizes a Decision Network for dynamic weighting of modalities and a Weighted Multi-Modal Feature Fusion module to handle incomplete data effectively. A new dataset, VESV, is also introduced.

Result: HFN outperformed state-of-the-art methods with 2.71% and 4.14% improvements in Marco F1 scores on the FakeTT and VESV datasets, respectively.

Conclusion: The proposed approach provides an effective and adaptable solution to detecting fake news on short video platforms, contributing to combating misinformation.

Abstract: The rapid proliferation of short video platforms has necessitated advanced
methods for detecting fake news. This need arises from the widespread influence
and ease of sharing misinformation, which can lead to significant societal
harm. Current methods often struggle with the dynamic and multimodal nature of
short video content. This paper presents HFN, Heterogeneous Fusion Net, a novel
multimodal framework that integrates video, audio, and text data to evaluate
the authenticity of short video content. HFN introduces a Decision Network that
dynamically adjusts modality weights during inference and a Weighted
Multi-Modal Feature Fusion module to ensure robust performance even with
incomplete data. Additionally, we contribute a comprehensive dataset VESV
(VEracity on Short Videos) specifically designed for short video fake news
detection. Experiments conducted on the FakeTT and newly collected VESV
datasets demonstrate improvements of 2.71% and 4.14% in Marco F1 over
state-of-the-art methods. This work establishes a robust solution capable of
effectively identifying fake news in the complex landscape of short video
platforms, paving the way for more reliable and comprehensive approaches in
combating misinformation.

</details>


### [117] [EyePCR: A Comprehensive Benchmark for Fine-Grained Perception, Knowledge Comprehension and Clinical Reasoning in Ophthalmic Surgery](https://arxiv.org/abs/2509.15596)
*Gui Wang,Yang Wennuo,Xusen Ma,Zehao Zhong,Zhuoru Wu,Ende Wu,Rong Qu,Wooi Ping Cheah,Jianfeng Ren,Linlin Shen*

Main category: cs.CV

TL;DR: The paper introduces EyePCR, a benchmark for evaluating multimodal language models (MLLMs) in ophthalmic surgery, with detailed annotations and tasks that simulate surgical cognition. EyePCR-MLLM, a domain-adapted model, shows competitive performance, revealing the current MLLMs' potential and limitations in clinical contexts.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding of how MLLMs perform in critical and domain-specific scenarios like surgical settings.

Method: Development of EyePCR, a comprehensive benchmark for ophthalmic surgery analysis, which includes perception, comprehension, and reasoning tasks supported by a richly annotated corpus and a model (EyePCR-MLLM) adapted to the domain.

Result: EyePCR enables detailed cognitive evaluation of surgical models. The adapted model, EyePCR-MLLM, performs competitively, surpassing open-source models in comprehension and reasoning and achieving the best accuracy in perception MCQs.

Conclusion: EyePCR highlights existing MLLMs' limitations in surgical cognition and paves the way for improving and benchmarking the clinical reliability of MLLMs in understanding surgical videos.

Abstract: MLLMs (Multimodal Large Language Models) have showcased remarkable
capabilities, but their performance in high-stakes, domain-specific scenarios
like surgical settings, remains largely under-explored. To address this gap, we
develop \textbf{EyePCR}, a large-scale benchmark for ophthalmic surgery
analysis, grounded in structured clinical knowledge to evaluate cognition
across \textit{Perception}, \textit{Comprehension} and \textit{Reasoning}.
EyePCR offers a richly annotated corpus with more than 210k VQAs, which cover
1048 fine-grained attributes for multi-view perception, medical knowledge graph
of more than 25k triplets for comprehension, and four clinically grounded
reasoning tasks. The rich annotations facilitate in-depth cognitive analysis,
simulating how surgeons perceive visual cues and combine them with domain
knowledge to make decisions, thus greatly improving models' cognitive ability.
In particular, \textbf{EyePCR-MLLM}, a domain-adapted variant of Qwen2.5-VL-7B,
achieves the highest accuracy on MCQs for \textit{Perception} among compared
models and outperforms open-source models in \textit{Comprehension} and
\textit{Reasoning}, rivalling commercial models like GPT-4.1. EyePCR reveals
the limitations of existing MLLMs in surgical cognition and lays the foundation
for benchmarking and enhancing clinical reliability of surgical video
understanding models.

</details>


### [118] [TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?](https://arxiv.org/abs/2509.15602)
*Zhongyuan Bao,Lejun Zhang*

Main category: cs.CV

TL;DR: The paper introduces TennisTV, a benchmark for evaluating multimodal large language models (MLLMs) on tennis video understanding, highlighting their limitations and areas for improvement.


<details>
  <summary>Details</summary>
Motivation: Multimodal large language models are effective at general video understanding but perform poorly in analyzing high-frequency, information-dense sports videos like tennis.

Method: A benchmark called TennisTV is developed, modeling rallies as sequences of stroke events and automating pipelines for event filtering and question generation across multiple tasks.

Result: 16 representative MLLMs were systematically assessed using TennisTV, exposing significant weaknesses in their understanding of tennis videos.

Conclusion: Key challenges for MLLMs include optimized frame-sampling density and improved temporal grounding to enhance reasoning capabilities.

Abstract: Multimodal large language models (MLLMs) excel at general video understanding
but struggle with fast, high-frequency sports like tennis, where rally clips
are short yet information-dense. To systematically evaluate MLLMs in this
challenging domain, we present TennisTV, the first and most comprehensive
benchmark for tennis video understanding. TennisTV models each rally as a
temporal-ordered sequence of consecutive stroke events, using automated
pipelines for filtering and question generation. It covers 8 tasks at rally and
stroke levels and includes 2,500 human-verified questions. Evaluating 16
representative MLLMs, we provide the first systematic assessment of tennis
video understanding. Results reveal substantial shortcomings and yield two key
insights: (i) frame-sampling density should be tailored and balanced across
tasks, and (ii) improving temporal grounding is essential for stronger
reasoning.

</details>


### [119] [Enhancing WSI-Based Survival Analysis with Report-Auxiliary Self-Distillation](https://arxiv.org/abs/2509.15608)
*Zheng Wang,Hong Liu,Zheng Wang,Danyi Li,Min Cen,Baptiste Magnier,Li Liang,Liansheng Wang*

Main category: cs.CV

TL;DR: This paper introduces Rasa, a framework that enhances WSI-based cancer survival analysis by incorporating textual data from pathology reports and self-distillation methods to address noisy features and limited data.


<details>
  <summary>Details</summary>
Motivation: Traditional WSI-based survival analysis struggles with noisy features and limited accessibility to critical prognostic data. Pathology reports offer complementary information, but their integration into WSI analysis is underexplored.

Method: The proposed Rasa framework uses large language models (LLMs) to extract relevant textual data from pathology reports, employs self-distillation to refine WSI features, and applies a risk-aware mix-up training strategy to enhance data diversity.

Result: Experiments on both collected (CRC) and public (TCGA-BRCA) datasets reveal that Rasa outperforms existing state-of-the-art methods in predicting cancer prognosis.

Conclusion: Rasa effectively combines textual data from pathology reports with WSI features, addressing challenges in survival analysis and improving prediction accuracy significantly.

Abstract: Survival analysis based on Whole Slide Images (WSIs) is crucial for
evaluating cancer prognosis, as they offer detailed microscopic information
essential for predicting patient outcomes. However, traditional WSI-based
survival analysis usually faces noisy features and limited data accessibility,
hindering their ability to capture critical prognostic features effectively.
Although pathology reports provide rich patient-specific information that could
assist analysis, their potential to enhance WSI-based survival analysis remains
largely unexplored. To this end, this paper proposes a novel Report-auxiliary
self-distillation (Rasa) framework for WSI-based survival analysis. First,
advanced large language models (LLMs) are utilized to extract fine-grained,
WSI-relevant textual descriptions from original noisy pathology reports via a
carefully designed task prompt. Next, a self-distillation-based pipeline is
designed to filter out irrelevant or redundant WSI features for the student
model under the guidance of the teacher model's textual knowledge. Finally, a
risk-aware mix-up strategy is incorporated during the training of the student
model to enhance both the quantity and diversity of the training data.
Extensive experiments carried out on our collected data (CRC) and public data
(TCGA-BRCA) demonstrate the superior effectiveness of Rasa against
state-of-the-art methods. Our code is available at
https://github.com/zhengwang9/Rasa.

</details>


### [120] [PCSR: Pseudo-label Consistency-Guided Sample Refinement for Noisy Correspondence Learning](https://arxiv.org/abs/2509.15623)
*Zhuoyao Liu,Yang Liu,Wentao Feng,Shudong Huang*

Main category: cs.CV

TL;DR: This paper introduces a framework called Pseudo-label Consistency-Guided Sample Refinement (PCSR) to improve cross-modal retrieval performance by addressing noisy image-text pair alignments.


<details>
  <summary>Details</summary>
Motivation: Cross-modal retrieval suffers from degraded performance due to noisy correspondences in real-world data, which are often overlooked or inadequately handled by existing methods.

Method: The proposed PCSR framework employs confidence-based estimation to distinguish clean and noisy pairs, refines noisy pairs using pseudo-label consistency, quantifies prediction stability using a Pseudo-label Consistency Score (PCS), and applies Adaptive Pair Optimization (APO) for tailored training strategies.

Result: Experiments on datasets like CC152K, MS-COCO, and Flickr30K demonstrate the framework's ability to enhance retrieval robustness under noisy supervision.

Conclusion: The PCSR framework effectively improves cross-modal retrieval by addressing noisy sample alignments through tailored refinement and optimization techniques.

Abstract: Cross-modal retrieval aims to align different modalities via semantic
similarity. However, existing methods often assume that image-text pairs are
perfectly aligned, overlooking Noisy Correspondences in real data. These
misaligned pairs misguide similarity learning and degrade retrieval
performance. Previous methods often rely on coarse-grained categorizations that
simply divide data into clean and noisy samples, overlooking the intrinsic
diversity within noisy instances. Moreover, they typically apply uniform
training strategies regardless of sample characteristics, resulting in
suboptimal sample utilization for model optimization. To address the above
challenges, we introduce a novel framework, called Pseudo-label
Consistency-Guided Sample Refinement (PCSR), which enhances correspondence
reliability by explicitly dividing samples based on pseudo-label consistency.
Specifically, we first employ a confidence-based estimation to distinguish
clean and noisy pairs, then refine the noisy pairs via pseudo-label consistency
to uncover structurally distinct subsets. We further proposed a Pseudo-label
Consistency Score (PCS) to quantify prediction stability, enabling the
separation of ambiguous and refinable samples within noisy pairs. Accordingly,
we adopt Adaptive Pair Optimization (APO), where ambiguous samples are
optimized with robust loss functions and refinable ones are enhanced via text
replacement during training. Extensive experiments on CC152K, MS-COCO and
Flickr30K validate the effectiveness of our method in improving retrieval
robustness under noisy supervision.

</details>


### [121] [pFedSAM: Personalized Federated Learning of Segment Anything Model for Medical Image Segmentation](https://arxiv.org/abs/2509.15638)
*Tong Wang,Xingyue Zhao,Linghao Zhuang,Haoyu Zhao,Jiayi Yin,Yuyang He,Gang Yu,Bo Lin*

Main category: cs.CV

TL;DR: The study proposes a Personalized Federated SAM framework for medical image segmentation in privacy-constrained and heterogeneous data settings.


<details>
  <summary>Details</summary>
Motivation: Privacy restrictions in medical image sharing hinder the performance of segmentation methods across institutions, driving the need for a federated approach.

Method: The framework employs a personalized strategy combining global parameter aggregation and domain-specific L-MoE features, alongside a decoupled global-local fine-tuning using knowledge distillation.

Result: Experiments demonstrate better segmentation performance, robust adaptation across domains, and minimized communication overhead.

Conclusion: The framework addresses the challenges of federated learning in medical image segmentation, improving results while overcoming privacy and heterogeneity hurdles.

Abstract: Medical image segmentation is crucial for computer-aided diagnosis, yet
privacy constraints hinder data sharing across institutions. Federated learning
addresses this limitation, but existing approaches often rely on lightweight
architectures that struggle with complex, heterogeneous data. Recently, the
Segment Anything Model (SAM) has shown outstanding segmentation capabilities;
however, its massive encoder poses significant challenges in federated
settings. In this work, we present the first personalized federated SAM
framework tailored for heterogeneous data scenarios in medical image
segmentation. Our framework integrates two key innovations: (1) a personalized
strategy that aggregates only the global parameters to capture cross-client
commonalities while retaining the designed L-MoE (Localized Mixture-of-Experts)
component to preserve domain-specific features; and (2) a decoupled
global-local fine-tuning mechanism that leverages a teacher-student paradigm
via knowledge distillation to bridge the gap between the global shared model
and the personalized local models, thereby mitigating overgeneralization.
Extensive experiments on two public datasets validate that our approach
significantly improves segmentation performance, achieves robust cross-domain
adaptation, and reduces communication overhead.

</details>


### [122] [UNIV: Unified Foundation Model for Infrared and Visible Modalities](https://arxiv.org/abs/2509.15642)
*Fangyuan Mao,Shuo Wang,Jilin Mei,Chen Min,Shun Lu,Fuyang Liu,Yu Hu*

Main category: cs.CV

TL;DR: This paper introduces a unified model for RGB-visible and infrared perception, resolving discrepancies in multimodal data processing with biologically inspired innovations.


<details>
  <summary>Details</summary>
Motivation: Existing pre-trained models for RGB-visible and infrared domains struggle in multimodal scenarios like autonomous vehicles. The goal is to achieve robust performance under varying weather conditions.

Method: The model, UNIV, utilizes Patch-wise Cross-modality Contrastive Learning (PCCL) for cross-modal alignment and dual-knowledge preservation via LoRA adapters and synchronous distillation on transformer-based architectures. A comprehensive benchmark dataset, MVIP, is also introduced.

Result: UNIV enhances infrared tasks (+1.7 mIoU for segmentation, +0.7 mAP for detection) while retaining 99%+ performance on RGB-visible tasks. It uses only 2% additional parameters.

Conclusion: UNIV successfully addresses multimodal perception challenges, demonstrating scalability and biological consistency, supported by its open-source implementation and benchmark dataset.

Abstract: The demand for joint RGB-visible and infrared perception is growing rapidly,
particularly to achieve robust performance under diverse weather conditions.
Although pre-trained models for RGB-visible and infrared data excel in their
respective domains, they often underperform in multimodal scenarios, such as
autonomous vehicles equipped with both sensors. To address this challenge, we
propose a biologically inspired UNified foundation model for Infrared and
Visible modalities (UNIV), featuring two key innovations. First, we introduce
Patch-wise Cross-modality Contrastive Learning (PCCL), an attention-guided
distillation framework that mimics retinal horizontal cells' lateral
inhibition, which enables effective cross-modal feature alignment while
remaining compatible with any transformer-based architecture. Second, our
dual-knowledge preservation mechanism emulates the retina's bipolar cell signal
routing - combining LoRA adapters (2% added parameters) with synchronous
distillation to prevent catastrophic forgetting, thereby replicating the
retina's photopic (cone-driven) and scotopic (rod-driven) functionality. To
support cross-modal learning, we introduce the MVIP dataset, the most
comprehensive visible-infrared benchmark to date. It contains 98,992 precisely
aligned image pairs spanning diverse scenarios. Extensive experiments
demonstrate UNIV's superior performance on infrared tasks (+1.7 mIoU in
semantic segmentation and +0.7 mAP in object detection) while maintaining 99%+
of the baseline performance on visible RGB tasks. Our code is available at
https://github.com/fangyuanmao/UNIV.

</details>


### [123] [GS-Scale: Unlocking Large-Scale 3D Gaussian Splatting Training via Host Offloading](https://arxiv.org/abs/2509.15645)
*Donghyun Lee,Dawoon Jeong,Jae W. Lee,Hongil Yoon*

Main category: cs.CV

TL;DR: GS-Scale drastically reduces GPU memory usage for 3D Gaussian Splatting while providing training speeds similar to GPU-only setups.


<details>
  <summary>Details</summary>
Motivation: Address the memory limitations in training high-quality, large-scale 3D Gaussian Splatting due to GPU memory constraints.

Method: Relocate Gaussian storage to host memory and introduce three system-level optimizations: selective offloading, parameter forwarding, and deferred optimizer updates.

Result: GS-Scale lowers GPU memory load by 3.3-5.6x, allowing consumer-grade GPUs to handle 4x more Gaussians for 23-35% perceptual image quality improvement.

Conclusion: The proposed system enables scalable and efficient training of 3D Gaussian Splatting on limited hardware like consumer GPUs.

Abstract: The advent of 3D Gaussian Splatting has revolutionized graphics rendering by
delivering high visual quality and fast rendering speeds. However, training
large-scale scenes at high quality remains challenging due to the substantial
memory demands required to store parameters, gradients, and optimizer states,
which can quickly overwhelm GPU memory. To address these limitations, we
propose GS-Scale, a fast and memory-efficient training system for 3D Gaussian
Splatting. GS-Scale stores all Gaussians in host memory, transferring only a
subset to the GPU on demand for each forward and backward pass. While this
dramatically reduces GPU memory usage, it requires frustum culling and
optimizer updates to be executed on the CPU, introducing slowdowns due to CPU's
limited compute and memory bandwidth. To mitigate this, GS-Scale employs three
system-level optimizations: (1) selective offloading of geometric parameters
for fast frustum culling, (2) parameter forwarding to pipeline CPU optimizer
updates with GPU computation, and (3) deferred optimizer update to minimize
unnecessary memory accesses for Gaussians with zero gradients. Our extensive
evaluations on large-scale datasets demonstrate that GS-Scale significantly
lowers GPU memory demands by 3.3-5.6x, while achieving training speeds
comparable to GPU without host offloading. This enables large-scale 3D Gaussian
Splatting training on consumer-grade GPUs; for instance, GS-Scale can scale the
number of Gaussians from 4 million to 18 million on an RTX 4070 Mobile GPU,
leading to 23-35% LPIPS (learned perceptual image patch similarity)
improvement.

</details>


### [124] [FingerSplat: Contactless Fingerprint 3D Reconstruction and Generation based on 3D Gaussian Splatting](https://arxiv.org/abs/2509.15648)
*Yuwei Jia,Yutang Lu,Zhe Cui,Fei Su*

Main category: cs.CV

TL;DR: The paper proposes a method using 3D Gaussian Splatting for improving contactless fingerprint recognition performance.


<details>
  <summary>Details</summary>
Motivation: Contactless fingerprint recognition underperforms compared to contact-based methods due to limited data with pose variations and lack of implicit 3D fingerprint representations.

Method: A novel framework using 3D Gaussian Splatting for 3D registration, reconstruction, and generation of fingerprints without requiring camera parameters.

Result: Experiments show accurate alignment and reconstruction of 3D fingerprints from sparse 2D images, leading to improved recognition quality.

Conclusion: This approach integrates 3D reconstruction and generation, providing a paradigm shift in contactless fingerprint recognition.

Abstract: Researchers have conducted many pioneer researches on contactless
fingerprints, yet the performance of contactless fingerprint recognition still
lags behind contact-based methods primary due to the insufficient contactless
fingerprint data with pose variations and lack of the usage of implicit 3D
fingerprint representations. In this paper, we introduce a novel contactless
fingerprint 3D registration, reconstruction and generation framework by
integrating 3D Gaussian Splatting, with the goal of offering a new paradigm for
contactless fingerprint recognition that integrates 3D fingerprint
reconstruction and generation. To our knowledge, this is the first work to
apply 3D Gaussian Splatting to the field of fingerprint recognition, and the
first to achieve effective 3D registration and complete reconstruction of
contactless fingerprints with sparse input images and without requiring camera
parameters information. Experiments on 3D fingerprint registration,
reconstruction, and generation prove that our method can accurately align and
reconstruct 3D fingerprints from 2D images, and sequentially generates
high-quality contactless fingerprints from 3D model, thus increasing the
performances for contactless fingerprint recognition.

</details>


### [125] [A PCA Based Model for Surface Reconstruction from Incomplete Point Clouds](https://arxiv.org/abs/2509.15675)
*Hao Liu*

Main category: cs.CV

TL;DR: This paper introduces a PCA-based model for reconstructing surfaces from incomplete point cloud data, overcoming challenges posed by data-missing regions.


<details>
  <summary>Details</summary>
Motivation: Surface reconstruction from incomplete point cloud data is challenging due to limitations like occlusions and light absorption. A reliable reconstruction model is needed in several fields.

Method: The model uses PCA to estimate normal surface information from existing data and employs it as a regularizer for reconstruction, alongside an operator-splitting method for effective problem-solving.

Result: Experiments reveal the model successfully infers structures in missing data areas and reconstructs surfaces, outperforming current methods.

Conclusion: The PCA-based approach effectively addresses surface reconstruction challenges, particularly for incomplete datasets, proving its superiority over existing methods.

Abstract: Point cloud data represents a crucial category of information for
mathematical modeling, and surface reconstruction from such data is an
important task across various disciplines. However, during the scanning
process, the collected point cloud data may fail to cover the entire surface
due to factors such as high light-absorption rate and occlusions, resulting in
incomplete datasets. Inferring surface structures in data-missing regions and
successfully reconstructing the surface poses a challenge. In this paper, we
present a Principal Component Analysis (PCA) based model for surface
reconstruction from incomplete point cloud data. Initially, we employ PCA to
estimate the normal information of the underlying surface from the available
point cloud data. This estimated normal information serves as a regularizer in
our model, guiding the reconstruction of the surface, particularly in areas
with missing data. Additionally, we introduce an operator-splitting method to
effectively solve the proposed model. Through systematic experimentation, we
demonstrate that our model successfully infers surface structures in
data-missing regions and well reconstructs the underlying surfaces,
outperforming existing methodologies.

</details>


### [126] [Camera Splatting for Continuous View Optimization](https://arxiv.org/abs/2509.15677)
*Gahye Lee,Hyomin Kim,Gwangjin Ju,Jooeun Son,Hyejeong Yoon,Seungyong Lee*

Main category: cs.CV

TL;DR: Camera Splatting introduces a novel view optimization framework for synthesizing complex view-dependent visuals by refining camera representations in 3D space.


<details>
  <summary>Details</summary>
Motivation: Improve novel view synthesis capabilities, especially for intricate textures and reflective phenomena.

Method: Uses 3D Gaussian-based camera splats observed by virtual cameras to refine views for better synthesis.

Result: Optimized views outperform Farthest View Sampling in capturing detailed textures and reflections.

Conclusion: Camera Splatting is a promising method for enhancing view synthesis quality in applications requiring accurate handling of complex visual phenomena.

Abstract: We propose Camera Splatting, a novel view optimization framework for novel
view synthesis. Each camera is modeled as a 3D Gaussian, referred to as a
camera splat, and virtual cameras, termed point cameras, are placed at 3D
points sampled near the surface to observe the distribution of camera splats.
View optimization is achieved by continuously and differentiably refining the
camera splats so that desirable target distributions are observed from the
point cameras, in a manner similar to the original 3D Gaussian splatting.
Compared to the Farthest View Sampling (FVS) approach, our optimized views
demonstrate superior performance in capturing complex view-dependent phenomena,
including intense metallic reflections and intricate textures such as text.

</details>


### [127] [CoPAD : Multi-source Trajectory Fusion and Cooperative Trajectory Prediction with Anchor-oriented Decoder in V2X Scenarios](https://arxiv.org/abs/2509.15984)
*Kangyu Wu,Jiaqi Qiao,Ya Zhang*

Main category: cs.CV

TL;DR: A novel framework named CoPAD is introduced for cooperative trajectory prediction in V2X scenarios, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Single-vehicle perception has limitations in trajectory prediction, thus requiring a cooperative approach.

Method: CoPAD uses a fusion module based on the Hungarian algorithm and Kalman filtering, along with a Past Time Attention module, mode attention module, and anchor-oriented decoder.

Result: Extensive experiments reveal that CoPAD achieves high accuracy and completeness on the DAIR-V2X-Seq dataset.

Conclusion: The proposed CoPAD framework enhances trajectory prediction in V2X systems through efficient multi-source data fusion and novel attention strategies.

Abstract: Recently, data-driven trajectory prediction methods have achieved remarkable
results, significantly advancing the development of autonomous driving.
However, the instability of single-vehicle perception introduces certain
limitations to trajectory prediction. In this paper, a novel lightweight
framework for cooperative trajectory prediction, CoPAD, is proposed. This
framework incorporates a fusion module based on the Hungarian algorithm and
Kalman filtering, along with the Past Time Attention (PTA) module, mode
attention module and anchor-oriented decoder (AoD). It effectively performs
early fusion on multi-source trajectory data from vehicles and road
infrastructure, enabling the trajectories with high completeness and accuracy.
The PTA module can efficiently capture potential interaction information among
historical trajectories, and the mode attention module is proposed to enrich
the diversity of predictions. Additionally, the decoder based on sparse anchors
is designed to generate the final complete trajectories. Extensive experiments
show that CoPAD achieves the state-of-the-art performance on the DAIR-V2X-Seq
dataset, validating the effectiveness of the model in cooperative trajectory
prediction in V2X scenarios.

</details>


### [128] [Layout Stroke Imitation: A Layout Guided Handwriting Stroke Generation for Style Imitation with Diffusion Model](https://arxiv.org/abs/2509.15678)
*Sidra Hanif,Longin Jan Latecki*

Main category: cs.CV

TL;DR: This paper presents a new method for handwriting stroke generation that incorporates calligraphic style and word layout to improve handwriting imitation, using a conditional diffusion model. The methodology outperforms current stroke generation techniques.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inconsistency in word spacing during handwriting stroke generation, a limitation in prior methods that did not explicitly account for word layout as a handwriting feature.

Method: The authors propose multi-scale attention features to capture local and global calligraphic style and include word layout for better word spacing. A conditional diffusion model is presented to predict handwriting strokes with temporal coordinate information.

Result: The proposed diffusion model surpasses the performance of state-of-the-art handwriting stroke generation methods and achieves competitive results compared to recent image generation techniques.

Conclusion: Incorporating calligraphic style and word layout in a conditional diffusion-based model results in significant improvements in handwriting stroke generation and style imitation.

Abstract: Handwriting stroke generation is crucial for improving the performance of
tasks such as handwriting recognition and writers order recovery. In
handwriting stroke generation, it is significantly important to imitate the
sample calligraphic style. The previous studies have suggested utilizing the
calligraphic features of the handwriting. However, they had not considered word
spacing (word layout) as an explicit handwriting feature, which results in
inconsistent word spacing for style imitation. Firstly, this work proposes
multi-scale attention features for calligraphic style imitation. These
multi-scale feature embeddings highlight the local and global style features.
Secondly, we propose to include the words layout, which facilitates word
spacing for handwriting stroke generation. Moreover, we propose a conditional
diffusion model to predict strokes in contrast to previous work, which directly
generated style images. Stroke generation provides additional temporal
coordinate information, which is lacking in image generation. Hence, our
proposed conditional diffusion model for stroke generation is guided by
calligraphic style and word layout for better handwriting imitation and stroke
generation in a calligraphic style. Our experimentation shows that the proposed
diffusion model outperforms the current state-of-the-art stroke generation and
is competitive with recent image generation networks.

</details>


### [129] [Towards Sharper Object Boundaries in Self-Supervised Depth Estimation](https://arxiv.org/abs/2509.15987)
*Aurélien Cecille,Stefan Duffner,Franck Davoine,Rémi Agier,Thibault Neveu*

Main category: cs.CV

TL;DR: The paper proposes a self-supervised approach for monocular depth estimation that achieves sharper object boundaries using a mixture distribution and uncertainty-aware loss functions.


<details>
  <summary>Details</summary>
Motivation: Depth estimation often results in blurred object boundaries, creating erroneous 3D points. The goal is to improve sharpness without requiring fine-grained supervision.

Method: The method uses a per-pixel depth mixture distribution and variance-aware loss functions to integrate depth uncertainty into existing pipelines, enabling crisp boundary estimation.

Result: The approach improves boundary sharpness by up to 35% and enhances point cloud quality on the KITTI and VKITTIv2 datasets, outperforming state-of-the-art methods.

Conclusion: The proposed method offers a significant improvement in boundary sharpness and 3D point cloud quality for monocular depth estimation, using only self-supervision.

Abstract: Accurate monocular depth estimation is crucial for 3D scene understanding,
but existing methods often blur depth at object boundaries, introducing
spurious intermediate 3D points. While achieving sharp edges usually requires
very fine-grained supervision, our method produces crisp depth discontinuities
using only self-supervision. Specifically, we model per-pixel depth as a
mixture distribution, capturing multiple plausible depths and shifting
uncertainty from direct regression to the mixture weights. This formulation
integrates seamlessly into existing pipelines via variance-aware loss functions
and uncertainty propagation. Extensive evaluations on KITTI and VKITTIv2 show
that our method achieves up to 35% higher boundary sharpness and improves point
cloud quality compared to state-of-the-art baselines.

</details>


### [130] [Saccadic Vision for Fine-Grained Visual Classification](https://arxiv.org/abs/2509.15688)
*Johann Schmidt,Sebastian Stober,Joachim Denzler,Paul Bodesheim*

Main category: cs.CV

TL;DR: The paper proposes a novel method inspired by human saccadic vision to address Fine-grained Visual Classification (FGVC) challenges by extracting coarse and focus information and utilizing selective attention mechanisms, achieving performance comparable to the state-of-the-art.


<details>
  <summary>Details</summary>
Motivation: Improving FGVC by addressing limitations such as high intra-class variability, limited inter-class differences, spatial redundancy, and complex localization requirements.

Method: A two-stage process: peripheral feature extraction, fixation patch sampling with non-maximum suppression, encoding using weight-shared encoders, and fusion via contextual selective attention.

Result: The proposed method achieves performance comparable to state-of-the-art FGVC models and outperforms baseline encoders across various benchmark and insect datasets.

Conclusion: Selective attention-based fusion and redundancy elimination effectively enhance feature representation, providing a simpler and competitive alternative to complex part-based FGVC methods.

Abstract: Fine-grained visual classification (FGVC) requires distinguishing between
visually similar categories through subtle, localized features - a task that
remains challenging due to high intra-class variability and limited inter-class
differences. Existing part-based methods often rely on complex localization
networks that learn mappings from pixel to sample space, requiring a deep
understanding of image content while limiting feature utility for downstream
tasks. In addition, sampled points frequently suffer from high spatial
redundancy, making it difficult to quantify the optimal number of required
parts. Inspired by human saccadic vision, we propose a two-stage process that
first extracts peripheral features (coarse view) and generates a sample map,
from which fixation patches are sampled and encoded in parallel using a
weight-shared encoder. We employ contextualized selective attention to weigh
the impact of each fixation patch before fusing peripheral and focus
representations. To prevent spatial collapse - a common issue in part-based
methods - we utilize non-maximum suppression during fixation sampling to
eliminate redundancy. Comprehensive evaluation on standard FGVC benchmarks
(CUB-200-2011, NABirds, Food-101 and Stanford-Dogs) and challenging insect
datasets (EU-Moths, Ecuador-Moths and AMI-Moths) demonstrates that our method
achieves comparable performance to state-of-the-art approaches while
consistently outperforming our baseline encoder.

</details>


### [131] [SCENEFORGE: Enhancing 3D-text alignment with Structured Scene Compositions](https://arxiv.org/abs/2509.15693)
*Cristian Sbrolli,Matteo Matteucci*

Main category: cs.CV

TL;DR: SceneForge enhances 3D-text contrastive learning by constructing structured multi-object scenes paired with coherent descriptions.


<details>
  <summary>Details</summary>
Motivation: Address the lack of large-scale 3D-text datasets and improve contrastive learning between 3D point clouds and text.

Method: SceneForge creates multi-object scenes using individual 3D shapes with spatial relations, paired with text descriptions refined by language models.

Result: Demonstrates performance improvements across tasks like zero-shot classification, few-shot segmentation, QA, retrieval, and spatial reasoning.

Conclusion: SceneForge's compositional augmentations are effective and model-agnostic, boosting performance across various scenarios.

Abstract: The whole is greater than the sum of its parts-even in 3D-text contrastive
learning. We introduce SceneForge, a novel framework that enhances contrastive
alignment between 3D point clouds and text through structured multi-object
scene compositions. SceneForge leverages individual 3D shapes to construct
multi-object scenes with explicit spatial relations, pairing them with coherent
multi-object descriptions refined by a large language model. By augmenting
contrastive training with these structured, compositional samples, SceneForge
effectively addresses the scarcity of large-scale 3D-text datasets,
significantly enriching data complexity and diversity. We systematically
investigate critical design elements, such as the optimal number of objects per
scene, the proportion of compositional samples in training batches, and scene
construction strategies. Extensive experiments demonstrate that SceneForge
delivers substantial performance gains across multiple tasks, including
zero-shot classification on ModelNet, ScanObjNN, Objaverse-LVIS, and ScanNet,
as well as few-shot part segmentation on ShapeNetPart. SceneForge's
compositional augmentations are model-agnostic, consistently improving
performance across multiple encoder architectures. Moreover, SceneForge
improves 3D visual question answering on ScanQA, generalizes robustly to
retrieval scenarios with increasing scene complexity, and showcases spatial
reasoning capabilities by adapting spatial configurations to align precisely
with textual instructions.

</details>


### [132] [ORIC: Benchmarking Object Recognition in Incongruous Context for Large Vision-Language Models](https://arxiv.org/abs/2509.15695)
*Zhaoyang Li,Zhan Ling,Yuchen Zhou,Hao Su*

Main category: cs.CV

TL;DR: The paper introduces ORIC, a benchmark to test Large Vision-Language Models (LVLMs) on their ability to recognize objects in unexpected contexts, highlighting issues like misidentification and hallucination.


<details>
  <summary>Details</summary>
Motivation: To address the persistent recognition errors in LVLMs when faced with incongruous object-context relationships, which can negatively impact their applications.

Method: The ORIC benchmark employs two sampling strategies: LLM-guided sampling for identifying contextually incongruous present objects, and CLIP-guided sampling for detecting plausible but nonexistent hallucinated objects.

Result: Evaluation of 18 LVLMs and two detection models through ORIC shows significant gaps in recognition under incongruous contexts, emphasizing the limitations in context-aware object recognition.

Conclusion: The study highlights the need for advancing LVLMs' capabilities in understanding and adapting to object-context relationships, urging further research in this area.

Abstract: Large Vision-Language Models (LVLMs) have made significant strides in image
caption, visual question answering, and robotics by integrating visual and
textual information. However, they remain prone to errors in incongruous
contexts, where objects appear unexpectedly or are absent when contextually
expected. This leads to two key recognition failures: object misidentification
and hallucination. To systematically examine this issue, we introduce the
Object Recognition in Incongruous Context Benchmark (ORIC), a novel benchmark
that evaluates LVLMs in scenarios where object-context relationships deviate
from expectations. ORIC employs two key strategies: (1) LLM-guided sampling,
which identifies objects that are present but contextually incongruous, and (2)
CLIP-guided sampling, which detects plausible yet nonexistent objects that are
likely to be hallucinated, thereby creating an incongruous context. Evaluating
18 LVLMs and two open-vocabulary detection models, our results reveal
significant recognition gaps, underscoring the challenges posed by contextual
incongruity. This work provides critical insights into LVLMs' limitations and
encourages further research on context-aware object recognition.

</details>


### [133] [Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance](https://arxiv.org/abs/2509.15704)
*Yuxuan Liang,Xu Li,Xiaolei Chen,Yi Zheng,Haotian Chen,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: The paper introduces Pyramid Token Pruning (PTP), a method to reduce computational complexity in large vision-language models (LVLMs) when processing high-resolution images without significant performance loss.


<details>
  <summary>Details</summary>
Motivation: LVLMs face challenges in efficiently handling high-resolution images due to the exponential increase in visual tokens and computational requirements.

Method: The authors propose Pyramid Token Pruning (PTP), a training-free approach that integrates bottom-up visual saliency and top-down instruction-guided importance to selectively retain tokens critical for multimodal tasks.

Result: Experiments on 13 benchmarks show that the PTP method significantly reduces computational overhead and inference latency while maintaining near-original performance.

Conclusion: PTP is an effective strategy for making LVLMs more efficient at handling high-resolution images, combining both visual saliency and task-specific guidance to optimize performance and computational cost.

Abstract: Large Vision-Language Models (LVLMs) have significantly advanced multimodal
understanding but still struggle with efficiently processing high-resolution
images. Recent approaches partition high-resolution images into multiple
sub-images, dramatically increasing the number of visual tokens and causing
exponential computational overhead during inference. To address these
limitations, we propose a training-free token pruning strategy, Pyramid Token
Pruning (PTP), that integrates bottom-up visual saliency at both region and
token levels with top-down instruction-guided importance. Inspired by human
visual attention mechanisms, PTP selectively retains more tokens from visually
salient regions and further leverages textual instructions to pinpoint tokens
most relevant to specific multimodal tasks. Extensive experiments across 13
diverse benchmarks demonstrate that our method substantially reduces
computational overhead and inference latency with minimal performance loss.

</details>


### [134] [SGMAGNet: A Baseline Model for 3D Cloud Phase Structure Reconstruction on a New Passive Active Satellite Benchmark](https://arxiv.org/abs/2509.15706)
*Chi Yang,Fu Wang,Xiaofei Yang,Hao Huang,Weijia Cao,Xiaowen Chu*

Main category: cs.CV

TL;DR: This paper introduces a dataset and a framework for predicting 3D cloud phase profiles from multimodal satellite observations, outperforming baseline models using SGMAGNet.


<details>
  <summary>Details</summary>
Motivation: Cloud phase profiles are essential for improving numerical weather prediction (NWP) as they influence radiative transfer and precipitation, highlighting the need for accurate retrieval methods.

Method: The method involves leveraging multimodal data—visible (VIS) and thermal infrared (TIR) imagery and vertical cloud phase profiles from spaceborne lidar and radar—and using SGMAGNet and other baseline architectures to predict 3D cloud phases.

Result: SGMAGNet demonstrates superior performance with Precision: 0.922, Recall: 0.858, F1-score: 0.763, and IoU: 0.617, outperforming UNet and SegNet baselines.

Conclusion: The proposed SGMAGNet framework effectively reconstructs complex 3D cloud phase profiles and shows promise for integration into NWP systems to enhance cloud microphysics parameterization.

Abstract: Cloud phase profiles are critical for numerical weather prediction (NWP), as
they directly affect radiative transfer and precipitation processes. In this
study, we present a benchmark dataset and a baseline framework for transforming
multimodal satellite observations into detailed 3D cloud phase structures,
aiming toward operational cloud phase profile retrieval and future integration
with NWP systems to improve cloud microphysics parameterization. The multimodal
observations consist of (1) high--spatiotemporal--resolution, multi-band
visible (VIS) and thermal infrared (TIR) imagery from geostationary satellites,
and (2) accurate vertical cloud phase profiles from spaceborne lidar
(CALIOP\slash CALIPSO) and radar (CPR\slash CloudSat). The dataset consists of
synchronized image--profile pairs across diverse cloud regimes, defining a
supervised learning task: given VIS/TIR patches, predict the corresponding 3D
cloud phase structure. We adopt SGMAGNet as the main model and compare it with
several baseline architectures, including UNet variants and SegNet, all
designed to capture multi-scale spatial patterns. Model performance is
evaluated using standard classification metrics, including Precision, Recall,
F1-score, and IoU. The results demonstrate that SGMAGNet achieves superior
performance in cloud phase reconstruction, particularly in complex multi-layer
and boundary transition regions. Quantitatively, SGMAGNet attains a Precision
of 0.922, Recall of 0.858, F1-score of 0.763, and an IoU of 0.617,
significantly outperforming all baselines across these key metrics.

</details>


### [135] [Toward Medical Deepfake Detection: A Comprehensive Dataset and Novel Method](https://arxiv.org/abs/2509.15711)
*Shuaibo Li,Zhaohu Xing,Hongqiu Wang,Pengfei Hao,Xingyu Li,Zekai Liu,Lei Zhu*

Main category: cs.CV

TL;DR: The paper presents MedForensics, a large dataset for detecting AI-generated medical images, and proposes DSKI, a dual-stage detection model tailored for medical forensics.


<details>
  <summary>Details</summary>
Motivation: Generative AI creates synthetic medical images that are risky for healthcare, prompting the need for tools to detect and counter these threats.

Method: The authors introduce MedForensics, the dataset, and DSKI, a two-component detector leveraging cross-domain adapters and forensic retrieval for accuracy enhancement.

Result: DSKI outperforms existing methods, achieving high detection accuracy across multiple medical modalities.

Conclusion: This work advances medical forensics with a novel dataset and model, addressing critical gaps in detecting fake medical images.

Abstract: The rapid advancement of generative AI in medical imaging has introduced both
significant opportunities and serious challenges, especially the risk that fake
medical images could undermine healthcare systems. These synthetic images pose
serious risks, such as diagnostic deception, financial fraud, and
misinformation. However, research on medical forensics to counter these threats
remains limited, and there is a critical lack of comprehensive datasets
specifically tailored for this field. Additionally, existing media forensic
methods, which are primarily designed for natural or facial images, are
inadequate for capturing the distinct characteristics and subtle artifacts of
AI-generated medical images. To tackle these challenges, we introduce
\textbf{MedForensics}, a large-scale medical forensics dataset encompassing six
medical modalities and twelve state-of-the-art medical generative models. We
also propose \textbf{DSKI}, a novel \textbf{D}ual-\textbf{S}tage
\textbf{K}nowledge \textbf{I}nfusing detector that constructs a vision-language
feature space tailored for the detection of AI-generated medical images. DSKI
comprises two core components: 1) a cross-domain fine-trace adapter (CDFA) for
extracting subtle forgery clues from both spatial and noise domains during
training, and 2) a medical forensic retrieval module (MFRM) that boosts
detection accuracy through few-shot retrieval during testing. Experimental
results demonstrate that DSKI significantly outperforms both existing methods
and human experts, achieving superior accuracy across multiple medical
modalities.

</details>


### [136] [TrueMoE: Dual-Routing Mixture of Discriminative Experts for Synthetic Image Detection](https://arxiv.org/abs/2509.15741)
*Laixin Zhang,Shuaibo Li,Wei Ma,Hongbin Zha*

Main category: cs.CV

TL;DR: The paper introduces TrueMoE, a framework to enhance synthetic image detection by using specialized subspaces rather than a unified discriminative space, improving generalization and robustness.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the limitations of current synthetic image detection methods that use a single discriminative space, which struggles with generalization to unseen generative patterns.

Method: The authors propose TrueMoE, which combines a dual-routing Mixture-of-Discriminative-Experts framework. It features a Discriminative Expert Array (DEA) with axes for manifold structure and perceptual granularity, and routes inputs adaptively via specialized mechanisms.

Result: TrueMoE shows superior performance in generalization and robustness across a wide range of generative models in extensive experiments.

Conclusion: The proposed framework effectively addresses the complexities of synthetic image detection by leveraging specialized and collaborative subspaces, offering a robust alternative to existing methods.

Abstract: The rapid progress of generative models has made synthetic image detection an
increasingly critical task. Most existing approaches attempt to construct a
single, universal discriminative space to separate real from fake content.
However, such unified spaces tend to be complex and brittle, often struggling
to generalize to unseen generative patterns. In this work, we propose TrueMoE,
a novel dual-routing Mixture-of-Discriminative-Experts framework that
reformulates the detection task as a collaborative inference across multiple
specialized and lightweight discriminative subspaces. At the core of TrueMoE is
a Discriminative Expert Array (DEA) organized along complementary axes of
manifold structure and perceptual granularity, enabling diverse forgery cues to
be captured across subspaces. A dual-routing mechanism, comprising a
granularity-aware sparse router and a manifold-aware dense router, adaptively
assigns input images to the most relevant experts. Extensive experiments across
a wide spectrum of generative models demonstrate that TrueMoE achieves superior
generalization and robustness.

</details>


### [137] [Hybrid Lie semi-group and cascade structures for the generalized Gaussian derivative model for visual receptive fields](https://arxiv.org/abs/2509.15748)
*Tony Lindeberg*

Main category: cs.CV

TL;DR: This paper explores how real-world geometric image transformations affect early visual processing, proposing receptive field designs to handle variability and deriving relationships for spatial and spatio-temporal receptive field responses.


<details>
  <summary>Details</summary>
Motivation: The variability of real-world image structures due to geometric transformations challenges accurate visual recognition, necessitating robust receptive field designs.

Method: The paper develops mathematical relationships for receptive field responses, including infinitesimal relations based on semi-groups and Lie groups, and macroscopic cascade smoothing properties using incremental filters.

Result: It provides insights into the relationships between spatial and spatio-temporal receptive field responses for different filter parameters and efficient computation mechanisms.

Conclusion: The study offers theoretical foundations for improved computational models in artificial and biological vision, facilitating the design of multi-parameter receptive fields.

Abstract: Because of the variabilities of real-world image structures under the natural
image transformations that arise when observing similar objects or
spatio-temporal events under different viewing conditions, the receptive field
responses computed in the earliest layers of the visual hierarchy may be
strongly influenced by such geometric image transformations. One way of
handling this variability is by basing the vision system on covariant receptive
field families, which expand the receptive field shapes over the degrees of
freedom in the image transformations.
  This paper addresses the problem of deriving relationships between spatial
and spatio-temporal receptive field responses obtained for different values of
the shape parameters in the resulting multi-parameter families of receptive
fields. For this purpose, we derive both (i) infinitesimal relationships,
roughly corresponding to a combination of notions from semi-groups and Lie
groups, as well as (ii) macroscopic cascade smoothing properties, which
describe how receptive field responses at coarser spatial and temporal scales
can be computed by applying smaller support incremental filters to the output
from corresponding receptive fields at finer spatial and temporal scales,
structurally related to the notion of Lie algebras, although with directional
preferences.
  The presented results provide (i) a deeper understanding of the relationships
between spatial and spatio-temporal receptive field responses for different
values of the filter parameters, which can be used for both (ii) designing more
efficient schemes for computing receptive field responses over populations of
multi-parameter families of receptive fields, as well as (iii)~formulating
idealized theoretical models of the computations of simple cells in biological
vision.

</details>


### [138] [FloorSAM: SAM-Guided Floorplan Reconstruction with Semantic-Geometric Fusion](https://arxiv.org/abs/2509.15750)
*Han Ye,Haofu Wang,Yunchi Zhang,Jiangjian Xiao,Yuqiang Jin,Jinyuan Liu,Wen-An Zhang,Uladzislau Sychou,Alexander Tuzikov,Vladislav Sobolevskii,Valerii Zakharov,Boris Sokolov,Minglei Fu*

Main category: cs.CV

TL;DR: This paper introduces FloorSAM, a framework utilizing point cloud density maps and SAM for reconstructing building floor plans, addressing issues faced by traditional methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve indoor navigation, BIM, and precise measurements by overcoming the challenges of noise, limited generalization, and loss of geometric details in traditional floor plan reconstruction methods.

Method: FloorSAM integrates LiDAR-based density maps, adaptive resolution projection, image enhancement, and SAM's zero-shot learning for robust segmentation and accurate reconstruction.

Result: Experiments on Giblayout and ISPRS datasets demonstrate that FloorSAM offers better accuracy and robustness against noise and complexity compared to existing methods.

Conclusion: FloorSAM effectively reconstructs accurate floor plans, enhances room segmentation, and recovers room topology relationships, proving superior results in diverse and challenging layouts.

Abstract: Reconstructing building floor plans from point cloud data is key for indoor
navigation, BIM, and precise measurements. Traditional methods like geometric
algorithms and Mask R-CNN-based deep learning often face issues with noise,
limited generalization, and loss of geometric details. We propose FloorSAM, a
framework that integrates point cloud density maps with the Segment Anything
Model (SAM) for accurate floor plan reconstruction from LiDAR data. Using
grid-based filtering, adaptive resolution projection, and image enhancement, we
create robust top-down density maps. FloorSAM uses SAM's zero-shot learning for
precise room segmentation, improving reconstruction across diverse layouts.
Room masks are generated via adaptive prompt points and multistage filtering,
followed by joint mask and point cloud analysis for contour extraction and
regularization. This produces accurate floor plans and recovers room
topological relationships. Tests on Giblayout and ISPRS datasets show better
accuracy, recall, and robustness than traditional methods, especially in noisy
and complex settings. Code and materials: github.com/Silentbarber/FloorSAM.

</details>


### [139] [Simulated Cortical Magnification Supports Self-Supervised Object Learning](https://arxiv.org/abs/2509.15751)
*Zhengyang Yu,Arthur Aubret,Chen Yu,Jochen Triesch*

Main category: cs.CV

TL;DR: This paper investigates the role of human-like foveated vision in improving self-supervised models for learning object representations using egocentric video data.


<details>
  <summary>Details</summary>
Motivation: To examine how incorporating foveated vision, which mimics human visual resolution differences across the field of view, impacts the development of semantic object representations in computational models.

Method: Applied models of foveation and cortical magnification to egocentric video datasets and trained bio-inspired self-supervised models with a time-based learning objective using these modified inputs.

Result: Modeling foveated vision improved the quality of learned object representations by making objects appear larger and optimizing the balance between central and peripheral visual detail.

Conclusion: Including foveated vision elements advances self-supervised learning models by aligning more closely with human visual processing, thereby enhancing their realism and performance.

Abstract: Recent self-supervised learning models simulate the development of semantic
object representations by training on visual experience similar to that of
toddlers. However, these models ignore the foveated nature of human vision with
high/low resolution in the center/periphery of the visual field. Here, we
investigate the role of this varying resolution in the development of object
representations. We leverage two datasets of egocentric videos that capture the
visual experience of humans during interactions with objects. We apply models
of human foveation and cortical magnification to modify these inputs, such that
the visual content becomes less distinct towards the periphery. The resulting
sequences are used to train two bio-inspired self-supervised learning models
that implement a time-based learning objective. Our results show that modeling
aspects of foveated vision improves the quality of the learned object
representations in this setting. Our analysis suggests that this improvement
comes from making objects appear bigger and inducing a better trade-off between
central and peripheral visual information. Overall, this work takes a step
towards making models of humans' learning of visual representations more
realistic and performant.

</details>


### [140] [MCOD: The First Challenging Benchmark for Multispectral Camouflaged Object Detection](https://arxiv.org/abs/2509.15753)
*Yang Li,Tingfa Xu,Shuyan Bai,Peifu Liu,Jianan Li*

Main category: cs.CV

TL;DR: The paper introduces MCOD, the first benchmark dataset for multispectral camouflaged object detection (COD), highlighting its advantages, challenges, and promising results in leveraging multispectral data for increased detection robustness.


<details>
  <summary>Details</summary>
Motivation: The motivation for this study is the limitations of existing RGB-based COD methods, particularly under challenging conditions. The lack of multispectral COD benchmark datasets impedes progress in leveraging rich spectral information for better foreground-background discrimination.

Method: The authors introduced MCOD, a multispectral COD benchmark dataset. It includes comprehensive challenge attributes, diverse real-world environments, and high-quality pixel-level annotations. They benchmarked eleven existing COD methods using MCOD.

Result: The study observed a consistent performance drop in COD methods due to increased task difficulty in MCOD. However, integrating multispectral modalities significantly alleviated this performance degradation, demonstrating the robustness offered by multispectral data.

Conclusion: MCOD provides a foundational resource for advancing research in multispectral camouflaged object detection by addressing real-world challenges and showcasing the benefits of spectral information for enhanced object detection.

Abstract: Camouflaged Object Detection (COD) aims to identify objects that blend
seamlessly into natural scenes. Although RGB-based methods have advanced, their
performance remains limited under challenging conditions. Multispectral
imagery, providing rich spectral information, offers a promising alternative
for enhanced foreground-background discrimination. However, existing COD
benchmark datasets are exclusively RGB-based, lacking essential support for
multispectral approaches, which has impeded progress in this area. To address
this gap, we introduce MCOD, the first challenging benchmark dataset
specifically designed for multispectral camouflaged object detection. MCOD
features three key advantages: (i) Comprehensive challenge attributes: It
captures real-world difficulties such as small object sizes and extreme
lighting conditions commonly encountered in COD tasks. (ii) Diverse real-world
scenarios: The dataset spans a wide range of natural environments to better
reflect practical applications. (iii) High-quality pixel-level annotations:
Each image is manually annotated with precise object masks and corresponding
challenge attribute labels. We benchmark eleven representative COD methods on
MCOD, observing a consistent performance drop due to increased task difficulty.
Notably, integrating multispectral modalities substantially alleviates this
degradation, highlighting the value of spectral information in enhancing
detection robustness. We anticipate MCOD will provide a strong foundation for
future research in multispectral camouflaged object detection. The dataset is
publicly accessible at https://github.com/yl2900260-bit/MCOD.

</details>


### [141] [Overview of PlantCLEF 2024: multi-species plant identification in vegetation plot images](https://arxiv.org/abs/2509.15768)
*Herve Goeau,Vincent Espitalier,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: The paper discusses the PlantCLEF 2024 challenge, a multi-label classification task that uses AI to identify plant species in ecological plot images.


<details>
  <summary>Details</summary>
Motivation: The goal is to improve efficiency in ecological studies, enabling standardized sampling and biodiversity assessment with AI, thus extending the scale and scope of research.

Method: The challenge provides a dataset of 1.7 million single-label plant images, a test set with expert-annotated multi-label images, and pre-trained vision transformer models. Participants use these for a multi-label classification task.

Result: The paper details the methods and models used by participants along with their achieved results in the challenge.

Conclusion: AI has significant potential to enhance efficiency in ecological studies. The competition serves as a valuable benchmark for assessing current and emerging techniques in multi-label plant species classification.

Abstract: Plot images are essential for ecological studies, enabling standardized
sampling, biodiversity assessment, long-term monitoring and remote, large-scale
surveys. Plot images are typically fifty centimetres or one square meter in
size, and botanists meticulously identify all the species found there. The
integration of AI could significantly improve the efficiency of specialists,
helping them to extend the scope and coverage of ecological studies. To
evaluate advances in this regard, the PlantCLEF 2024 challenge leverages a new
test set of thousands of multi-label images annotated by experts and covering
over 800 species. In addition, it provides a large training set of 1.7 million
individual plant images as well as state-of-the-art vision transformer models
pre-trained on this data. The task is evaluated as a (weakly-labeled)
multi-label classification task where the aim is to predict all the plant
species present on a high-resolution plot image (using the single-label
training data). In this paper, we provide an detailed description of the data,
the evaluation methodology, the methods and models employed by the participants
and the results achieved.

</details>


### [142] [Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation](https://arxiv.org/abs/2509.15772)
*Weimin Bai,Yubo Li,Weijian Luo,Wenzheng Chen,He Sun*

Main category: cs.CV

TL;DR: The paper introduces VLM3D, a text-to-3D generation framework that uses large vision-language models as semantic and spatial priors to overcome limitations of SDS-based methods in fine semantic details and spatial accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing SDS-based text-to-3D methods face challenges in fine-grained semantic alignment and 3D spatial consistency due to their reliance on CLIP-style text encoders and 2D diffusion priors.

Method: The proposed VLM3D framework integrates vision-language models into the SDS pipeline, leveraging their superior language-grounded supervision and spatial understanding to address fine-grained prompts and geometric inconsistencies.

Result: VLM3D, instantiated with the Qwen2.5-VL model, outperforms previous SDS-based methods on the GPTeval3D benchmark across various metrics including semantic fidelity, geometric coherence, and spatial correctness.

Conclusion: VLM3D effectively resolves the limitations of existing SDS approaches, introducing a significant enhancement in both semantic and spatial quality for text-to-3D generation, especially in multi-object scenarios.

Abstract: Score Distillation Sampling (SDS) enables high-quality text-to-3D generation
by supervising 3D models through the denoising of multi-view 2D renderings,
using a pretrained text-to-image diffusion model to align with the input prompt
and ensure 3D consistency. However, existing SDS-based methods face two
fundamental limitations: (1) their reliance on CLIP-style text encoders leads
to coarse semantic alignment and struggles with fine-grained prompts; and (2)
2D diffusion priors lack explicit 3D spatial constraints, resulting in
geometric inconsistencies and inaccurate object relationships in multi-object
scenes. To address these challenges, we propose VLM3D, a novel text-to-3D
generation framework that integrates large vision-language models (VLMs) into
the SDS pipeline as differentiable semantic and spatial priors. Unlike standard
text-to-image diffusion priors, VLMs leverage rich language-grounded
supervision that enables fine-grained prompt alignment. Moreover, their
inherent vision language modeling provides strong spatial understanding, which
significantly enhances 3D consistency for single-object generation and improves
relational reasoning in multi-object scenes. We instantiate VLM3D based on the
open-source Qwen2.5-VL model and evaluate it on the GPTeval3D benchmark.
Experiments across diverse objects and complex scenes show that VLM3D
significantly outperforms prior SDS-based methods in semantic fidelity,
geometric coherence, and spatial correctness.

</details>


### [143] [Enriched Feature Representation and Motion Prediction Module for MOSEv2 Track of 7th LSVOS Challenge: 3rd Place Solution](https://arxiv.org/abs/2509.15781)
*Chang Soo Lim,Joonyoung Moon,Donghyeon Cho*

Main category: cs.CV

TL;DR: The paper introduces SCOPE, a video object segmentation framework combining the strengths of two existing methods, Cutie and SAM2, supplemented with a motion prediction module for stability.


<details>
  <summary>Details</summary>
Motivation: Video object segmentation is a challenging task relevant in areas like video editing and autonomous driving. Existing methods have limitations in feature capacity and temporal modeling, motivating the authors to combine their strengths.

Method: The authors replace the encoder in Cutie with the ViT encoder from SAM2 and add a motion prediction module for temporal stability. They also employ an ensemble strategy integrating Cutie, SAM2, and their proposed variant.

Result: The proposed SCOPE framework achieved 3rd place in the MOSEv2 track of the 7th Large-Scale Video Object Segmentation (LSVOS) Challenge.

Conclusion: SCOPE showcases the benefits of combining enriched feature representation and effective temporal modeling to achieve robust video object segmentation.

Abstract: Video object segmentation (VOS) is a challenging task with wide applications
such as video editing and autonomous driving. While Cutie provides strong
query-based segmentation and SAM2 offers enriched representations via a
pretrained ViT encoder, each has limitations in feature capacity and temporal
modeling. In this report, we propose a framework that integrates their
complementary strengths by replacing the encoder of Cutie with the ViT encoder
of SAM2 and introducing a motion prediction module for temporal stability. We
further adopt an ensemble strategy combining Cutie, SAM2, and our variant,
achieving 3rd place in the MOSEv2 track of the 7th LSVOS Challenge. We refer to
our final model as SCOPE (SAM2-CUTIE Object Prediction Ensemble). This
demonstrates the effectiveness of enriched feature representation and motion
prediction for robust video object segmentation. The code is available at
https://github.com/2025-LSVOS-3rd-place/MOSEv2_3rd_place.

</details>


### [144] [Ideal Registration? Segmentation is All You Need](https://arxiv.org/abs/2509.15784)
*Xiang Chen,Fengting Zhang,Qinghao Liu,Min Liu,Kun Wu,Yaonan Wang,Hang Zhang*

Main category: cs.CV

TL;DR: The study proposes SegReg, a deep learning framework that improves image registration by adaptively handling region-specific anatomical deformations, outperforming existing methods across multiple clinical scenarios.


<details>
  <summary>Details</summary>
Motivation: Current image registration methods struggle to handle regionally varying anatomical deformations due to globally uniform smoothness constraints.

Method: The SegReg framework segments medical images into subregions, applies registration to these localized areas, and integrates the partial deformation fields into a global field for adaptive anatomical motion handling.

Result: SegReg achieved 98.23% Dice score on critical anatomy alignment and showed a performance improvement of 2-12% over existing methods in cardiac, abdominal, and lung image registration.

Conclusion: SegReg introduces a segmentation-driven adaptive approach to make image registration more accurate and robust, effectively tying registration accuracy to segmentation quality.

Abstract: Deep learning has revolutionized image registration by its ability to handle
diverse tasks while achieving significant speed advantages over conventional
approaches. Current approaches, however, often employ globally uniform
smoothness constraints that fail to accommodate the complex, regionally varying
deformations characteristic of anatomical motion. To address this limitation,
we propose SegReg, a Segmentation-driven Registration framework that implements
anatomically adaptive regularization by exploiting region-specific deformation
patterns. Our SegReg first decomposes input moving and fixed images into
anatomically coherent subregions through segmentation. These localized domains
are then processed by the same registration backbone to compute optimized
partial deformation fields, which are subsequently integrated into a global
deformation field. SegReg achieves near-perfect structural alignment (98.23%
Dice on critical anatomies) using ground-truth segmentation, and outperforms
existing methods by 2-12% across three clinical registration scenarios
(cardiac, abdominal, and lung images) even with automatic segmentation. Our
SegReg demonstrates a near-linear dependence of registration accuracy on
segmentation quality, transforming the registration challenge into a
segmentation problem. The source code will be released upon manuscript
acceptance.

</details>


### [145] [CBPNet: A Continual Backpropagation Prompt Network for Alleviating Plasticity Loss on Edge Devices](https://arxiv.org/abs/2509.15785)
*Runjie Shao,Boyu Diao,Zijia An,Ruiqi Liu,Yongjun Xu*

Main category: cs.CV

TL;DR: The paper introduces CBPNet, a continual learning framework enhancing plasticity for edge devices, achieving state-of-the-art results on benchmarks with minimal additional parameters.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of frozen pretrained models and mitigate plasticity loss in continual learning on edge devices.

Method: Develop CBPNet and incorporate an Efficient CBP Block for adaptive reinitialization of underutilized parameters, boosting model plasticity.

Result: CBPNet improves average accuracy on Split CIFAR-100 by over 1% and achieves state-of-the-art accuracy (69.41%) on Split ImageNet-R while using less than 0.2% additional parameters.

Conclusion: CBPNet restores learning vitality in edge-device continual learning settings, proving its efficiency and effectiveness.

Abstract: To meet the demands of applications like robotics and autonomous driving that
require real-time responses to dynamic environments, efficient continual
learning methods suitable for edge devices have attracted increasing attention.
In this transition, using frozen pretrained models with prompts has become a
mainstream strategy to combat catastrophic forgetting. However, this approach
introduces a new critical bottleneck: plasticity loss, where the model's
ability to learn new knowledge diminishes due to the frozen backbone and the
limited capacity of prompt parameters. We argue that the reduction in
plasticity stems from a lack of update vitality in underutilized parameters
during the training process. To this end, we propose the Continual
Backpropagation Prompt Network (CBPNet), an effective and parameter efficient
framework designed to restore the model's learning vitality. We innovatively
integrate an Efficient CBP Block that counteracts plasticity decay by
adaptively reinitializing these underutilized parameters. Experimental results
on edge devices demonstrate CBPNet's effectiveness across multiple benchmarks.
On Split CIFAR-100, it improves average accuracy by over 1% against a strong
baseline, and on the more challenging Split ImageNet-R, it achieves a state of
the art accuracy of 69.41%. This is accomplished by training additional
parameters that constitute less than 0.2% of the backbone's size, validating
our approach.

</details>


### [146] [FoBa: A Foreground-Background co-Guided Method and New Benchmark for Remote Sensing Semantic Change Detection](https://arxiv.org/abs/2509.15788)
*Haotian Zhang,Han Guo,Keyan Chen,Hao Chen,Zhengxia Zou,Zhenwei Shi*

Main category: cs.CV

TL;DR: This paper introduces a new benchmark dataset, LevirSCD, and proposes a novel SCD method (FoBa) to tackle challenges in semantic change detection.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome challenges in remote sensing SCD, such as lack of diverse datasets and ineffective utilization of change information in existing methods.

Method: The authors create the LevirSCD dataset featuring fine-grained change categories and types, and propose the FoBa method using foreground-background guidance and a Gated Interaction Fusion module.

Result: FoBa exhibits improved performance compared to current SOTA methods, showing significant SeK metric gains on multiple datasets.

Conclusion: This paper provides both a new benchmark dataset and an effective SCD method, enhancing practical applications of semantic change detection in remote sensing.

Abstract: Despite the remarkable progress achieved in remote sensing semantic change
detection (SCD), two major challenges remain. At the data level, existing SCD
datasets suffer from limited change categories, insufficient change types, and
a lack of fine-grained class definitions, making them inadequate to fully
support practical applications. At the methodological level, most current
approaches underutilize change information, typically treating it as a
post-processing step to enhance spatial consistency, which constrains further
improvements in model performance. To address these issues, we construct a new
benchmark for remote sensing SCD, LevirSCD. Focused on the Beijing area, the
dataset covers 16 change categories and 210 specific change types, with more
fine-grained class definitions (e.g., roads are divided into unpaved and paved
roads). Furthermore, we propose a foreground-background co-guided SCD (FoBa)
method, which leverages foregrounds that focus on regions of interest and
backgrounds enriched with contextual information to guide the model
collaboratively, thereby alleviating semantic ambiguity while enhancing its
ability to detect subtle changes. Considering the requirements of bi-temporal
interaction and spatial consistency in SCD, we introduce a Gated Interaction
Fusion (GIF) module along with a simple consistency loss to further enhance the
model's detection performance. Extensive experiments on three datasets (SECOND,
JL1, and the proposed LevirSCD) demonstrate that FoBa achieves competitive
results compared to current SOTA methods, with improvements of 1.48%, 3.61%,
and 2.81% in the SeK metric, respectively. Our code and dataset are available
at https://github.com/zmoka-zht/FoBa.

</details>


### [147] [Minimal Semantic Sufficiency Meets Unsupervised Domain Generalization](https://arxiv.org/abs/2509.15791)
*Tan Pan,Kaiyu Guo,Dongli Xu,Zhaorui Tan,Chen Jiang,Deshu Chen,Xin Guo,Brian C. Lovell,Limei Han,Yuan Cheng,Mahsa Baktashmotlagh*

Main category: cs.CV

TL;DR: The paper addresses a new unsupervised domain generalization (UDG) framework, proposing a Minimal Sufficient Semantic Representation (MS-UDG) to improve generalization capabilities in self-supervised learning models by disentangling semantics from variations without relying on category or domain labels.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of current unsupervised domain generalization methods, particularly the dependency on domain labels and challenges in fully disentangling semantic information from irrelevant variations.

Method: MS-UDG employs a theoretical framework based on information theory principles, integrating InfoNCE-based objectives for semantic sufficiency and novel disentanglement and reconstruction mechanisms to achieve minimality in representation learning.

Result: The MS-UDG model achieves state-of-the-art performance on widely-recognized unsupervised domain-generalization benchmarks, outperforming existing methods like SSL and UDG even without using category or domain labels.

Conclusion: Optimizing for sufficiency and minimality in semantic representation improves out-of-distribution generalization, making MS-UDG a robust framework for real-world unsupervised learning tasks.

Abstract: The generalization ability of deep learning has been extensively studied in
supervised settings, yet it remains less explored in unsupervised scenarios.
Recently, the Unsupervised Domain Generalization (UDG) task has been proposed
to enhance the generalization of models trained with prevalent unsupervised
learning techniques, such as Self-Supervised Learning (SSL). UDG confronts the
challenge of distinguishing semantics from variations without category labels.
Although some recent methods have employed domain labels to tackle this issue,
such domain labels are often unavailable in real-world contexts. In this paper,
we address these limitations by formalizing UDG as the task of learning a
Minimal Sufficient Semantic Representation: a representation that (i) preserves
all semantic information shared across augmented views (sufficiency), and (ii)
maximally removes information irrelevant to semantics (minimality). We
theoretically ground these objectives from the perspective of information
theory, demonstrating that optimizing representations to achieve sufficiency
and minimality directly reduces out-of-distribution risk. Practically, we
implement this optimization through Minimal-Sufficient UDG (MS-UDG), a
learnable model by integrating (a) an InfoNCE-based objective to achieve
sufficiency; (b) two complementary components to promote minimality: a novel
semantic-variation disentanglement loss and a reconstruction-based mechanism
for capturing adequate variation. Empirically, MS-UDG sets a new
state-of-the-art on popular unsupervised domain-generalization benchmarks,
consistently outperforming existing SSL and UDG methods, without category or
domain labels during representation learning.

</details>


### [148] [Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks](https://arxiv.org/abs/2509.16163)
*Het Patel,Muzammil Allie,Qian Zhang,Jia Chen,Evangelos E. Papalexakis*

Main category: cs.CV

TL;DR: The paper proposes a tensor decomposition method to defend against adversarial attacks on vision-language models, achieving significant performance recovery without retraining or architectural changes.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models (VLMs) face vulnerabilities to adversarial attacks, necessitating an effective yet resource-efficient defense mechanism.

Method: The proposed defense utilizes tensor decomposition and reconstruction of vision encoder representations to filter adversarial noise. It does not require model retraining or changes to the architecture.

Result: Experiments with CLIP on datasets like COCO and Flickr30K demonstrate restored performance following adversarial attacks: 12.3% recovery on Flickr30K (Recall@1 raised from 7.5% to 19.8%) and 8.1% recovery on COCO (accuracy improved from 3.8% to 11.9%).

Conclusion: This tensor decomposition method is lightweight, practical, and enhances robustness against adversarial attacks for pre-trained VLMs with minimal performance overhead.

Abstract: Vision language models (VLMs) excel in multimodal understanding but are prone
to adversarial attacks. Existing defenses often demand costly retraining or
significant architecture changes. We introduce a lightweight defense using
tensor decomposition suitable for any pre-trained VLM, requiring no retraining.
By decomposing and reconstructing vision encoder representations, it filters
adversarial noise while preserving meaning. Experiments with CLIP on COCO and
Flickr30K show improved robustness. On Flickr30K, it restores 12.3\%
performance lost to attacks, raising Recall@1 accuracy from 7.5\% to 19.8\%. On
COCO, it recovers 8.1\% performance, improving accuracy from 3.8\% to 11.9\%.
Analysis shows Tensor Train decomposition with low rank (8-32) and low residual
strength ($\alpha=0.1-0.2$) is optimal. This method is a practical,
plug-and-play solution with minimal overhead for existing VLMs.

</details>


### [149] [TASAM: Terrain-and-Aware Segment Anything Model for Temporal-Scale Remote Sensing Segmentation](https://arxiv.org/abs/2509.15795)
*Tianyang Wang,Xi Xiao,Gaofei Chen,Hanzhang Chi,Qi Zhang,Guo Cheng,Yingrui Ji*

Main category: cs.CV

TL;DR: This paper introduces TASAM, an extension of the Segment Anything Model (SAM) tailored for remote sensing image segmentation, overcoming challenges like complex terrain and temporal dynamics.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of SAM in remote sensing data segmentation, focusing on challenges such as terrain complexity, varying scales, and time-dependent changes.

Method: TASAM integrates three modules: (1) a terrain-aware adapter adding elevation priors, (2) a temporal prompt generator accounting for land-cover changes, and (3) a multi-scale fusion strategy for detailed object delineation, all without retraining the SAM backbone.

Result: TASAM shows significant performance improvements on three remote sensing benchmarks—LoveDA, iSAID, and WHU-CD—outperforming SAM and task-specific models with minimal computational burden.

Conclusion: The approach demonstrates the effectiveness of domain-specific adaptations for foundation models, providing an efficient and scalable solution for remote sensing segmentation challenges.

Abstract: Segment Anything Model (SAM) has demonstrated impressive zero-shot
segmentation capabilities across natural image domains, but it struggles to
generalize to the unique challenges of remote sensing data, such as complex
terrain, multi-scale objects, and temporal dynamics. In this paper, we
introduce TASAM, a terrain and temporally-aware extension of SAM designed
specifically for high-resolution remote sensing image segmentation. TASAM
integrates three lightweight yet effective modules: a terrain-aware adapter
that injects elevation priors, a temporal prompt generator that captures
land-cover changes over time, and a multi-scale fusion strategy that enhances
fine-grained object delineation. Without retraining the SAM backbone, our
approach achieves substantial performance gains across three remote sensing
benchmarks-LoveDA, iSAID, and WHU-CD-outperforming both zero-shot SAM and
task-specific models with minimal computational overhead. Our results highlight
the value of domain-adaptive augmentation for foundation models and offer a
scalable path toward more robust geospatial segmentation.

</details>


### [150] [ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding](https://arxiv.org/abs/2509.15800)
*Kehua Chen*

Main category: cs.CV

TL;DR: This paper proposes ChronoForge-RL, a framework for video understanding that addresses inefficiencies in frame processing and keyframe selection.


<details>
  <summary>Details</summary>
Motivation: The video understanding field faces challenges of computational inefficiency and identifying meaningful keyframes through naive strategies.

Method: ChronoForge-RL introduces Temporal Apex Distillation (TAD) for keyframe selection and KeyFrame-aware Group Relative Policy Optimization (KF-GRPO) for temporal reasoning.

Result: ChronoForge-RL achieves superior accuracy (69.1% on VideoMME and 52.7% on LVBench) and computational efficiency, outperforming models with far larger parameters.

Conclusion: The proposed framework significantly improves video understanding, offering scalable solutions for efficient processing and robust performance.

Abstract: Current state-of-the-art video understanding methods typically struggle with
two critical challenges: (1) the computational infeasibility of processing
every frame in dense video content and (2) the difficulty in identifying
semantically significant frames through naive uniform sampling strategies. In
this paper, we propose a novel video understanding framework, called
ChronoForge-RL, which combines Temporal Apex Distillation (TAD) and
KeyFrame-aware Group Relative Policy Optimization (KF-GRPO) to tackle these
issues. Concretely, we introduce a differentiable keyframe selection mechanism
that systematically identifies semantic inflection points through a three-stage
process to enhance computational efficiency while preserving temporal
information. Then, two particular modules are proposed to enable effective
temporal reasoning: Firstly, TAD leverages variation scoring, inflection
detection, and prioritized distillation to select the most informative frames.
Secondly, we introduce KF-GRPO which implements a contrastive learning paradigm
with a saliency-enhanced reward mechanism that explicitly incentivizes models
to leverage both frame content and temporal relationships. Finally, our
proposed ChronoForge-RL achieves 69.1% on VideoMME and 52.7% on LVBench
compared to baseline methods, clearly surpassing previous approaches while
enabling our 7B parameter model to achieve performance comparable to 72B
parameter alternatives.

</details>


### [151] [MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer](https://arxiv.org/abs/2509.16197)
*Yanghao Li,Rui Qian,Bowen Pan,Haotian Zhang,Haoshuo Huang,Bowen Zhang,Jialing Tong,Haoxuan You,Xianzhi Du,Zhe Gan,Hyunjik Kim,Chao Jia,Zhenbang Wang,Yinfei Yang,Mingfei Gao,Zi-Yi Dou,Wenze Hu,Chang Gao,Dongxu Li,Philipp Dufter,Zirui Wang,Guoli Yin,Zhengdong Zhang,Chen Chen,Yang Zhao,Ruoming Pang,Zhifeng Chen*

Main category: cs.CV

TL;DR: Manzano is a unified multimodal LLM that can understand and generate visual content without the performance trade-offs of past open-source models.


<details>
  <summary>Details</summary>
Motivation: To resolve performance trade-offs in multimodal large language models capable of both understanding and generating visual content.

Method: Manzano employs a hybrid image tokenizer, a shared vision encoder, lightweight adapters, a unified autoregressive LLM, and an auxiliary diffusion decoder, coupled with a unified training recipe.

Result: Manzano demonstrates state-of-the-art performance among unified models and competitive results with specialist models, especially on text-rich tasks.

Conclusion: The design choice of a hybrid tokenizer reduces task conflicts and scales effectively, enabling balanced and scalable learning of both understanding and generation capabilities.

Abstract: Unified multimodal Large Language Models (LLMs) that can both understand and
generate visual content hold immense potential. However, existing open-source
models often suffer from a performance trade-off between these capabilities. We
present Manzano, a simple and scalable unified framework that substantially
reduces this tension by coupling a hybrid image tokenizer with a well-curated
training recipe. A single shared vision encoder feeds two lightweight adapters
that produce continuous embeddings for image-to-text understanding and discrete
tokens for text-to-image generation within a common semantic space. A unified
autoregressive LLM predicts high-level semantics in the form of text and image
tokens, with an auxiliary diffusion decoder subsequently translating the image
tokens into pixels. The architecture, together with a unified training recipe
over understanding and generation data, enables scalable joint learning of both
capabilities. Manzano achieves state-of-the-art results among unified models,
and is competitive with specialist models, particularly on text-rich
evaluation. Our studies show minimal task conflicts and consistent gains from
scaling model size, validating our design choice of a hybrid tokenizer.

</details>


### [152] [CIDER: A Causal Cure for Brand-Obsessed Text-to-Image Models](https://arxiv.org/abs/2509.15803)
*Fangjian Shen,Zifeng Liang,Chao Wang,Wushao Wen*

Main category: cs.CV

TL;DR: The paper proposes CIDER, a framework to mitigate "brand bias" in text-to-image (T2I) models by refining prompts at inference-time without retraining, maintaining image quality and neutrality.


<details>
  <summary>Details</summary>
Motivation: Text-to-image (T2I) models have an unexplored "brand bias" issue, where they over-represent dominant commercial brands, raising ethical and legal concerns.

Method: The proposed CIDER framework uses a lightweight detector to identify branded content and a Vision-Language Model (VLM) to generate stylistically diverse, unbiased alternatives. It requires no retraining of the T2I model.

Result: Extensive experiments demonstrate that CIDER effectively reduces both explicit and implicit brand biases in T2I models while preserving image quality and aesthetic appeal.

Conclusion: CIDER offers a practical, model-agnostic solution to address brand bias, enabling the creation of more equitable and trustworthy generative AI systems.

Abstract: Text-to-image (T2I) models exhibit a significant yet under-explored "brand
bias", a tendency to generate contents featuring dominant commercial brands
from generic prompts, posing ethical and legal risks. We propose CIDER, a
novel, model-agnostic framework to mitigate bias at inference-time through
prompt refinement to avoid costly retraining. CIDER uses a lightweight detector
to identify branded content and a Vision-Language Model (VLM) to generate
stylistically divergent alternatives. We introduce the Brand Neutrality Score
(BNS) to quantify this issue and perform extensive experiments on leading T2I
models. Results show CIDER significantly reduces both explicit and implicit
biases while maintaining image quality and aesthetic appeal. Our work offers a
practical solution for more original and equitable content, contributing to the
development of trustworthy generative AI.

</details>


### [153] [Boosting Active Learning with Knowledge Transfer](https://arxiv.org/abs/2509.15805)
*Tianyang Wang,Xi Xiao,Gaofei Chen,Xiaoying Liao,Guo Cheng,Yingrui Ji*

Main category: cs.CV

TL;DR: The paper proposes a knowledge transfer-based method for uncertainty estimation in Active Learning (AL) using a teacher-student model, validated on computer vision tasks and cryo-ET challenges.


<details>
  <summary>Details</summary>
Motivation: Existing methods for uncertainty estimation in AL often rely on complex auxiliary models and advanced training setups, which are challenging to design and train, especially for domain-specific tasks like cryo-ET in computational biology.

Method: The authors introduce a teacher-student framework where the teacher represents the task model in AL, and the student is an auxiliary task-agnostic model learning from the teacher. Both models are trained simultaneously, with uncertainty measured based on the distance between their outputs.

Result: The proposed method demonstrates effectiveness and efficiency through extensive experiments on standard computer vision tasks and the domain-specific cryo-ET challenges.

Conclusion: The method simplifies uncertainty estimation in AL, providing a generalizable and task-agnostic approach without requiring complex training mechanisms, making it practical for a wide range of tasks.

Abstract: Uncertainty estimation is at the core of Active Learning (AL). Most existing
methods resort to complex auxiliary models and advanced training fashions to
estimate uncertainty for unlabeled data. These models need special design and
hence are difficult to train especially for domain tasks, such as Cryo-Electron
Tomography (cryo-ET) classification in computational biology. To address this
challenge, we propose a novel method using knowledge transfer to boost
uncertainty estimation in AL. Specifically, we exploit the teacher-student mode
where the teacher is the task model in AL and the student is an auxiliary model
that learns from the teacher. We train the two models simultaneously in each AL
cycle and adopt a certain distance between the model outputs to measure
uncertainty for unlabeled data. The student model is task-agnostic and does not
rely on special training fashions (e.g. adversarial), making our method
suitable for various tasks. More importantly, we demonstrate that data
uncertainty is not tied to concrete value of task loss but closely related to
the upper-bound of task loss. We conduct extensive experiments to validate the
proposed method on classical computer vision tasks and cryo-ET challenges. The
results demonstrate its efficacy and efficiency.

</details>


### [154] [LC-SLab -- An Object-based Deep Learning Framework for Large-scale Land Cover Classification from Satellite Imagery and Sparse In-situ Labels](https://arxiv.org/abs/2509.15868)
*Johannes Leonhardt,Juergen Gall,Ribana Roscher*

Main category: cs.CV

TL;DR: The paper introduces LC-SLab, a deep learning framework for object-based land cover classification to enhance accuracy and map coherence in scenarios of sparse supervision.


<details>
  <summary>Details</summary>
Motivation: The motivation for this research is the limitations of current deep learning approaches in land cover mapping, which are prone to fragmented and noisy predictions due to sparse spatial coverage of open in-situ datasets.

Method: The method involves LC-SLab, a deep learning framework that utilizes object-based classification with graph neural networks and pre-trained features, and supports both input-level and output-level aggregation strategies.

Result: Object-based methods achieved comparable or better accuracy than pixel-wise models and generated more coherent maps. Input-level aggregation was more effective for small datasets, while output-level aggregation excelled with larger datasets.

Conclusion: LC-SLab demonstrates practical utility by outperforming existing land cover products and improving both accuracy and map coherence in scenarios with sparse supervision.

Abstract: Large-scale land cover maps generated using deep learning play a critical
role across a wide range of Earth science applications. Open in-situ datasets
from principled land cover surveys offer a scalable alternative to manual
annotation for training such models. However, their sparse spatial coverage
often leads to fragmented and noisy predictions when used with existing deep
learning-based land cover mapping approaches. A promising direction to address
this issue is object-based classification, which assigns labels to semantically
coherent image regions rather than individual pixels, thereby imposing a
minimum mapping unit. Despite this potential, object-based methods remain
underexplored in deep learning-based land cover mapping pipelines, especially
in the context of medium-resolution imagery and sparse supervision. To address
this gap, we propose LC-SLab, the first deep learning framework for
systematically exploring object-based deep learning methods for large-scale
land cover classification under sparse supervision. LC-SLab supports both
input-level aggregation via graph neural networks, and output-level aggregation
by postprocessing results from established semantic segmentation models.
Additionally, we incorporate features from a large pre-trained network to
improve performance on small datasets. We evaluate the framework on annual
Sentinel-2 composites with sparse LUCAS labels, focusing on the tradeoff
between accuracy and fragmentation, as well as sensitivity to dataset size. Our
results show that object-based methods can match or exceed the accuracy of
common pixel-wise models while producing substantially more coherent maps.
Input-level aggregation proves more robust on smaller datasets, whereas
output-level aggregation performs best with more data. Several configurations
of LC-SLab also outperform existing land cover products, highlighting the
framework's practical utility.

</details>


### [155] [Zero-Shot Visual Grounding in 3D Gaussians via View Retrieval](https://arxiv.org/abs/2509.15871)
*Liwei Liao,Xufeng Li,Xiaoyun Zheng,Boning Liu,Feng Gao,Ronggang Wang*

Main category: cs.CV

TL;DR: The paper introduces GVR, a zero-shot framework transforming 3D visual grounding into 2D retrieval for 3D Gaussian Splatting, overcoming per-scene training and annotation demands.


<details>
  <summary>Details</summary>
Motivation: Address challenges in 3D visual grounding, notably issues with spatial texture representation in 3D Gaussian Splatting and the reliance on large annotated datasets.

Method: The GVR framework uses object-level view retrieval across multiple views, bypassing the need for detailed 3D annotations and per-scene training.

Result: Experiments show that GVR achieves state-of-the-art performance in 3D visual grounding while avoiding per-scene training.

Conclusion: GVR lays a strong foundation for zero-shot 3D visual grounding research by addressing major limitations and demonstrating robust performance.

Abstract: 3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on text
prompts, which is essential for applications such as robotics. However,
existing 3DVG methods encounter two main challenges: first, they struggle to
handle the implicit representation of spatial textures in 3D Gaussian Splatting
(3DGS), making per-scene training indispensable; second, they typically require
larges amounts of labeled data for effective training. To this end, we propose
\underline{G}rounding via \underline{V}iew \underline{R}etrieval (GVR), a novel
zero-shot visual grounding framework for 3DGS to transform 3DVG as a 2D
retrieval task that leverages object-level view retrieval to collect grounding
clues from multiple views, which not only avoids the costly process of 3D
annotation, but also eliminates the need for per-scene training. Extensive
experiments demonstrate that our method achieves state-of-the-art visual
grounding performance while avoiding per-scene training, providing a solid
foundation for zero-shot 3DVG research. Video demos can be found in
https://github.com/leviome/GVR_demos.

</details>


### [156] [ENSAM: an efficient foundation model for interactive segmentation of 3D medical images](https://arxiv.org/abs/2509.15874)
*Elias Stenhede,Agnar Martin Bjørnstad,Arian Ranjbar*

Main category: cs.CV

TL;DR: ENSAM is a lightweight model for universal 3D medical image segmentation designed to work well with limited data and computational resources, outperforming other baseline models in key metrics.


<details>
  <summary>Details</summary>
Motivation: To address the need for an efficient, promptable model capable of accurate 3D medical image segmentation across multiple modalities despite limited computational and data resources.

Method: ENSAM employs a SegResNet-based encoder combined with a prompt encoder and mask decoder in a U-Net-style architecture. It incorporates latent cross-attention, relative positional encoding, normalized attention, and the Muon optimizer for enhanced training efficiency.

Result: The model achieved competitive performance metrics (e.g., DSC AUC of 2.404, final DSC of 0.627) and outperformed or matched baseline models in the CVPR 2025 segmentation challenge, ranking 5th overall and first among models without pretrained weights.

Conclusion: ENSAM demonstrates its suitability for efficient 3D medical image segmentation with limited resources, with key components like relative positional encodings and the Muon optimizer playing critical roles in performance improvements.

Abstract: We present ENSAM (Equivariant, Normalized, Segment Anything Model), a
lightweight and promptable model for universal 3D medical image segmentation.
ENSAM combines a SegResNet-based encoder with a prompt encoder and mask decoder
in a U-Net-style architecture, using latent cross-attention, relative
positional encoding, normalized attention, and the Muon optimizer for training.
ENSAM is designed to achieve good performance under limited data and
computational budgets, and is trained from scratch on under 5,000 volumes from
multiple modalities (CT, MRI, PET, ultrasound, microscopy) on a single 32 GB
GPU in 6 hours. As part of the CVPR 2025 Foundation Models for Interactive 3D
Biomedical Image Segmentation Challenge, ENSAM was evaluated on hidden test set
with multimodal 3D medical images, obtaining a DSC AUC of 2.404, NSD AUC of
2.266, final DSC of 0.627, and final NSD of 0.597, outperforming two previously
published baseline models (VISTA3D, SAM-Med3D) and matching the third (SegVol),
surpassing its performance in final DSC but trailing behind in the other three
metrics. In the coreset track of the challenge, ENSAM ranks 5th of 10 overall
and best among the approaches not utilizing pretrained weights. Ablation
studies confirm that our use of relative positional encodings and the Muon
optimizer each substantially speed up convergence and improve segmentation
quality.

</details>


### [157] [Self-Supervised Cross-Modal Learning for Image-to-Point Cloud Registration](https://arxiv.org/abs/2509.15882)
*Xingmei Wang,Xiaoyu Hu,Chengkai Huang,Ziyan Zeng,Guohao Nie,Quan Z. Sheng,Lina Yao*

Main category: cs.CV

TL;DR: The paper addresses challenges in image-to-point cloud registration for autonomous systems by introducing CrossI2P, a self-supervised framework that improves accuracy and robustness through a novel pipeline combining cross-modal learning and two-stage registration.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle to accurately register 2D textures from images to 3D structures from point clouds due to semantic-geometric differences and issues with local optima.

Method: CrossI2P employs contrastive learning for feature embeddings, a coarse-to-fine registration system with two stages, and dynamically balanced training mechanisms to align 2D and 3D sensor modalities effectively.

Result: CrossI2P achieves substantial performance gains, outperforming previous approaches by 23.7% on KITTI Odometry and by 37.9% on nuScenes benchmarks.

Conclusion: The proposed framework successfully bridges 2D and 3D perceptions, enhancing robustness and precision in autonomous systems through innovative alignment and registration processes.

Abstract: Bridging 2D and 3D sensor modalities is critical for robust perception in
autonomous systems. However, image-to-point cloud (I2P) registration remains
challenging due to the semantic-geometric gap between texture-rich but
depth-ambiguous images and sparse yet metrically precise point clouds, as well
as the tendency of existing methods to converge to local optima. To overcome
these limitations, we introduce CrossI2P, a self-supervised framework that
unifies cross-modal learning and two-stage registration in a single end-to-end
pipeline. First, we learn a geometric-semantic fused embedding space via
dual-path contrastive learning, enabling annotation-free, bidirectional
alignment of 2D textures and 3D structures. Second, we adopt a coarse-to-fine
registration paradigm: a global stage establishes superpoint-superpixel
correspondences through joint intra-modal context and cross-modal interaction
modeling, followed by a geometry-constrained point-level refinement for precise
registration. Third, we employ a dynamic training mechanism with gradient
normalization to balance losses for feature alignment, correspondence
refinement, and pose estimation. Extensive experiments demonstrate that
CrossI2P outperforms state-of-the-art methods by 23.7% on the KITTI Odometry
benchmark and by 37.9% on nuScenes, significantly improving both accuracy and
robustness.

</details>


### [158] [RACap: Relation-Aware Prompting for Lightweight Retrieval-Augmented Image Captioning](https://arxiv.org/abs/2509.15883)
*Xiaosheng Long,Hanyu Wang,Zhentao Song,Kun Luo,Hongde Liu*

Main category: cs.CV

TL;DR: RACap is a retrieval-augmented image captioning model that enhances semantic understanding by incorporating structured relational semantics and heterogeneous object identification.


<details>
  <summary>Details</summary>
Motivation: Current image captioning methods fail at fine-grained relationship modeling and explicit semantic relationships between image objects.

Method: RACap introduces structured relation semantics from retrieval captions and identifies various objects in images to enhance relational understanding.

Result: RACap achieves superior performance over previous lightweight models, using only 10.8M trainable parameters.

Conclusion: RACap effectively addresses the limitations in current models, offering improved semantic consistency and relational expressiveness in image captioning.

Abstract: Recent retrieval-augmented image captioning methods incorporate external
knowledge to compensate for the limitations in comprehending complex scenes.
However, current approaches face challenges in relation modeling: (1) the
representation of semantic prompts is too coarse-grained to capture
fine-grained relationships; (2) these methods lack explicit modeling of image
objects and their semantic relationships. To address these limitations, we
propose RACap, a relation-aware retrieval-augmented model for image captioning,
which not only mines structured relation semantics from retrieval captions, but
also identifies heterogeneous objects from the image. RACap effectively
retrieves structured relation features that contain heterogeneous visual
information to enhance the semantic consistency and relational expressiveness.
Experimental results show that RACap, with only 10.8M trainable parameters,
achieves superior performance compared to previous lightweight captioning
models.

</details>


### [159] [RangeSAM: Leveraging Visual Foundation Models for Range-View repesented LiDAR segmentation](https://arxiv.org/abs/2509.15886)
*Paul Julius Kühn,Duc Anh Nguyen,Arjan Kuijper,Holger Graf,Dieter Fellner,Saptarshi Neil Sinha*

Main category: cs.CV

TL;DR: The paper introduces a novel method for LiDAR point cloud segmentation by adapting VFMs, specifically SAM2, to operate in the range view, demonstrating competitive performance and offering scalability advantages.


<details>
  <summary>Details</summary>
Motivation: To overcome the computational and real-time inefficiency of voxel- and point-based methods, and to explore the potential of range-view methods empowered by VFMs for 3D segmentation.

Method: The authors adapt SAM2, a state-of-the-art VFM, to range view LiDAR segmentation. Architectural modifications include emphasizing horizontal spatial dependencies, customizing encoder configurations for spherical projections, and adapting mechanisms to capture unique spatial patterns and discontinuities.

Result: The framework achieves competitive results on SemanticKITTI benchmark while retaining the efficiency and simplicity of 2D-centric approaches.

Conclusion: This study demonstrates the potential of VFMs as versatile backbones for 3D segmentation and marks progress towards unified, foundation-model-driven LiDAR segmentation.

Abstract: Point cloud segmentation is central to autonomous driving and 3D scene
understanding. While voxel- and point-based methods dominate recent research
due to their compatibility with deep architectures and ability to capture
fine-grained geometry, they often incur high computational cost, irregular
memory access, and limited real-time efficiency. In contrast, range-view
methods, though relatively underexplored - can leverage mature 2D semantic
segmentation techniques for fast and accurate predictions. Motivated by the
rapid progress in Visual Foundation Models (VFMs) for captioning, zero-shot
recognition, and multimodal tasks, we investigate whether SAM2, the current
state-of-the-art VFM for segmentation tasks, can serve as a strong backbone for
LiDAR point cloud segmentation in the range view. We present , to our
knowledge, the first range-view framework that adapts SAM2 to 3D segmentation,
coupling efficient 2D feature extraction with standard
projection/back-projection to operate on point clouds. To optimize SAM2 for
range-view representations, we implement several architectural modifications to
the encoder: (1) a novel module that emphasizes horizontal spatial dependencies
inherent in LiDAR range images, (2) a customized configuration of tailored to
the geometric properties of spherical projections, and (3) an adapted mechanism
in the encoder backbone specifically designed to capture the unique spatial
patterns and discontinuities present in range-view pseudo-images. Our approach
achieves competitive performance on SemanticKITTI while benefiting from the
speed, scalability, and deployment simplicity of 2D-centric pipelines. This
work highlights the viability of VFMs as general-purpose backbones for 3D
perception and opens a path toward unified, foundation-model-driven LiDAR
segmentation. Results lets us conclude that range-view segmentation methods
using VFMs leads to promising results.

</details>


### [160] [Global Regulation and Excitation via Attention Tuning for Stereo Matching](https://arxiv.org/abs/2509.15891)
*Jiahao Li,Xinhong Chen,Zhengmin Jiang,Qian Zhou,Yung-Hui Li,Jianping Wang*

Main category: cs.CV

TL;DR: The paper introduces the GREAT framework, enhancing stereo-matching methods with global context and geometric information for better performance in challenging ill-posed regions.


<details>
  <summary>Details</summary>
Motivation: Existing iterative stereo-matching algorithms struggle in ill-posed regions due to lack of global context and geometric detail in their refinement processes.

Method: The GREAT framework introduces three types of attention modules: Spatial Attention (SA), Matching Attention (MA), and Volume Attention (VA), which together enhance the robustness of the cost-volume by utilizing global context and geometric details.

Result: The GREAT framework significantly improves the performance of stereo-matching algorithms in ill-posed regions. GREAT-IGEV achieves top rankings on major benchmarks like Scene Flow, KITTI 2015, and ETH3D, and second place on the Middlebury benchmark.

Conclusion: GREAT effectively boosts the performance of iterative stereo-matching algorithms by incorporating global and geometric contexts, proving its universality and effectiveness across methods.

Abstract: Stereo matching achieves significant progress with iterative algorithms like
RAFT-Stereo and IGEV-Stereo. However, these methods struggle in ill-posed
regions with occlusions, textureless, or repetitive patterns, due to a lack of
global context and geometric information for effective iterative refinement. To
enable the existing iterative approaches to incorporate global context, we
propose the Global Regulation and Excitation via Attention Tuning (GREAT)
framework which encompasses three attention modules. Specifically, Spatial
Attention (SA) captures the global context within the spatial dimension,
Matching Attention (MA) extracts global context along epipolar lines, and
Volume Attention (VA) works in conjunction with SA and MA to construct a more
robust cost-volume excited by global context and geometric details. To verify
the universality and effectiveness of this framework, we integrate it into
several representative iterative stereo-matching methods and validate it
through extensive experiments, collectively denoted as GREAT-Stereo. This
framework demonstrates superior performance in challenging ill-posed regions.
Applied to IGEV-Stereo, among all published methods, our GREAT-IGEV ranks first
on the Scene Flow test set, KITTI 2015, and ETH3D leaderboards, and achieves
second on the Middlebury benchmark. Code is available at
https://github.com/JarvisLee0423/GREAT-Stereo.

</details>


### [161] [Deep Feedback Models](https://arxiv.org/abs/2509.15905)
*David Calhas,Arlindo L. Oliveira*

Main category: cs.CV

TL;DR: Deep Feedback Models (DFMs) enhance neural network performance by iteratively refining internal states via feedback, demonstrating superiority in noisy or data-limited settings.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve neural networks' robustness to noise and ability to generalize with limited data by mimicking biological decision-making mechanisms with feedback loops.

Method: DFMs introduce a feedback mechanism modeled as a differential equation within a recurrent neural network, stabilized using exponential decay for convergence. Their effectiveness is evaluated in object recognition, segmentation tasks, and medical imaging.

Result: DFMs outperform traditional feedforward architectures, particularly under low data or high noise conditions, while also showing robustness in medical imaging against various noise corruptions.

Conclusion: The study highlights the critical role of feedback in neural networks for achieving robustness, stability, and generalizable learning capabilities.

Abstract: Deep Feedback Models (DFMs) are a new class of stateful neural networks that
combine bottom up input with high level representations over time. This
feedback mechanism introduces dynamics into otherwise static architectures,
enabling DFMs to iteratively refine their internal state and mimic aspects of
biological decision making. We model this process as a differential equation
solved through a recurrent neural network, stabilized via exponential decay to
ensure convergence. To evaluate their effectiveness, we measure DFMs under two
key conditions: robustness to noise and generalization with limited data. In
both object recognition and segmentation tasks, DFMs consistently outperform
their feedforward counterparts, particularly in low data or high noise regimes.
In addition, DFMs translate to medical imaging settings, while being robust
against various types of noise corruption. These findings highlight the
importance of feedback in achieving stable, robust, and generalizable learning.
Code is available at https://github.com/DCalhas/deep_feedback_models.

</details>


### [162] [Sparse Multiview Open-Vocabulary 3D Detection](https://arxiv.org/abs/2509.15924)
*Olivier Moliner,Viktor Larsson,Kalle Åström*

Main category: cs.CV

TL;DR: The paper addresses open-vocabulary 3D object detection using limited RGB images, employing a training-free approach via pre-trained 2D models to achieve competitive performance without costly 3D learning.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D object detection relies on fixed categories and extensive 3D-specific learning, which limits flexibility and practicality in scenarios with sparse inputs.

Method: The approach uses pre-trained 2D foundation models to lift 2D detections into 3D proposals and optimizes them for featuremetric consistency across sparse views, eliminating the need for complex 3D-specific techniques.

Result: The proposed method performs competitively with state-of-the-art techniques in densely sampled setups and significantly outperforms them in sparse-view scenarios.

Conclusion: The training-free pipeline leverages existing 2D models effectively, enabling robust open-vocabulary 3D object detection in challenging sparse-view settings.

Abstract: The ability to interpret and comprehend a 3D scene is essential for many
vision and robotics systems. In numerous applications, this involves 3D object
detection, i.e.~identifying the location and dimensions of objects belonging to
a specific category, typically represented as bounding boxes. This has
traditionally been solved by training to detect a fixed set of categories,
which limits its use. In this work, we investigate open-vocabulary 3D object
detection in the challenging yet practical sparse-view setting, where only a
limited number of posed RGB images are available as input. Our approach is
training-free, relying on pre-trained, off-the-shelf 2D foundation models
instead of employing computationally expensive 3D feature fusion or requiring
3D-specific learning. By lifting 2D detections and directly optimizing 3D
proposals for featuremetric consistency across views, we fully leverage the
extensive training data available in 2D compared to 3D. Through standard
benchmarks, we demonstrate that this simple pipeline establishes a powerful
baseline, performing competitively with state-of-the-art techniques in densely
sampled scenarios while significantly outperforming them in the sparse-view
setting.

</details>


### [163] [PAN: Pillars-Attention-Based Network for 3D Object Detection](https://arxiv.org/abs/2509.15935)
*Ruan Bispo,Dane Mitrev,Letizia Mariotti,Clément Botty,Denver Humphrey,Anthony Scanlan,Ciarán Eising*

Main category: cs.CV

TL;DR: This paper introduces a novel camera-radar fusion architecture for 3D object detection in real-time by utilizing radar point cloud advantages like distance estimation and speed information.


<details>
  <summary>Details</summary>
Motivation: To develop a low-cost, real-time, and robust alternative for 3D object detection that performs well under adverse weather and lighting conditions, leveraging the advantages of radar data.

Method: The authors propose a new architecture with a backbone for mapping radar pillar features and introduce a self-attention mechanism to model radar point dependencies. They simplify the convolutional layer to improve inference time.

Result: The proposed method achieved state-of-the-art performance with an NDS metric of 58.2 using ResNet-50 and set a new benchmark for inference time on the nuScenes dataset.

Conclusion: The study showcases a significant advancement in camera-radar fusion for 3D object detection, achieving both accuracy and efficiency and providing a feasible alternative to camera-lidar fusion.

Abstract: Camera-radar fusion offers a robust and low-cost alternative to Camera-lidar
fusion for the 3D object detection task in real-time under adverse weather and
lighting conditions. However, currently, in the literature, it is possible to
find few works focusing on this modality and, most importantly, developing new
architectures to explore the advantages of the radar point cloud, such as
accurate distance estimation and speed information. Therefore, this work
presents a novel and efficient 3D object detection algorithm using cameras and
radars in the bird's-eye-view (BEV). Our algorithm exploits the advantages of
radar before fusing the features into a detection head. A new backbone is
introduced, which maps the radar pillar features into an embedded dimension. A
self-attention mechanism allows the backbone to model the dependencies between
the radar points. We are using a simplified convolutional layer to replace the
FPN-based convolutional layers used in the PointPillars-based architectures
with the main goal of reducing inference time. Our results show that with this
modification, our approach achieves the new state-of-the-art in the 3D object
detection problem, reaching 58.2 of the NDS metric for the use of ResNet-50,
while also setting a new benchmark for inference time on the nuScenes dataset
for the same category.

</details>


### [164] [A multi-temporal multi-spectral attention-augmented deep convolution neural network with contrastive learning for crop yield prediction](https://arxiv.org/abs/2509.15966)
*Shalini Dangi,Surya Karthikeya Mullapudi,Chandravardhan Singh Raghaw,Shahid Shafi Dar,Mohammad Zia Ur Rehman,Nagendra Kumar*

Main category: cs.CV

TL;DR: The paper introduces MTMS-YieldNet, a novel neural network for precise agricultural yield prediction, leveraging multi-temporal and multi-spectral remote sensing data.


<details>
  <summary>Details</summary>
Motivation: Climate change impacts agricultural yield prediction by altering conditions like weather and soil, necessitating improved predictive tools.

Method: The proposed MTMS-YieldNet integrates multi-spectral and spatio-temporal data using contrastive learning to capture spatial-spectral patterns and dependencies.

Result: MTMS-YieldNet outperforms seven state-of-the-art methods with low MAPE scores (0.336 on Sentinel-1, 0.353 on Landsat-8, 0.331 on Sentinel-2), ensuring better yield predictions.

Conclusion: MTMS-YieldNet significantly enhances yield prediction accuracy, aiding farmers with impactful insights for improved agricultural decision-making.

Abstract: Precise yield prediction is essential for agricultural sustainability and
food security. However, climate change complicates accurate yield prediction by
affecting major factors such as weather conditions, soil fertility, and farm
management systems. Advances in technology have played an essential role in
overcoming these challenges by leveraging satellite monitoring and data
analysis for precise yield estimation. Current methods rely on spatio-temporal
data for predicting crop yield, but they often struggle with multi-spectral
data, which is crucial for evaluating crop health and growth patterns. To
resolve this challenge, we propose a novel Multi-Temporal Multi-Spectral Yield
Prediction Network, MTMS-YieldNet, that integrates spectral data with
spatio-temporal information to effectively capture the correlations and
dependencies between them. While existing methods that rely on pre-trained
models trained on general visual data, MTMS-YieldNet utilizes contrastive
learning for feature discrimination during pre-training, focusing on capturing
spatial-spectral patterns and spatio-temporal dependencies from remote sensing
data. Both quantitative and qualitative assessments highlight the excellence of
the proposed MTMS-YieldNet over seven existing state-of-the-art methods.
MTMS-YieldNet achieves MAPE scores of 0.336 on Sentinel-1, 0.353 on Landsat-8,
and an outstanding 0.331 on Sentinel-2, demonstrating effective yield
prediction performance across diverse climatic and seasonal conditions. The
outstanding performance of MTMS-YieldNet improves yield predictions and
provides valuable insights that can assist farmers in making better decisions,
potentially improving crop yields.

</details>


### [165] [Shedding Light on Depth: Explainability Assessment in Monocular Depth Estimation](https://arxiv.org/abs/2509.15980)
*Lorenzo Cirillo,Claudio Schiavella,Lorenzo Papa,Paolo Russo,Irene Amerini*

Main category: cs.CV

TL;DR: The paper investigates the explainability of Monocular Depth Estimation (MDE) networks using three feature attribution methods and proposes a new metric called Attribution Fidelity to assess their reliability.


<details>
  <summary>Details</summary>
Motivation: Although explainable AI is widely used to understand deep learning decisions, MDE remains underexplored in terms of its explainability, despite its practical importance in real-world applications.

Method: The authors investigate existing explainability methods (Saliency Maps, Integrated Gradients, and Attention Rollout) on two MDE models of varying complexity. They also develop a new evaluation metric, Attribution Fidelity, to better measure the quality of explanations.

Result: Findings show that Saliency Maps perform well for lightweight MDE models, while Integrated Gradients work better for deeper ones. Attribution Fidelity proves to be an effective metric for evaluating the reliability of visual explanations.

Conclusion: The study advances the understanding of explainable MDE by demonstrating the effectiveness of certain methods and introducing a new metric to ensure explanation reliability, filling a gap in MDE explainability assessment.

Abstract: Explainable artificial intelligence is increasingly employed to understand
the decision-making process of deep learning models and create trustworthiness
in their adoption. However, the explainability of Monocular Depth Estimation
(MDE) remains largely unexplored despite its wide deployment in real-world
applications. In this work, we study how to analyze MDE networks to map the
input image to the predicted depth map. More in detail, we investigate
well-established feature attribution methods, Saliency Maps, Integrated
Gradients, and Attention Rollout on different computationally complex models
for MDE: METER, a lightweight network, and PixelFormer, a deep network. We
assess the quality of the generated visual explanations by selectively
perturbing the most relevant and irrelevant pixels, as identified by the
explainability methods, and analyzing the impact of these perturbations on the
model's output. Moreover, since existing evaluation metrics can have some
limitations in measuring the validity of visual explanations for MDE, we
additionally introduce the Attribution Fidelity. This metric evaluates the
reliability of the feature attribution by assessing their consistency with the
predicted depth map. Experimental results demonstrate that Saliency Maps and
Integrated Gradients have good performance in highlighting the most important
input features for MDE lightweight and deep models, respectively. Furthermore,
we show that Attribution Fidelity effectively identifies whether an
explainability method fails to produce reliable visual maps, even in scenarios
where conventional metrics might suggest satisfactory results.

</details>


### [166] [DAFTED: Decoupled Asymmetric Fusion of Tabular and Echocardiographic Data for Cardiac Hypertension Diagnosis](https://arxiv.org/abs/2509.15990)
*Jérémie Stym-Popper,Nathan Painchaud,Clément Rambour,Pierre-Yves Courand,Nicolas Thome,Olivier Bernard*

Main category: cs.CV

TL;DR: The paper introduces an asymmetric fusion strategy for multimodal data to improve medical diagnosis, achieving over 90% AUC in validation on patient data.


<details>
  <summary>Details</summary>
Motivation: To enhance diagnostic accuracy in medical applications by effectively integrating multimodal data.

Method: An asymmetric fusion strategy was developed, focusing on a primary modality and integrating secondary ones by separating shared and modality-specific information.

Result: Validated on 239 patients' data, the method surpassed existing approaches with an AUC exceeding 90%.

Conclusion: This approach provides a significant benchmark for potential clinical implementation in multimodal diagnostics.

Abstract: Multimodal data fusion is a key approach for enhancing diagnosis in medical
applications. We propose an asymmetric fusion strategy starting from a primary
modality and integrating secondary modalities by disentangling shared and
modality-specific information. Validated on a dataset of 239 patients with
echocardiographic time series and tabular records, our model outperforms
existing methods, achieving an AUC over 90%. This improvement marks a crucial
benchmark for clinical use.

</details>


### [167] [Towards Robust Visual Continual Learning with Multi-Prototype Supervision](https://arxiv.org/abs/2509.16011)
*Xiwei Liu,Yulong Li,Yichen Li,Xinlin Zhuang,Haolin Yang,Huifa Li,Imran Razzak*

Main category: cs.CV

TL;DR: This paper introduces MuproCL, a framework that uses multiple semantic prototypes instead of a single target to enhance visual Continual Learning guided by language models.


<details>
  <summary>Details</summary>
Motivation: Address issues of semantic ambiguity and intra-class visual diversity when relying on a single semantic target in language-guided continual learning.

Method: MuproCL leverages a lightweight LLM agent for category disambiguation and visual-modal expansion, generating multiple context-aware semantic prototypes combined with a LogSumExp aggregation mechanism for adaptive alignment.

Result: MuproCL improves performance and robustness in visual continual learning tasks, as demonstrated by comprehensive experiments.

Conclusion: Replacing a single semantic target with multiple adaptive prototypes enhances language-guided continual learning's effectiveness and reliability.

Abstract: Language-guided supervision, which utilizes a frozen semantic target from a
Pretrained Language Model (PLM), has emerged as a promising paradigm for visual
Continual Learning (CL). However, relying on a single target introduces two
critical limitations: 1) semantic ambiguity, where a polysemous category name
results in conflicting visual representations, and 2) intra-class visual
diversity, where a single prototype fails to capture the rich variety of visual
appearances within a class. To this end, we propose MuproCL, a novel framework
that replaces the single target with multiple, context-aware prototypes.
Specifically, we employ a lightweight LLM agent to perform category
disambiguation and visual-modal expansion to generate a robust set of semantic
prototypes. A LogSumExp aggregation mechanism allows the vision model to
adaptively align with the most relevant prototype for a given image. Extensive
experiments across various CL baselines demonstrate that MuproCL consistently
enhances performance and robustness, establishing a more effective path for
language-guided continual learning.

</details>


### [168] [DistillMatch: Leveraging Knowledge Distillation from Vision Foundation Model for Multimodal Image Matching](https://arxiv.org/abs/2509.16017)
*Meng Yang,Fan Fan,Zizhuo Li,Songchu Deng,Yong Ma,Jiayi Ma*

Main category: cs.CV

TL;DR: DistillMatch is a multimodal image matching method using knowledge distillation from Vision Foundation Models (VFMs), outperforming existing algorithms through enhanced feature extraction and data augmentation.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge in multimodal image matching where significant appearance differences between modalities hinder correspondence identification, compounded by the lack of high-quality datasets.

Method: DistillMatch utilizes VFMs to distill knowledge into a lightweight student model, enabling extraction of robust semantic features for matching. It incorporates modality-specific information and employs V2I-GAN to enhance generalization via pseudo-infrared data augmentation.

Result: Experimental results demonstrate that DistillMatch achieves superior performance compared to existing algorithms on public multimodal image datasets.

Conclusion: The proposed method leverages VFMs and innovative augmentations to create a robust and adaptable multimodal image matching framework, advancing cross-modal perception and analysis.

Abstract: Multimodal image matching seeks pixel-level correspondences between images of
different modalities, crucial for cross-modal perception, fusion and analysis.
However, the significant appearance differences between modalities make this
task challenging. Due to the scarcity of high-quality annotated datasets,
existing deep learning methods that extract modality-common features for
matching perform poorly and lack adaptability to diverse scenarios. Vision
Foundation Model (VFM), trained on large-scale data, yields generalizable and
robust feature representations adapted to data and tasks of various modalities,
including multimodal matching. Thus, we propose DistillMatch, a multimodal
image matching method using knowledge distillation from VFM. DistillMatch
employs knowledge distillation to build a lightweight student model that
extracts high-level semantic features from VFM (including DINOv2 and DINOv3) to
assist matching across modalities. To retain modality-specific information, it
extracts and injects modality category information into the other modality's
features, which enhances the model's understanding of cross-modal correlations.
Furthermore, we design V2I-GAN to boost the model's generalization by
translating visible to pseudo-infrared images for data augmentation.
Experiments show that DistillMatch outperforms existing algorithms on public
datasets.

</details>


### [169] [Generalized Deep Multi-view Clustering via Causal Learning with Partially Aligned Cross-view Correspondence](https://arxiv.org/abs/2509.16022)
*Xihong Yang,Siwei Wang,Jiaqi Jin,Fangdi Wang,Tianrui Liu,Yueming Jin,Xinwang Liu,En Zhu,Kunlun He*

Main category: cs.CV

TL;DR: The paper introduces CauMVC, a causal multi-view clustering network to address challenges in clustering when data is partially aligned between different views.


<details>
  <summary>Details</summary>
Motivation: Existing multi-view clustering methods struggle when the data alignment across views is only partial, which hampers clustering results in real-world scenarios.

Method: The authors leverage causal modeling to treat partially aligned data as an intervention, employing a Variational Auto-Encoder for feature extraction and using a decoder for inference. A contrastive regularizer is added to capture correlations between samples.

Result: Empirical tests show that CauMVC is effective and generalizes well for both fully and partially aligned datasets.

Conclusion: This is the first study to use causal learning for generalized multi-view clustering, demonstrating improved performance even under challenging data alignment scenarios.

Abstract: Multi-view clustering (MVC) aims to explore the common clustering structure
across multiple views. Many existing MVC methods heavily rely on the assumption
of view consistency, where alignments for corresponding samples across
different views are ordered in advance. However, real-world scenarios often
present a challenge as only partial data is consistently aligned across
different views, restricting the overall clustering performance. In this work,
we consider the model performance decreasing phenomenon caused by data order
shift (i.e., from fully to partially aligned) as a generalized multi-view
clustering problem. To tackle this problem, we design a causal multi-view
clustering network, termed CauMVC. We adopt a causal modeling approach to
understand multi-view clustering procedure. To be specific, we formulate the
partially aligned data as an intervention and multi-view clustering with
partially aligned data as an post-intervention inference. However, obtaining
invariant features directly can be challenging. Thus, we design a Variational
Auto-Encoder for causal learning by incorporating an encoder from existing
information to estimate the invariant features. Moreover, a decoder is designed
to perform the post-intervention inference. Lastly, we design a contrastive
regularizer to capture sample correlations. To the best of our knowledge, this
paper is the first work to deal generalized multi-view clustering via causal
learning. Empirical experiments on both fully and partially aligned data
illustrate the strong generalization and effectiveness of CauMVC.

</details>


### [170] [GLip: A Global-Local Integrated Progressive Framework for Robust Visual Speech Recognition](https://arxiv.org/abs/2509.16031)
*Tianyue Wang,Shuang Yang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: This paper introduces GLip, a novel framework for Visual Speech Recognition (VSR) that integrates global and local features to address real-world challenges such as occlusions and pose changes, demonstrating superior performance on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to improve VSR methods' robustness against real-world challenges like illumination variations, occlusions, blurring, and pose changes, which are often overlooked in existing approaches.

Method: The GLip framework features a dual-path architecture combining global and local feature extractions. It employs a two-stage progressive learning process: coarse alignment of visual features in the first stage and refinement via a Contextual Enhancement Module (CEM) in the second stage, further leveraging audio-visual data.

Result: GLip outperforms existing methods on major benchmarks (LRS2 and LRS3) and also shows its effectiveness on a newly proposed challenging Mandarin dataset.

Conclusion: The GLip framework demonstrates enhanced robustness against adverse visual conditions and achieves superior performance, making it a significant advancement for VSR tasks.

Abstract: Visual speech recognition (VSR), also known as lip reading, is the task of
recognizing speech from silent video. Despite significant advancements in VSR
over recent decades, most existing methods pay limited attention to real-world
visual challenges such as illumination variations, occlusions, blurring, and
pose changes. To address these challenges, we propose GLip, a Global-Local
Integrated Progressive framework designed for robust VSR. GLip is built upon
two key insights: (i) learning an initial \textit{coarse} alignment between
visual features across varying conditions and corresponding speech content
facilitates the subsequent learning of \textit{precise} visual-to-speech
mappings in challenging environments; (ii) under adverse conditions, certain
local regions (e.g., non-occluded areas) often exhibit more discriminative cues
for lip reading than global features. To this end, GLip introduces a dual-path
feature extraction architecture that integrates both global and local features
within a two-stage progressive learning framework. In the first stage, the
model learns to align both global and local visual features with corresponding
acoustic speech units using easily accessible audio-visual data, establishing a
coarse yet semantically robust foundation. In the second stage, we introduce a
Contextual Enhancement Module (CEM) to dynamically integrate local features
with relevant global context across both spatial and temporal dimensions,
refining the coarse representations into precise visual-speech mappings. Our
framework uniquely exploits discriminative local regions through a progressive
learning strategy, demonstrating enhanced robustness against various visual
challenges and consistently outperforming existing methods on the LRS2 and LRS3
benchmarks. We further validate its effectiveness on a newly introduced
challenging Mandarin dataset.

</details>


### [171] [Graph-based Point Cloud Surface Reconstruction using B-Splines](https://arxiv.org/abs/2509.16050)
*Stuti Pathak,Rhys G. Evans,Gunther Steenackers,Rudi Penne*

Main category: cs.CV

TL;DR: This paper proposes a surface reconstruction method for noisy point clouds using a network that predicts both the location and the number of control points, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing algorithms struggle with noisy real-world point clouds due to dependence on ground truth normals or approximations, and fixed control point strategies for B-splines are inadequate in capturing surface complexity.

Method: A Dictionary-Guided Graph Convolutional Network is used to predict both the number and locations of B-spline control points directly from noisy point cloud data, bypassing the need for normals.

Result: The method outperforms state-of-the-art surface reconstruction techniques both qualitatively and quantitatively according to widely-used metrics.

Conclusion: This novel approach generates smooth and accurate surfaces for noisy point clouds without relying on normals, offering a significant improvement over existing methods.

Abstract: Generating continuous surfaces from discrete point cloud data is a
fundamental task in several 3D vision applications. Real-world point clouds are
inherently noisy due to various technical and environmental factors. Existing
data-driven surface reconstruction algorithms rely heavily on ground truth
normals or compute approximate normals as an intermediate step. This dependency
makes them extremely unreliable for noisy point cloud datasets, even if the
availability of ground truth training data is ensured, which is not always the
case. B-spline reconstruction techniques provide compact surface
representations of point clouds and are especially known for their smoothening
properties. However, the complexity of the surfaces approximated using
B-splines is directly influenced by the number and location of the spline
control points. Existing spline-based modeling methods predict the locations of
a fixed number of control points for a given point cloud, which makes it very
difficult to match the complexity of its underlying surface. In this work, we
develop a Dictionary-Guided Graph Convolutional Network-based surface
reconstruction strategy where we simultaneously predict both the location and
the number of control points for noisy point cloud data to generate smooth
surfaces without the use of any point normals. We compare our reconstruction
method with several well-known as well as recent baselines by employing
widely-used evaluation metrics, and demonstrate that our method outperforms all
of them both qualitatively and quantitatively.

</details>


### [172] [Language-Instructed Reasoning for Group Activity Detection via Multimodal Large Language Model](https://arxiv.org/abs/2509.16054)
*Jihua Peng,Qianxiong Xu,Yichen Liu,Chenxi Liu,Cheng Long,Rui Zhao,Ziyue Li*

Main category: cs.CV

TL;DR: The paper introduces LIR-GAD, a framework leveraging multimodal large language models to improve group activity detection tasks.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning-based methods for group activity detection struggle with contextual reasoning and explainability due to reliance on visual features.

Method: The authors propose to enhance MLLM's vocabulary with activity-level and cluster-specific tokens, along with a novel fusion module to integrate language instructions and visual features.

Result: Quantitative and qualitative experiments show that LIR-GAD delivers superior performance on group activity detection compared to prior methods.

Conclusion: LIR-GAD improves explainability and contextual reasoning in group activity detection through multimodal integration using large language models.

Abstract: Group activity detection (GAD) aims to simultaneously identify group members
and categorize their collective activities within video sequences. Existing
deep learning-based methods develop specialized architectures (e.g.,
transformer networks) to model the dynamics of individual roles and semantic
dependencies between individuals and groups. However, they rely solely on
implicit pattern recognition from visual features and struggle with contextual
reasoning and explainability. In this work, we propose LIR-GAD, a novel
framework of language-instructed reasoning for GAD via Multimodal Large
Language Model (MLLM). Our approach expand the original vocabulary of MLLM by
introducing an activity-level <ACT> token and multiple cluster-specific <GROUP>
tokens. We process video frames alongside two specially designed tokens and
language instructions, which are then integrated into the MLLM. The pretrained
commonsense knowledge embedded in the MLLM enables the <ACT> token and <GROUP>
tokens to effectively capture the semantic information of collective activities
and learn distinct representational features of different groups, respectively.
Also, we introduce a multi-label classification loss to further enhance the
<ACT> token's ability to learn discriminative semantic representations. Then,
we design a Multimodal Dual-Alignment Fusion (MDAF) module that integrates
MLLM's hidden embeddings corresponding to the designed tokens with visual
features, significantly enhancing the performance of GAD. Both quantitative and
qualitative experiments demonstrate the superior performance of our proposed
method in GAD taks.

</details>


### [173] [See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model](https://arxiv.org/abs/2509.16087)
*Pengteng Li,Pinhao Song,Wuyang Li,Weiyu Guo,Huizai Yao,Yijie Xu,Dugang Liu,Hui Xiong*

Main category: cs.CV

TL;DR: SEE&TREK introduces a training-free framework to improve spatial understanding in Multimodal Large Language Models (MLLMs) using visual input exclusively, showcasing notable performance improvements.


<details>
  <summary>Details</summary>
Motivation: Spatial reasoning in MLLMs is underexplored when limited to visual-only data. SEE&TREK aims to enhance spatial understanding without relying on additional modalities such as depth or point clouds.

Method: SEE&TREK employs Maximum Semantic Richness Sampling for visual diversity, using a perception model to select semantically meaningful keyframes, and simulates visual trajectories to encode spatial and temporal coherence into these frames.

Result: The framework consistently boosts MLLM performance in spatial reasoning tasks, with a maximum improvement of +3.5%.

Conclusion: SEE&TREK offers a promising, training-free solution to enhance spatial intelligence in MLLMs, focusing exclusively on visual data and enabling seamless integration into existing models.

Abstract: We introduce SEE&TREK, the first training-free prompting framework tailored
to enhance the spatial understanding of Multimodal Large Language Models
(MLLMS) under vision-only constraints. While prior efforts have incorporated
modalities like depth or point clouds to improve spatial reasoning, purely
visualspatial understanding remains underexplored. SEE&TREK addresses this gap
by focusing on two core principles: increasing visual diversity and motion
reconstruction. For visual diversity, we conduct Maximum Semantic Richness
Sampling, which employs an off-the-shell perception model to extract
semantically rich keyframes that capture scene structure. For motion
reconstruction, we simulate visual trajectories and encode relative spatial
positions into keyframes to preserve both spatial relations and temporal
coherence. Our method is training&GPU-free, requiring only a single forward
pass, and can be seamlessly integrated into existing MLLM'S. Extensive
experiments on the VSI-B ENCH and STI-B ENCH show that S EE &T REK consistently
boosts various MLLM S performance across diverse spatial reasoning tasks with
the most +3.5% improvement, offering a promising path toward stronger spatial
intelligence.

</details>


### [174] [Blind-Spot Guided Diffusion for Self-supervised Real-World Denoising](https://arxiv.org/abs/2509.16091)
*Shen Cheng,Haipeng Li,Haibin Huang,Xiaohong Liu,Shuaicheng Liu*

Main category: cs.CV

TL;DR: Blind-Spot Guided Diffusion is a new self-supervised image denoising approach that uses a dual-branch diffusion framework for better accuracy and detail preservation, achieving state-of-the-art performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations in existing denoising methods, specifically the pixel discontinuities and loss of local detail in blind-spot networks (BSNs) and the difficulty in adapting diffusion models to self-supervised denoising.

Method: The paper introduces a dual-branch diffusion framework that combines a BSN-based diffusion branch for generating semi-clean images with a conventional diffusion branch for noise structure analysis. Self-supervised training is enhanced by using the BSN branch to guide sampling while preserving image detail.

Result: The method achieves state-of-the-art denoising performance on real-world datasets (SIDD and DND), surpassing existing self-supervised approaches.

Conclusion: The framework successfully overcomes key challenges in self-supervised denoising, preserving local details and handling noise effectively, proving its practical applicability and high performance.

Abstract: In this work, we present Blind-Spot Guided Diffusion, a novel self-supervised
framework for real-world image denoising. Our approach addresses two major
challenges: the limitations of blind-spot networks (BSNs), which often
sacrifice local detail and introduce pixel discontinuities due to spatial
independence assumptions, and the difficulty of adapting diffusion models to
self-supervised denoising. We propose a dual-branch diffusion framework that
combines a BSN-based diffusion branch, generating semi-clean images, with a
conventional diffusion branch that captures underlying noise distributions. To
enable effective training without paired data, we use the BSN-based branch to
guide the sampling process, capturing noise structure while preserving local
details. Extensive experiments on the SIDD and DND datasets demonstrate
state-of-the-art performance, establishing our method as a highly effective
self-supervised solution for real-world denoising. Code and pre-trained models
are released at: https://github.com/Sumching/BSGD.

</details>


### [175] [AdaSports-Traj: Role- and Domain-Aware Adaptation for Multi-Agent Trajectory Modeling in Sports](https://arxiv.org/abs/2509.16095)
*Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: AdaSports-Traj improves trajectory prediction by addressing role and domain discrepancies in sports, leveraging a role- and domain-aware adapter alongside hierarchical contrastive learning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address trajectory prediction challenges in multi-agent sports scenarios caused by structural heterogeneity and dynamic domain gaps.

Method: The framework uses a Role- and Domain-Aware Adapter for conditional adjustment of latent representations and Hierarchical Contrastive Learning for disentangled latent structures.

Result: AdaSports-Traj achieves strong performance in unified and cross-domain trajectory prediction, validated through experiments on diverse sports datasets.

Conclusion: AdaSports-Traj effectively handles distributional discrepancies in sports trajectory prediction, enhancing generalization across roles and domains.

Abstract: Trajectory prediction in multi-agent sports scenarios is inherently
challenging due to the structural heterogeneity across agent roles (e.g.,
players vs. ball) and dynamic distribution gaps across different sports
domains. Existing unified frameworks often fail to capture these structured
distributional shifts, resulting in suboptimal generalization across roles and
domains. We propose AdaSports-Traj, an adaptive trajectory modeling framework
that explicitly addresses both intra-domain and inter-domain distribution
discrepancies in sports. At its core, AdaSports-Traj incorporates a Role- and
Domain-Aware Adapter to conditionally adjust latent representations based on
agent identity and domain context. Additionally, we introduce a Hierarchical
Contrastive Learning objective, which separately supervises role-sensitive and
domain-aware representations to encourage disentangled latent structures
without introducing optimization conflict. Experiments on three diverse sports
datasets, Basketball-U, Football-U, and Soccer-U, demonstrate the effectiveness
of our adaptive design, achieving strong performance in both unified and
cross-domain trajectory prediction settings.

</details>


### [176] [SegDINO3D: 3D Instance Segmentation Empowered by Both Image-Level and Object-Level 2D Features](https://arxiv.org/abs/2509.16098)
*Jinyuan Qu,Hongyang Li,Xingyu Chen,Shilong Liu,Yukai Shi,Tianhe Ren,Ruitao Jing,Lei Zhang*

Main category: cs.CV

TL;DR: This paper introduces SegDINO3D, a Transformer-based framework designed for 3D instance segmentation using enriched 2D and 3D representations, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of limited 3D training data by fully integrating and utilizing pre-trained 2D detection models for enhancing 3D segmentation.

Method: SegDINO3D leverages a Transformer encoder-decoder architecture using both 3D point clouds and associated 2D images. The encoder enriches 3D points with features from 2D image views, while the decoder uses cross-attention between 3D box queries and compact representations extracted from 2D object queries.

Result: SegDINO3D achieved state-of-the-art results on benchmarks such as ScanNetV2 and ScanNet200, with significant performance improvements of +8.7 and +6.8 mAP on ScanNet200.

Conclusion: SegDINO3D effectively bridges 2D and 3D representations, demonstrating significant advancements in 3D instance segmentation and surpassing prior methods in benchmark performance.

Abstract: In this paper, we present SegDINO3D, a novel Transformer encoder-decoder
framework for 3D instance segmentation. As 3D training data is generally not as
sufficient as 2D training images, SegDINO3D is designed to fully leverage 2D
representation from a pre-trained 2D detection model, including both
image-level and object-level features, for improving 3D representation.
SegDINO3D takes both a point cloud and its associated 2D images as input. In
the encoder stage, it first enriches each 3D point by retrieving 2D image
features from its corresponding image views and then leverages a 3D encoder for
3D context fusion. In the decoder stage, it formulates 3D object queries as 3D
anchor boxes and performs cross-attention from 3D queries to 2D object queries
obtained from 2D images using the 2D detection model. These 2D object queries
serve as a compact object-level representation of 2D images, effectively
avoiding the challenge of keeping thousands of image feature maps in the memory
while faithfully preserving the knowledge of the pre-trained 2D model. The
introducing of 3D box queries also enables the model to modulate
cross-attention using the predicted boxes for more precise querying. SegDINO3D
achieves the state-of-the-art performance on the ScanNetV2 and ScanNet200 3D
instance segmentation benchmarks. Notably, on the challenging ScanNet200
dataset, SegDINO3D significantly outperforms prior methods by +8.7 and +6.8 mAP
on the validation and hidden test sets, respectively, demonstrating its
superiority.

</details>


### [177] [RadarGaussianDet3D: An Efficient and Effective Gaussian-based 3D Detector with 4D Automotive Radars](https://arxiv.org/abs/2509.16119)
*Weiyi Xiong,Bing Zhu,Tao Huang,Zewei Zheng*

Main category: cs.CV

TL;DR: The paper introduces RadarGaussianDet3D, a Gaussian-based 3D detector for 4D automotive radar data for enhanced detection and real-time processing in autonomous driving.


<details>
  <summary>Details</summary>
Motivation: The work aims to address the limitations of existing 4D radar-based 3D detectors, which suffer from sparse feature maps, sub-optimal bounding box optimization, and slow inference speeds on embedded devices.

Method: The proposed RadarGaussianDet3D utilizes Gaussian primitives and distributions for better point representation and bounding box optimization. It introduces a Point Gaussian Encoder (PGE) for efficient BEV feature extraction and employs 3D Gaussian Splatting (3DGS) for denser feature maps. A new loss function, Box Gaussian Loss (BGL), is designed for improved bounding box optimization.

Result: RadarGaussianDet3D achieves state-of-the-art detection accuracy on TJ4DRadSet and View-of-Delft datasets while delivering significantly faster inference, suitable for real-time applications.

Conclusion: RadarGaussianDet3D demonstrates the potential for real-time 3D object detection in autonomous driving, overcoming limitations of prior methods with its efficient and effective design.

Abstract: 4D automotive radars have gained increasing attention for autonomous driving
due to their low cost, robustness, and inherent velocity measurement
capability. However, existing 4D radar-based 3D detectors rely heavily on
pillar encoders for BEV feature extraction, where each point contributes to
only a single BEV grid, resulting in sparse feature maps and degraded
representation quality. In addition, they also optimize bounding box attributes
independently, leading to sub-optimal detection accuracy. Moreover, their
inference speed, while sufficient for high-end GPUs, may fail to meet the
real-time requirement on vehicle-mounted embedded devices. To overcome these
limitations, an efficient and effective Gaussian-based 3D detector, namely
RadarGaussianDet3D is introduced, leveraging Gaussian primitives and
distributions as intermediate representations for radar points and bounding
boxes. In RadarGaussianDet3D, a novel Point Gaussian Encoder (PGE) is designed
to transform each point into a Gaussian primitive after feature aggregation and
employs the 3D Gaussian Splatting (3DGS) technique for BEV rasterization,
yielding denser feature maps. PGE exhibits exceptionally low latency, owing to
the optimized algorithm for point feature aggregation and fast rendering of
3DGS. In addition, a new Box Gaussian Loss (BGL) is proposed, which converts
bounding boxes into 3D Gaussian distributions and measures their distance to
enable more comprehensive and consistent optimization. Extensive experiments on
TJ4DRadSet and View-of-Delft demonstrate that RadarGaussianDet3D achieves
state-of-the-art detection accuracy while delivering substantially faster
inference, highlighting its potential for real-time deployment in autonomous
driving.

</details>


### [178] [BaseReward: A Strong Baseline for Multimodal Reward Model](https://arxiv.org/abs/2509.16127)
*Yi-Fan Zhang,Haihua Yang,Huanyu Zhang,Yang Shi,Zezhou Chen,Haochen Tian,Chaoyou Fu,Haotian Wang,Kai Wu,Bo Cui,Xu Wang,Jianfei Pan,Haotian Wang,Zhang Zhang,Liang Wang*

Main category: cs.CV

TL;DR: This paper introduces a systematic guide for building state-of-the-art Multimodal Reward Models (MRMs) to align Multimodal Large Language Models (MLLMs) with human preferences. It presents 'BaseReward' as a new SOTA MRM and demonstrates its superiority on benchmarks and real-world tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the lack of a systematic guide for constructing high-performance Multimodal Reward Models (MRMs) crucial for aligning Multimodal Large Language Models (MLLMs) with human preferences.

Method: The paper systematically explores various aspects of MRM development, including modeling paradigms, reward head architecture, training strategies, data curation, backbone models, scalability, and ensemble methods. Leveraging these insights, the authors introduce 'BaseReward', built upon the Qwen2.5-VL backbone and trained on curated multimodal and text-only datasets.

Result: The proposed 'BaseReward' model sets new state-of-the-art (SOTA) performance on major MRM benchmarks like MM-RLHF-Reward Bench, VL-Reward Bench, and Multimodal Reward Bench, outperforming previous models.

Conclusion: The paper delivers the high-performing 'BaseReward' MRM and provides an empirically-backed guide for developing robust reward models, enhancing the community's ability to improve the next generation of Multimodal Large Language Models.

Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has made
aligning them with human preferences a critical challenge. Reward Models (RMs)
are a core technology for achieving this goal, but a systematic guide for
building state-of-the-art Multimodal Reward Models (MRMs) is currently lacking
in both academia and industry. Through exhaustive experimental analysis, this
paper aims to provide a clear ``recipe'' for constructing high-performance
MRMs. We systematically investigate every crucial component in the MRM
development pipeline, including \textit{reward modeling paradigms} (e.g.,
Naive-RM, Critic-based RM, and Generative RM), \textit{reward head
architecture}, \textit{training strategies}, \textit{data curation} (covering
over ten multimodal and text-only preference datasets), \textit{backbone model}
and \textit{model scale}, and \textit{ensemble methods}.
  Based on these experimental insights, we introduce \textbf{BaseReward}, a
powerful and efficient baseline for multimodal reward modeling. BaseReward
adopts a simple yet effective architecture, built upon a {Qwen2.5-VL} backbone,
featuring an optimized two-layer reward head, and is trained on a carefully
curated mixture of high-quality multimodal and text-only preference data. Our
results show that BaseReward establishes a new SOTA on major benchmarks such as
MM-RLHF-Reward Bench, VL-Reward Bench, and Multimodal Reward Bench,
outperforming previous models. Furthermore, to validate its practical utility
beyond static benchmarks, we integrate BaseReward into a real-world
reinforcement learning pipeline, successfully enhancing an MLLM's performance
across various perception, reasoning, and conversational tasks. This work not
only delivers a top-tier MRM but, more importantly, provides the community with
a clear, empirically-backed guide for developing robust reward models for the
next generation of MLLMs.

</details>


### [179] [Recovering Parametric Scenes from Very Few Time-of-Flight Pixels](https://arxiv.org/abs/2509.16132)
*Carter Sifferman,Yiquan Li,Yiming Li,Fangzhou Mu,Michael Gleicher,Mohit Gupta,Yin Li*

Main category: cs.CV

TL;DR: This paper focuses on reconstructing the geometry of 3D parametric scenes using sparse depth measurements from low-resolution time-of-flight sensors, achieving pose estimation and geometry recovery with minimal data input.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of using low-cost, single-pixel time-of-flight sensors to recover 3D scene geometries with high accuracy, especially given these sensors' low spatial resolution and sparse measurements.

Method: The proposed method combines feed-forward prediction for initial scene parameter inference and differentiable rendering for iterative refinement in an analysis-by-synthesis framework. Hardware prototypes were developed for validation.

Result: The method successfully recovered object poses and simple parametric scene geometries based on sparse measurements both in simulations and controlled real-world experiments.

Conclusion: The study demonstrates the feasibility of using low-resolution time-of-flight sensors to recover 3D scene geometry under strong priors, opening a promising direction for low-cost, efficient 3D reconstruction and pose estimation.

Abstract: We aim to recover the geometry of 3D parametric scenes using very few depth
measurements from low-cost, commercially available time-of-flight sensors.
These sensors offer very low spatial resolution (i.e., a single pixel), but
image a wide field-of-view per pixel and capture detailed time-of-flight data
in the form of time-resolved photon counts. This time-of-flight data encodes
rich scene information and thus enables recovery of simple scenes from sparse
measurements. We investigate the feasibility of using a distributed set of few
measurements (e.g., as few as 15 pixels) to recover the geometry of simple
parametric scenes with a strong prior, such as estimating the 6D pose of a
known object. To achieve this, we design a method that utilizes both
feed-forward prediction to infer scene parameters, and differentiable rendering
within an analysis-by-synthesis framework to refine the scene parameter
estimate. We develop hardware prototypes and demonstrate that our method
effectively recovers object pose given an untextured 3D model in both
simulations and controlled real-world captures, and show promising initial
results for other parametric scenes. We additionally conduct experiments to
explore the limits and capabilities of our imaging solution.

</details>


### [180] [AcT2I: Evaluating and Improving Action Depiction in Text-to-Image Models](https://arxiv.org/abs/2509.16141)
*Vatsal Malaviya,Agneet Chatterjee,Maitreya Patel,Yezhou Yang,Chitta Baral*

Main category: cs.CV

TL;DR: Existing Text-to-Image (T2I) models struggle with generating accurate images for action-centric prompts due to insufficient contextual understanding. This paper introduces AcT2I, a benchmark for evaluation, and proposes a training-free knowledge distillation method using enhanced prompts.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the shortcomings of T2I models in generating nuanced and contextually accurate images for complex action-centric scenes.

Method: A benchmark (AcT2I) is introduced for systematic evaluation, and a training-free knowledge distillation approach utilizing enhanced prompts is developed.

Result: Enhanced prompts improve T2I model performance by up to 72% on contextual accuracy in image generation.

Conclusion: Integrating linguistic knowledge systematically allows T2I models to better depict complex reasoning and contextual details in image generation.

Abstract: Text-to-Image (T2I) models have recently achieved remarkable success in
generating images from textual descriptions. However, challenges still persist
in accurately rendering complex scenes where actions and interactions form the
primary semantic focus. Our key observation in this work is that T2I models
frequently struggle to capture nuanced and often implicit attributes inherent
in action depiction, leading to generating images that lack key contextual
details. To enable systematic evaluation, we introduce AcT2I, a benchmark
designed to evaluate the performance of T2I models in generating images from
action-centric prompts. We experimentally validate that leading T2I models do
not fare well on AcT2I. We further hypothesize that this shortcoming arises
from the incomplete representation of the inherent attributes and contextual
dependencies in the training corpora of existing T2I models. We build upon this
by developing a training-free, knowledge distillation technique utilizing Large
Language Models to address this limitation. Specifically, we enhance prompts by
incorporating dense information across three dimensions, observing that
injecting prompts with temporal details significantly improves image generation
accuracy, with our best model achieving an increase of 72%. Our findings
highlight the limitations of current T2I methods in generating images that
require complex reasoning and demonstrate that integrating linguistic knowledge
in a systematic way can notably advance the generation of nuanced and
contextually accurate images.

</details>


### [181] [Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal Large Language Models](https://arxiv.org/abs/2509.16149)
*Renjie Pi,Kehao Miao,Li Peihang,Runtao Liu,Jiahui Gao,Jipeng Zhang,Xiaofang Zhou*

Main category: cs.CV

TL;DR: The paper discusses the "sycophantic modality gap" observed in multimodal large language models (MLLMs), where visual-based behaviors overly conform to user instructions. The authors propose Sycophantic Reflective Tuning (SRT) for addressing this issue.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the heightened visual sycophantic behavior in MLLMs compared to text-based LLMs, which can affect reliable interpretation and output quality.

Method: The authors experiment with supervised fine-tuning and propose Sycophantic Reflective Tuning (SRT), enabling the model to discern between misleading and corrective user instructions.

Result: Using SRT, MLLMs demonstrate reduced sycophantic behavior toward misleading inputs while maintaining responsiveness to corrective instructions, overcoming the stubbornness from naive fine-tuning.

Conclusion: SRT effectively balances mitigating visual sycophantic behavior while retaining the ability to respond accurately to user corrections, enhancing the model's interpretive reliability.

Abstract: Multimodal large language models (MLLMs) have demonstrated extraordinary
capabilities in conducting conversations based on image inputs. However, we
observe that MLLMs exhibit a pronounced form of visual sycophantic behavior.
While similar behavior has also been noted in text-based large language models
(LLMs), it becomes significantly more prominent when MLLMs process image
inputs. We refer to this phenomenon as the "sycophantic modality gap." To
better understand this issue, we further analyze the factors that contribute to
the exacerbation of this gap. To mitigate the visual sycophantic behavior, we
first experiment with naive supervised fine-tuning to help the MLLM resist
misleading instructions from the user. However, we find that this approach also
makes the MLLM overly resistant to corrective instructions (i.e., stubborn even
if it is wrong). To alleviate this trade-off, we propose Sycophantic Reflective
Tuning (SRT), which enables the MLLM to engage in reflective reasoning,
allowing it to determine whether a user's instruction is misleading or
corrective before drawing a conclusion. After applying SRT, we observe a
significant reduction in sycophantic behavior toward misleading instructions,
without resulting in excessive stubbornness when receiving corrective
instructions.

</details>


### [182] [UniMRSeg: Unified Modality-Relax Segmentation via Hierarchical Self-Supervised Compensation](https://arxiv.org/abs/2509.16170)
*Xiaoqi Zhao,Youwei Pang,Chenyang Yu,Lihe Zhang,Huchuan Lu,Shijian Lu,Georges El Fakhri,Xiaofeng Liu*

Main category: cs.CV

TL;DR: UniMRSeg introduces a unified solution for multi-modal image segmentation under incomplete modalities by hierarchically compensating representation gaps, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current approaches for multi-modal image segmentation struggle with incomplete or corrupted modalities, requiring specialized models for different scenarios, which increase deployment costs.

Method: The UniMRSeg framework uses Hierarchical Self-Supervised Compensation (HSSC), which includes hybrid shuffled-masking augmentation for modality reconstruction, modality-invariant contrastive learning, a reverse attention adapter for enhancing perceptual semantics, and hybrid consistency fine-tuning.

Result: UniMRSeg demonstrated superior performance compared to current methods in MRI-based brain tumor segmentation, RGB-D semantic segmentation, and RGB-D/T salient object segmentation.

Conclusion: UniMRSeg provides a cost-effective, high-performing solution for segmentation tasks under varying modality scenarios, resolving issues of model redundancy and performance stability.

Abstract: Multi-modal image segmentation faces real-world deployment challenges from
incomplete/corrupted modalities degrading performance. While existing methods
address training-inference modality gaps via specialized per-combination
models, they introduce high deployment costs by requiring exhaustive model
subsets and model-modality matching. In this work, we propose a unified
modality-relax segmentation network (UniMRSeg) through hierarchical
self-supervised compensation (HSSC). Our approach hierarchically bridges
representation gaps between complete and incomplete modalities across input,
feature and output levels. % First, we adopt modality reconstruction with the
hybrid shuffled-masking augmentation, encouraging the model to learn the
intrinsic modality characteristics and generate meaningful representations for
missing modalities through cross-modal fusion. % Next, modality-invariant
contrastive learning implicitly compensates the feature space distance among
incomplete-complete modality pairs. Furthermore, the proposed lightweight
reverse attention adapter explicitly compensates for the weak perceptual
semantics in the frozen encoder. Last, UniMRSeg is fine-tuned under the hybrid
consistency constraint to ensure stable prediction under all modality
combinations without large performance fluctuations. Without bells and
whistles, UniMRSeg significantly outperforms the state-of-the-art methods under
diverse missing modality scenarios on MRI-based brain tumor segmentation, RGB-D
semantic segmentation, RGB-D/T salient object segmentation. The code will be
released at https://github.com/Xiaoqi-Zhao-DLUT/UniMRSeg.

</details>


### [183] [Fast OTSU Thresholding Using Bisection Method](https://arxiv.org/abs/2509.16179)
*Sai Varun Kodathala*

Main category: cs.CV

TL;DR: The paper enhances the Otsu thresholding algorithm by using the bisection method to significantly reduce computation time while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The computational inefficiency caused by the exhaustive search method in Otsu's algorithm limits its application in real-time and large-scale operations.

Method: The authors employ the bisection method, leveraging the unimodal nature of the between-class variance function, to reduce computational complexity from O(L) to O(log L).

Result: The optimized algorithm achieved a 91.63% reduction in variance computations, a 97.21% reduction in iterations, exact threshold matches in 66.67% of cases, and deviations within 5 gray levels for 95.83% of cases across 48 test images.

Conclusion: The optimized algorithm minimizes computational demands while ensuring accuracy, making it suitable for real-time large-scale image processing without compromising the integrity of the Otsu method.

Abstract: The Otsu thresholding algorithm represents a fundamental technique in image
segmentation, yet its computational efficiency is severely limited by
exhaustive search requirements across all possible threshold values. This work
presents an optimized implementation that leverages the bisection method to
exploit the unimodal characteristics of the between-class variance function.
Our approach reduces the computational complexity from O(L) to O(log L)
evaluations while preserving segmentation accuracy. Experimental validation on
48 standard test images demonstrates a 91.63% reduction in variance
computations and 97.21% reduction in algorithmic iterations compared to
conventional exhaustive search. The bisection method achieves exact threshold
matches in 66.67% of test cases, with 95.83% exhibiting deviations within 5
gray levels. The algorithm maintains universal convergence within theoretical
logarithmic bounds while providing deterministic performance guarantees
suitable for real-time applications. This optimization addresses critical
computational bottlenecks in large-scale image processing systems without
compromising the theoretical foundations or segmentation quality of the
original Otsu method.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [184] [PCCL: Photonic circuit-switched collective communication for distributed ML](https://arxiv.org/abs/2509.15450)
*Abhishek Vijaya Kumar,Arjun Devraj,Rachee Singh*

Main category: cs.DC

TL;DR: PCCL is a library that dynamically reconfigures network topology to eliminate congestion in distributed ML, offering up to 3X speedup in GPU communication and 1.3X improvement in training throughput.


<details>
  <summary>Details</summary>
Motivation: Distributed ML faces inefficiencies due to congestion and dilation in GPU communications, which degrade the performance of collective algorithms in practice.

Method: PCCL dynamically reconfigures network topologies to match communication patterns of collective algorithms, using hardware-agnostic optimization to balance reconfiguration delays and congestion costs.

Result: Demonstrated up to 3X speedup on communication across 128 GPUs and a 1.3X increase in end-to-end training throughput under diverse workloads and configurations.

Conclusion: PCCL proves practical and effective for improving ML system performance by dynamically and intelligently adapting network topologies to remove communication bottlenecks.

Abstract: Modern distributed ML suffers from a fundamental gap between the theoretical
and realized performance of collective communication algorithms due to
congestion and hop-count induced dilation in practical GPU clusters. We present
PCCL, a Photonic Collective Communication Library that reconfigures the network
topology to match the communication patterns of collective algorithms, thereby
eliminating congestion and dilation by creating direct, contention-free
circuits between communicating GPUs. Unlike prior approaches that synthesize
algorithms for specific network topologies and collectives, PCCL generalizes to
any collective primitive and any topology by adapting the network to match each
algorithm's communication pattern. PCCL's key innovation lies in its
hardware-agnostic optimization framework that intelligently decides when to
reconfigure based on the trade-off between network reconfiguration delay and
congestion/dilation costs, making it practical across different optical
hardware with varying switching speeds. Our evaluation demonstrates that PCCL
achieves up to 3X speedup over state-of-the-art algorithms on 128 GPUs across
various workloads, buffer sizes, and topologies, translating to a 1.3X speedup
in end-to-end training throughput.

</details>


### [185] [Angelfish: Consensus with Optimal Throughput and Latency Across the Leader-DAG Spectrum](https://arxiv.org/abs/2509.15847)
*Qianyu Yu,Giuliano Losa,Nibesh Shrestha,Xuechao Wang*

Main category: cs.DC

TL;DR: Angelfish is a hybrid blockchain consensus protocol combining leader-based and DAG-based approaches to optimize both latency and throughput, outperforming existing methods across varying loads.


<details>
  <summary>Details</summary>
Motivation: Modern blockchain systems face a trade-off between low latency and high throughput in eventually-synchronous BFT consensus, necessitating new design approaches.

Method: Angelfish introduces dynamic adjustment to a hybrid protocol, enabling parties to use lightweight, best-effort broadcast voting instead of more resource-intensive mechanisms, reducing communication and improving system adaptability.

Result: Empirical evaluations show that Angelfish achieves state-of-the-art throughput and matches the low latency of leader-based protocols under moderate loads.

Conclusion: Angelfish delivers the best of both worlds, effectively addressing the design trade-offs in BFT consensus systems by adapting smoothly across protocol designs.

Abstract: To maximize performance, many modern blockchain systems rely on
eventually-synchronous, Byzantine fault-tolerant (BFT) consensus protocols. Two
protocol designs have emerged in this space: protocols that minimize latency
using a leader that drives both data dissemination and consensus, and protocols
that maximize throughput using a separate, asynchronous data dissemination
layer. Recent protocols such as Partially-Synchronous Bullshark and Sailfish
combine elements of both approaches by using a DAG to enable parallel data
dissemination and a leader that paces DAG formation. This improves latency
while achieving state-of-the-art throughput. Yet the latency of leader-based
protocols is still better under moderate loads.
  We present Angelfish, a hybrid protocol that adapts smoothly across this
design space, from leader-based to Sailfish-like DAG-based consensus. Angelfish
lets a dynamically-adjusted subset of parties use best-effort broadcast to
issue lightweight votes instead of reliably broadcasting costlier DAG vertices.
This reduces communication, helps lagging nodes catch up, and lowers latency in
practice compared to prior DAG-based protocols. Our empirical evaluation shows
that Angelfish attains state-of-the-art peak throughput while matching the
latency of leader-based protocols under moderate throughput, delivering the
best of both worlds.

</details>


### [186] [Efficient Pre-Training of LLMs via Topology-Aware Communication Alignment on More Than 9600 GPUs](https://arxiv.org/abs/2509.15940)
*Guoliang He,Youhe Jiang,Wencong Xiao,Kaihua Jiang,Shuguang Wang,Jun Wang,Zixian Du,Zhuo Jiang,Xinlei Zhang,Binhang Yuan,Eiko Yoneki*

Main category: cs.DC

TL;DR: Arnold is a scheduling system designed to improve the performance of large language model (LLM) pre-training by aligning communication patterns with data center topology, resulting in a 10.6% performance increase for large GPU clusters.


<details>
  <summary>Details</summary>
Motivation: LLM pre-training faces challenges due to sparse, high-volume communication patterns in large GPU clusters, leading to bandwidth contention and suboptimal scheduling.

Method: The authors studied network topology’s impact on LLM pre-training and developed a scheduling algorithm that aligns communication patterns with data center architecture.

Result: Simulation experiments showed a 1.67x reduction in maximum communication spread, and production training with over 9600 GPUs yielded a 10.6% improvement in performance.

Conclusion: Arnold effectively addresses LLM-specific communication issues in large-scale GPU clusters, optimizing performance through intelligent scheduling aligned with physical data center topology.

Abstract: The scaling law for large language models (LLMs) depicts that the path
towards machine intelligence necessitates training at large scale. Thus,
companies continuously build large-scale GPU clusters, and launch training jobs
that span over thousands of computing nodes. However, LLM pre-training presents
unique challenges due to its complex communication patterns, where GPUs
exchange data in sparse yet high-volume bursts within specific groups.
Inefficient resource scheduling exacerbates bandwidth contention, leading to
suboptimal training performance. This paper presents Arnold, a scheduling
system summarizing our experience to effectively align LLM communication
patterns with data center topology at scale. An in-depth characteristic study
is performed to identify the impact of physical network topology to LLM
pre-training jobs. Based on the insights, we develop a scheduling algorithm to
effectively align communication patterns with the physical network topology in
modern data centers. Through simulation experiments, we show the effectiveness
of our algorithm in reducing the maximum spread of communication groups by up
to $1.67$x. In production training, our scheduling system improves the
end-to-end performance by $10.6\%$ when training with more than $9600$ GPUs, a
significant improvement for our training pipeline.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [187] [Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to Multi-Scale Problems](https://arxiv.org/abs/2509.15448)
*Saeed Amizadeh,Sara Abdali,Yinheng Li,Kazuhito Koishida*

Main category: cs.LG

TL;DR: The paper proposes a mathematical approach to represent multi-modal, multi-scale data and derive a hierarchical attention mechanism for transformers, enabling them to handle hierarchical and multi-modal data more effectively.


<details>
  <summary>Details</summary>
Motivation: Current methods incorporating hierarchy and multi-modality into transformers rely on ad hoc heuristics, which are not generalizable to different data structures. The paper aims to establish a systematic approach.

Method: A new mathematical construct for multi-modal, multi-scale data is introduced and the attention mechanics are derived through entropy minimization principles. An efficient dynamic programming-based algorithm is proposed to compute the attention mechanism.

Result: The proposed hierarchical attention mechanism enhances transformers' capability to work with hierarchical/multi-modal data. It also improves the efficiency of pre-trained models in zero-shot settings.

Conclusion: The new attention mechanism derived from first principles optimally combines hierarchical and geometric inductive biases, leading to more generalizable and efficient transformer models.

Abstract: Transformers and their attention mechanism have been revolutionary in the
field of Machine Learning. While originally proposed for the language data,
they quickly found their way to the image, video, graph, etc. data modalities
with various signal geometries. Despite this versatility, generalizing the
attention mechanism to scenarios where data is presented at different scales
from potentially different modalities is not straightforward. The attempts to
incorporate hierarchy and multi-modality within transformers are largely based
on ad hoc heuristics, which are not seamlessly generalizable to similar
problems with potentially different structures. To address this problem, in
this paper, we take a fundamentally different approach: we first propose a
mathematical construct to represent multi-modal, multi-scale data. We then
mathematically derive the neural attention mechanics for the proposed construct
from the first principle of entropy minimization. We show that the derived
formulation is optimal in the sense of being the closest to the standard
Softmax attention while incorporating the inductive biases originating from the
hierarchical/geometric information of the problem. We further propose an
efficient algorithm based on dynamic programming to compute our derived
attention mechanism. By incorporating it within transformers, we show that the
proposed hierarchical attention mechanism not only can be employed to train
transformer models in hierarchical/multi-modal settings from scratch, but it
can also be used to inject hierarchical information into classical, pre-trained
transformer models post training, resulting in more efficient models in
zero-shot manner.

</details>


### [188] [Pre-Forgettable Models: Prompt Learning as a Native Mechanism for Unlearning](https://arxiv.org/abs/2509.15230)
*Rutger Hendrix,Giovanni Patanè,Leonardo G. Russo,Simone Carnemolla,Giovanni Bellitto,Federica Proietto Salanitri,Concetto Spampinato,Matteo Pennisi*

Main category: cs.LG

TL;DR: The paper introduces a prompt-based learning framework for multimedia analysis that enables seamless unlearning by removing specific prompts, avoiding retraining or data access.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for ethically responsive AI systems that comply with privacy regulations like GDPR by enabling efficient unlearning mechanisms.

Method: The method involves designing a prompt-based learning framework where class-level data is bound to specific prompt tokens, allowing unlearning by simply removing those tokens.

Result: The framework retains predictive performance on retained classes while effectively erasing forgotten classes. It ensures strong privacy and security, including resistance to membership inference attacks and protection against knowledge extraction.

Conclusion: The proposed method embeds built-in unlearning capability into AI architectures, meeting regulatory requirements and enabling scalable, ethical deployments in sensitive environments.

Abstract: Foundation models have transformed multimedia analysis by enabling robust and
transferable representations across diverse modalities and tasks. However,
their static deployment conflicts with growing societal and regulatory demands
-- particularly the need to unlearn specific data upon request, as mandated by
privacy frameworks such as the GDPR. Traditional unlearning approaches,
including retraining, activation editing, or distillation, are often
computationally expensive, fragile, and ill-suited for real-time or
continuously evolving systems. In this paper, we propose a paradigm shift:
rethinking unlearning not as a retroactive intervention but as a built-in
capability. We introduce a prompt-based learning framework that unifies
knowledge acquisition and removal within a single training phase. Rather than
encoding information in model weights, our approach binds class-level semantics
to dedicated prompt tokens. This design enables instant unlearning simply by
removing the corresponding prompt -- without retraining, model modification, or
access to original data. Experiments demonstrate that our framework preserves
predictive performance on retained classes while effectively erasing forgotten
ones. Beyond utility, our method exhibits strong privacy and security
guarantees: it is resistant to membership inference attacks, and prompt removal
prevents any residual knowledge extraction, even under adversarial conditions.
This ensures compliance with data protection principles and safeguards against
unauthorized access to forgotten information, making the framework suitable for
deployment in sensitive and regulated environments. Overall, by embedding
removability into the architecture itself, this work establishes a new
foundation for designing modular, scalable and ethically responsive AI models.

</details>


### [189] [Instance Generation for Meta-Black-Box Optimization through Latent Space Reverse Engineering](https://arxiv.org/abs/2509.15810)
*Chen Wang,Zeyuan Ma,Zhiguang Cao,Yue-Jiao Gong*

Main category: cs.LG

TL;DR: This paper introduces LSRE, a method to enhance instance diversity for training Meta-Black-Box Optimization (MetaBBO) algorithms, which improves their generalization performance.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the limitations of existing training sets, such as CoCo-BBOB, which lack diversity and pose risks of overfitting in MetaBBO algorithms. Increased diversity in training problem sets can enhance their adaptability and performance on unseen instances.

Method: LSRE employs an autoencoder to transform high-dimensional problem features into a 2D latent space, performs uniform-grid sampling in this latent space for diverse representations, and uses genetic programming to engineer corresponding problem formulas. This leads to the creation of a diversified problem set called Diverse-BBO.

Result: Experiments show that Diverse-BBO allows MetaBBOs to achieve superior generalization performance on both synthetic and realistic scenarios compared to traditional training sets like CoCo-BBOB.

Conclusion: LSRE and Diverse-BBO enhance the generalization capabilities of MetaBBOs by providing a more diverse set of training instances, reducing the risk of overfitting and improving adaptability across problem categories.

Abstract: To relieve intensive human-expertise required to design optimization
algorithms, recent Meta-Black-Box Optimization (MetaBBO) researches leverage
generalization strength of meta-learning to train neural network-based
algorithm design policies over a predefined training problem set, which
automates the adaptability of the low-level optimizers on unseen problem
instances. Currently, a common training problem set choice in existing MetaBBOs
is well-known benchmark suites CoCo-BBOB. Although such choice facilitates the
MetaBBO's development, problem instances in CoCo-BBOB are more or less limited
in diversity, raising the risk of overfitting of MetaBBOs, which might further
results in poor generalization. In this paper, we propose an instance
generation approach, termed as \textbf{LSRE}, which could generate diverse
training problem instances for MetaBBOs to learn more generalizable policies.
LSRE first trains an autoencoder which maps high-dimensional problem features
into a 2-dimensional latent space. Uniform-grid sampling in this latent space
leads to hidden representations of problem instances with sufficient diversity.
By leveraging a genetic-programming approach to search function formulas with
minimal L2-distance to these hidden representations, LSRE reverse engineers a
diversified problem set, termed as \textbf{Diverse-BBO}. We validate the
effectiveness of LSRE by training various MetaBBOs on Diverse-BBO and observe
their generalization performances on either synthetic or realistic scenarios.
Extensive experimental results underscore the superiority of Diverse-BBO to
existing training set choices in MetaBBOs. Further ablation studies not only
demonstrate the effectiveness of design choices in LSRE, but also reveal
interesting insights on instance diversity and MetaBBO's generalization.

</details>


### [190] [A Multi-Scale Graph Neural Process with Cross-Drug Co-Attention for Drug-Drug Interactions Prediction](https://arxiv.org/abs/2509.15256)
*Zimo Yan,Jie Zhang,Zheng Xie,Yiping Song,Hao Li*

Main category: cs.LG

TL;DR: MPNP-DDI is a drug interaction prediction framework that uses multi-scale graph representations and integrated uncertainty estimation to deliver accurate and confident predictions.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in drug-drug interaction prediction methods, particularly difficulties in capturing multi-scale structural features and lacking mechanisms for uncertainty assessment.

Method: The proposed framework, MPNP-DDI, employs a message-passing mechanism to learn multi-scale graph representations and employs a cross-drug co-attention mechanism for embedding generation, alongside a neural process module for uncertainty analysis.

Result: Experiments show MPNP-DDI surpasses state-of-the-art baselines in predicting drug-drug interactions on benchmark datasets.

Conclusion: MPNP-DDI is a promising computational approach for pharmacovigilance and precision medicine, delivering accurate and uncertainty-aware predictions through its multi-scale structural focus.

Abstract: Accurate prediction of drug-drug interactions (DDI) is crucial for medication
safety and effective drug development. However, existing methods often struggle
to capture structural information across different scales, from local
functional groups to global molecular topology, and typically lack mechanisms
to quantify prediction confidence. To address these limitations, we propose
MPNP-DDI, a novel Multi-scale Graph Neural Process framework. The core of
MPNP-DDI is a unique message-passing scheme that, by being iteratively applied,
learns a hierarchy of graph representations at multiple scales. Crucially, a
cross-drug co-attention mechanism then dynamically fuses these multi-scale
representations to generate context-aware embeddings for interacting drug
pairs, while an integrated neural process module provides principled
uncertainty estimation. Extensive experiments demonstrate that MPNP-DDI
significantly outperforms state-of-the-art baselines on benchmark datasets. By
providing accurate, generalizable, and uncertainty-aware predictions built upon
multi-scale structural features, MPNP-DDI represents a powerful computational
tool for pharmacovigilance, polypharmacy risk assessment, and precision
medicine.

</details>


### [191] [Generative AI Meets Wireless Sensing: Towards Wireless Foundation Model](https://arxiv.org/abs/2509.15258)
*Zheng Yang,Guoxuan Chi,Chenshu Wu,Hanyu Liu,Yuchong Gao,Yunhao Liu,Jie Xu,Tony Xiao Han*

Main category: cs.LG

TL;DR: This paper surveys the integration of Generative AI (GenAI) into wireless sensing, exploring its modes of integration and the applicability of generative models.


<details>
  <summary>Details</summary>
Motivation: To advance wireless sensing applications by integrating Generative AI for improved data synthesis, domain adaptation, and overall sensing performance.

Method: The paper investigates two integration modes of GenAI in wireless sensing: as a plugin for task-specific models and as a direct solver for sensing tasks. It also analyzes generative model types like GANs, VAEs, and diffusion models.

Result: The authors outline the applicability and advantages of generative models in enhancing wireless sensing tasks and identify challenges in GenAI's application.

Conclusion: The paper proposes development towards a wireless foundation model offering scalable, adaptable, and efficient signal understanding for diverse wireless sensing tasks.

Abstract: Generative Artificial Intelligence (GenAI) has made significant advancements
in fields such as computer vision (CV) and natural language processing (NLP),
demonstrating its capability to synthesize high-fidelity data and improve
generalization. Recently, there has been growing interest in integrating GenAI
into wireless sensing systems. By leveraging generative techniques such as data
augmentation, domain adaptation, and denoising, wireless sensing applications,
including device localization, human activity recognition, and environmental
monitoring, can be significantly improved. This survey investigates the
convergence of GenAI and wireless sensing from two complementary perspectives.
First, we explore how GenAI can be integrated into wireless sensing pipelines,
focusing on two modes of integration: as a plugin to augment task-specific
models and as a solver to directly address sensing tasks. Second, we analyze
the characteristics of mainstream generative models, such as Generative
Adversarial Networks (GANs), Variational Autoencoders (VAEs), and diffusion
models, and discuss their applicability and unique advantages across various
wireless sensing tasks. We further identify key challenges in applying GenAI to
wireless sensing and outline a future direction toward a wireless foundation
model: a unified, pre-trained design capable of scalable, adaptable, and
efficient signal understanding across diverse sensing tasks.

</details>


### [192] [Inference Offloading for Cost-Sensitive Binary Classification at the Edge](https://arxiv.org/abs/2509.15674)
*Vishnu Narayanan Moothedath,Umang Agarwal,Umeshraja N,James Richard Gross,Jaya Prakash Champati,Sharayu Moharir*

Main category: cs.LG

TL;DR: This paper investigates optimizing binary classification in an edge intelligence system with hierarchical inference, balancing local accuracy and remote offloading costs through an adaptive, model-agnostic, and feedback-driven online framework called H2T2.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of effectively balancing the trade-off between classification accuracy and offloading costs in edge intelligence systems, where false negatives are more costly than false positives.

Method: The proposed method uses an online learning framework with adaptive confidence thresholds for the local model, determining whether a sample is classified locally or offloaded. A closed-form solution is provided for calibrated models, and a general policy (H2T2) is introduced for uncalibrated models, which learns adaptively during real-time inference.

Result: H2T2 outperforms naive and single-threshold policies in simulations on real-world datasets. It even occasionally beats offline optimal solutions. It is robust to distribution shifts and adapts well to mismatched classifiers.

Conclusion: The H2T2 policy offers an effective, adaptive solution for hierarchical inference systems by achieving a balance between accuracy and offloading costs with sublinear regret, making it practical and reliable under real-world conditions.

Abstract: We focus on a binary classification problem in an edge intelligence system
where false negatives are more costly than false positives. The system has a
compact, locally deployed model, which is supplemented by a larger, remote
model, which is accessible via the network by incurring an offloading cost. For
each sample, our system first uses the locally deployed model for inference.
Based on the output of the local model, the sample may be offloaded to the
remote model. This work aims to understand the fundamental trade-off between
classification accuracy and these offloading costs within such a hierarchical
inference (HI) system. To optimize this system, we propose an online learning
framework that continuously adapts a pair of thresholds on the local model's
confidence scores. These thresholds determine the prediction of the local model
and whether a sample is classified locally or offloaded to the remote model. We
present a closed-form solution for the setting where the local model is
calibrated. For the more general case of uncalibrated models, we introduce
H2T2, an online two-threshold hierarchical inference policy, and prove it
achieves sublinear regret. H2T2 is model-agnostic, requires no training, and
learns in the inference phase using limited feedback. Simulations on real-world
datasets show that H2T2 consistently outperforms naive and single-threshold HI
policies, sometimes even surpassing offline optima. The policy also
demonstrates robustness to distribution shifts and adapts effectively to
mismatched classifiers.

</details>


### [193] [IEFS-GMB: Gradient Memory Bank-Guided Feature Selection Based on Information Entropy for EEG Classification of Neurological Disorders](https://arxiv.org/abs/2509.15259)
*Liang Zhang,Hanyang Dong,Jia-Hong Gao,Yi Sun,Kuntao Xiao,Wanli Yang,Zhao Lv,Shurong Sheng*

Main category: cs.LG

TL;DR: This paper introduces IEFS-GMB, a novel EEG feature selection method using information entropy and gradient memory banks, to improve classification accuracy and interpretability for neurological disorder diagnosis.


<details>
  <summary>Details</summary>
Motivation: To address the performance challenges in EEG classification due to low signal-to-noise ratios and the lack of robust, interpretability-focused EEG-specific feature selection methods.

Method: The proposed method, IEFS-GMB, creates a dynamic memory bank of historical gradients, computes feature importance using information entropy, and applies weighted selection of EEG features, enhancing neural network encoders.

Result: IEFS-GMB improved classification accuracy by 0.64% to 6.45% across four datasets, outperforming baseline models and four alternative feature selection methods.

Conclusion: IEFS-GMB enhances the diagnostic utility of neural network encoders for EEG analysis, making it a promising tool for clinical applications due to its improved accuracy, robustness, and interpretability.

Abstract: Deep learning-based EEG classification is crucial for the automated detection
of neurological disorders, improving diagnostic accuracy and enabling early
intervention. However, the low signal-to-noise ratio of EEG signals limits
model performance, making feature selection (FS) vital for optimizing
representations learned by neural network encoders. Existing FS methods are
seldom designed specifically for EEG diagnosis; many are architecture-dependent
and lack interpretability, limiting their applicability. Moreover, most rely on
single-iteration data, resulting in limited robustness to variability. To
address these issues, we propose IEFS-GMB, an Information Entropy-based Feature
Selection method guided by a Gradient Memory Bank. This approach constructs a
dynamic memory bank storing historical gradients, computes feature importance
via information entropy, and applies entropy-based weighting to select
informative EEG features. Experiments on four public neurological disease
datasets show that encoders enhanced with IEFS-GMB achieve accuracy
improvements of 0.64% to 6.45% over baseline models. The method also
outperforms four competing FS techniques and improves model interpretability,
supporting its practical use in clinical settings.

</details>


### [194] [FedHK-MVFC: Federated Heat Kernel Multi-View Clustering](https://arxiv.org/abs/2509.15844)
*Kristina P. Sinaga*

Main category: cs.LG

TL;DR: The paper introduces a framework for multi-view clustering in healthcare data using quantum field theory concepts, specifically heat-kernel coefficients, for both centralized and federated privacy-preserving analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable advanced geometry-aware federated learning for analyzing sensitive medical data while ensuring data privacy and clinical relevance.

Method: The framework involves the Heat Kernel Distance (HKD) transformation with guaranteed convergence, algorithms like HK-MVFC for centralized analysis, and FedHK-MVFC for secure federated learning using differential privacy and secure aggregation.

Result: Tests on synthetic data show improved clustering accuracy (8–12%), reduced communication (70%), and efficiency retention (98.2%). Validation on 10,000 patient records demonstrates practical utility in healthcare data analytics.

Conclusion: The study sets a new standard for privacy-preserving, geometry-aware federated learning in healthcare, combining theoretical rigor with clinical applicability.

Abstract: In the realm of distributed AI and privacy-focused medical applications, we
propose a framework for multi-view clustering that links quantum field theory
with federated healthcare analytics. Our method uses heat-kernel coefficients
from spectral analysis to convert Euclidean distances into geometry-aware
similarity measures, capturing the structure of diverse medical data. We lay
this out through the Heat Kernel Distance (HKD) transformation with convergence
guarantees. Two algorithms are developed: Heat Kernel-Enhanced Multi-View Fuzzy
Clustering (HK-MVFC) for central analysis, and Federated Heat Kernel Multi-View
Fuzzy Clustering (FedHK-MVFC) for secure, privacy-preserving learning across
hospitals using differential privacy and secure aggregation to facilitate
HIPAA-compliant collaboration. Tests on synthetic datasets of cardiovascular
patients show an $8-12 \%$ increase in clustering accuracy, $70 \%$ reduced
communication, and $98.2 \%$ efficiency retention over centralized methods.
Validated on 10,000 patient records across two hospitals, it proves useful for
collaborative phenotyping involving ECG, cardiac imaging, and behavioral data.
Our theoretical contributions include update rules with proven convergence,
adaptive view weighting, and privacy-preserving protocols. This presents a new
standard for geometry-aware federated learning in healthcare, turning advanced
math into workable solutions for analyzing sensitive medical data while
ensuring both rigor and clinical relevance.

</details>


### [195] [A Weak Supervision Approach for Monitoring Recreational Drug Use Effects in Social Media](https://arxiv.org/abs/2509.15266)
*Lucía Prieto-Santamaría,Alba Cortés Iglesias,Claudio Vidal Giné,Fermín Fernández Calderón,Óscar M. Lozano,Alejandro Rodríguez-González*

Main category: cs.LG

TL;DR: The paper uses Twitter data to analyze real-world effects of recreational drugs (ecstasy, GHB, and 2C-B) by extracting user-reported experiences and predicting whether tweets indicate positive or negative effects using advanced machine learning techniques.


<details>
  <summary>Details</summary>
Motivation: Traditional surveillance systems underrepresent user experiences related to recreational drug effects. Leveraging social media data provides a rich source of information to better understand these experiences.

Method: Researchers employed slang term lists and MetaMap for biomedical concept extraction to weakly annotate tweets. They used expert-guided heuristics for polarity labeling, performed statistical analyses, and applied machine learning techniques (e.g., cost-sensitive learning and synthetic oversampling) to predict tweet polarity.

Result: The study identified over 92,000 tweets mentioning the substances and developed machine learning models with strong predictive performance. An eXtreme Gradient Boosting model achieved high accuracy (F1=0.885, AUPRC=0.934).

Conclusion: Twitter can be effectively used to detect drug-specific phenotypic effects, and polarity classification models offer high-accuracy tools for pharmacovigilance and real-time drug effect monitoring.

Abstract: Understanding the real-world effects of recreational drug use remains a
critical challenge in public health and biomedical research, especially as
traditional surveillance systems often underrepresent user experiences. In this
study, we leverage social media (specifically Twitter) as a rich and unfiltered
source of user-reported effects associated with three emerging psychoactive
substances: ecstasy, GHB, and 2C-B. By combining a curated list of slang terms
with biomedical concept extraction via MetaMap, we identified and weakly
annotated over 92,000 tweets mentioning these substances. Each tweet was
labeled with a polarity reflecting whether it reported a positive or negative
effect, following an expert-guided heuristic process. We then performed
descriptive and comparative analyses of the reported phenotypic outcomes across
substances and trained multiple machine learning classifiers to predict
polarity from tweet content, accounting for strong class imbalance using
techniques such as cost-sensitive learning and synthetic oversampling. The top
performance on the test set was obtained from eXtreme Gradient Boosting with
cost-sensitive learning (F1 = 0.885, AUPRC = 0.934). Our findings reveal that
Twitter enables the detection of substance-specific phenotypic effects, and
that polarity classification models can support real-time pharmacovigilance and
drug effect characterization with high accuracy.

</details>


### [196] [ToFU: Transforming How Federated Learning Systems Forget User Data](https://arxiv.org/abs/2509.15861)
*Van-Tuan Tran,Hong-Hanh Nguyen-Le,Quoc-Viet Pham*

Main category: cs.LG

TL;DR: This paper introduces a framework called ToFU (Transformation-guided Federated Unlearning) to address privacy risks in federated learning models by reducing data memorization and improving unlearning efficiency.


<details>
  <summary>Details</summary>
Motivation: Federated learning systems pose privacy risks as neural networks may unintentionally memorize sensitive training data, exposing models to inference and reconstruction attacks.

Method: The authors propose a framework called ToFU, integrating transformations during the learning process to limit data memorization and simplify the process of data unlearning later on.

Result: Experiments on benchmarks like CIFAR-10, CIFAR-100, and MUFAC demonstrate that ToFU improves unlearning performance, enhances existing unlearning methods, and reduces the time for unlearning.

Conclusion: ToFU establishes an effective framework designed to simplify data unlearning by inherently controlling memorization during the training phase of federated learning models.

Abstract: Neural networks unintentionally memorize training data, creating privacy
risks in federated learning (FL) systems, such as inference and reconstruction
attacks on sensitive data. To mitigate these risks and to comply with privacy
regulations, Federated Unlearning (FU) has been introduced to enable
participants in FL systems to remove their data's influence from the global
model. However, current FU methods primarily act post-hoc, struggling to
efficiently erase information deeply memorized by neural networks. We argue
that effective unlearning necessitates a paradigm shift: designing FL systems
inherently amenable to forgetting. To this end, we propose a
learning-to-unlearn Transformation-guided Federated Unlearning (ToFU) framework
that incorporates transformations during the learning process to reduce
memorization of specific instances. Our theoretical analysis reveals how
transformation composition provably bounds instance-specific information,
directly simplifying subsequent unlearning. Crucially, ToFU can work as a
plug-and-play framework that improves the performance of existing FU methods.
Experiments on CIFAR-10, CIFAR-100, and the MUFAC benchmark show that ToFU
outperforms existing FU baselines, enhances performance when integrated with
current methods, and reduces unlearning time.

</details>


### [197] [Modeling Transformers as complex networks to analyze learning dynamics](https://arxiv.org/abs/2509.15269)
*Elisabetta Rocchetti*

Main category: cs.LG

TL;DR: The paper explores learning dynamics in Large Language Models using Complex Network Theory, representing model components as a graph to uncover evolving structural phases during training.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand how LLMs acquire complex capabilities during training, a central question in mechanistic interpretability.

Method: A Transformer-based LLM is represented as a directed, weighted graph based on intervention-based ablation. Graph metrics are tracked across training checkpoints to analyze the structural evolution of the model.

Result: The study identifies distinct phases: exploration, consolidation, and refinement in network evolution, along with stable hierarchies of information spreaders and dynamic configurations of information gatherers.

Conclusion: The work demonstrates the utility of a network-based perspective for visualizing and understanding principles behind functional circuit formation within LLMs.

Abstract: The process by which Large Language Models (LLMs) acquire complex
capabilities during training remains a key open question in mechanistic
interpretability. This project investigates whether these learning dynamics can
be characterized through the lens of Complex Network Theory (CNT). I introduce
a novel methodology to represent a Transformer-based LLM as a directed,
weighted graph where nodes are the model's computational components (attention
heads and MLPs) and edges represent causal influence, measured via an
intervention-based ablation technique. By tracking the evolution of this
component-graph across 143 training checkpoints of the Pythia-14M model on a
canonical induction task, I analyze a suite of graph-theoretic metrics. The
results reveal that the network's structure evolves through distinct phases of
exploration, consolidation, and refinement. Specifically, I identify the
emergence of a stable hierarchy of information spreader components and a
dynamic set of information gatherer components, whose roles reconfigure at key
learning junctures. This work demonstrates that a component-level network
perspective offers a powerful macroscopic lens for visualizing and
understanding the self-organizing principles that drive the formation of
functional circuits in LLMs.

</details>


### [198] [RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation](https://arxiv.org/abs/2509.15965)
*Chao Yu,Yuanqing Wang,Zhen Guo,Hao Lin,Si Xu,Hongzhi Zang,Quanlu Zhang,Yongji Wu,Chunyang Zhu,Junhao Hu,Zixiao Huang,Mingjie Wei,Yuqing Xie,Ke Yang,Bo Dai,Zhexuan Xu,Xiangyuan Wang,Xu Fu,Zhihao Liu,Kang Chen,Weilin Liu,Gang Liu,Boxun Li,Jianlei Yang,Zhi Yang,Guohao Dai,Yu Wang*

Main category: cs.LG

TL;DR: The paper introduces RLinf, an efficient RL training system leveraging a novel design approach called M2Flow for optimizing execution flows, yielding significant speedup in RL task training.


<details>
  <summary>Details</summary>
Motivation: Current RL training systems suffer from low hardware utilization and slow training due to system inflexibility in handling heterogeneous RL workflows.

Method: The authors propose RLinf, built upon the M2Flow paradigm which dynamically transforms RL workflows into optimized execution flows through techniques like context switching, elastic pipelining, and profiling-guided scheduling.

Result: Evaluations on reasoning and embodied RL tasks show RLinf achieving 1.1x-2.13x speedup in training throughput compared to state-of-the-art systems.

Conclusion: RLinf offers a system-level solution that significantly improves RL training efficiency, addressing flexibility challenges and setting a higher performance benchmark in the field.

Abstract: Reinforcement learning (RL) has demonstrated immense potential in advancing
artificial general intelligence, agentic intelligence, and embodied
intelligence. However, the inherent heterogeneity and dynamicity of RL
workflows often lead to low hardware utilization and slow training on existing
systems. In this paper, we present RLinf, a high-performance RL training system
based on our key observation that the major roadblock to efficient RL training
lies in system flexibility. To maximize flexibility and efficiency, RLinf is
built atop a novel RL system design paradigm called macro-to-micro flow
transformation (M2Flow), which automatically breaks down high-level,
easy-to-compose RL workflows at both the temporal and spatial dimensions, and
recomposes them into optimized execution flows. Supported by RLinf worker's
adaptive communication capability, we devise context switching and elastic
pipelining to realize M2Flow transformation, and a profiling-guided scheduling
policy to generate optimal execution plans. Extensive evaluations on both
reasoning RL and embodied RL tasks demonstrate that RLinf consistently
outperforms state-of-the-art systems, achieving 1.1x-2.13x speedup in
end-to-end training throughput.

</details>


### [199] [Partial Column Generation with Graph Neural Networks for Team Formation and Routing](https://arxiv.org/abs/2509.15275)
*Giacomo Dall'Olio,Rainer Kolisch,Yaoxin Wu*

Main category: cs.LG

TL;DR: A novel partial column generation strategy leveraging graph neural networks is proposed to solve the team formation and routing problem efficiently.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of optimizing team formation and routing in large-scale problems with applications in airport, healthcare, and maintenance domains, for which traditional methods are computationally demanding.

Method: The authors introduce a machine learning model based on graph neural networks to predict and prioritize pricing problems likely to produce columns with negative reduced costs.

Result: Experiments validate that the proposed method improves performance and outperforms traditional approaches under tight time constraints, particularly for complex cases.

Conclusion: The study proposes an innovative machine learning-based strategy that optimizes computational efforts in solving team formation and routing problems and demonstrates superior efficiency compared to existing methods.

Abstract: The team formation and routing problem is a challenging optimization problem
with several real-world applications in fields such as airport, healthcare, and
maintenance operations. To solve this problem, exact solution methods based on
column generation have been proposed in the literature. In this paper, we
propose a novel partial column generation strategy for settings with multiple
pricing problems, based on predicting which ones are likely to yield columns
with a negative reduced cost. We develop a machine learning model tailored to
the team formation and routing problem that leverages graph neural networks for
these predictions. Computational experiments demonstrate that applying our
strategy enhances the solution method and outperforms traditional partial
column generation approaches from the literature, particularly on hard
instances solved under a tight time limit.

</details>


### [200] [Top-$k$ Feature Importance Ranking](https://arxiv.org/abs/2509.15420)
*Yuxi Chen,Tiffany Tang,Genevera Allen*

Main category: cs.LG

TL;DR: The paper introduces RAMPART, a new algorithm focusing on accurately ranking important features in machine learning.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of methods specifically tailored for accurately ranking important features in interpretable machine learning, a critical task in scientific discovery and decision-making.

Method: RAMPART combines adaptive sequential halving to refine focus on key features and efficient ensembling through observation and feature subsampling. It leverages existing feature importance measures but optimizes explicitly for ranking accuracy.

Result: The authors provide theoretical guarantees for RAMPART's accuracy in top-k feature ranking and demonstrate its superior performance through simulations and a genomics case study.

Conclusion: RAMPART is a robust and effective framework for ranking important features, outperforming traditional methods and applicable in high-dimensional data analysis.

Abstract: Accurate ranking of important features is a fundamental challenge in
interpretable machine learning with critical applications in scientific
discovery and decision-making. Unlike feature selection and feature importance,
the specific problem of ranking important features has received considerably
less attention. We introduce RAMPART (Ranked Attributions with MiniPatches And
Recursive Trimming), a framework that utilizes any existing feature importance
measure in a novel algorithm specifically tailored for ranking the top-$k$
features. Our approach combines an adaptive sequential halving strategy that
progressively focuses computational resources on promising features with an
efficient ensembling technique using both observation and feature subsampling.
Unlike existing methods that convert importance scores to ranks as
post-processing, our framework explicitly optimizes for ranking accuracy. We
provide theoretical guarantees showing that RAMPART achieves the correct
top-$k$ ranking with high probability under mild conditions, and demonstrate
through extensive simulation studies that RAMPART consistently outperforms
popular feature importance methods, concluding with a high-dimensional genomics
case study.

</details>


### [201] [Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning](https://arxiv.org/abs/2509.15279)
*Chi Liu,Derek Li,Yan Shu,Robin Chen,Derek Duan,Teng Fang,Bryan Dai*

Main category: cs.LG

TL;DR: The paper introduces Fleming-R1, a model designed for expert medical reasoning by integrating reasoning-oriented data strategies, structured initializations, and reinforcement learning techniques.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in achieving expert-level clinical reasoning in large language models, focusing on accurate answers and transparent reasoning processes.

Method: They utilized a combination of Reasoning-Oriented Data Strategy (RODS), Chain-of-Thought cold starts, and a two-stage Reinforcement Learning from Verifiable Rewards (RLVR) framework for training the model.

Result: Fleming-R1 surpasses larger baselines in parameter efficiency, with the 7B model outperforming similar models and the 32B model achieving near parity with GPT-4o while outperforming other open-source alternatives.

Conclusion: The findings suggest that structured data design, reasoning-oriented initialization, and verifiable reinforcement learning significantly enhance clinical reasoning, advocating for safer and transparent deployment of medical AI models.

Abstract: While large language models show promise in medical applications, achieving
expert-level clinical reasoning remains challenging due to the need for both
accurate answers and transparent reasoning processes. To address this
challenge, we introduce Fleming-R1, a model designed for verifiable medical
reasoning through three complementary innovations. First, our
Reasoning-Oriented Data Strategy (RODS) combines curated medical QA datasets
with knowledge-graph-guided synthesis to improve coverage of underrepresented
diseases, drugs, and multi-hop reasoning chains. Second, we employ
Chain-of-Thought (CoT) cold start to distill high-quality reasoning
trajectories from teacher models, establishing robust inference priors. Third,
we implement a two-stage Reinforcement Learning from Verifiable Rewards (RLVR)
framework using Group Relative Policy Optimization, which consolidates core
reasoning skills while targeting persistent failure modes through adaptive
hard-sample mining. Across diverse medical benchmarks, Fleming-R1 delivers
substantial parameter-efficient improvements: the 7B variant surpasses much
larger baselines, while the 32B model achieves near-parity with GPT-4o and
consistently outperforms strong open-source alternatives. These results
demonstrate that structured data design, reasoning-oriented initialization, and
verifiable reinforcement learning can advance clinical reasoning beyond simple
accuracy optimization. We release Fleming-R1 publicly to promote transparent,
reproducible, and auditable progress in medical AI, enabling safer deployment
in high-stakes clinical environments.

</details>


### [202] [Personalized Federated Learning with Heat-Kernel Enhanced Tensorized Multi-View Clustering](https://arxiv.org/abs/2509.16101)
*Kristina P. Sinaga*

Main category: cs.LG

TL;DR: The paper introduces a novel personalized federated learning framework using heat-kernel enhanced tensorized fuzzy clustering and tensor decomposition for high-dimensional multi-view data.


<details>
  <summary>Details</summary>
Motivation: To efficiently handle high-dimensional multi-view data and preserve privacy in federated learning by leveraging advanced tensor decomposition techniques.

Method: Utilizes heat-kernel coefficients, tensor decomposition techniques like Tucker and CANDECOMP/PARAFAC, and a dual-level optimization scheme for local clustering and federated aggregation of tensor factors.

Result: The proposed method allows for efficient and privacy-preserving multi-view data clustering with reduced communication costs, revealing hidden patterns in client-specific data.

Conclusion: The framework robustly handles high-dimensional data, integrates privacy mechanisms, and achieves communication savings while enhancing federated learning capabilities.

Abstract: We present a robust personalized federated learning framework that leverages
heat-kernel enhanced tensorized multi-view fuzzy c-means clustering with
advanced tensor decomposition techniques. Our approach integrates heat-kernel
coefficients adapted from quantum field theory with Tucker decomposition and
canonical polyadic decomposition (CANDECOMP/PARAFAC) to transform conventional
distance metrics and efficiently represent high-dimensional multi-view
structures. The framework employs matriculation and vectorization techniques to
facilitate the discovery of hidden structures and multilinear relationships via
N-way generalized tensors. The proposed method introduces a dual-level
optimization scheme: local heat-kernel enhanced fuzzy clustering with tensor
decomposition operating on order-N input tensors, and federated aggregation of
tensor factors with privacy-preserving personalization mechanisms. The local
stage employs tensorized kernel Euclidean distance transformations and Tucker
decomposition to discover client-specific patterns in multi-view tensor data,
while the global aggregation process coordinates tensor factors (core tensors
and factor matrices) across clients through differential privacy-preserving
protocols. This tensorized approach enables efficient handling of
high-dimensional multi-view data with significant communication savings through
low-rank tensor approximations.

</details>


### [203] [Hybrid unary-binary design for multiplier-less printed Machine Learning classifiers](https://arxiv.org/abs/2509.15316)
*Giorgos Armeniakos,Theodoros Mantzakidis,Dimitrios Soudris*

Main category: cs.LG

TL;DR: This study introduces a hybrid unary-binary architecture for implementing ML classifiers in printed electronics, improving area and power efficiency with minimal accuracy trade-offs.


<details>
  <summary>Details</summary>
Motivation: Printed electronics offer cost-efficient machine learning hardware, but their large feature sizes restrict classifier complexity. The authors seek alternatives to enhance efficiency while leveraging PE's low costs.

Method: A hybrid unary-binary architecture is proposed to simplify circuit designs and eliminate encoders. Additionally, architecture-aware training is employed to optimize area and power efficiency.

Result: Evaluation over six datasets demonstrated 46% reductions in area and 39% reductions in power consumption, with negligible accuracy loss compared to other state-of-the-art designs.

Conclusion: The proposed hybrid architecture successfully enhances efficiency for ML classifiers in printed electronics, pushing the boundaries of hardware optimization for PE applications.

Abstract: Printed Electronics (PE) provide a flexible, cost-efficient alternative to
silicon for implementing machine learning (ML) circuits, but their large
feature sizes limit classifier complexity. Leveraging PE's low fabrication and
NRE costs, designers can tailor hardware to specific ML models, simplifying
circuit design. This work explores alternative arithmetic and proposes a hybrid
unary-binary architecture that removes costly encoders and enables efficient,
multiplier-less execution of MLP classifiers. We also introduce
architecture-aware training to further improve area and power efficiency.
Evaluation on six datasets shows average reductions of 46% in area and 39% in
power, with minimal accuracy loss, surpassing other state-of-the-art MLP
designs.

</details>


### [204] [Kuramoto Orientation Diffusion Models](https://arxiv.org/abs/2509.15328)
*Yue Song,T. Anderson Keller,Sevan Brodjian,Takeru Miyato,Yisong Yue,Pietro Perona,Max Welling*

Main category: cs.LG

TL;DR: The paper introduces a generative model leveraging Kuramoto dynamics to improve structured image generation, especially for orientation-rich datasets.


<details>
  <summary>Details</summary>
Motivation: Standard generative models struggle with coherent angular patterns like fingerprints and textures.

Method: A score-based generative model using stochastic Kuramoto dynamics to synchronize and then desynchronize phase variables for image generation.

Result: The method enhances generation quality on orientation-rich datasets and achieves competitive results on general image benchmarks.

Conclusion: Biologically-inspired synchronization dynamics effectively integrate structured priors into generative modeling, promising advancements in image synthesis.

Abstract: Orientation-rich images, such as fingerprints and textures, often exhibit
coherent angular directional patterns that are challenging to model using
standard generative approaches based on isotropic Euclidean diffusion.
Motivated by the role of phase synchronization in biological systems, we
propose a score-based generative model built on periodic domains by leveraging
stochastic Kuramoto dynamics in the diffusion process. In neural and physical
systems, Kuramoto models capture synchronization phenomena across coupled
oscillators -- a behavior that we re-purpose here as an inductive bias for
structured image generation. In our framework, the forward process performs
\textit{synchronization} among phase variables through globally or locally
coupled oscillator interactions and attraction to a global reference phase,
gradually collapsing the data into a low-entropy von Mises distribution. The
reverse process then performs \textit{desynchronization}, generating diverse
patterns by reversing the dynamics with a learned score function. This approach
enables structured destruction during forward diffusion and a hierarchical
generation process that progressively refines global coherence into fine-scale
details. We implement wrapped Gaussian transition kernels and periodicity-aware
networks to account for the circular geometry. Our method achieves competitive
results on general image benchmarks and significantly improves generation
quality on orientation-dense datasets like fingerprints and textures.
Ultimately, this work demonstrates the promise of biologically inspired
synchronization dynamics as structured priors in generative modeling.

</details>


### [205] [Global Pre-fixing, Local Adjusting: A Simple yet Effective Contrastive Strategy for Continual Learning](https://arxiv.org/abs/2509.15347)
*Jia Tang,Xinrui Wang,Songcan Chen*

Main category: cs.LG

TL;DR: The paper introduces GPLASC, a strategy for supervised contrastive learning in continual learning to alleviate catastrophic forgetting by addressing both inter-task and intra-task feature confusion through structured representation.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the limitations in performance caused by feature confusion in both inter-task and intra-task levels when using contrastive approaches in continual learning.

Method: The proposed method, GPLASC, involves dividing the hypersphere of representations into non-overlapping regions to avoid task-level confusion. These regions are structured using Equiangular Tight Frames (ETFs) at both inter-task and intra-task levels.

Result: The proposed GPLASC approach ensures discriminative features for both inter-task and intra-task levels, significantly reducing feature confusion and improving performance on continual learning benchmarks.

Conclusion: GPLASC can seamlessly integrate into existing contrastive continual learning frameworks, enhancing transferable and less forgetful representations as validated by extensive experiments.

Abstract: Continual learning (CL) involves acquiring and accumulating knowledge from
evolving tasks while alleviating catastrophic forgetting. Recently, leveraging
contrastive loss to construct more transferable and less forgetful
representations has been a promising direction in CL. Despite advancements,
their performance is still limited due to confusion arising from both
inter-task and intra-task features. To address the problem, we propose a simple
yet effective contrastive strategy named \textbf{G}lobal \textbf{P}re-fixing,
\textbf{L}ocal \textbf{A}djusting for \textbf{S}upervised \textbf{C}ontrastive
learning (GPLASC). Specifically, to avoid task-level confusion, we divide the
entire unit hypersphere of representations into non-overlapping regions, with
the centers of the regions forming an inter-task pre-fixed \textbf{E}quiangular
\textbf{T}ight \textbf{F}rame (ETF). Meanwhile, for individual tasks, our
method helps regulate the feature structure and form intra-task adjustable ETFs
within their respective allocated regions. As a result, our method
\textit{simultaneously} ensures discriminative feature structures both between
tasks and within tasks and can be seamlessly integrated into any existing
contrastive continual learning framework. Extensive experiments validate its
effectiveness.

</details>


### [206] [ThermalGuardian: Temperature-Aware Testing of Automotive Deep Learning Frameworks](https://arxiv.org/abs/2509.15815)
*Yinglong Zou,Juan Zhai,Chunrong Fang,Zhenyu Chen*

Main category: cs.LG

TL;DR: The paper introduces ThermalGuardian, a testing method tailored for automotive deep learning frameworks, addressing quality issues caused by GPU temperature fluctuations.


<details>
  <summary>Details</summary>
Motivation: Current deep learning frameworks for autonomous driving face critical performance and quality issues under extreme vehicular temperature conditions, which aren't addressed by existing testing methods.

Method: ThermalGuardian introduces model mutation rules targeting temperature-sensitive operators, simulates GPU temperature variations using Newton's law of cooling, and adjusts GPU frequency based on real-time temperature.

Result: ThermalGuardian successfully identifies quality flaws such as operator delays, precision errors, and synchronization issues due to temperature-induced GPU frequency changes.

Conclusion: The approach provides a significant advancement in testing automotive deep learning frameworks, ensuring robust performance across temperature-varying environments.

Abstract: Deep learning models play a vital role in autonomous driving systems,
supporting critical functions such as environmental perception. To accelerate
model inference, these deep learning models' deployment relies on automotive
deep learning frameworks, for example, PaddleInference in Apollo and TensorRT
in AutoWare. However, unlike deploying deep learning models on the cloud,
vehicular environments experience extreme ambient temperatures varying from
-40{\deg}C to 50{\deg}C, significantly impacting GPU temperature. Additionally,
heats generated when computing further lead to the GPU temperature increase.
These temperature fluctuations lead to dynamic GPU frequency adjustments
through mechanisms such as DVFS. However, automotive deep learning frameworks
are designed without considering the impact of temperature-induced frequency
variations. When deployed on temperature-varying GPUs, these frameworks suffer
critical quality issues: compute-intensive operators face delays or errors,
high/mixed-precision operators suffer from precision errors, and time-series
operators suffer from synchronization issues. The above quality issues cannot
be detected by existing deep learning framework testing methods because they
ignore temperature's effect on the deep learning framework quality. To bridge
this gap, we propose ThermalGuardian, the first automotive deep learning
framework testing method under temperature-varying environments. Specifically,
ThermalGuardian generates test input models using model mutation rules
targeting temperature-sensitive operators, simulates GPU temperature
fluctuations based on Newton's law of cooling, and controls GPU frequency based
on real-time GPU temperature.

</details>


### [207] [Probabilistic Conformal Coverage Guarantees in Small-Data Settings](https://arxiv.org/abs/2509.15349)
*Petrus H. Zwart*

Main category: cs.LG

TL;DR: The paper proposes Small Sample Beta Correction (SSBC) to enhance conformal prediction's coverage guarantees by addressing variance over calibration draws.


<details>
  <summary>Details</summary>
Motivation: Conformal prediction suffers from variability in coverage across calibration samples, which hampers effective risk control in applications.

Method: The authors propose SSBC, a plug-and-play adjustment leveraging the finite-sample distribution of conformal coverage to provide probabilistic guarantees for desired coverage levels.

Result: SSBC ensures that, with a user-defined probability, the deployed predictor meets desired coverage across calibration draws.

Conclusion: By incorporating SSBC, conformal prediction methods achieve more reliable coverage, improving practical risk control and usability.

Abstract: Conformal prediction provides distribution-free prediction sets with
guaranteed marginal coverage. However, in split conformal prediction this
guarantee is training-conditional only in expectation: across many calibration
draws, the average coverage equals the nominal level, but the realized coverage
for a single calibration set may vary substantially. This variance undermines
effective risk control in practical applications. Here we introduce the Small
Sample Beta Correction (SSBC), a plug-and-play adjustment to the conformal
significance level that leverages the exact finite-sample distribution of
conformal coverage to provide probabilistic guarantees, ensuring that with
user-defined probability over the calibration draw, the deployed predictor
achieves at least the desired coverage.

</details>


### [208] [Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification](https://arxiv.org/abs/2509.15591)
*Zinan Lin,Enshu Liu,Xuefei Ning,Junyi Zhu,Wenyu Wang,Sergey Yekhanin*

Main category: cs.LG

TL;DR: The paper presents the Latent Zoning Network (LZN), a unified framework addressing generative modeling, representation learning, and classification in machine learning, showing improvements across diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: To unify the solutions for generative modeling, representation learning, and classification, enabling simplification of ML workflows and fostering task synergy.

Method: The paper introduces LZN, which uses a shared Gaussian latent space to integrate tasks. Encoders encode data into disjoint latent zones, while decoders map latents back to data. ML tasks are expressed by combining encoders and decoders.

Result: LZN shows promising outcomes: it improves existing image generation methods, outperforms leading representation learning methods, and performs joint generation and classification with high accuracy and performance improvements.

Conclusion: LZN demonstrates that a unified ML framework is feasible and effective, offering advancements across multiple ML problems while simplifying pipeline architectures.

Abstract: Generative modeling, representation learning, and classification are three
core problems in machine learning (ML), yet their state-of-the-art (SoTA)
solutions remain largely disjoint. In this paper, we ask: Can a unified
principle address all three? Such unification could simplify ML pipelines and
foster greater synergy across tasks. We introduce Latent Zoning Network (LZN)
as a step toward this goal. At its core, LZN creates a shared Gaussian latent
space that encodes information across all tasks. Each data type (e.g., images,
text, labels) is equipped with an encoder that maps samples to disjoint latent
zones, and a decoder that maps latents back to data. ML tasks are expressed as
compositions of these encoders and decoders: for example, label-conditional
image generation uses a label encoder and image decoder; image embedding uses
an image encoder; classification uses an image encoder and label decoder. We
demonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN
can enhance existing models (image generation): When combined with the SoTA
Rectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without
modifying the training objective. (2) LZN can solve tasks independently
(representation learning): LZN can implement unsupervised representation
learning without auxiliary loss functions, outperforming the seminal MoCo and
SimCLR methods by 9.3% and 0.2%, respectively, on downstream linear
classification on ImageNet. (3) LZN can solve multiple tasks simultaneously
(joint generation and classification): With image and label encoders/decoders,
LZN performs both tasks jointly by design, improving FID and achieving SoTA
classification accuracy on CIFAR10. The code and trained models are available
at https://github.com/microsoft/latent-zoning-networks. The project website is
at https://zinanlin.me/blogs/latent_zoning_networks.html.

</details>


### [209] [Predicting Language Models' Success at Zero-Shot Probabilistic Prediction](https://arxiv.org/abs/2509.15356)
*Kevin Ren,Santiago Cortes-Gomez,Carlos Miguel Patiño,Ananya Joshi,Ruiqi Lyu,Jingjing Tang,Alistair Turcan,Khurram Yamin,Steven Wu,Bryan Wilder*

Main category: cs.LG

TL;DR: This paper studies the zero-shot predictive power of large language models (LLMs) across various tasks and develops metrics to assess their suitability for new prediction tasks.


<details>
  <summary>Details</summary>
Motivation: To understand when users can trust LLMs to provide high-quality predictions for specific tasks, as their performance varies greatly across datasets and tasks.

Method: The authors conducted a large-scale empirical study evaluating LLMs' predictive performance on a diverse array of tabular prediction tasks. They also designed metrics to estimate task suitability for LLMs without requiring labeled data.

Result: LLMs showed inconsistent performance across tasks and datasets, but when they performed well in base tasks, their probability predictions became robust indicators of prediction accuracy. New task-suitability metrics also provided strong signals for LLM performance.

Conclusion: LLMs can excel in certain prediction tasks but fail in others. New task-specific metrics provide a useful framework for predicting their suitability without labeled data.

Abstract: Recent work has investigated the capabilities of large language models (LLMs)
as zero-shot models for generating individual-level characteristics (e.g., to
serve as risk models or augment survey datasets). However, when should a user
have confidence that an LLM will provide high-quality predictions for their
particular task? To address this question, we conduct a large-scale empirical
study of LLMs' zero-shot predictive capabilities across a wide range of tabular
prediction tasks. We find that LLMs' performance is highly variable, both on
tasks within the same dataset and across different datasets. However, when the
LLM performs well on the base prediction task, its predicted probabilities
become a stronger signal for individual-level accuracy. Then, we construct
metrics to predict LLMs' performance at the task level, aiming to distinguish
between tasks where LLMs may perform well and where they are likely unsuitable.
We find that some of these metrics, each of which are assessed without labeled
data, yield strong signals of LLMs' predictive performance on new tasks.

</details>


### [210] [Stochastic Sample Approximations of (Local) Moduli of Continuity](https://arxiv.org/abs/2509.15368)
*Rodion Nazarov,Allen Gehret,Robert Shorten,Jakub Marecek*

Main category: cs.LG

TL;DR: The paper introduces a stochastic approximation for moduli of local continuity to evaluate neural network robustness and repeated use fairness.


<details>
  <summary>Details</summary>
Motivation: To improve the study of neural network robustness and fairness when repeatedly applied in closed-loop models.

Method: The authors revisit the link between generalized derivatives and moduli of local continuity, and propose a non-uniform stochastic sample approximation method.

Result: The paper provides a structured approach to analyzing local continuity for assessing neural network robustness and fairness.

Conclusion: Moduli of local continuity and its stochastic approximation play a critical role in understanding and ensuring neural network reliability and fairness in iterative scenarios.

Abstract: Modulus of local continuity is used to evaluate the robustness of neural
networks and fairness of their repeated uses in closed-loop models. Here, we
revisit a connection between generalized derivatives and moduli of local
continuity, and present a non-uniform stochastic sample approximation for
moduli of local continuity. This is of importance in studying robustness of
neural networks and fairness of their repeated uses.

</details>


### [211] [Information Geometry of Variational Bayes](https://arxiv.org/abs/2509.15641)
*Mohammad Emtiyaz Khan*

Main category: cs.LG

TL;DR: The paper explores the connection between information geometry and variational Bayes, presenting the Bayesian Learning Rule (BLR) to simplify computations and enhance VB for machine learning tasks.


<details>
  <summary>Details</summary>
Motivation: To emphasize the shared foundations of information geometry and variational Bayes and encourage exploration at their intersection, particularly in machine learning.

Method: Using the Bayesian Learning Rule (BLR) to highlight and leverage natural gradients, simplifying Bayes' rule, generalizing quadratic surrogates, and enabling scalability of VB algorithms for large language models.

Result: The paper shows advantages of BLR, such as improved VB computation methods, new generalizations of gradient-based surrogates, and expanded applicability to large-scale machine learning models.

Conclusion: These insights aim to inspire further research at the intersection of information geometry and Bayesian methods to advance the field of machine learning.

Abstract: We highlight a fundamental connection between information geometry and
variational Bayes (VB) and discuss its consequences for machine learning. Under
certain conditions, a VB solution always requires estimation or computation of
natural gradients. We show several consequences of this fact by using the
natural-gradient descent algorithm of Khan and Rue (2023) called the Bayesian
Learning Rule (BLR). These include (i) a simplification of Bayes' rule as
addition of natural gradients, (ii) a generalization of quadratic surrogates
used in gradient-based methods, and (iii) a large-scale implementation of VB
algorithms for large language models. Neither the connection nor its
consequences are new but we further emphasize the common origins of the two
fields of information geometry and Bayes with a hope to facilitate more work at
the intersection of the two fields.

</details>


### [212] [Adversarial generalization of unfolding (model-based) networks](https://arxiv.org/abs/2509.15370)
*Vicky Kouni*

Main category: cs.LG

TL;DR: This paper investigates the adversarial robustness of unfolding networks, which are interpretable models used in solving inverse problems like compressed sensing, by developing theoretical generalization bounds and performing experiments.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical understanding of how unfolding networks perform under adversarial attacks, especially given the critical applications of these networks in domains like medical imaging and cryptography, where robustness is essential.

Method: The authors study 
$l_2$-norm constrained adversarial attacks (using the fast gradient sign method) on overparameterized unfolding networks. They develop a new framework to estimate adversarial Rademacher complexity and use it to derive generalization error bounds tight with respect to attack levels. They also validate their findings with real-world experiments.

Result: They derive tight, theoretical generalization error bounds for unfolding networks against adversarial perturbations and validate the theory with consistent experimental results. They also uncover how overparameterization can enhance robustness.

Conclusion: This work provides the first theoretical foundation for adversarial generalization in unfolding networks and highlights how overparameterization may be leveraged to robustify neural networks efficiently.

Abstract: Unfolding networks are interpretable networks emerging from iterative
algorithms, incorporate prior knowledge of data structure, and are designed to
solve inverse problems like compressed sensing, which deals with recovering
data from noisy, missing observations. Compressed sensing finds applications in
critical domains, from medical imaging to cryptography, where adversarial
robustness is crucial to prevent catastrophic failures. However, a solid
theoretical understanding of the performance of unfolding networks in the
presence of adversarial attacks is still in its infancy. In this paper, we
study the adversarial generalization of unfolding networks when perturbed with
$l_2$-norm constrained attacks, generated by the fast gradient sign method.
Particularly, we choose a family of state-of-the-art overaparameterized
unfolding networks and deploy a new framework to estimate their adversarial
Rademacher complexity. Given this estimate, we provide adversarial
generalization error bounds for the networks under study, which are tight with
respect to the attack level. To our knowledge, this is the first theoretical
analysis on the adversarial generalization of unfolding networks. We further
present a series of experiments on real-world data, with results corroborating
our derived theory, consistently for all data. Finally, we observe that the
family's overparameterization can be exploited to promote adversarial
robustness, shedding light on how to efficiently robustify neural networks.

</details>


### [213] [Generalization and Optimization of SGD with Lookahead](https://arxiv.org/abs/2509.15776)
*Kangcheng Li,Yunwen Lei*

Main category: cs.LG

TL;DR: The paper provides a stability and generalization analysis of the Lookahead optimizer with minibatch SGD, improving upon prior restrictions like Lipschitz continuity assumptions.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand the generalization capabilities of the Lookahead optimizer, addressing gaps in current theoretical studies regarding optimization-generalization relationships.

Method: The paper employs a rigorous stability analysis, using on-average model stability to derive generalization bounds for convex and strongly convex problems without relying on Lipschitz continuity assumptions.

Result: The analysis highlights a linear speedup in generalization bounds concerning batch size for convex problems.

Conclusion: This work extends theoretical understanding and offers more realistic generalization insights for the Lookahead optimizer, emphasizing its broader applicability.

Abstract: The Lookahead optimizer enhances deep learning models by employing a
dual-weight update mechanism, which has been shown to improve the performance
of underlying optimizers such as SGD. However, most theoretical studies focus
on its convergence on training data, leaving its generalization capabilities
less understood. Existing generalization analyses are often limited by
restrictive assumptions, such as requiring the loss function to be globally
Lipschitz continuous, and their bounds do not fully capture the relationship
between optimization and generalization. In this paper, we address these issues
by conducting a rigorous stability and generalization analysis of the Lookahead
optimizer with minibatch SGD. We leverage on-average model stability to derive
generalization bounds for both convex and strongly convex problems without the
restrictive Lipschitzness assumption. Our analysis demonstrates a linear
speedup with respect to the batch size in the convex setting.

</details>


### [214] [Learning in Stackelberg Mean Field Games: A Non-Asymptotic Analysis](https://arxiv.org/abs/2509.15392)
*Sihan Zeng,Benjamin Patrick Evans,Sujay Bhatt,Leo Ardon,Sumitra Ganesh,Alec Koppel*

Main category: cs.LG

TL;DR: The paper develops AC-SMFG, an actor-critic algorithm for optimizing policies in Stackelberg mean field games, addressing previous limitations in convergence guarantees and sample efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations in solving Stackelberg mean field games, such as restrictive independence assumptions, inefficiency in sample usage, and lack of finite-time convergence guarantees.

Method: The researchers propose a single-loop actor-critic algorithm (AC-SMFG) that alternates between gradient updates for the leader, a representative follower, and the mean field, and operates on continuously generated Markovian samples.

Result: AC-SMFG achieves finite-time and finite-sample convergence to a stationary point of the Stackelberg objective under a gradient alignment condition. Simulation results show better policy quality and faster convergence compared to existing baselines.

Conclusion: AC-SMFG offers a more effective and efficient approach to policy optimization in Stackelberg mean field games with proven convergence guarantees and improved performance.

Abstract: We study policy optimization in Stackelberg mean field games (MFGs), a
hierarchical framework for modeling the strategic interaction between a single
leader and an infinitely large population of homogeneous followers. The
objective can be formulated as a structured bi-level optimization problem, in
which the leader needs to learn a policy maximizing its reward, anticipating
the response of the followers. Existing methods for solving these (and related)
problems often rely on restrictive independence assumptions between the
leader's and followers' objectives, use samples inefficiently due to
nested-loop algorithm structure, and lack finite-time convergence guarantees.
To address these limitations, we propose AC-SMFG, a single-loop actor-critic
algorithm that operates on continuously generated Markovian samples. The
algorithm alternates between (semi-)gradient updates for the leader, a
representative follower, and the mean field, and is simple to implement in
practice. We establish the finite-time and finite-sample convergence of the
algorithm to a stationary point of the Stackelberg objective. To our knowledge,
this is the first Stackelberg MFG algorithm with non-asymptotic convergence
guarantees. Our key assumption is a "gradient alignment" condition, which
requires that the full policy gradient of the leader can be approximated by a
partial component of it, relaxing the existing leader-follower independence
assumption. Simulation results in a range of well-established economics
environments demonstrate that AC-SMFG outperforms existing multi-agent and MFG
learning baselines in policy quality and convergence speed.

</details>


### [215] [VMDNet: Time Series Forecasting with Leakage-Free Samplewise Variational Mode Decomposition and Multibranch Decoding](https://arxiv.org/abs/2509.15394)
*Weibin Feng,Ran Tao,John Cartlidge,Jin Zheng*

Main category: cs.LG

TL;DR: VMDNet enhances time series forecasting by addressing information leakage and improving hyperparameter tuning in Variational Mode Decomposition (VMD), achieving state-of-the-art results on energy datasets.


<details>
  <summary>Details</summary>
Motivation: Address existing limitations in Variational Mode Decomposition (VMD), such as information leakage and inappropriate hyperparameter tuning, to better capture structured periodic patterns in time series forecasting.

Method: A causality-preserving framework involving sample-wise VMD, frequency-aware embeddings, parallel temporal convolutional networks (TCNs), and a bilevel Stackelberg-inspired optimization approach for hyperparameter selection.

Result: VMDNet achieved state-of-the-art results on energy datasets with strong periodicity and maintained robustness under weak periodicity.

Conclusion: VMDNet effectively captures periodic patterns and addresses VMD's shortcomings, making it highly suitable for time series forecasting with periodic components.

Abstract: In time series forecasting, capturing recurrent temporal patterns is
essential; decomposition techniques make such structure explicit and thereby
improve predictive performance. Variational Mode Decomposition (VMD) is a
powerful signal-processing method for periodicity-aware decomposition and has
seen growing adoption in recent years. However, existing studies often suffer
from information leakage and rely on inappropriate hyperparameter tuning. To
address these issues, we propose VMDNet, a causality-preserving framework that
(i) applies sample-wise VMD to avoid leakage; (ii) represents each decomposed
mode with frequency-aware embeddings and decodes it using parallel temporal
convolutional networks (TCNs), ensuring mode independence and efficient
learning; and (iii) introduces a bilevel, Stackelberg-inspired optimisation to
adaptively select VMD's two core hyperparameters: the number of modes (K) and
the bandwidth penalty (alpha). Experiments on two energy-related datasets
demonstrate that VMDNet achieves state-of-the-art results when periodicity is
strong, showing clear advantages in capturing structured periodic patterns
while remaining robust under weak periodicity.

</details>


### [216] [The Alignment Bottleneck](https://arxiv.org/abs/2509.15932)
*Wenjun Cao*

Main category: cs.LG

TL;DR: Large language models improve with scaling, but feedback-based alignment struggles due to resource limitations. This study models the alignment process as a two-stage cascade, introducing capacity-based bounds and providing analytical insights on alignment and optimization challenges.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address systematic deviations in intended behavior during feedback-based alignment, drawing inspiration from bounded rationality principles. It seeks to understand judgment under resource-limited constraints.

Method: The authors model the alignment loop as a two-stage cascade and derive theoretical bounds such as a data size-independent Fano lower bound and a PAC-Bayes upper bound, utilizing concepts like cognitive capacity and total capacity.

Result: They establish a unified capacity-based performance interval for alignment, revealing that aligning complex targets requires growing capacity. Adding more data alone does not surpass the bound, and excess optimization often leads to undesirable behaviors.

Conclusion: Alignment should be approached as interface engineering, emphasizing capacity allocation, task complexity management, and information expenditure decisions.

Abstract: Large language models improve with scale, yet feedback-based alignment still
exhibits systematic deviations from intended behavior. Motivated by bounded
rationality in economics and cognitive science, we view judgment as
resource-limited and feedback as a constrained channel. On this basis, we model
the loop as a two-stage cascade $U \to H \to Y$ given $S$, with cognitive
capacity $C_{\text{cog}|S}$ and average total capacity
$\bar{C}_{\text{tot}|S}$. Our main result is a capacity-coupled Alignment
Performance Interval. It pairs a data size-independent Fano lower bound proved
on a separable codebook mixture with a PAC-Bayes upper bound whose KL term is
controlled by the same channel via $m \, \bar{C}_{\text{tot}|S}$. The PAC-Bayes
bound becomes an upper bound on the same true risk when the canonical
observable loss is used and the dataset is drawn from the same mixture. Under
these matched conditions, both limits are governed by a single capacity.
Consequences include that, with value complexity and capacity fixed, adding
labels alone cannot cross the bound; attaining lower risk on more complex
targets requires capacity that grows with $\log M$; and once useful signal
saturates capacity, further optimization tends to fit channel regularities,
consistent with reports of sycophancy and reward hacking. The analysis views
alignment as interface engineering: measure and allocate limited capacity,
manage task complexity, and decide where information is spent.

</details>


### [217] [Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization](https://arxiv.org/abs/2509.15399)
*Xiaochuan Gong,Jie Hao,Mingrui Liu*

Main category: cs.LG

TL;DR: This paper introduces adaptive algorithms for stochastic hierarchical optimization problems, achieving sharp convergence rates without needing prior knowledge of noise levels.


<details>
  <summary>Details</summary>
Motivation: Existing methods for hierarchical optimization lack adaptability in stochastic settings, failing to achieve optimal convergence rates across varying gradient noise levels.

Method: The proposed solution combines momentum normalization with adaptive parameter choices, applied to nonconvex-strongly-concave and nonconvex-strongly-convex problems, achieving automatic adaptivity in optimization.

Result: The algorithms achieved convergence rates of $\widetilde{O}(1/\sqrt{T} + \sqrt{\bar{\sigma}}/T^{1/4})$ for the gradient norm, applicable without needing prior noise magnitude knowledge.

Conclusion: These adaptive algorithms are groundbreaking for stochastic hierarchical optimization, providing robust performance in both low and high-noise settings with empirical validation across synthetic and deep learning tasks.

Abstract: Hierarchical optimization refers to problems with interdependent decision
variables and objectives, such as minimax and bilevel formulations. While
various algorithms have been proposed, existing methods and analyses lack
adaptivity in stochastic optimization settings: they cannot achieve optimal
convergence rates across a wide spectrum of gradient noise levels without prior
knowledge of the noise magnitude. In this paper, we propose novel adaptive
algorithms for two important classes of stochastic hierarchical optimization
problems: nonconvex-strongly-concave minimax optimization and
nonconvex-strongly-convex bilevel optimization. Our algorithms achieve sharp
convergence rates of $\widetilde{O}(1/\sqrt{T} + \sqrt{\bar{\sigma}}/T^{1/4})$
in $T$ iterations for the gradient norm, where $\bar{\sigma}$ is an upper bound
on the stochastic gradient noise. Notably, these rates are obtained without
prior knowledge of the noise level, thereby enabling automatic adaptivity in
both low and high-noise regimes. To our knowledge, this work provides the first
adaptive and sharp convergence guarantees for stochastic hierarchical
optimization. Our algorithm design combines the momentum normalization
technique with novel adaptive parameter choices. Extensive experiments on
synthetic and deep learning tasks demonstrate the effectiveness of our proposed
algorithms.

</details>


### [218] [Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations](https://arxiv.org/abs/2509.15981)
*Yujie Zhu,Charles A. Hepburn,Matthew Thorpe,Giovanni Montana*

Main category: cs.LG

TL;DR: SPReD improves reinforcement learning with sparse rewards using uncertainty-aware imitation of demonstrations, enabling substantial performance gains.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the challenge of efficiently using demonstrations to accelerate learning in reinforcement learning tasks that involve sparse rewards.

Method: SPReD uses ensemble methods to model Q-value distributions and introduces two uncertainty-aware approaches: probabilistic comparison and advantage-based scaling, employing continuous regularisation weights.

Result: SPReD improves performance across eight robotics tasks, exhibiting up to 14x improvement in complex scenarios compared to existing methods.

Conclusion: SPReD demonstrates the effectiveness of uncertainty-aware imitation strategies, ensuring robustness to demonstration quality and quantity.

Abstract: In reinforcement learning with sparse rewards, demonstrations can accelerate
learning, but determining when to imitate them remains challenging. We propose
Smooth Policy Regularisation from Demonstrations (SPReD), a framework that
addresses the fundamental question: when should an agent imitate a
demonstration versus follow its own policy? SPReD uses ensemble methods to
explicitly model Q-value distributions for both demonstration and policy
actions, quantifying uncertainty for comparisons. We develop two complementary
uncertainty-aware methods: a probabilistic approach estimating the likelihood
of demonstration superiority, and an advantage-based approach scaling imitation
by statistical significance. Unlike prevailing methods (e.g. Q-filter) that
make binary imitation decisions, SPReD applies continuous,
uncertainty-proportional regularisation weights, reducing gradient variance
during training. Despite its computational simplicity, SPReD achieves
remarkable gains in experiments across eight robotics tasks, outperforming
existing approaches by up to a factor of 14 in complex tasks while maintaining
robustness to demonstration quality and quantity. Our code is available at
https://github.com/YujieZhu7/SPReD.

</details>


### [219] [Exploring multimodal implicit behavior learning for vehicle navigation in simulated cities](https://arxiv.org/abs/2509.15400)
*Eric Aislan Antonelo,Gustavo Claudio Karl Couto,Christian Möller*

Main category: cs.LG

TL;DR: The paper introduces DA-IBC, an approach using energy-based models combined with augmentations to address multimodality in driving decisions.


<details>
  <summary>Details</summary>
Motivation: Standard BC struggles with multimodal decision scenarios in driving tasks, where multiple valid actions exist for a single scenario.

Method: The authors propose Data-Augmented Implicit Behavioral Cloning (DA-IBC) using EBMs with expert action perturbations and improved derivative-free inference initialization.

Result: Experiments in CARLA simulator reveal DA-IBC outperforms standard IBC and successfully models multimodal driving behavior.

Conclusion: DA-IBC enables better representation of multimodal actions compared to BC, addressing the limitations in urban driving tasks.

Abstract: Standard Behavior Cloning (BC) fails to learn multimodal driving decisions,
where multiple valid actions exist for the same scenario. We explore Implicit
Behavioral Cloning (IBC) with Energy-Based Models (EBMs) to better capture this
multimodality. We propose Data-Augmented IBC (DA-IBC), which improves learning
by perturbing expert actions to form the counterexamples of IBC training and
using better initialization for derivative-free inference. Experiments in the
CARLA simulator with Bird's-Eye View inputs demonstrate that DA-IBC outperforms
standard IBC in urban driving tasks designed to evaluate multimodal behavior
learning in a test environment. The learned energy landscapes are able to
represent multimodal action distributions, which BC fails to achieve.

</details>


### [220] [Random Matrix Theory-guided sparse PCA for single-cell RNA-seq data](https://arxiv.org/abs/2509.15429)
*Victor Chardès*

Main category: cs.LG

TL;DR: This paper introduces an improved Random Matrix Theory (RMT)-based method for sparse principal component analysis (SPCA) in single-cell RNA-seq data, addressing challenges like noise and variability.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve dimensionality reduction in noisy single-cell RNA-seq datasets, addressing limitations of traditional PCA methods while maintaining robustness and interpretability.

Method: A novel biwhitening method inspired by the Sinkhorn-Knopp algorithm is proposed to stabilize variance. An RMT-based criterion is then used to select sparsity levels, making SPCA nearly parameter-free.

Result: The approach improves principal subspace reconstruction and outperforms PCA-, autoencoder-, and diffusion-based methods in classifying cell types across various single-cell RNA-seq technologies.

Conclusion: The proposed approach enhances the utility of SPCA, offering a robust and interpretable alternative to traditional dimensionality reduction methods for single-cell RNA-seq analyses.

Abstract: Single-cell RNA-seq provides detailed molecular snapshots of individual cells
but is notoriously noisy. Variability stems from biological differences, PCR
amplification bias, limited sequencing depth, and low capture efficiency,
making it challenging to adapt computational pipelines to heterogeneous
datasets or evolving technologies. As a result, most studies still rely on
principal component analysis (PCA) for dimensionality reduction, valued for its
interpretability and robustness. Here, we improve upon PCA with a Random Matrix
Theory (RMT)-based approach that guides the inference of sparse principal
components using existing sparse PCA algorithms. We first introduce a novel
biwhitening method, inspired by the Sinkhorn-Knopp algorithm, that
simultaneously stabilizes variance across genes and cells. This enables the use
of an RMT-based criterion to automatically select the sparsity level, rendering
sparse PCA nearly parameter-free. Our mathematically grounded approach retains
the interpretability of PCA while enabling robust, hands-off inference of
sparse principal components. Across seven single-cell RNA-seq technologies and
four sparse PCA algorithms, we show that this method systematically improves
the reconstruction of the principal subspace and consistently outperforms PCA-,
autoencoder-, and diffusion-based methods in cell-type classification tasks.

</details>


### [221] [Computing Linear Regions in Neural Networks with Skip Connections](https://arxiv.org/abs/2509.15441)
*Johnny Joyce,Jan Verschelde*

Main category: cs.LG

TL;DR: The paper explores the use of tropical arithmetic for analyzing neural networks and presents algorithms to compute the regions where the networks are linear.


<details>
  <summary>Details</summary>
Motivation: To better understand the properties of neural networks, specifically their piecewise linear structure, and address challenges like overfitting and the advantages of skip connections.

Method: The authors use tropical arithmetic to represent piecewise linear activation functions and apply tropical geometry. Algorithms are proposed to determine linear regions in neural networks.

Result: The study provides insights into neural network training difficulties, including issues like overfitting, and highlights the benefits of architectural features like skip connections.

Conclusion: Representing neural networks with tropical mathematics is useful for understanding their properties and addressing key training challenges.

Abstract: Neural networks are important tools in machine learning. Representing
piecewise linear activation functions with tropical arithmetic enables the
application of tropical geometry. Algorithms are presented to compute regions
where the neural networks are linear maps. Through computational experiments,
we provide insights on the difficulty to train neural networks, in particular
on the problems of overfitting and on the benefits of skip connections.

</details>


### [222] [IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for LLMs](https://arxiv.org/abs/2509.15455)
*Junchen Zhao,Ali Derakhshan,Dushyant Bharadwaj,Jayden Kana Hyman,Junhao Dong,Sangeetha Abdu Jyothi,Ian Harris*

Main category: cs.LG

TL;DR: The paper introduces methods to optimize mixed-precision quantization for Large Language Models (LLMs), achieving significant perplexity improvements while operating under reduced precision constraints.


<details>
  <summary>Details</summary>
Motivation: To make the deployment of multi-billion parameter LLMs feasible in low-resource and on-device scenarios by improving mixed-precision quantization methods below four bits, considering inter-layer interactions.

Method: They propose Shapley-based Progressive Quantization Estimation (SPQE) to evaluate layer sensitivities and inter-layer interactions, and Interaction-aware Mixed-Precision Quantization (IMPQ), which optimizes precision assignments using binary quadratic optimization.

Result: IMPQ improves scalability and performance across models like Llama-3, Gemma-2, and Qwen-3 using different PTQ backends. It significantly reduces perplexity by 20–80% compared to baselines as precision shrinks.

Conclusion: By considering inter-layer interactions, IMPQ provides a more efficient quantization approach, making low-precision LLM deployment feasible while maintaining performance superiority.

Abstract: Large Language Models (LLMs) promise impressive capabilities, yet their
multi-billion-parameter scale makes on-device or low-resource deployment
prohibitive. Mixed-precision quantization offers a compelling solution, but
existing methods struggle when the average precision drops below four bits, as
they rely on isolated, layer-specific metrics that overlook critical
inter-layer interactions affecting overall performance. In this paper, we
propose two innovations to address these limitations. First, we frame the
mixed-precision quantization problem as a cooperative game among layers and
introduce Shapley-based Progressive Quantization Estimation (SPQE) to
efficiently obtain accurate Shapley estimates of layer sensitivities and
inter-layer interactions. Second, building upon SPQE, we propose
Interaction-aware Mixed-Precision Quantization (IMPQ) which translates these
Shapley estimates into a binary quadratic optimization formulation, assigning
either 2 or 4-bit precision to layers under strict memory constraints.
Comprehensive experiments conducted on Llama-3, Gemma-2, and Qwen-3 models
across three independent PTQ backends (Quanto, HQQ, GPTQ) demonstrate IMPQ's
scalability and consistently superior performance compared to methods relying
solely on isolated metrics. Across average precisions spanning 4 bit down to 2
bit, IMPQ cuts Perplexity by 20 to 80 percent relative to the best baseline,
with the margin growing as the bit-width tightens.

</details>


### [223] [Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs](https://arxiv.org/abs/2509.15464)
*Junhong Lin,Song Wang,Xiaojie Guo,Julian Shun,Yada Zhu*

Main category: cs.LG

TL;DR: This paper introduces EvoReasoner and EvoKG to enhance the ability of large language models (LLMs) in temporal reasoning and adapt knowledge graphs (KGs) to evolving real-world data, achieving competitive performance on dynamic question answering benchmarks.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges in reasoning with temporally shifting and inconsistent knowledge. Addressing these issues is critical for maintaining up-to-date and accurate AI systems.

Method: The authors propose EvoReasoner for temporal-aware multi-hop reasoning and EvoKG, a module for noise-tolerant KG evolution. EvoKG uses confidence-based contradiction resolution and temporal trend tracking to update KGs from unstructured data.

Result: EvoReasoner combined with EvoKG outperformed both prompting-based and KG-enhanced baselines, enabling smaller LLMs to achieve performance levels comparable to much larger models. The approach excelled in temporal QA benchmarks and dynamic KG updating tasks.

Conclusion: Integrating temporal reasoning and KG evolution enhances the robustness and temporal adaptability of LLMs, bridging the performance gap across models and enabling effective application in dynamic settings.

Abstract: Large language models (LLMs) excel at many language understanding tasks but
struggle to reason over knowledge that evolves. To address this, recent work
has explored augmenting LLMs with knowledge graphs (KGs) to provide structured,
up-to-date information. However, many existing approaches assume a static
snapshot of the KG and overlook the temporal dynamics and factual
inconsistencies inherent in real-world data. To address the challenge of
reasoning over temporally shifting knowledge, we propose EvoReasoner, a
temporal-aware multi-hop reasoning algorithm that performs global-local entity
grounding, multi-route decomposition, and temporally grounded scoring. To
ensure that the underlying KG remains accurate and up-to-date, we introduce
EvoKG, a noise-tolerant KG evolution module that incrementally updates the KG
from unstructured documents through confidence-based contradiction resolution
and temporal trend tracking. We evaluate our approach on temporal QA benchmarks
and a novel end-to-end setting where the KG is dynamically updated from raw
documents. Our method outperforms both prompting-based and KG-enhanced
baselines, effectively narrowing the gap between small and large LLMs on
dynamic question answering. Notably, an 8B-parameter model using our approach
matches the performance of a 671B model prompted seven months later. These
results highlight the importance of combining temporal reasoning with KG
evolution for robust and up-to-date LLM performance. Our code is publicly
available at github.com/junhongmit/TREK.

</details>


### [224] [Solar Forecasting with Causality: A Graph-Transformer Approach to Spatiotemporal Dependencies](https://arxiv.org/abs/2509.15481)
*Yanan Niu,Demetri Psaltis,Christophe Moser,Luisa Lambertini*

Main category: cs.LG

TL;DR: SolarCAST uses historical solar irradiance data and neural networks to improve solar forecasting accuracy without relying on costly hardware.


<details>
  <summary>Details</summary>
Motivation: Effectively managing renewable energy requires accurate solar forecasting, but existing methods often depend on expensive and specialized resources.

Method: SolarCAST uses neural components to address factors influencing irradiance correlations, such as synchronous variables, latent weather patterns, and time-lagged effects.

Result: SolarCAST surpasses commercial and academic forecasting baselines, achieving a 25.9% error reduction over Solcast.

Conclusion: This model demonstrates a cost-effective and generalizable approach for accurate localized solar forecasting.

Abstract: Accurate solar forecasting underpins effective renewable energy management.
We present SolarCAST, a causally informed model predicting future global
horizontal irradiance (GHI) at a target site using only historical GHI from
site X and nearby stations S - unlike prior work that relies on sky-camera or
satellite imagery requiring specialized hardware and heavy preprocessing. To
deliver high accuracy with only public sensor data, SolarCAST models three
classes of confounding factors behind X-S correlations using scalable neural
components: (i) observable synchronous variables (e.g., time of day, station
identity), handled via an embedding module; (ii) latent synchronous factors
(e.g., regional weather patterns), captured by a spatio-temporal graph neural
network; and (iii) time-lagged influences (e.g., cloud movement across
stations), modeled with a gated transformer that learns temporal shifts. It
outperforms leading time-series and multimodal baselines across diverse
geographical conditions, and achieves a 25.9% error reduction over the top
commercial forecaster, Solcast. SolarCAST offers a lightweight, practical, and
generalizable solution for localized solar forecasting.

</details>


### [225] [FRAUDGUESS: Spotting and Explaining New Types of Fraud in Million-Scale Financial Data](https://arxiv.org/abs/2509.15493)
*Robson L. F. Cordeiro,Meng-Chieh Lee,Christos Faloutsos*

Main category: cs.LG

TL;DR: The paper proposes FRAUDGUESS for detecting and justifying novel fraudulent transactions using clustering, visualization, and interactive dashboards.


<details>
  <summary>Details</summary>
Motivation: Fraud detection requires identifying both known and unknown types of fraudulent behaviors in financial transactions while providing evidence that assists experts in validation.

Method: FRAUDGUESS identifies fraudulent transactions as micro-clusters in a specialized feature space and supports justification using visualization tools, heatmaps, and interactive dashboards.

Result: FRAUDGUESS was tested on a million-scale financial dataset and identified three new behaviors, two of which were flagged as suspicious or fraudulent, catching hundreds of missed fraudulent transactions.

Conclusion: FRAUDGUESS is effective in detecting and analyzing fraudulent transactions and is being considered for deployment in a financial institution, showcasing its real-life applicability.

Abstract: Given a set of financial transactions (who buys from whom, when, and for how
much), as well as prior information from buyers and sellers, how can we find
fraudulent transactions? If we have labels for some transactions for known
types of fraud, we can build a classifier. However, we also want to find new
types of fraud, still unknown to the domain experts ('Detection'). Moreover, we
also want to provide evidence to experts that supports our opinion
('Justification'). In this paper, we propose FRAUDGUESS, to achieve two goals:
(a) for 'Detection', it spots new types of fraud as micro-clusters in a
carefully designed feature space; (b) for 'Justification', it uses
visualization and heatmaps for evidence, as well as an interactive dashboard
for deep dives. FRAUDGUESS is used in real life and is currently considered for
deployment in an Anonymous Financial Institution (AFI). Thus, we also present
the three new behaviors that FRAUDGUESS discovered in a real, million-scale
financial dataset. Two of these behaviors are deemed fraudulent or suspicious
by domain experts, catching hundreds of fraudulent transactions that would
otherwise go un-noticed.

</details>


### [226] [Detail Across Scales: Multi-Scale Enhancement for Full Spectrum Neural Representations](https://arxiv.org/abs/2509.15494)
*Yuan Ni,Zhantao Chen,Cheng Peng,Rajan Plumley,Chun Hong Yoon,Jana B. Thayer,Joshua J. Turner*

Main category: cs.LG

TL;DR: The paper introduces WIEN-INR, a wavelet-informed system that enhances implicit neural representations (INRs) to better capture multi-scale and high-frequency details in scientific datasets.


<details>
  <summary>Details</summary>
Motivation: Current compact implicit neural representations (INRs) struggle with representing fine textures and high-frequency details in scientific data, limiting their effectiveness.

Method: WIEN-INR employs a wavelet-informed multi-scale framework and a specialized kernel network at the finest resolution level to improve detail reconstruction while keeping the model compact and efficient.

Result: WIEN-INR achieves superior reconstruction fidelity compared to existing methods while maintaining smaller network sizes, as verified by experiments on diverse scientific datasets.

Conclusion: The proposed WIEN-INR framework extends the utility of compact INRs to applications requiring efficient retention of fine details, making it a practical tool for high-fidelity scientific data representation.

Abstract: Implicit neural representations (INRs) have emerged as a compact and
parametric alternative to discrete array-based data representations, encoding
information directly in neural network weights to enable resolution-independent
representation and memory efficiency. However, existing INR approaches, when
constrained to compact network sizes, struggle to faithfully represent the
multi-scale structures, high-frequency information, and fine textures that
characterize the majority of scientific datasets. To address this limitation,
we propose WIEN-INR, a wavelet-informed implicit neural representation that
distributes modeling across different resolution scales and employs a
specialized kernel network at the finest scale to recover subtle details. This
multi-scale architecture allows for the use of smaller networks to retain the
full spectrum of information while preserving the training efficiency and
reducing storage cost. Through extensive experiments on diverse scientific
datasets spanning different scales and structural complexities, WIEN-INR
achieves superior reconstruction fidelity while maintaining a compact model
size. These results demonstrate WIEN-INR as a practical neural representation
framework for high-fidelity scientific data encoding, extending the
applicability of INRs to domains where efficient preservation of fine detail is
essential.

</details>


### [227] [Mental Accounts for Actions: EWA-Inspired Attention in Decision Transformers](https://arxiv.org/abs/2509.15498)
*Zahra Aref,Narayan B. Mandayam*

Main category: cs.LG

TL;DR: The paper proposes EWA-VQ-ODT, an enhancement for Online Decision Transformers, improving sample efficiency and average return in reinforcement learning through a lightweight module with action-specific memories.


<details>
  <summary>Details</summary>
Motivation: Transformers in RL struggle with modeling long-term action outcomes efficiently, and standard attention mechanisms lack explicit memory for actions, limiting effectiveness in continuous-control tasks.

Method: The authors add the Experience-Weighted Attraction with Vector Quantization (EWA-VQ) module to Online Decision Transformers. The module maintains per-action attraction values stored in a vector-quantized grid and updates them dynamically based on decay and rewards. These values are used to bias attention columns tied to action tokens without altering the backbone architecture.

Result: On standard continuous-control benchmarks, EWA-VQ-ODT demonstrates improved sample efficiency and average returns compared to baseline ODT, especially during the early stages of training.

Conclusion: The EWA-VQ-ODT module enhances Online Decision Transformers by addressing inefficiencies in long-term action learning. The module is computationally efficient, interpretable, and theoretically supported, showcasing potential for broader RL applications.

Abstract: Transformers have emerged as a compelling architecture for sequential
decision-making by modeling trajectories via self-attention. In reinforcement
learning (RL), they enable return-conditioned control without relying on value
function approximation. Decision Transformers (DTs) exploit this by casting RL
as supervised sequence modeling, but they are restricted to offline data and
lack exploration. Online Decision Transformers (ODTs) address this limitation
through entropy-regularized training on on-policy rollouts, offering a stable
alternative to traditional RL methods like Soft Actor-Critic, which depend on
bootstrapped targets and reward shaping. Despite these advantages, ODTs use
standard attention, which lacks explicit memory of action-specific outcomes.
This leads to inefficiencies in learning long-term action effectiveness.
Inspired by cognitive models such as Experience-Weighted Attraction (EWA), we
propose Experience-Weighted Attraction with Vector Quantization for Online
Decision Transformers (EWA-VQ-ODT), a lightweight module that maintains
per-action mental accounts summarizing recent successes and failures.
Continuous actions are routed via direct grid lookup to a compact
vector-quantized codebook, where each code stores a scalar attraction updated
online through decay and reward-based reinforcement. These attractions modulate
attention by biasing the columns associated with action tokens, requiring no
change to the backbone or training objective. On standard continuous-control
benchmarks, EWA-VQ-ODT improves sample efficiency and average return over ODT,
particularly in early training. The module is computationally efficient,
interpretable via per-code traces, and supported by theoretical guarantees that
bound the attraction dynamics and its impact on attention drift.

</details>


### [228] [Policy Gradient Optimzation for Bayesian-Risk MDPs with General Convex Losses](https://arxiv.org/abs/2509.15509)
*Xiaoshuang Wang,Yifan Lin,Enlu Zhou*

Main category: cs.LG

TL;DR: The study develops a Bayesian policy gradient method for Markov decision processes (MDPs) under epistemic uncertainty, achieving strong convergence properties without relying on Bellman equations.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of MDPs with general loss functions and unknown parameters, especially under epistemic uncertainty, using a Bayesian approach.

Method: A policy gradient optimization method based on the dual representation of coherent risk measures and an extended envelope theorem, enabling convergence analysis.

Result: Achieved a convergence rate of $O(T^{-1/2}+r^{-1/2})$, extended the algorithm to episodic settings, and established bounds to achieve an error bound of $O(\epsilon)$ per episode.

Conclusion: The proposed policy gradient method effectively tackles MDPs without Bellman equations and demonstrates theoretical convergence guarantees and scalability to episodic frameworks.

Abstract: Motivated by many application problems, we consider Markov decision processes
(MDPs) with a general loss function and unknown parameters. To mitigate the
epistemic uncertainty associated with unknown parameters, we take a Bayesian
approach to estimate the parameters from data and impose a coherent risk
functional (with respect to the Bayesian posterior distribution) on the loss.
Since this formulation usually does not satisfy the interchangeability
principle, it does not admit Bellman equations and cannot be solved by
approaches based on dynamic programming. Therefore, We propose a policy
gradient optimization method, leveraging the dual representation of coherent
risk measures and extending the envelope theorem to continuous cases. We then
show the stationary analysis of the algorithm with a convergence rate of
$O(T^{-1/2}+r^{-1/2})$, where $T$ is the number of policy gradient iterations
and $r$ is the sample size of the gradient estimator. We further extend our
algorithm to an episodic setting, and establish the global convergence of the
extended algorithm and provide bounds on the number of iterations needed to
achieve an error bound $O(\epsilon)$ in each episode.

</details>


### [229] [KoopCast: Trajectory Forecasting via Koopman Operators](https://arxiv.org/abs/2509.15513)
*Jungjin Lee,Jaeuk Shin,Gihwan Kim,Joonho Han,Insoon Yang*

Main category: cs.LG

TL;DR: The paper introduces KoopCast, a new trajectory forecasting model using Koopman operator theory, designed for both accuracy and efficiency in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: The goal is to improve trajectory forecasting by integrating both predictive accuracy and interpretability while ensuring computational efficiency.

Method: A two-stage model is proposed: initially estimating high-probability long-term goals, followed by incorporating dynamics via Koopman operator theory for linear predictions in a transformed nonlinear space.

Result: KoopCast was tested on datasets like ETH/UCY, Waymo, and nuScenes, achieving high accuracy, interpretability, and low computational latency across all.

Conclusion: KoopCast combines theoretical rigor with practical advantages, offering a trajectory forecasting model that is accurate, interpretable, and efficient for dynamic environments.

Abstract: We present KoopCast, a lightweight yet efficient model for trajectory
forecasting in general dynamic environments. Our approach leverages Koopman
operator theory, which enables a linear representation of nonlinear dynamics by
lifting trajectories into a higher-dimensional space. The framework follows a
two-stage design: first, a probabilistic neural goal estimator predicts
plausible long-term targets, specifying where to go; second, a Koopman
operator-based refinement module incorporates intention and history into a
nonlinear feature space, enabling linear prediction that dictates how to go.
This dual structure not only ensures strong predictive accuracy but also
inherits the favorable properties of linear operators while faithfully
capturing nonlinear dynamics. As a result, our model offers three key
advantages: (i) competitive accuracy, (ii) interpretability grounded in Koopman
spectral theory, and (iii) low-latency deployment. We validate these benefits
on ETH/UCY, the Waymo Open Motion Dataset, and nuScenes, which feature rich
multi-agent interactions and map-constrained nonlinear motion. Across
benchmarks, KoopCast consistently delivers high predictive accuracy together
with mode-level interpretability and practical efficiency.

</details>


### [230] [Manifold Dimension Estimation: An Empirical Study](https://arxiv.org/abs/2509.15517)
*Zelong Bi,Pierre Lafaye de Micheaux*

Main category: cs.LG

TL;DR: This paper surveys the manifold hypothesis and dimension estimation methods, evaluating them using experiments on various datasets.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of systematic evaluation in dimension estimation of low-dimensional manifolds within high-dimensional data.

Method: The authors review theoretical foundations, analyze eight estimators through experiments under various conditions, and compare their performances on real-world and synthetic datasets.

Result: They find that simpler dimension estimation methods work well across diverse conditions, and introduce a structured approach to fine-tune estimators based on datasets.

Conclusion: The study provides practical insights for both researchers and practitioners, emphasizing effective and simple solutions to dimension estimation challenges.

Abstract: The manifold hypothesis suggests that high-dimensional data often lie on or
near a low-dimensional manifold. Estimating the dimension of this manifold is
essential for leveraging its structure, yet existing work on dimension
estimation is fragmented and lacks systematic evaluation. This article provides
a comprehensive survey for both researchers and practitioners. We review
often-overlooked theoretical foundations and present eight representative
estimators. Through controlled experiments, we analyze how individual factors
such as noise, curvature, and sample size affect performance. We also compare
the estimators on diverse synthetic and real-world datasets, introducing a
principled approach to dataset-specific hyperparameter tuning. Our results
offer practical guidance and suggest that, for a problem of this generality,
simpler methods often perform better.

</details>


### [231] [Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem](https://arxiv.org/abs/2509.15519)
*Chao Li,Bingkun Bao,Yang Gao*

Main category: cs.LG

TL;DR: This paper introduces Dynamics-Aware Context (DAC) for fully decentralized cooperative multi-agent reinforcement learning, addressing non-stationarity and relative overgeneralization.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of non-stationarity and relative overgeneralization in decentralized cooperative multi-agent reinforcement learning.

Method: Proposed Dynamics-Aware Context (DAC), modeling tasks with Contextual Markov Decision Processes and using latent variables for context-based dynamics modeling.

Result: DAC demonstrated superior performance over baseline methods in experiments like matrix games, predator-prey tasks, and SMAC.

Conclusion: DAC effectively tackles decentralized learning issues by leveraging context-based modeling and optimistic marginal value function for cooperative actions.

Abstract: This paper studies fully decentralized cooperative multi-agent reinforcement
learning, where each agent solely observes the states, its local actions, and
the shared rewards. The inability to access other agents' actions often leads
to non-stationarity during value function updates and relative
overgeneralization during value function estimation, hindering effective
cooperative policy learning. However, existing works fail to address both
issues simultaneously, due to their inability to model the joint policy of
other agents in a fully decentralized setting. To overcome this limitation, we
propose a novel method named Dynamics-Aware Context (DAC), which formalizes the
task, as locally perceived by each agent, as an Contextual Markov Decision
Process, and further addresses both non-stationarity and relative
overgeneralization through dynamics-aware context modeling. Specifically, DAC
attributes the non-stationary local task dynamics of each agent to switches
between unobserved contexts, each corresponding to a distinct joint policy.
Then, DAC models the step-wise dynamics distribution using latent variables and
refers to them as contexts. For each agent, DAC introduces a context-based
value function to address the non-stationarity issue during value function
update. For value function estimation, an optimistic marginal value is derived
to promote the selection of cooperative actions, thereby addressing the
relative overgeneralization issue. Experimentally, we evaluate DAC on various
cooperative tasks (including matrix game, predator and prey, and SMAC), and its
superior performance against multiple baselines validates its effectiveness.

</details>


### [232] [Universal Learning of Stochastic Dynamics for Exact Belief Propagation using Bernstein Normalizing Flows](https://arxiv.org/abs/2509.15533)
*Peter Amorese,Morteza Lahijanian*

Main category: cs.LG

TL;DR: The paper introduces a model combining normalizing flows and Bernstein polynomials for belief propagation in nonlinear stochastic systems.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of predicting future states under uncertainty with unknown system models and complex dynamics.

Method: Utilizes a combination of normalizing flows for flexible density estimation and Bernstein polynomials for maintaining analytical tractability.

Result: Demonstrates superior performance of the proposed model over existing methods, especially in complex nonlinear systems with non-standard noise.

Conclusion: The proposed model successfully approximates nonlinear stochastic dynamics while supporting analytical belief propagation.

Abstract: Predicting the distribution of future states in a stochastic system, known as
belief propagation, is fundamental to reasoning under uncertainty. However,
nonlinear dynamics often make analytical belief propagation intractable,
requiring approximate methods. When the system model is unknown and must be
learned from data, a key question arises: can we learn a model that (i)
universally approximates general nonlinear stochastic dynamics, and (ii)
supports analytical belief propagation? This paper establishes the theoretical
foundations for a class of models that satisfy both properties. The proposed
approach combines the expressiveness of normalizing flows for density
estimation with the analytical tractability of Bernstein polynomials. Empirical
results show the efficacy of our learned model over state-of-the-art
data-driven methods for belief propagation, especially for highly non-linear
systems with non-additive, non-Gaussian noise.

</details>


### [233] [Nonconvex Decentralized Stochastic Bilevel Optimization under Heavy-Tailed Noises](https://arxiv.org/abs/2509.15543)
*Xinwen Zhang,Yihan Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: The paper introduces a novel decentralized stochastic bilevel optimization algorithm for nonconvex problems under heavy-tailed noise, providing strong theoretical guarantees and effective experimental results.


<details>
  <summary>Details</summary>
Motivation: Existing decentralized optimization methods rely on strong assumptions like strong convexity and finite gradient variance, which are often not met in real-life machine learning scenarios.

Method: The authors propose a normalized stochastic variance-reduced bilevel gradient descent algorithm that avoids clipping operations. They establish convergence rates by bounding interdependent gradient sequences under heavy-tailed noise conditions.

Result: Their algorithm is the first decentralized bilevel optimization approach with theoretical guarantees under heavy-tailed noises. Experimental results demonstrate its effectiveness in such challenging conditions.

Conclusion: The proposed algorithm addresses the limitations of prior methods and proves to be effective in handling heavy-tailed noise in decentralized nonconvex bilevel optimization problems.

Abstract: Existing decentralized stochastic optimization methods assume the lower-level
loss function is strongly convex and the stochastic gradient noise has finite
variance. These strong assumptions typically are not satisfied in real-world
machine learning models. To address these limitations, we develop a novel
decentralized stochastic bilevel optimization algorithm for the nonconvex
bilevel optimization problem under heavy-tailed noises. Specifically, we
develop a normalized stochastic variance-reduced bilevel gradient descent
algorithm, which does not rely on any clipping operation. Moreover, we
establish its convergence rate by innovatively bounding interdependent gradient
sequences under heavy-tailed noises for nonconvex decentralized bilevel
optimization problems. As far as we know, this is the first decentralized
bilevel optimization algorithm with rigorous theoretical guarantees under
heavy-tailed noises. The extensive experimental results confirm the
effectiveness of our algorithm in handling heavy-tailed noises.

</details>


### [234] [PolyJuice Makes It Real: Black-Box, Universal Red Teaming for Synthetic Image Detectors](https://arxiv.org/abs/2509.15551)
*Sepehr Dehdashtian,Mashrur M. Morshed,Jacob H. Seidman,Gaurav Bharaj,Vishnu Naresh Boddeti*

Main category: cs.LG

TL;DR: PolyJuice is a black-box, image-agnostic red-teaming method to deceive synthetic image detectors (SIDs) by exploiting distribution shifts and universally steering generated images toward failure modes.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing red-teaming methods, which require white-box access and are computationally expensive, in improving synthetic image detectors (SIDs).

Method: PolyJuice exploits the latent space distribution shift to offline identify attack directions and universally steer images towards SID's failure modes, while maintaining efficiency through resolution scaling.

Result: PolyJuice significantly improves the ability of synthetic images to deceive SIDs, showing up to 84% success in deception, and enhances the performance of original SIDs when detectors are retrained on PolyJuice-augmented datasets.

Conclusion: PolyJuice provides an efficient and effective method for testing and improving SIDs, highlighting its potential for advancing defenses against text-to-image generated synthetic image risks.

Abstract: Synthetic image detectors (SIDs) are a key defense against the risks posed by
the growing realism of images from text-to-image (T2I) models. Red teaming
improves SID's effectiveness by identifying and exploiting their failure modes
via misclassified synthetic images. However, existing red-teaming solutions (i)
require white-box access to SIDs, which is infeasible for proprietary
state-of-the-art detectors, and (ii) generate image-specific attacks through
expensive online optimization. To address these limitations, we propose
PolyJuice, the first black-box, image-agnostic red-teaming method for SIDs,
based on an observed distribution shift in the T2I latent space between samples
correctly and incorrectly classified by the SID. PolyJuice generates attacks by
(i) identifying the direction of this shift through a lightweight offline
process that only requires black-box access to the SID, and (ii) exploiting
this direction by universally steering all generated images towards the SID's
failure modes. PolyJuice-steered T2I models are significantly more effective at
deceiving SIDs (up to 84%) compared to their unsteered counterparts. We also
show that the steering directions can be estimated efficiently at lower
resolutions and transferred to higher resolutions using simple interpolation,
reducing computational overhead. Finally, tuning SID models on
PolyJuice-augmented datasets notably enhances the performance of the detectors
(up to 30%).

</details>


### [235] [The Multi-Query Paradox in Zeroth-Order Optimization](https://arxiv.org/abs/2509.15552)
*Wei Lin,Qingyu Song,Hong Xu*

Main category: cs.LG

TL;DR: This paper focuses on zeroth-order (ZO) optimization and investigates the problem of query allocation under a fixed query budget. It compares two aggregation methods and shows that their suitability depends on the method used.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the under-explored problem of how to allocate a fixed query budget in ZO optimization, given the trade-off between the number of queries per iteration and the total number of iterations.

Method: The paper examines two aggregation methods, ZO-Avg and ZO-Align, and derives their convergence rates in different settings (strongly convex, convex, non-convex, and stochastic). The trade-offs are analyzed systematically through theoretical and experimental validation.

Result: The analysis shows a dichotomy: ZO-Avg is optimized by a single-query approach, while ZO-Align benefits from multi-query approaches. The findings reveal a strict dependence on the aggregation method used.

Conclusion: The study clarifies that the query allocation decision boils down to selecting specific classical algorithms based on the aggregation method, rather than optimizing intermediate query sizes. This has theoretical and practical implications.

Abstract: Zeroth-order (ZO) optimization provides a powerful framework for problems
where explicit gradients are unavailable and have to be approximated using only
queries to function value. The prevalent single-query approach is simple, but
suffers from high estimation variance, motivating a multi-query paradigm to
improves estimation accuracy. This, however, creates a critical trade-off:
under a fixed budget of queries (i.e. cost), queries per iteration and the
total number of optimization iterations are inversely proportional to one
another. How to best allocate this budget is a fundamental, under-explored
question.
  This work systematically resolves this query allocation problem. We analyze
two aggregation methods: the de facto simple averaging (ZO-Avg), and a new
Projection Alignment method (ZO-Align) we derive from local surrogate
minimization. By deriving convergence rates for both methods that make the
dependence on the number of queries explicit across strongly convex, convex,
non-convex, and stochastic settings, we uncover a stark dichotomy: For ZO-Avg,
we prove that using more than one query per iteration is always
query-inefficient, rendering the single-query approach optimal. On the
contrary, ZO-Align generally performs better with more queries per iteration,
resulting in a full-subspace estimation as the optimal approach. Thus, our work
clarifies that the multi-query problem boils down to a choice not about an
intermediate query size, but between two classic algorithms, a choice dictated
entirely by the aggregation method used. These theoretical findings are also
consistently validated by extensive experiments.

</details>


### [236] [Reward Hacking Mitigation using Verifiable Composite Rewards](https://arxiv.org/abs/2509.15557)
*Mirza Farhan Bin Tarek,Rahmatollah Beheshti*

Main category: cs.LG

TL;DR: The paper extends Reinforcement Learning from Verifiable Rewards (RLVR) to mitigate reward hacking in medical domain question-answering applications.


<details>
  <summary>Details</summary>
Motivation: To address vulnerabilities, such as reward hacking, in RLVR when applied to medical domain question-answering tasks.

Method: Introduced a composite reward function with penalties for behaviors like skipping reasoning steps or using exploitative reasoning formats.

Result: The modified RLVR approach showed better reasoning formats, reduced reward hacking, and maintained good accuracy against baseline models.

Conclusion: Enhancing RLVR with a composite reward model improves reliability and reduces harmful behaviors in medical question-answering applications.

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has recently shown that
large language models (LLMs) can develop their own reasoning without direct
supervision. However, applications in the medical domain, specifically for
question answering, are susceptible to significant reward hacking during the
reasoning phase. Our work addresses two primary forms of this behavior: i)
providing a final answer without preceding reasoning, and ii) employing
non-standard reasoning formats to exploit the reward mechanism. To mitigate
these, we introduce a composite reward function with specific penalties for
these behaviors. Our experiments show that extending RLVR with our proposed
reward model leads to better-formatted reasoning with less reward hacking and
good accuracy compared to the baselines. This approach marks a step toward
reducing reward hacking and enhancing the reliability of models utilizing RLVR.

</details>


### [237] [Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning](https://arxiv.org/abs/2509.15561)
*Om Naphade,Saksham Bansal,Parikshit Pareek*

Main category: cs.LG

TL;DR: The paper introduces a framework using Small LLMs for Hyper-parameter Tuning (HPT), achieving near-GPT-4 performance with lower computational costs.


<details>
  <summary>Details</summary>
Motivation: To address the high computational cost and complexity of using Large Language Models (LLMs) exceeding 100 billion parameters for HPT in machine learning.

Method: The framework employs the Trajectory Context Summarizer (TCS), a deterministic block that converts raw training trajectories into structured contexts, enabling effective analysis by small LLMs.

Result: Using small LLMs (phi4:reasoning14B and qwen2.5-coder:32B), the proposed framework achieves average performance within ~0.9 percentage points of GPT-4 across six tasks under a limited trial budget.

Conclusion: Small, locally-run LLMs with the proposed TCS can match the reliability of large models like GPT-4 for HPT, significantly reducing computational demands.

Abstract: Hyper-parameter Tuning (HPT) is a necessary step in machine learning (ML)
pipelines but becomes computationally expensive and opaque with larger models.
Recently, Large Language Models (LLMs) have been explored for HPT, yet most
rely on models exceeding 100 billion parameters. We propose an Expert Block
Framework for HPT using Small LLMs. At its core is the Trajectory Context
Summarizer (TCS), a deterministic block that transforms raw training
trajectories into structured context, enabling small LLMs to analyze
optimization progress with reliability comparable to larger models. Using two
locally-run LLMs (phi4:reasoning14B and qwen2.5-coder:32B) and a 10-trial
budget, our TCS-enabled HPT pipeline achieves average performance within ~0.9
percentage points of GPT-4 across six diverse tasks.

</details>


### [238] [How many classes do we need to see for novel class discovery?](https://arxiv.org/abs/2509.15585)
*Akanksha Sarkar,Been Kim,Jennifer J. Sun*

Main category: cs.LG

TL;DR: The paper establishes an experimental framework to investigate factors affecting novel class discovery (NCD) using a controlled dataset, revealing that discovery benefits plateau as the number of known classes increases.


<details>
  <summary>Details</summary>
Motivation: Understanding and improving the ability of ML models to identify new class structures in complex datasets with entangled variations.

Method: The experimental setup uses the dSprites dataset with procedurally-generated modifying factors to control and study the influence of known/unknown class proportions and class coverage on discovery.

Result: Discovery performance shows diminishing returns as the number of known classes increases, suggesting saturation in the benefit of additional known classes.

Conclusion: The findings inform cost-benefit analyses for practitioners, provide insights into class discovery, and prompt future exploration of NCD in complex, real-world datasets.

Abstract: Novel class discovery is essential for ML models to adapt to evolving
real-world data, with applications ranging from scientific discovery to
robotics. However, these datasets contain complex and entangled factors of
variation, making a systematic study of class discovery difficult. As a result,
many fundamental questions are yet to be answered on why and when new class
discoveries are more likely to be successful. To address this, we propose a
simple controlled experimental framework using the dSprites dataset with
procedurally generated modifying factors. This allows us to investigate what
influences successful class discovery. In particular, we study the relationship
between the number of known/unknown classes and discovery performance, as well
as the impact of known class 'coverage' on discovering new classes. Our
empirical results indicate that the benefit of the number of known classes
reaches a saturation point beyond which discovery performance plateaus. The
pattern of diminishing return across different settings provides an insight for
cost-benefit analysis for practitioners and a starting point for more rigorous
future research of class discovery on complex real-world datasets.

</details>


### [239] [Personalized Prediction By Learning Halfspace Reference Classes Under Well-Behaved Distribution](https://arxiv.org/abs/2509.15592)
*Jizhou Huang,Brendan Juba*

Main category: cs.LG

TL;DR: The paper introduces a personalized prediction scheme using sparse linear classifiers for sub-populations, evaluated theoretically and with benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: The increasing use of complex machine learning models in high-stakes applications, such as healthcare, drives the need for accurate yet interpretable predictions.

Method: The study designs a PAC-learning algorithm for sub-populations characterized by halfspaces, employs a reference-class learning algorithm, and integrates sparse linear list learners.

Result: The paper establishes an upper bound for personalized prediction and demonstrates algorithm efficacy via standard datasets.

Conclusion: Personalized sparse linear predictors serve as competitive, interpretable models for specific sub-populations.

Abstract: In machine learning applications, predictive models are trained to serve
future queries across the entire data distribution. Real-world data often
demands excessively complex models to achieve competitive performance, however,
sacrificing interpretability. Hence, the growing deployment of machine learning
models in high-stakes applications, such as healthcare, motivates the search
for methods for accurate and explainable predictions. This work proposes a
Personalized Prediction scheme, where an easy-to-interpret predictor is learned
per query. In particular, we wish to produce a "sparse linear" classifier with
competitive performance specifically on some sub-population that includes the
query point. The goal of this work is to study the PAC-learnability of this
prediction model for sub-populations represented by "halfspaces" in a
label-agnostic setting. We first give a distribution-specific PAC-learning
algorithm for learning reference classes for personalized prediction. By
leveraging both the reference-class learning algorithm and a list learner of
sparse linear representations, we prove the first upper bound,
$O(\mathrm{opt}^{1/4} )$, for personalized prediction with sparse linear
classifiers and homogeneous halfspace subsets. We also evaluate our algorithms
on a variety of standard benchmark data sets.

</details>


### [240] [Efficient Extractive Text Summarization for Online News Articles Using Machine Learning](https://arxiv.org/abs/2509.15614)
*Sajib Biswas,Milon Biswas,Arunima Mandal,Fatema Tabassum Liza,Joy Sarker*

Main category: cs.LG

TL;DR: This paper explores extractive text summarization using machine learning techniques, with LSTM networks showing superior performance. It uses BERT embeddings and the Cornell Newsroom dataset to establish high-quality summaries.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the issue of information overload and enhance accessibility and user engagement by improving extractive text summarization techniques.

Method: The paper employs BERT embeddings for data transformation, frames summarization as a binary classification task, and compares models like logistic regression, feed-forward neural networks, and LSTM.

Result: LSTM networks outperformed other models and baseline methods, achieving higher metrics like F1 score and ROUGE-1 for summarization accuracy.

Conclusion: Automated summarization using LSTM networks greatly improves content management systems for online news platforms by delivering concise and coherent summaries that enhance user experiences.

Abstract: In the age of information overload, content management for online news
articles relies on efficient summarization to enhance accessibility and user
engagement. This article addresses the challenge of extractive text
summarization by employing advanced machine learning techniques to generate
concise and coherent summaries while preserving the original meaning. Using the
Cornell Newsroom dataset, comprising 1.3 million article-summary pairs, we
developed a pipeline leveraging BERT embeddings to transform textual data into
numerical representations. By framing the task as a binary classification
problem, we explored various models, including logistic regression,
feed-forward neural networks, and long short-term memory (LSTM) networks. Our
findings demonstrate that LSTM networks, with their ability to capture
sequential dependencies, outperform baseline methods like Lede-3 and simpler
models in F1 score and ROUGE-1 metrics. This study underscores the potential of
automated summarization in improving content management systems for online news
platforms, enabling more efficient content organization and enhanced user
experiences.

</details>


### [241] [Toward Efficient Influence Function: Dropout as a Compression Tool](https://arxiv.org/abs/2509.15651)
*Yuchen Zhang,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: This paper proposes a method to compute influence functions efficiently using dropout for gradient compression, addressing computational and memory challenges in large-scale models.


<details>
  <summary>Details</summary>
Motivation: Understanding training data's impact on machine learning models is critical for transparency and data selection, but influence function computation is resource-intensive.

Method: A novel approach employing dropout as a gradient compression mechanism to alleviate the computational and memory challenges in influence function calculation.

Result: Theoretical and experimental evaluations show that the method maintains key components of data influence and allows application to large-scale models.

Conclusion: The proposed method efficiently computes influence functions while preserving their effectiveness, offering scalability for modern machine learning models.

Abstract: Assessing the impact the training data on machine learning models is crucial
for understanding the behavior of the model, enhancing the transparency, and
selecting training data. Influence function provides a theoretical framework
for quantifying the effect of training data points on model's performance given
a specific test data. However, the computational and memory costs of influence
function presents significant challenges, especially for large-scale models,
even when using approximation methods, since the gradients involved in
computation are as large as the model itself. In this work, we introduce a
novel approach that leverages dropout as a gradient compression mechanism to
compute the influence function more efficiently. Our method significantly
reduces computational and memory overhead, not only during the influence
function computation but also in gradient compression process. Through
theoretical analysis and empirical validation, we demonstrate that our method
could preserves critical components of the data influence and enables its
application to modern large-scale models.

</details>


### [242] [Nonconvex Regularization for Feature Selection in Reinforcement Learning](https://arxiv.org/abs/2509.15652)
*Kyohei Suzuki,Konstantinos Slavakis*

Main category: cs.LG

TL;DR: The paper introduces a batch algorithm for feature selection in RL with guaranteed theoretical convergence, using the PMC penalty to address biases and a novel FRBS algorithm for problem-solving.


<details>
  <summary>Details</summary>
Motivation: Address the challenges in feature selection within RL, with a focus on minimizing estimation bias and improving performance in noisy scenarios.

Method: The method extends LSTD policy evaluation using a sparsity-inducing PMC penalty, paired with FRBS algorithm for solving nonmonotone-inclusion problems.

Result: The algorithm outperforms existing feature-selection methods in benchmarks, especially in the presence of noisy features.

Conclusion: The proposed approach offers a robust solution for efficient feature selection in RL, with theoretical guarantees and superior empirical performance.

Abstract: This work proposes an efficient batch algorithm for feature selection in
reinforcement learning (RL) with theoretical convergence guarantees. To
mitigate the estimation bias inherent in conventional regularization schemes,
the first contribution extends policy evaluation within the classical
least-squares temporal-difference (LSTD) framework by formulating a
Bellman-residual objective regularized with the sparsity-inducing, nonconvex
projected minimax concave (PMC) penalty. Owing to the weak convexity of the PMC
penalty, this formulation can be interpreted as a special instance of a general
nonmonotone-inclusion problem. The second contribution establishes novel
convergence conditions for the forward-reflected-backward splitting (FRBS)
algorithm to solve this class of problems. Numerical experiments on benchmark
datasets demonstrate that the proposed approach substantially outperforms
state-of-the-art feature-selection methods, particularly in scenarios with many
noisy features.

</details>


### [243] [KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning](https://arxiv.org/abs/2509.15676)
*Vaibhav Singh,Soumya Suvra Ghosal,Kapu Nirmal Joshua,Soumyabrata Pal,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: This paper focuses on improving example selection for in-context learning in large language models using an information-theoretic approach, and demonstrates better task performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of nearest-neighbor methods in high-dimensional spaces for selecting task-specific examples for in-context learning, which impacts performance due to poor generalization and lack of diversity.

Method: The authors propose a principled example selection framework that uses information theory to minimize prediction errors for a specific query instance, employing a greedy algorithm with submodular approximation, kernel tricks, and diversity-based regularization.

Result: The method achieves better performance compared to traditional retrieval approaches across various classification tasks, showcasing improvements in real-world, label-scarce environments.

Conclusion: Structure-aware and diverse example selection significantly enhances the effectiveness of in-context learning in adapting large language models to new tasks.

Abstract: In-context learning (ICL) has emerged as a powerful paradigm for adapting
large language models (LLMs) to new and data-scarce tasks using only a few
carefully selected task-specific examples presented in the prompt. However,
given the limited context size of LLMs, a fundamental question arises: Which
examples should be selected to maximize performance on a given user query?
While nearest-neighbor-based methods like KATE have been widely adopted for
this purpose, they suffer from well-known drawbacks in high-dimensional
embedding spaces, including poor generalization and a lack of diversity. In
this work, we study this problem of example selection in ICL from a principled,
information theory-driven perspective. We first model an LLM as a linear
function over input embeddings and frame the example selection task as a
query-specific optimization problem: selecting a subset of exemplars from a
larger example bank that minimizes the prediction error on a specific query.
This formulation departs from traditional generalization-focused learning
theoretic approaches by targeting accurate prediction for a specific query
instance. We derive a principled surrogate objective that is approximately
submodular, enabling the use of a greedy algorithm with an approximation
guarantee. We further enhance our method by (i) incorporating the kernel trick
to operate in high-dimensional feature spaces without explicit mappings, and
(ii) introducing an optimal design-based regularizer to encourage diversity in
the selected examples. Empirically, we demonstrate significant improvements
over standard retrieval methods across a suite of classification tasks,
highlighting the benefits of structure-aware, diverse example selection for ICL
in real-world, label-scarce scenarios.

</details>


### [244] [RMT-KD: Random Matrix Theoretic Causal Knowledge Distillation](https://arxiv.org/abs/2509.15724)
*Davide Ettori,Nastaran Darabi,Sureshkumar Senthilkumar,Amit Ranjan Trivedi*

Main category: cs.LG

TL;DR: RMT-KD leverages Random Matrix Theory for knowledge distillation to compress deep learning models, achieving reduced network size with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Large deep learning models are costly to deploy at the edge due to their size and computational requirements.

Method: RMT-KD uses Random Matrix Theory-based spectral properties to iteratively reduce network size while maintaining accuracy through self-distillation.

Result: RMT-KD achieves up to 80% parameter reduction, 2.8x faster inference, and halved power consumption with only 2% accuracy loss.

Conclusion: RMT-KD is a mathematically grounded and efficient approach to deep learning model compression, improving deployability without significant performance loss.

Abstract: Large deep learning models such as BERT and ResNet achieve state-of-the-art
performance but are costly to deploy at the edge due to their size and compute
demands. We present RMT-KD, a compression method that leverages Random Matrix
Theory (RMT) for knowledge distillation to iteratively reduce network size.
Instead of pruning or heuristic rank selection, RMT-KD preserves only
informative directions identified via the spectral properties of hidden
representations. RMT-based causal reduction is applied layer by layer with
self-distillation to maintain stability and accuracy. On GLUE, AG News, and
CIFAR-10, RMT-KD achieves up to 80% parameter reduction with only 2% accuracy
loss, delivering 2.8x faster inference and nearly halved power consumption.
These results establish RMT-KD as a mathematically grounded approach to network
distillation.

</details>


### [245] [EigenTrack: Spectral Activation Feature Tracking for Hallucination and Out-of-Distribution Detection in LLMs and VLMs](https://arxiv.org/abs/2509.15735)
*Davide Ettori,Nastaran Darabi,Sina Tayebati,Ranganath Krishnan,Mahesh Subedar,Omesh Tickoo,Amit Ranjan Trivedi*

Main category: cs.LG

TL;DR: EigenTrack is a real-time mechanism for detecting hallucination and OOD errors in large language models by analyzing the spectral geometry of hidden activations.


<details>
  <summary>Details</summary>
Motivation: Hallucination and out-of-distribution errors are persistent challenges in the utility of large language models, raising the need for interpretable and efficient detection tools.

Method: EigenTrack proposes using covariance-spectrum statistics like entropy, eigenvalue gaps, and KL divergence, streamed into a lightweight recurrent classifier to monitor changes in hidden layer representation dynamics.

Result: The method effectively identifies temporal shifts in representation structure, enabling it to detect errors before they become visible at the output level, without requiring resampling or multiple forward passes.

Conclusion: EigenTrack offers an interpretable, efficient solution that maintains temporal and global context while balancing accuracy and latency for improved detection of model issues.

Abstract: Large language models (LLMs) offer broad utility but remain prone to
hallucination and out-of-distribution (OOD) errors. We propose EigenTrack, an
interpretable real-time detector that uses the spectral geometry of hidden
activations, a compact global signature of model dynamics. By streaming
covariance-spectrum statistics such as entropy, eigenvalue gaps, and KL
divergence from random baselines into a lightweight recurrent classifier,
EigenTrack tracks temporal shifts in representation structure that signal
hallucination and OOD drift before surface errors appear. Unlike black- and
grey-box methods, it needs only a single forward pass without resampling.
Unlike existing white-box detectors, it preserves temporal context, aggregates
global signals, and offers interpretable accuracy-latency trade-offs.

</details>


### [246] [Aircraft Fuel Flow Modelling with Ageing Effects: From Parametric Corrections to Neural Networks](https://arxiv.org/abs/2509.15736)
*Gabriel Jarry,Ramon Dalmau,Philippe Very,Junzi Sun*

Main category: cs.LG

TL;DR: This paper examines methods to integrate aircraft engine ageing effects into fuel-flow predictions for improved operational and environmental assessments.


<details>
  <summary>Details</summary>
Motivation: Current fuel-flow models fail to account for performance deterioration due to aircraft ageing, limiting their accuracy in planning and environmental analysis.

Method: Multiple approaches are explored, including physics-based models, empirical corrections, and neural network architectures with ageing as a key variable, tested on 19,000 flights from nine Airbus A320-214 airframes.

Result: Models incorporating age-based corrections show reduced bias and improved prediction accuracy compared to baseline models, which underestimate fuel consumption for older aircraft.

Conclusion: Integrating ageing effects into prediction models enhances reliability but requires larger datasets including detailed maintenance records for improved generalization.

Abstract: Accurate modelling of aircraft fuel-flow is crucial for both operational
planning and environmental impact assessment, yet standard parametric models
often neglect performance deterioration that occurs as aircraft age. This paper
investigates multiple approaches to integrate engine ageing effects into
fuel-flow prediction for the Airbus A320-214, using a comprehensive dataset of
approximately nineteen thousand Quick Access Recorder flights from nine
distinct airframes with varying years in service. We systematically evaluate
classical physics-based models, empirical correction coefficients, and
data-driven neural network architectures that incorporate age either as an
input feature or as an explicit multiplicative bias. Results demonstrate that
while baseline models consistently underestimate fuel consumption for older
aircraft, the use of age-dependent correction factors and neural models
substantially reduces bias and improves prediction accuracy. Nevertheless,
limitations arise from the small number of airframes and the lack of detailed
maintenance event records, which constrain the representativeness and
generalization of age-based corrections. This study emphasizes the importance
of accounting for the effects of ageing in parametric and machine learning
frameworks to improve the reliability of operational and environmental
assessments. The study also highlights the need for more diverse datasets that
can capture the complexity of real-world engine deterioration.

</details>


### [247] [GUI-ReWalk: Massive Data Generation for GUI Agent via Stochastic Exploration and Intent-Aware Reasoning](https://arxiv.org/abs/2509.15738)
*Musen Lin,Minghao Liu,Taoran Lu,Lichen Yuan,Yiwei Liu,Haonan Xu,Yu Miao,Yuhao Chao,Zhaojian Li*

Main category: cs.LG

TL;DR: GUI-ReWalk introduces a strategy for generating realistic and diverse GUI interaction trajectories, improving automation in GUI agents.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality trajectory data limits the progress of GUI agents in achieving end-to-end automation in digital environments.

Method: GUI-ReWalk uses a multi-stage process that begins with stochastic exploration mimicking human trial-error and transitions into a reasoning-driven phase, generating purposeful interactions and multi-stride task workflows.

Result: The approach improves diversity and structure in interaction flows, achieving superior results on benchmarks such as Screenspot-Pro, OSWorld-G, and others.

Conclusion: GUI-ReWalk establishes itself as a scalable, data-efficient tool for creating realistic data, aiding GUI agent research and practical automation.

Abstract: Graphical User Interface (GUI) Agents, powered by large language and
vision-language models, hold promise for enabling end-to-end automation in
digital environments. However, their progress is fundamentally constrained by
the scarcity of scalable, high-quality trajectory data. Existing data
collection strategies either rely on costly and inconsistent manual annotations
or on synthetic generation methods that trade off between diversity and
meaningful task coverage. To bridge this gap, we present GUI-ReWalk: a
reasoning-enhanced, multi-stage framework for synthesizing realistic and
diverse GUI trajectories. GUI-ReWalk begins with a stochastic exploration phase
that emulates human trial-and-error behaviors, and progressively transitions
into a reasoning-guided phase where inferred goals drive coherent and
purposeful interactions. Moreover, it supports multi-stride task generation,
enabling the construction of long-horizon workflows across multiple
applications. By combining randomness for diversity with goal-aware reasoning
for structure, GUI-ReWalk produces data that better reflects the intent-aware,
adaptive nature of human-computer interaction. We further train Qwen2.5-VL-7B
on the GUI-ReWalk dataset and evaluate it across multiple benchmarks, including
Screenspot-Pro, OSWorld-G, UI-Vision, AndroidControl, and GUI-Odyssey. Results
demonstrate that GUI-ReWalk enables superior coverage of diverse interaction
flows, higher trajectory entropy, and more realistic user intent. These
findings establish GUI-ReWalk as a scalable and data-efficient framework for
advancing GUI agent research and enabling robust real-world automation.

</details>


### [248] [Incremental Multistep Forecasting of Battery Degradation Using Pseudo Targets](https://arxiv.org/abs/2509.15740)
*Jonathan Adam Rico,Nagarajan Raghavan,Senthilnath Jayavelu*

Main category: cs.LG

TL;DR: The paper introduces iFSNet, an online adaptive model for battery prognosis that refines forecasts incrementally using pseudo targets, aiming to overcome limitations of offline models.


<details>
  <summary>Details</summary>
Motivation: Existing offline ML models for battery prognosis need retraining when encountering new data distributions. A demand arises for adaptive online methods that perform continuous learning.

Method: iFSNet employs sample-by-sample learning using a linear regressor to extrapolate pseudo targets. It calculates loss incrementally and updates using FSNet’s associative memory and adaptive structures.

Result: iFSNet achieved impressive results on battery degradation datasets, attaining low errors (RMSE and MAE), underscoring its efficacy in handling varied degradation trajectories.

Conclusion: iFSNet provides an effective solution for real-time multistep forecasting in battery prognosis, addressing challenges in existing incremental approaches and ensuring ongoing adaptability.

Abstract: Data-driven models accurately perform early battery prognosis to prevent
equipment failure and further safety hazards. Most existing machine learning
(ML) models work in offline mode which must consider their retraining
post-deployment every time new data distribution is encountered. Hence, there
is a need for an online ML approach where the model can adapt to varying
distributions. However, existing online incremental multistep forecasts are a
great challenge as there is no way to correct the model of its forecasts at the
current instance. Also, these methods need to wait for a considerable amount of
time to acquire enough streaming data before retraining. In this study, we
propose iFSNet (incremental Fast and Slow learning Network) which is a modified
version of FSNet for a single-pass mode (sample-by-sample) to achieve multistep
forecasting using pseudo targets. It uses a simple linear regressor of the
input sequence to extrapolate pseudo future samples (pseudo targets) and
calculate the loss from the rest of the forecast and keep updating the model.
The model benefits from the associative memory and adaptive structure
mechanisms of FSNet, at the same time the model incrementally improves by using
pseudo targets. The proposed model achieved 0.00197 RMSE and 0.00154 MAE on
datasets with smooth degradation trajectories while it achieved 0.01588 RMSE
and 0.01234 MAE on datasets having irregular degradation trajectories with
capacity regeneration spikes.

</details>


### [249] [On Optimal Steering to Achieve Exact Fairness](https://arxiv.org/abs/2509.15759)
*Mohit Sharma,Amit Jayant Deshpande,Chiranjib Bhattacharyya,Rajiv Ratn Shah*

Main category: cs.LG

TL;DR: The paper addresses fairness in machine learning by defining and optimizing feature distributions to achieve ideal group-fair outcomes via efficient algorithms and empirically demonstrates fairness improvement without compromising utility.


<details>
  <summary>Details</summary>
Motivation: To solve the 'bias in, bias out' issue in fair ML by creating fair feature distributions or LLM representations that guarantee group-fair outcomes.

Method: Define 'ideal distributions' with no fairness-utility trade-off, formulate an optimization program in KL-divergence, and provide algorithms for steering distributions towards fairness.

Result: Empirical tests show improved fairness and, in some cases, utility enhancement for both synthetic and real-world data, such as reducing bias in occupation predictions using LLM representations.

Conclusion: Optimal steering of distributions can effectively enhance fairness while maintaining or improving utility in machine learning models.

Abstract: To fix the 'bias in, bias out' problem in fair machine learning, it is
important to steer feature distributions of data or internal representations of
Large Language Models (LLMs) to ideal ones that guarantee group-fair outcomes.
Previous work on fair generative models and representation steering could
greatly benefit from provable fairness guarantees on the model output. We
define a distribution as ideal if the minimizer of any cost-sensitive risk on
it is guaranteed to have exact group-fair outcomes (e.g., demographic parity,
equal opportunity)-in other words, it has no fairness-utility trade-off. We
formulate an optimization program for optimal steering by finding the nearest
ideal distribution in KL-divergence, and provide efficient algorithms for it
when the underlying distributions come from well-known parametric families
(e.g., normal, log-normal). Empirically, our optimal steering techniques on
both synthetic and real-world datasets improve fairness without diminishing
utility (and sometimes even improve utility). We demonstrate affine steering of
LLM representations to reduce bias in multi-class classification, e.g.,
occupation prediction from a short biography in Bios dataset (De-Arteaga et
al.). Furthermore, we steer internal representations of LLMs towards desired
outputs so that it works equally well across different groups.

</details>


### [250] [Learning to Optimize Capacity Planning in Semiconductor Manufacturing](https://arxiv.org/abs/2509.15767)
*Philipp Andelfinger,Jieyi Bi,Qiuyu Zhu,Jianan Zhou,Bo Zhang,Fei Fei Zhang,Chew Wye Chan,Boon Ping Gan,Wentong Cai,Jie Zhang*

Main category: cs.LG

TL;DR: The paper introduces a neural network-based capacity planning model for semiconductor manufacturing, improving throughput and cycle time by capturing machine-level interactions.


<details>
  <summary>Details</summary>
Motivation: To address limitations of heuristic methods in semiconductor manufacturing by better accounting for complex machine-process interactions and avoiding production bottlenecks.

Method: The proposed model uses a deep reinforcement learning framework with a heterogeneous graph neural network to capture diverse interrelationships in machine-level production.

Result: Tests on Intel's Minifab model and another testbed show throughput and cycle time improvements of approximately 1.8% in the largest scenario.

Conclusion: The model demonstrates potential advantages over heuristic approaches by enabling more proactive and optimized decision-making in capacity planning.

Abstract: In manufacturing, capacity planning is the process of allocating production
resources in accordance with variable demand. The current industry practice in
semiconductor manufacturing typically applies heuristic rules to prioritize
actions, such as future change lists that account for incoming machine and
recipe dedications. However, while offering interpretability, heuristics cannot
easily account for the complex interactions along the process flow that can
gradually lead to the formation of bottlenecks. Here, we present a neural
network-based model for capacity planning on the level of individual machines,
trained using deep reinforcement learning. By representing the policy using a
heterogeneous graph neural network, the model directly captures the diverse
relationships among machines and processing steps, allowing for proactive
decision-making. We describe several measures taken to achieve sufficient
scalability to tackle the vast space of possible machine-level actions.
  Our evaluation results cover Intel's small-scale Minifab model and
preliminary experiments using the popular SMT2020 testbed. In the largest
tested scenario, our trained policy increases throughput and decreases cycle
time by about 1.8% each.

</details>


### [251] [Monte Carlo Tree Diffusion with Multiple Experts for Protein Design](https://arxiv.org/abs/2509.15796)
*Xuefeng Liu,Mingxuan Cao,Songhao Jiang,Xiao Luo,Xiaotian Duan,Mengdi Wang,Tobin R. Sosnick,Jinbo Xu,Rick Stevens*

Main category: cs.LG

TL;DR: This paper introduces MCTD-ME, a novel approach combining masked diffusion models and tree search to enhance protein design, surpassing previous methods in sequence recovery and structural similarity.


<details>
  <summary>Details</summary>
Motivation: Address challenges in protein design related to long-range dependencies and large search spaces, improving efficiency and accuracy.

Method: Developed MCTD-ME, integrating diffusion models with multi-token planning and multi-expert guidance for enhanced exploration and protein design.

Result: The approach demonstrated superior performance in sequence recovery and structural similarity benchmarks, particularly for longer proteins.

Conclusion: MCTD-ME is versatile and sets a new benchmark in protein design, with potential applications in de novo engineering and molecular generation.

Abstract: The goal of protein design is to generate amino acid sequences that fold into
functional structures with desired properties. Prior methods combining
autoregressive language models with Monte Carlo Tree Search (MCTS) struggle
with long-range dependencies and suffer from an impractically large search
space. We propose MCTD-ME, Monte Carlo Tree Diffusion with Multiple Experts,
which integrates masked diffusion models with tree search to enable multi-token
planning and efficient exploration. Unlike autoregressive planners, MCTD-ME
uses biophysical-fidelity-enhanced diffusion denoising as the rollout engine,
jointly revising multiple positions and scaling to large sequence spaces. It
further leverages experts of varying capacities to enrich exploration, guided
by a pLDDT-based masking schedule that targets low-confidence regions while
preserving reliable residues. We propose a novel multi-expert selection rule
(PH-UCT-ME) extends predictive-entropy UCT to expert ensembles. On the inverse
folding task (CAMEO and PDB benchmarks), MCTD-ME outperforms single-expert and
unguided baselines in both sequence recovery (AAR) and structural similarity
(scTM), with gains increasing for longer proteins and benefiting from
multi-expert guidance. More generally, the framework is model-agnostic and
applicable beyond inverse folding, including de novo protein engineering and
multi-objective molecular generation.

</details>


### [252] [On the Convergence of Muon and Beyond](https://arxiv.org/abs/2509.15816)
*Da Chang,Yongxiang Liu,Ganzhao Yuan*

Main category: cs.LG

TL;DR: The paper introduces Muon-VR2, a variance-reduced variant of the Muon optimizer, achieving optimal convergence rates for matrix-structured parameter training.


<details>
  <summary>Details</summary>
Motivation: To address the gap between the Muon optimizer's strong empirical success and its suboptimal theoretical convergence rate in stochastic non-convex settings.

Method: Construct and analyze Muon-VR2 by integrating a variance-reduction mechanism and provide rigorous convergence proofs under both stochastic non-convex conditions and the Polyak-Łojasiewicz condition.

Result: Muon-VR2 achieves an optimal convergence rate of $\tilde{\mathcal{O}}(T^{-1/3})$ and demonstrates improved theoretical guarantees and performance in vision and language tasks.

Conclusion: The work establishes the first theoretical proof of optimality for a Muon-style optimizer, paving the way for development of faster and more efficient variants.

Abstract: The Muon optimizer has demonstrated remarkable empirical success in handling
matrix-structured parameters for training neural networks. However, a
significant gap persists between its practical performance and theoretical
understanding. Existing analyses indicate that the standard Muon variant
achieves only a suboptimal convergence rate of $\mathcal{O}(T^{-1/4})$ in
stochastic non-convex settings, where $T$ denotes the number of iterations. To
explore the theoretical limits of the Muon framework, we construct and analyze
a variance-reduced variant, termed Muon-VR2. We provide the first rigorous
proof that incorporating a variance-reduction mechanism enables Muon-VR2 to
attain an optimal convergence rate of $\tilde{\mathcal{O}}(T^{-1/3})$, thereby
matching the theoretical lower bound for this class of problems. Moreover, our
analysis establishes convergence guarantees for Muon variants under the
Polyak-{\L}ojasiewicz (P{\L}) condition. Extensive experiments on vision
(CIFAR-10) and language (C4) benchmarks corroborate our theoretical findings on
per-iteration convergence. Overall, this work provides the first proof of
optimality for a Muon-style optimizer and clarifies the path toward developing
more practically efficient, accelerated variants.

</details>


### [253] [SolarCrossFormer: Improving day-ahead Solar Irradiance Forecasting by Integrating Satellite Imagery and Ground Sensors](https://arxiv.org/abs/2509.15827)
*Baptiste Schubnel,Jelena Simeunović,Corentin Tissier,Pierre-Jean Alet,Rafael E. Carrillo*

Main category: cs.LG

TL;DR: The SolarCrossFormer model significantly improves solar irradiance forecasts by leveraging deep learning and combining diverse data inputs.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for high-resolution, accurate solar irradiance forecasts required to integrate solar power effectively into the energy grid.

Method: The method involves using SolarCrossFormer, a deep learning model based on graph neural networks, which combines satellite imagery and ground data to improve forecast accuracy and resolution.

Result: Experimental results reveal SolarCrossFormer achieves a normalized mean absolute error of 6.1%, performing on par with commercial weather forecasting services.

Conclusion: SolarCrossFormer is effective, robust, and adaptable, offering precise forecasts even in the absence of local input data, which positions it as a viable solution for solar energy challenges in Switzerland.

Abstract: Accurate day-ahead forecasts of solar irradiance are required for the
large-scale integration of solar photovoltaic (PV) systems into the power grid.
However, current forecasting solutions lack the temporal and spatial resolution
required by system operators. In this paper, we introduce SolarCrossFormer, a
novel deep learning model for day-ahead irradiance forecasting, that combines
satellite images and time series from a ground-based network of meteorological
stations. SolarCrossFormer uses novel graph neural networks to exploit the
inter- and intra-modal correlations of the input data and improve the accuracy
and resolution of the forecasts. It generates probabilistic forecasts for any
location in Switzerland with a 15-minute resolution for horizons up to 24 hours
ahead. One of the key advantages of SolarCrossFormer its robustness in real
life operations. It can incorporate new time-series data without retraining the
model and, additionally, it can produce forecasts for locations without input
data by using only their coordinates. Experimental results over a dataset of
one year and 127 locations across Switzerland show that SolarCrossFormer yield
a normalized mean absolute error of 6.1 % over the forecasting horizon. The
results are competitive with those achieved by a commercial numerical weather
prediction service.

</details>


### [254] [HyP-ASO: A Hybrid Policy-based Adaptive Search Optimization Framework for Large-Scale Integer Linear Programs](https://arxiv.org/abs/2509.15828)
*Ning Xu,Junkai Zhang,Yang Wu,Huigen Ye,Hua Xu,Huiling Xu,Yifan Zhang*

Main category: cs.LG

TL;DR: HyP-ASO is a novel hybrid framework using reinforcement learning for faster and efficient Integer Linear Programs (ILPs) solving, outperforming modern methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for solving large-scale Integer Linear Programs are slow and inefficient due to the NP-hard nature of ILPs, and even the Large Neighborhood Search (LNS)-based frameworks struggle to create highly effective neighborhoods.

Method: Proposed HyP-ASO, a hybrid framework combining a customized formula to calculate variable selection probabilities and a reinforcement learning-based policy network to predict neighborhood sizes for improved optimization.

Result: Extensive experiments show that HyP-ASO outperforms existing LNS-based methods in both speed and effectiveness for large-scale ILPs while being lightweight and scalable.

Conclusion: The HyP-ASO framework is an effective and scalable solution for solving large-scale ILPs, showcasing significant improvements over traditional and contemporary techniques.

Abstract: Directly solving large-scale Integer Linear Programs (ILPs) using traditional
solvers is slow due to their NP-hard nature. While recent frameworks based on
Large Neighborhood Search (LNS) can accelerate the solving process, their
performance is often constrained by the difficulty in generating sufficiently
effective neighborhoods. To address this challenge, we propose HyP-ASO, a
hybrid policy-based adaptive search optimization framework that combines a
customized formula with deep Reinforcement Learning (RL). The formula leverages
feasible solutions to calculate the selection probabilities for each variable
in the neighborhood generation process, and the RL policy network predicts the
neighborhood size. Extensive experiments demonstrate that HyP-ASO significantly
outperforms existing LNS-based approaches for large-scale ILPs. Additional
experiments show it is lightweight and highly scalable, making it well-suited
for solving large-scale ILPs.

</details>


### [255] [Tsururu: A Python-based Time Series Forecasting Strategies Library](https://arxiv.org/abs/2509.15843)
*Alina Kostromina,Kseniia Kuvshinova,Aleksandr Yugay,Andrey Savchenko,Dmitry Simakov*

Main category: cs.LG

TL;DR: The paper introduces Tsururu, a Python library aimed at improving time series forecasting through flexible combinations of approaches and integration with various models.


<details>
  <summary>Details</summary>
Motivation: Address the gap in optimal training approach selection for time series forecasting, bridging research and practical applications.

Method: Development of the Tsururu library, which allows flexible combinations of global/multivariate approaches and forecasting strategies.

Result: Tsururu enables seamless integration with various forecasting models, making it easier to apply to both research and industry settings.

Conclusion: Tsururu enhances the adaptability and usability of time series forecasting by providing flexible and comprehensive tools accessible through its library.

Abstract: While current time series research focuses on developing new models, crucial
questions of selecting an optimal approach for training such models are
underexplored. Tsururu, a Python library introduced in this paper, bridges SoTA
research and industry by enabling flexible combinations of global and
multivariate approaches and multi-step-ahead forecasting strategies. It also
enables seamless integration with various forecasting models. Available at
https://github.com/sb-ai-lab/tsururu .

</details>


### [256] [EvoBrain: Dynamic Multi-channel EEG Graph Modeling for Time-evolving Brain Network](https://arxiv.org/abs/2509.15857)
*Rikuto Kotoge,Zheng Chen,Tasuku Kimura,Yasuko Matsubara,Takufumi Yanagisawa,Haruhiko Kishima,Yasushi Sakurai*

Main category: cs.LG

TL;DR: Dynamic GNNs are promising for seizure detection in EEG data but struggle with evolving brain connectivity and modeling temporal-graph interactions. EvoBrain overcomes these issues using a novel dynamic modeling approach.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in capturing brain state dynamics, such as seizure progression, due to limitations in static graphs and inconsistent modeling of temporal-graph interactions.

Method: Developed EvoBrain, a model combining a Mamba architecture with GCN enhanced by Laplacian Positional Encoding, explicitly including dynamic graph structures where nodes and edges evolve. Conducted theoretical analysis to validate method.

Result: EvoBrain showed significant performance improvements: AUROC increased by 23% and F1 score by 30% compared to the baseline dynamic GNN model.

Conclusion: EvoBrain provides a more effective approach to seizure detection and early prediction by explicitly modeling brain dynamics and connectivity.

Abstract: Dynamic GNNs, which integrate temporal and spatial features in
Electroencephalography (EEG) data, have shown great potential in automating
seizure detection. However, fully capturing the underlying dynamics necessary
to represent brain states, such as seizure and non-seizure, remains a
non-trivial task and presents two fundamental challenges. First, most existing
dynamic GNN methods are built on temporally fixed static graphs, which fail to
reflect the evolving nature of brain connectivity during seizure progression.
Second, current efforts to jointly model temporal signals and graph structures
and, more importantly, their interactions remain nascent, often resulting in
inconsistent performance. To address these challenges, we present the first
theoretical analysis of these two problems, demonstrating the effectiveness and
necessity of explicit dynamic modeling and time-then-graph dynamic GNN method.
Building on these insights, we propose EvoBrain, a novel seizure detection
model that integrates a two-stream Mamba architecture with a GCN enhanced by
Laplacian Positional Encoding, following neurological insights. Moreover,
EvoBrain incorporates explicitly dynamic graph structures, allowing both nodes
and edges to evolve over time. Our contributions include (a) a theoretical
analysis proving the expressivity advantage of explicit dynamic modeling and
time-then-graph over other approaches, (b) a novel and efficient model that
significantly improves AUROC by 23% and F1 score by 30%, compared with the
dynamic GNN baseline, and (c) broad evaluations of our method on the
challenging early seizure prediction tasks.

</details>


### [257] [Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data](https://arxiv.org/abs/2509.15859)
*Nakul Sharma*

Main category: cs.LG

TL;DR: The paper addresses imbalanced classification datasets by introducing an efficient framework that uses Vision Foundation Models to synthesize data for training simple linear classifiers, achieving strong results with reduced computational demands.


<details>
  <summary>Details</summary>
Motivation: Imbalanced datasets often lead to poor performance for minority classes, and existing methods using foundation models fail to close the performance gap or require considerable computational resources.

Method: The authors utilize the semantic latent space of Vision Foundation Models to generate synthetic data, combine it with real data, and apply a simple linear classifier for long-tail classification.

Result: Their method establishes new state-of-the-art results on the CIFAR-100-LT benchmark and demonstrates competitiveness on the Places-LT benchmark.

Conclusion: The proposed approach is computationally efficient and effective for tackling long-tail classification tasks, utilizing fewer trainable parameters while achieving outstanding performance.

Abstract: Imbalanced classification datasets pose significant challenges in machine
learning, often leading to biased models that perform poorly on
underrepresented classes. With the rise of foundation models, recent research
has focused on the full, partial, and parameter-efficient fine-tuning of these
models to deal with long-tail classification. Despite the impressive
performance of these works on the benchmark datasets, they still fail to close
the gap with the networks trained using the balanced datasets and still require
substantial computational resources, even for relatively smaller datasets.
Underscoring the importance of computational efficiency and simplicity, in this
work we propose a novel framework that leverages the rich semantic latent space
of Vision Foundation Models to generate synthetic data and train a simple
linear classifier using a mixture of real and synthetic data for long-tail
classification. The computational efficiency gain arises from the number of
trainable parameters that are reduced to just the number of parameters in the
linear model. Our method sets a new state-of-the-art for the CIFAR-100-LT
benchmark and demonstrates strong performance on the Places-LT benchmark,
highlighting the effectiveness and adaptability of our simple and effective
approach.

</details>


### [258] [SAGE: Semantic-Aware Shared Sampling for Efficient Diffusion](https://arxiv.org/abs/2509.15865)
*Haoran Zhao,Tong Bai,Lei Huang,Xiaoyu Liang*

Main category: cs.LG

TL;DR: The paper introduces SAGE, a framework to reduce the sampling cost of diffusion models by sharing sampling steps for similar queries, achieving efficiency without compromising quality. Experiments show significant cost reduction and improved generation quality.


<details>
  <summary>Details</summary>
Motivation: There is a need to address the high sampling cost of diffusion models without decreasing the quality of generation.

Method: SAGE employs a semantic-aware shared sampling approach and a specific training strategy to enable efficiency while preserving the quality of diffusion model sampling.

Result: SAGE achieves a 25.5% reduction in sampling cost, along with improvements including 5.0% lower FID (better quality), 5.4% higher CLIP score (better alignment), and 160% higher diversity compared to baseline methods.

Conclusion: SAGE demonstrates that shared sampling across semantically alike queries in diffusion models can significantly enhance efficiency while maintaining or improving the quality of results.

Abstract: Diffusion models manifest evident benefits across diverse domains, yet their
high sampling cost, requiring dozens of sequential model evaluations, remains a
major limitation. Prior efforts mainly accelerate sampling via optimized
solvers or distillation, which treat each query independently. In contrast, we
reduce total number of steps by sharing early-stage sampling across
semantically similar queries. To enable such efficiency gains without
sacrificing quality, we propose SAGE, a semantic-aware shared sampling
framework that integrates a shared sampling scheme for efficiency and a
tailored training strategy for quality preservation. Extensive experiments show
that SAGE reduces sampling cost by 25.5%, while improving generation quality
with 5.0% lower FID, 5.4% higher CLIP, and 160% higher diversity over
baselines.

</details>


### [259] [From Data to Diagnosis: A Large, Comprehensive Bone Marrow Dataset and AI Methods for Childhood Leukemia Prediction](https://arxiv.org/abs/2509.15895)
*Henning Höfener,Farina Kock,Martina Pontones,Tabita Ghete,David Pfrang,Nicholas Dickel,Meik Kunz,Daniela P. Schacherer,David A. Clunie,Andrey Fedorov,Max Westphal,Markus Metzler*

Main category: cs.LG

TL;DR: The study presents a public leukemia bone marrow dataset and AI methods spanning cell detection, classification, and diagnosis prediction, aiming to streamline the complex diagnosis process.


<details>
  <summary>Details</summary>
Motivation: Manual leukemia diagnosis using bone marrow morphology is time-consuming and complicated, necessitating AI solutions that encompass the entire diagnostic pipeline.

Method: The researchers created a large public dataset with detailed annotations, developed AI models for cell detection, classification, and diagnosis prediction, and rigorously evaluated their performance using metrics like precision, AUC, and F1-scores.

Result: AI models showed high accuracy with a cell detection precision of 0.96, 33-class cell classification AUC of 0.98 and F1-score of 0.61, and diagnosis prediction F1-score of 0.90.

Conclusion: The dataset and AI methods promise enhanced leukemia diagnostic precision, offering significant potential for further research and improved patient care outcomes.

Abstract: Leukemia diagnosis primarily relies on manual microscopic analysis of bone
marrow morphology supported by additional laboratory parameters, making it
complex and time consuming. While artificial intelligence (AI) solutions have
been proposed, most utilize private datasets and only cover parts of the
diagnostic pipeline. Therefore, we present a large, high-quality, publicly
available leukemia bone marrow dataset spanning the entire diagnostic process,
from cell detection to diagnosis. Using this dataset, we further propose
methods for cell detection, cell classification, and diagnosis prediction. The
dataset comprises 246 pediatric patients with diagnostic, clinical and
laboratory information, over 40 000 cells with bounding box annotations and
more than 28 000 of these with high-quality class labels, making it the most
comprehensive dataset publicly available. Evaluation of the AI models yielded
an average precision of 0.96 for the cell detection, an area under the curve of
0.98, and an F1-score of 0.61 for the 33-class cell classification, and a mean
F1-score of 0.90 for the diagnosis prediction using predicted cell counts.
While the proposed approaches demonstrate their usefulness for AI-assisted
diagnostics, the dataset will foster further research and development in the
field, ultimately contributing to more precise diagnoses and improved patient
outcomes.

</details>


### [260] [Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds](https://arxiv.org/abs/2509.15915)
*Remo Sasso,Michelangelo Conserva,Dominik Jeurissen,Paulo Rauber*

Main category: cs.LG

TL;DR: The paper examines the integration of foundation models (FMs) into reinforcement learning, exploring two strategies—foundation world models (FWMs) and foundation agents (FAs)—and evaluates their effectiveness using grid-world environments.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning often requires extensive interactions, which can be impractical in real-world applications. Foundation models, with their extensive knowledge and reasoning capabilities, could address the limitation of low sample efficiency in reinforcement learning, but how to effectively integrate them remains uncertain.

Method: The authors evaluate two integration strategies of FMs into reinforcement learning: (1) Foundation World Models (FWMs) that leverage FM knowledge to enable simulations for training and evaluation; and (2) Foundation Agents (FAs) that utilize FM reasoning for decision-making. These strategies are empirically tested in grid-world environments aligned to the capabilities of current large language models (LLMs).

Result: The study finds that improvements in large language models (LLMs) lead to better-performing FWMs and FAs. Foundation Agents (FAs) derived from current LLMs deliver excellent policies in simple environments, while FWMs demonstrate strong potential in handling more complex scenarios involving partial observability and stochastic elements.

Conclusion: Integrating foundation models into reinforcement learning has significant potential. FWMs and FAs enhance sample efficiency and decision-making, with FWMs especially promising for complex, uncertain environments.

Abstract: While reinforcement learning from scratch has shown impressive results in
solving sequential decision-making tasks with efficient simulators, real-world
applications with expensive interactions require more sample-efficient agents.
Foundation models (FMs) are natural candidates to improve sample efficiency as
they possess broad knowledge and reasoning capabilities, but it is yet unclear
how to effectively integrate them into the reinforcement learning framework. In
this paper, we anticipate and, most importantly, evaluate two promising
strategies. First, we consider the use of foundation world models (FWMs) that
exploit the prior knowledge of FMs to enable training and evaluating agents
with simulated interactions. Second, we consider the use of foundation agents
(FAs) that exploit the reasoning capabilities of FMs for decision-making. We
evaluate both approaches empirically in a family of grid-world environments
that are suitable for the current generation of large language models (LLMs).
Our results suggest that improvements in LLMs already translate into better
FWMs and FAs; that FAs based on current LLMs can already provide excellent
policies for sufficiently simple environments; and that the coupling of FWMs
and reinforcement learning agents is highly promising for more complex settings
with partial observability and stochastic elements.

</details>


### [261] [EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-grained Emotions](https://arxiv.org/abs/2509.15986)
*Xinchen Wan,Jinhua Liang,Huan Zhang*

Main category: cs.LG

TL;DR: EmoHeal is a personalized digital tool addressing emotional wellness using fine-grained emotion detection and music therapy principles, showing significant improvement in mood and therapeutic accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing mental wellness tools are overly static and generalized, failing to address specific emotional challenges like pre-sleep anxiety, which affects over 1.5 billion people globally.

Method: EmoHeal uses an XLM-RoBERTa model to detect 27 emotions, maps them to musical therapy parameters with a knowledge graph, and employs CLAMP3 to retrieve audiovisual content for emotional transition.

Result: A study with 40 participants showed substantial mood improvement and high accuracy in emotional recognition, with a strong correlation to therapeutic outcomes (r=0.72, p<0.001).

Conclusion: EmoHeal demonstrates the feasibility and effectiveness of using emotion-aware AI systems grounded in music therapy, showing promise for scalable digital wellness solutions.

Abstract: Existing digital mental wellness tools often overlook the nuanced emotional
states underlying everyday challenges. For example, pre-sleep anxiety affects
more than 1.5 billion people worldwide, yet current approaches remain largely
static and "one-size-fits-all", failing to adapt to individual needs. In this
work, we present EmoHeal, an end-to-end system that delivers personalized,
three-stage supportive narratives. EmoHeal detects 27 fine-grained emotions
from user text with a fine-tuned XLM-RoBERTa model, mapping them to musical
parameters via a knowledge graph grounded in music therapy principles (GEMS,
iso-principle). EmoHeal retrieves audiovisual content using the CLAMP3 model to
guide users from their current state toward a calmer one
("match-guide-target"). A within-subjects study (N=40) demonstrated significant
supportive effects, with participants reporting substantial mood improvement
(M=4.12, p<0.001) and high perceived emotion recognition accuracy (M=4.05,
p<0.001). A strong correlation between perceived accuracy and therapeutic
outcome (r=0.72, p<0.001) validates our fine-grained approach. These findings
establish the viability of theory-driven, emotion-aware digital wellness tools
and provides a scalable AI blueprint for operationalizing music therapy
principles.

</details>


### [262] [Enhancing Generative Auto-bidding with Offline Reward Evaluation and Policy Search](https://arxiv.org/abs/2509.15927)
*Zhiyu Mou,Yiqin Lv,Miao Xu,Cheems Wang,Yixiu Mao,Qichen Ye,Chao Li,Rongquan Bai,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.LG

TL;DR: The paper introduces AIGB-Pearl, an advanced auto-bidding method that integrates generative planning and policy optimization, coupled with a non-bootstrapped trajectory evaluator to iteratively enhance bidding performance.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from improving upon AIGB methods by addressing their limitations in fine-grained generation quality evaluation and exploration beyond static datasets.

Method: The proposed method includes a trajectory evaluator assigning rewards and guiding policy search, utilizing a Large Language Model-based architecture, hybrid losses for score learning, and integrating expert feedback adaptively.

Result: Extensive experiments on both simulated and real-world advertising systems demonstrate the superior and state-of-the-art performance of AIGB-Pearl.

Conclusion: AIGB-Pearl is an effective solution for overcoming traditional auto-bidding limitations, showing its potential in achieving superior advertising performance and promising broader applicability.

Abstract: Auto-bidding is an essential tool for advertisers to enhance their
advertising performance. Recent progress has shown that AI-Generated Bidding
(AIGB), which formulates the auto-bidding as a trajectory generation task and
trains a conditional diffusion-based planner on offline data, achieves superior
and stable performance compared to typical offline reinforcement learning
(RL)-based auto-bidding methods. However, existing AIGB methods still encounter
a performance bottleneck due to their neglect of fine-grained generation
quality evaluation and inability to explore beyond static datasets. To address
this, we propose AIGB-Pearl (\emph{Planning with EvAluator via RL}), a novel
method that integrates generative planning and policy optimization. The key to
AIGB-Pearl is to construct a non-bootstrapped \emph{trajectory evaluator} to
assign rewards and guide policy search, enabling the planner to optimize its
generation quality iteratively through interaction. Furthermore, to enhance
trajectory evaluator accuracy in offline settings, we incorporate three key
techniques: (i) a Large Language Model (LLM)-based architecture for better
representational capacity, (ii) hybrid point-wise and pair-wise losses for
better score learning, and (iii) adaptive integration of expert feedback for
better generalization ability. Extensive experiments on both simulated and
real-world advertising systems demonstrate the state-of-the-art performance of
our approach.

</details>


### [263] [SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection](https://arxiv.org/abs/2509.16060)
*Maithili Joshi,Palash Nandi,Tanmoy Chakraborty*

Main category: cs.LG

TL;DR: The paper introduces SABER, a white-box jailbreak method targeting Large Language Models' safety mechanisms to significantly improve attack efficiency while minimally affecting model perplexity.


<details>
  <summary>Details</summary>
Motivation: To address vulnerabilities in safety-aligned Large Language Models, specifically those exploited by jailbreak attacks.

Method: The authors propose SABER, a technique that creates residual connections between two intermediate layers in models for bypassing safety mechanisms.

Result: SABER improves attack performance by 51% on the HarmBench test set compared to the best baseline, with minimal perplexity changes.

Conclusion: The study demonstrates a novel vulnerability in LLMs' alignment mechanisms and provides an effective method to exploit this, urging considerations for enhancing the robustness of LLMs.

Abstract: Large Language Models (LLMs) with safe-alignment training are powerful
instruments with robust language comprehension capabilities. These models
typically undergo meticulous alignment procedures involving human feedback to
ensure the acceptance of safe inputs while rejecting harmful or unsafe ones.
However, despite their massive scale and alignment efforts, LLMs remain
vulnerable to jailbreak attacks, where malicious users manipulate the model to
produce harmful outputs that it was explicitly trained to avoid. In this study,
we find that the safety mechanisms in LLMs are predominantly embedded in the
middle-to-late layers. Building on this insight, we introduce a novel white-box
jailbreak method, SABER (Safety Alignment Bypass via Extra Residuals), which
connects two intermediate layers $s$ and $e$ such that $s < e$, through a
residual connection. Our approach achieves a 51% improvement over the
best-performing baseline on the HarmBench test set. Furthermore, SABER induces
only a marginal shift in perplexity when evaluated on the HarmBench validation
set. The source code is publicly available at
https://github.com/PalGitts/SABER.

</details>


### [264] [Improving Monte Carlo Tree Search for Symbolic Regression](https://arxiv.org/abs/2509.15929)
*Zhengyao Huang,Daniel Zhengyu Huang,Tiannan Xiao,Dina Ma,Zhenyu Ming,Hao Shi,Yuanhui Wen*

Main category: cs.LG

TL;DR: This paper proposes an enhanced Monte Carlo Tree Search (MCTS) framework for symbolic regression by introducing extreme bandit allocation strategies and state-jumping actions to improve efficiency and robustness.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges of discovering optimal mathematical expressions for symbolic regression, a task dominated by genetic programming but limited by search inefficiencies.

Method: The authors enhance MCTS by introducing extreme bandit allocation for better reward optimization and state-jumping actions like mutation and crossover for broader search transitions.

Result: Numerical studies demonstrate competitive performance against leading symbolic regression methods, achieving favorable accuracy and complexity trade-offs.

Conclusion: The proposed framework advances symbolic regression by improving search techniques, resulting in robust and efficient model discovery, with code publicly available for further exploration.

Abstract: Symbolic regression aims to discover concise, interpretable mathematical
expressions that satisfy desired objectives, such as fitting data, posing a
highly combinatorial optimization problem. While genetic programming has been
the dominant approach, recent efforts have explored reinforcement learning
methods for improving search efficiency. Monte Carlo Tree Search (MCTS), with
its ability to balance exploration and exploitation through guided search, has
emerged as a promising technique for symbolic expression discovery. However,
its traditional bandit strategies and sequential symbol construction often
limit performance. In this work, we propose an improved MCTS framework for
symbolic regression that addresses these limitations through two key
innovations: (1) an extreme bandit allocation strategy tailored for identifying
globally optimal expressions, with finite-time performance guarantees under
polynomial reward decay assumptions; and (2) evolution-inspired state-jumping
actions such as mutation and crossover, which enable non-local transitions to
promising regions of the search space. These state-jumping actions also reshape
the reward landscape during the search process, improving both robustness and
efficiency. We conduct a thorough numerical study to the impact of these
improvements and benchmark our approach against existing symbolic regression
methods on a variety of datasets, including both ground-truth and black-box
datasets. Our approach achieves competitive performance with state-of-the-art
libraries in terms of recovery rate, attains favorable positions on the Pareto
frontier of accuracy versus model complexity. Code is available at
https://github.com/PKU-CMEGroup/MCTS-4-SR.

</details>


### [265] [Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences](https://arxiv.org/abs/2509.16189)
*Andrew Kyle Lampinen,Martin Engelcke,Yuxuan Li,Arslan Chaudhry,James L. McClelland*

Main category: cs.LG

TL;DR: The paper explores why machine learning systems struggle with generalization, focusing on their inability to perform latent learning. By drawing on cognitive science, the authors suggest episodic memory integration and retrieval mechanisms as solutions.


<details>
  <summary>Details</summary>
Motivation: Machine learning systems often fail to generalize well, which is a significant limitation compared to natural intelligence.

Method: The paper analyzes failures in generalization, connects them to cognitive principles like latent learning, and proposes incorporating episodic memory and retrieval mechanisms to improve ML systems.

Result: The study shows better generalization in ML systems when equipped with oracle retrieval mechanisms and highlights components like in-context learning that facilitate effective use of retrieved information.

Conclusion: Machine learning systems can overcome data inefficiency and improve generalization by adopting methods inspired by cognitive science, particularly episodic memory and retrieval strategies.

Abstract: When do machine learning systems fail to generalize, and what mechanisms
could improve their generalization? Here, we draw inspiration from cognitive
science to argue that one weakness of machine learning systems is their failure
to exhibit latent learning -- learning information that is not relevant to the
task at hand, but that might be useful in a future task. We show how this
perspective links failures ranging from the reversal curse in language modeling
to new findings on agent-based navigation. We then highlight how cognitive
science points to episodic memory as a potential part of the solution to these
issues. Correspondingly, we show that a system with an oracle retrieval
mechanism can use learning experiences more flexibly to generalize better
across many of these challenges. We also identify some of the essential
components for effectively using retrieval, including the importance of
within-example in-context learning for acquiring the ability to use information
across retrieved examples. In summary, our results illustrate one possible
contributor to the relative data inefficiency of current machine learning
systems compared to natural intelligence, and help to understand how retrieval
methods can complement parametric learning to improve generalization.

</details>


### [266] [Bayesian Physics Informed Neural Networks for Reliable Transformer Prognostics](https://arxiv.org/abs/2509.15933)
*Ibai Ramirez,Jokin Alcibar,Joel Pino,Mikel Sanz,David Pardo,Jose I. Aizpurua*

Main category: cs.LG

TL;DR: This paper proposes a Bayesian Physics-Informed Neural Network (B-PINN) for improved prognostics in transformer insulation degradation, integrating physics-based models with uncertainty-aware predictions.


<details>
  <summary>Details</summary>
Motivation: To address the limited application of Scientific Machine Learning in prognostics by tackling the challenges of incorporating partial differential equations (PDEs) and robust uncertainty quantification.

Method: Development of a Bayesian PINN framework that integrates Bayesian Neural Networks within the PINN structure to model transformer ageing driven by thermal stress and includes testing of various prior distributions.

Result: The B-PINN outperformed the dropout-PINN baseline, demonstrating enhanced reliability in prognostic predictions and better quantification of predictive uncertainty.

Conclusion: The proposed B-PINN methodology effectively supports robust decision-making in power asset maintenance through its uncertainty-aware predictions and physical knowledge integration.

Abstract: Scientific Machine Learning (SciML) integrates physics and data into the
learning process, offering improved generalization compared with purely
data-driven models. Despite its potential, applications of SciML in prognostics
remain limited, partly due to the complexity of incorporating partial
differential equations (PDEs) for ageing physics and the scarcity of robust
uncertainty quantification methods. This work introduces a Bayesian
Physics-Informed Neural Network (B-PINN) framework for probabilistic
prognostics estimation. By embedding Bayesian Neural Networks into the PINN
architecture, the proposed approach produces principled, uncertainty-aware
predictions. The method is applied to a transformer ageing case study, where
insulation degradation is primarily driven by thermal stress. The heat
diffusion PDE is used as the physical residual, and different prior
distributions are investigated to examine their impact on predictive posterior
distributions and their ability to encode a priori physical knowledge. The
framework is validated against a finite element model developed and tested with
real measurements from a solar power plant. Results, benchmarked against a
dropout-PINN baseline, show that the proposed B-PINN delivers more reliable
prognostic predictions by accurately quantifying predictive uncertainty. This
capability is crucial for supporting robust and informed maintenance
decision-making in critical power assets.

</details>


### [267] [UniTac2Pose: A Unified Approach Learned in Simulation for Category-level Visuotactile In-hand Pose Estimation](https://arxiv.org/abs/2509.15934)
*Mingdong Wu,Long Yang,Jin Liu,Weiyao Huang,Lehong Wu,Zelin Chen,Daolin Ma,Hao Dong*

Main category: cs.LG

TL;DR: This paper presents a new three-stage framework for object pose estimation using a unified energy-based diffusion model that improves performance in unseen CAD models and integrates different tasks into a unified system.


<details>
  <summary>Details</summary>
Motivation: Current methods for in-hand pose estimation struggle with achieving high precision and generalization to unseen CAD models, which is essential for many industrial and everyday tasks.

Method: The proposed framework includes three stages—sampling and pre-ranking pose candidates, iterative refinement, and post-ranking using a unified energy-based diffusion model trained on simulated data. A render-compare architecture enhances sim-to-real transfer.

Result: Experiments show the method outperforms baseline techniques like regression and matching, demonstrates strong generalization to new CAD models, and performs robustly across diverse real-world tasks.

Conclusion: The approach successfully integrates object pose estimation, pose tracking, and uncertainty estimation into a cohesive method, showcasing its robustness and generalizability.

Abstract: Accurate estimation of the in-hand pose of an object based on its CAD model
is crucial in both industrial applications and everyday tasks, ranging from
positioning workpieces and assembling components to seamlessly inserting
devices like USB connectors. While existing methods often rely on regression,
feature matching, or registration techniques, achieving high precision and
generalizability to unseen CAD models remains a significant challenge. In this
paper, we propose a novel three-stage framework for in-hand pose estimation.
The first stage involves sampling and pre-ranking pose candidates, followed by
iterative refinement of these candidates in the second stage. In the final
stage, post-ranking is applied to identify the most likely pose candidates.
These stages are governed by a unified energy-based diffusion model, which is
trained solely on simulated data. This energy model simultaneously generates
gradients to refine pose estimates and produces an energy scalar that
quantifies the quality of the pose estimates. Additionally, borrowing the idea
from the computer vision domain, we incorporate a render-compare architecture
within the energy-based score network to significantly enhance sim-to-real
performance, as demonstrated by our ablation studies. We conduct comprehensive
experiments to show that our method outperforms conventional baselines based on
regression, matching, and registration techniques, while also exhibiting strong
intra-category generalization to previously unseen CAD models. Moreover, our
approach integrates tactile object pose estimation, pose tracking, and
uncertainty estimation into a unified framework, enabling robust performance
across a variety of real-world conditions.

</details>


### [268] [Targeted Fine-Tuning of DNN-Based Receivers via Influence Functions](https://arxiv.org/abs/2509.15950)
*Marko Tuononen,Heikki Penttinen,Ville Hautamäki*

Main category: cs.LG

TL;DR: This paper introduces influence functions to improve the performance of deep learning-based wireless receivers.


<details>
  <summary>Details</summary>
Motivation: The authors aim to harness influence functions for better interpretability and targeted fine-tuning of deep learning-based wireless receivers.

Method: The method involves applying influence functions to identify impactful training samples and using targeted fine-tuning with a binary cross-entropy loss for improvement.

Result: The approach improves bit error rates in single-target scenarios, while multi-target adaptation was less effective.

Conclusion: Influence functions offer interpretability and efficiency benefits for wireless receiver adaptation, with promising single-target performance and room for improvement in multi-target settings.

Abstract: We present the first use of influence functions for deep learning-based
wireless receivers. Applied to DeepRx, a fully convolutional receiver,
influence analysis reveals which training samples drive bit predictions,
enabling targeted fine-tuning of poorly performing cases. We show that
loss-relative influence with capacity-like binary cross-entropy loss and
first-order updates on beneficial samples most consistently improves bit error
rate toward genie-aided performance, outperforming random fine-tuning in
single-target scenarios. Multi-target adaptation proved less effective,
underscoring open challenges. Beyond experiments, we connect influence to
self-influence corrections and propose a second-order, influence-aligned update
strategy. Our results establish influence functions as both an interpretability
tool and a basis for efficient receiver adaptation.

</details>


### [269] [Adversarial Graph Fusion for Incomplete Multi-view Semi-supervised Learning with Tensorial Imputation](https://arxiv.org/abs/2509.15955)
*Zhangqi Jiang,Tingjin Luo,Xu Yang,Xinyan Liang*

Main category: cs.LG

TL;DR: The paper tackles the Sub-Cluster Problem (SCP) in graph-based multi-view semi-supervised learning caused by missing samples and introduces AGF-TI to address this using adversarial graph fusion and tensor-learning techniques.


<details>
  <summary>Details</summary>
Motivation: Traditional graph-based methods struggle with view missing in multi-view semi-supervised learning, as missing samples can disrupt local structures and degrade classification accuracy.

Method: AGF-TI employs adversarial graph fusion to counter distorted structures, tensor-based low-rank learning for incomplete structure recovery, and anchors to reduce computational cost. An efficient optimization algorithm ensures convergence.

Result: Experimental results demonstrate AGF-TI's superior performance over existing techniques across diverse datasets.

Conclusion: AGF-TI effectively mitigates SCP by improving graph fusion, recovering incomplete structures, and boosting classification outcomes, underscoring its potential in real-world applications.

Abstract: View missing remains a significant challenge in graph-based multi-view
semi-supervised learning, hindering their real-world applications. To address
this issue, traditional methods introduce a missing indicator matrix and focus
on mining partial structure among existing samples in each view for label
propagation (LP). However, we argue that these disregarded missing samples
sometimes induce discontinuous local structures, i.e., sub-clusters, breaking
the fundamental smoothness assumption in LP. Consequently, such a Sub-Cluster
Problem (SCP) would distort graph fusion and degrade classification
performance. To alleviate SCP, we propose a novel incomplete multi-view
semi-supervised learning method, termed AGF-TI. Firstly, we design an
adversarial graph fusion scheme to learn a robust consensus graph against the
distorted local structure through a min-max framework. By stacking all
similarity matrices into a tensor, we further recover the incomplete structure
from the high-order consistency information based on the low-rank tensor
learning. Additionally, the anchor-based strategy is incorporated to reduce the
computational complexity. An efficient alternative optimization algorithm
combining a reduced gradient descent method is developed to solve the
formulated objective, with theoretical convergence. Extensive experimental
results on various datasets validate the superiority of our proposed AGF-TI as
compared to state-of-the-art methods. Code is available at
https://github.com/ZhangqiJiang07/AGF_TI.

</details>


### [270] [Inverse Optimization Latent Variable Models for Learning Costs Applied to Route Problems](https://arxiv.org/abs/2509.15999)
*Alan A. Lahoud,Erik Schaffernicht,Johannes A. Stork*

Main category: cs.LG

TL;DR: The paper introduces IO-LVM, a model that learns latent representations of constrained optimization problems (COPs) and reconstructs feasible solutions using solvers.


<details>
  <summary>Details</summary>
Motivation: To address challenges in learning representations for COP solutions, especially in enforcing output constraints during decoding.

Method: Proposes IO-LVM, combining latent space learning of COPs' cost functions with a solver loop and gradient estimation using Fenchel-Young loss.

Result: IO-LVM shows competence in reconstructing paths, predicting distributions, and providing interpretable latent representations using datasets from real-world and synthetic applications.

Conclusion: IO-LVM advances learning for COPs by enabling a distributional understanding of cost functions, diverse solution modeling, and practical applications.

Abstract: Learning representations for solutions of constrained optimization problems
(COPs) with unknown cost functions is challenging, as models like (Variational)
Autoencoders struggle to enforce constraints when decoding structured outputs.
We propose an Inverse Optimization Latent Variable Model (IO-LVM) that learns a
latent space of COP cost functions from observed solutions and reconstructs
feasible outputs by solving a COP with a solver in the loop. Our approach
leverages estimated gradients of a Fenchel-Young loss through a
non-differentiable deterministic solver to shape the latent space. Unlike
standard Inverse Optimization or Inverse Reinforcement Learning methods, which
typically recover a single or context-specific cost function, IO-LVM captures a
distribution over cost functions, enabling the identification of diverse
solution behaviors arising from different agents or conditions not available
during the training process. We validate our method on real-world datasets of
ship and taxi routes, as well as paths in synthetic graphs, demonstrating its
ability to reconstruct paths and cycles, predict their distributions, and yield
interpretable latent representations.

</details>


### [271] [Predicting the descent into extremism and terrorism](https://arxiv.org/abs/2509.16014)
*R. O. Lane,W. J. Holmes,C. J. Taylor,H. M. State-Davey,A. J. Wragge*

Main category: cs.LG

TL;DR: The paper presents a system to analyze and track statements for extremist or terrorist intent using machine learning, achieving high detection accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of identifying extremist or terrorist intentions in online statements using an automated, machine learning-based system.

Method: The authors collected quotes from various sources, encoded them using the Universal Sentence Encoder, and trained a support vector machine classifier with 10-fold cross-validation. Tracking algorithms and visualization tools were then applied.

Result: The system achieved 81% accuracy in detecting extremism and 97% in detecting terrorism from a dataset of 839 quotes, outperforming n-gram baselines.

Conclusion: The proposed approach demonstrates promise not only in accurately detecting attitudes related to extremism and terrorism but also in tracking temporal changes in mindset through analysis.

Abstract: This paper proposes an approach for automatically analysing and tracking
statements in material gathered online and detecting whether the authors of the
statements are likely to be involved in extremism or terrorism. The proposed
system comprises: online collation of statements that are then encoded in a
form amenable to machine learning (ML), an ML component to classify the encoded
text, a tracker, and a visualisation system for analysis of results. The
detection and tracking concept has been tested using quotes made by terrorists,
extremists, campaigners, and politicians, obtained from wikiquote.org. A set of
features was extracted for each quote using the state-of-the-art Universal
Sentence Encoder (Cer et al. 2018), which produces 512-dimensional vectors. The
data were used to train and test a support vector machine (SVM) classifier
using 10-fold cross-validation. The system was able to correctly detect
intentions and attitudes associated with extremism 81% of the time and
terrorism 97% of the time, using a dataset of 839 quotes. This accuracy was
higher than that which was achieved for a simple baseline system based on
n-gram text features. Tracking techniques were also used to perform a temporal
analysis of the data, with each quote considered to be a noisy measurement of a
person's state of mind. It was demonstrated that the tracking algorithms were
able to detect both trends over time and sharp changes in attitude that could
be attributed to major events.

</details>


### [272] [Time-adaptive SympNets for separable Hamiltonian systems](https://arxiv.org/abs/2509.16026)
*Konrad Janik,Peter Benner*

Main category: cs.LG

TL;DR: The paper extends adaptive symplectic integrators (TSympNets) for non-autonomous Hamiltonian systems, proving their universal approximation for separable systems and rectifying a mistake in a prior theorem.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning methods for Hamiltonian systems require fixed step-size training data, which is impractical as real-world data often involves irregular sampling.

Method: The authors adapt and extend TSympNets for non-autonomous Hamiltonian systems, provide theoretical proof of universal approximation, and conduct numerical experiments, besides correcting an error in a prior theorem.

Result: The paper demonstrates that TSympNets can universally approximate separable Hamiltonian systems but cannot extend this property to non-separable ones. Errors in a key previous theorem were identified and corrected.

Conclusion: TSympNets offer a powerful tool for modeling separable Hamiltonian systems with irregular sampling. However, their limitations for non-separable systems highlight areas for future exploration.

Abstract: Measurement data is often sampled irregularly i.e. not on equidistant time
grids. This is also true for Hamiltonian systems. However, existing machine
learning methods, which learn symplectic integrators, such as SympNets [20] and
H\'enonNets [4] still require training data generated by fixed step sizes. To
learn time-adaptive symplectic integrators, an extension to SympNets, which we
call TSympNets, was introduced in [20]. We adapt the architecture of TSympNets
and extend them to non-autonomous Hamiltonian systems. So far the approximation
qualities of TSympNets were unknown. We close this gap by providing a universal
approximation theorem for separable Hamiltonian systems and show that it is not
possible to extend it to non-separable Hamiltonian systems. To investigate
these theoretical approximation capabilities, we perform different numerical
experiments. Furthermore we fix a mistake in a proof of a substantial theorem
[25, Theorem 2] for the approximation of symplectic maps in general, but
specifically for symplectic machine learning methods.

</details>


### [273] [Automated Constitutive Model Discovery by Pairing Sparse Regression Algorithms with Model Selection Criteria](https://arxiv.org/abs/2509.16040)
*Jorge-Humberto Urrea-Quintero,David Anton,Laura De Lorenzis,Henning Wessels*

Main category: cs.LG

TL;DR: The paper proposes and evaluates a framework for automated discovery of constitutive models using sparse regression algorithms and model selection criteria, achieving high accuracy for isotropic and anisotropic materials.


<details>
  <summary>Details</summary>
Motivation: Traditional calibration methods for constitutive models are limited, and an automated approach could significantly improve model generation by balancing sparsity, predictive performance, and computational cost.

Method: The study pairs three sparse regression algorithms (LASSO, LARS, OMP) with three model selection criteria ($K$-fold CV, AIC, BIC), creating nine unique combinations. These are systematically applied to isotropic and anisotropic hyperelasticity datasets, including synthetic and experimental data.

Result: All nine algorithm-criterion combinations were found to perform consistently well across different material types, producing accurate and reliable constitutive models.

Conclusion: The framework provides a robust and versatile methodology for discovering constitutive models, demonstrating the capability to extend beyond $\ell_1$-based approaches and delivering highly accurate results.

Abstract: The automated discovery of constitutive models from data has recently emerged
as a promising alternative to the traditional model calibration paradigm. In
this work, we present a fully automated framework for constitutive model
discovery that systematically pairs three sparse regression algorithms (Least
Absolute Shrinkage and Selection Operator (LASSO), Least Angle Regression
(LARS), and Orthogonal Matching Pursuit (OMP)) with three model selection
criteria: $K$-fold cross-validation (CV), Akaike Information Criterion (AIC),
and Bayesian Information Criterion (BIC). This pairing yields nine distinct
algorithms for model discovery and enables a systematic exploration of the
trade-off between sparsity, predictive performance, and computational cost.
While LARS serves as an efficient path-based solver for the
$\ell_1$-constrained problem, OMP is introduced as a tractable heuristic for
$\ell_0$-regularized selection. The framework is applied to both isotropic and
anisotropic hyperelasticity, utilizing both synthetic and experimental
datasets. Results reveal that all nine algorithm-criterion combinations perform
consistently well for the discovery of isotropic and anisotropic materials,
yielding highly accurate constitutive models. These findings broaden the range
of viable discovery algorithms beyond $\ell_1$-based approaches such as LASSO.

</details>


### [274] [Communications to Circulations: 3D Wind Field Retrieval and Real-Time Prediction Using 5G GNSS Signals and Deep Learning](https://arxiv.org/abs/2509.16068)
*Yuchen Ye,Hong Liang,Chaoxia Yuan,Mingyu Li,Aoqi Zhou,Chunqing Shang,Hua Cai,Peixi Liu,Kezuan Wang,Yifeng Zheng*

Main category: cs.LG

TL;DR: This paper presents G-WindCast, a deep learning framework using 5G GNSS signals for accurate, cost-effective, and scalable 3D atmospheric wind field forecasting.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for obtaining high-resolution atmospheric wind data face challenges due to technological limitations, computational expense, and biases in numerical weather prediction models.

Method: The study leverages deep learning techniques, specifically Forward Neural Networks and Transformer networks, that utilize variations in 5G GNSS signal strength to predict 3D wind fields.

Result: Initial results indicated high accuracy in wind retrieval and forecasting, showing robustness across forecast horizons and pressure levels, and outperforming ERA5 reanalysis in some metrics.

Conclusion: The findings emphasize the potential of integrating non-traditional data sources and advanced machine learning methods to revolutionize environmental monitoring and real-time atmospheric applications.

Abstract: Accurate atmospheric wind field information is crucial for various
applications, including weather forecasting, aviation safety, and disaster risk
reduction. However, obtaining high spatiotemporal resolution wind data remains
challenging due to limitations in traditional in-situ observations and remote
sensing techniques, as well as the computational expense and biases of
numerical weather prediction (NWP) models. This paper introduces G-WindCast, a
novel deep learning framework that leverages signal strength variations from 5G
Global Navigation Satellite System (GNSS) signals to retrieve and forecast
three-dimensional (3D) atmospheric wind fields. The framework utilizes Forward
Neural Networks (FNN) and Transformer networks to capture complex, nonlinear,
and spatiotemporal relationships between GNSS-derived features and wind
dynamics. Our preliminary results demonstrate promising accuracy in both wind
retrieval and short-term wind forecasting (up to 30 minutes lead time), with
skill scores comparable to high-resolution NWP outputs in certain scenarios.
The model exhibits robustness across different forecast horizons and pressure
levels, and its predictions for wind speed and direction show superior
agreement with observations compared to concurrent ERA5 reanalysis data.
Furthermore, we show that the system can maintain excellent performance for
localized forecasting even with a significantly reduced number of GNSS stations
(e.g., around 100), highlighting its cost-effectiveness and scalability. This
interdisciplinary approach underscores the transformative potential of
exploiting non-traditional data sources and deep learning for advanced
environmental monitoring and real-time atmospheric applications.

</details>


### [275] [MTS-DMAE: Dual-Masked Autoencoder for Unsupervised Multivariate Time Series Representation Learning](https://arxiv.org/abs/2509.16078)
*Yi Xu,Yitian Zhang,Yun Fu*

Main category: cs.LG

TL;DR: This paper introduces Dual-Masked Autoencoder (DMAE), a novel framework leveraging masked modeling tasks for robust unsupervised multivariate time-series representation learning.


<details>
  <summary>Details</summary>
Motivation: The need for compact and informative representations from multivariate time series data without relying on labels, enabling its application to various downstream tasks.

Method: A dual-masked autoencoder framework combining two pretext tasks: reconstructing masked values and predicting latent representations with teacher guidance, accompanied by a feature-level alignment constraint.

Result: DMAE demonstrated consistent and superior performance over competitive baselines across classification, regression, and forecasting tasks.

Conclusion: DMAE effectively learns temporally coherent and semantically rich multivariate time-series representations through its novel dual-masked framework.

Abstract: Unsupervised multivariate time series (MTS) representation learning aims to
extract compact and informative representations from raw sequences without
relying on labels, enabling efficient transfer to diverse downstream tasks. In
this paper, we propose Dual-Masked Autoencoder (DMAE), a novel masked
time-series modeling framework for unsupervised MTS representation learning.
DMAE formulates two complementary pretext tasks: (1) reconstructing masked
values based on visible attributes, and (2) estimating latent representations
of masked features, guided by a teacher encoder. To further improve
representation quality, we introduce a feature-level alignment constraint that
encourages the predicted latent representations to align with the teacher's
outputs. By jointly optimizing these objectives, DMAE learns temporally
coherent and semantically rich representations. Comprehensive evaluations
across classification, regression, and forecasting tasks demonstrate that our
approach achieves consistent and superior performance over competitive
baselines.

</details>


### [276] [Rethinking Molecule Synthesizability with Chain-of-Reaction](https://arxiv.org/abs/2509.16084)
*Seul Lee,Karsten Kreis,Srimukh Prasad Veccham,Meng Liu,Danny Reidenbach,Saee Paliwal,Weili Nie,Arash Vahdat*

Main category: cs.LG

TL;DR: ReaSyn is a generative framework designed to address challenges in synthesizable molecule generation by leveraging the concept of chain-of-reaction reasoning and reinforcement learning to optimize synthetic pathways.


<details>
  <summary>Details</summary>
Motivation: Existing molecular generative models struggle with synthesizable molecule generation due to the vast combinatorial space and poor molecular optimization performance.

Method: ReaSyn introduces the chain-of-reaction (CoR) notation, supervised training, reinforcement learning-based finetuning, and test-time compute scaling to navigate synthesizable chemical spaces effectively.

Result: ReaSyn achieved top performance in synthesizable molecule reconstruction and optimization, with superior hit expansion compared to existing methods.

Conclusion: ReaSyn successfully addresses limitations in existing generative methods, showcasing its ability to explore synthesizable chemical spaces and optimize molecules effectively.

Abstract: A well-known pitfall of molecular generative models is that they are not
guaranteed to generate synthesizable molecules. There have been considerable
attempts to address this problem, but given the exponentially large
combinatorial space of synthesizable molecules, existing methods have shown
limited coverage of the space and poor molecular optimization performance. To
tackle these problems, we introduce ReaSyn, a generative framework for
synthesizable projection where the model explores the neighborhood of given
molecules in the synthesizable space by generating pathways that result in
synthesizable analogs. To fully utilize the chemical knowledge contained in the
synthetic pathways, we propose a novel perspective that views synthetic
pathways akin to reasoning paths in large language models (LLMs). Specifically,
inspired by chain-of-thought (CoT) reasoning in LLMs, we introduce the
chain-of-reaction (CoR) notation that explicitly states reactants, reaction
types, and intermediate products for each step in a pathway. With the CoR
notation, ReaSyn can get dense supervision in every reaction step to explicitly
learn chemical reaction rules during supervised training and perform
step-by-step reasoning. In addition, to further enhance the reasoning
capability of ReaSyn, we propose reinforcement learning (RL)-based finetuning
and goal-directed test-time compute scaling tailored for synthesizable
projection. ReaSyn achieves the highest reconstruction rate and pathway
diversity in synthesizable molecule reconstruction and the highest optimization
performance in synthesizable goal-directed molecular optimization, and
significantly outperforms previous synthesizable projection methods in
synthesizable hit expansion. These results highlight ReaSyn's superior ability
to navigate combinatorially-large synthesizable chemical space.

</details>


### [277] [Randomized Smoothing Meets Vision-Language Models](https://arxiv.org/abs/2509.16088)
*Emmanouil Seferis,Changshun Wu,Stefanos Kollias,Saddek Bensalem,Chih-Hong Cheng*

Main category: cs.LG

TL;DR: The paper extends randomized smoothing (RS) for its application on generative models by connecting their outputs to classification tasks and derives theoretical robustness guarantees.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in robustness certification for generative models using randomized smoothing, which is well-studied for classifiers but unclear for models with sequence-based outputs.

Method: The authors propose connecting generative outputs to classification tasks using an oracle and develop a theory that defines robustness radius and scaling laws based on sample efficiency under bounded errors.

Result: The paper shows that robustness certification is feasible for state-of-the-art generative models, demonstrating its validity against robust adversarial attacks like jailbreak-style attempts.

Conclusion: This research validates randomized smoothing as both practical and computationally efficient for certifying robustness in advanced generative models like VLMs.

Abstract: Randomized smoothing (RS) is one of the prominent techniques to ensure the
correctness of machine learning models, where point-wise robustness
certificates can be derived analytically. While RS is well understood for
classification, its application to generative models is unclear, since their
outputs are sequences rather than labels. We resolve this by connecting
generative outputs to an oracle classification task and showing that RS can
still be enabled: the final response can be classified as a discrete action
(e.g., service-robot commands in VLAs), as harmful vs. harmless (content
moderation or toxicity detection in VLMs), or even applying oracles to cluster
answers into semantically equivalent ones. Provided that the error rate for the
oracle classifier comparison is bounded, we develop the theory that associates
the number of samples with the corresponding robustness radius. We further
derive improved scaling laws analytically relating the certified radius and
accuracy to the number of samples, showing that the earlier result of 2 to 3
orders of magnitude fewer samples sufficing with minimal loss remains valid
even under weaker assumptions. Together, these advances make robustness
certification both well-defined and computationally feasible for
state-of-the-art VLMs, as validated against recent jailbreak-style adversarial
attacks.

</details>


### [278] [DiffusionNFT: Online Diffusion Reinforcement with Forward Process](https://arxiv.org/abs/2509.16117)
*Kaiwen Zheng,Huayu Chen,Haotian Ye,Haoxiang Wang,Qinsheng Zhang,Kai Jiang,Hang Su,Stefano Ermon,Jun Zhu,Ming-Yu Liu*

Main category: cs.LG

TL;DR: Online RL for diffusion models, DiffusionNFT, improves efficiency, using flow matching without CFG and likelihood estimation.


<details>
  <summary>Details</summary>
Motivation: Online RL has proven useful in language models, but adapting it to diffusion models faces intractable likelihood challenges.

Method: DiffusionNFT introduces policy optimization using flow matching methods and contrasts positive/negative generations.

Result: The method is up to 25× faster than FlowGRPO, improves efficiency in gradient optimization, and achieves superior GenEval scores.

Conclusion: DiffusionNFT makes diffusion model RL more straightforward, efficient, and robust, bypassing complex integration and likelihood estimation.

Abstract: Online reinforcement learning (RL) has been central to post-training language
models, but its extension to diffusion models remains challenging due to
intractable likelihoods. Recent works discretize the reverse sampling process
to enable GRPO-style training, yet they inherit fundamental drawbacks,
including solver restrictions, forward-reverse inconsistency, and complicated
integration with classifier-free guidance (CFG). We introduce Diffusion
Negative-aware FineTuning (DiffusionNFT), a new online RL paradigm that
optimizes diffusion models directly on the forward process via flow matching.
DiffusionNFT contrasts positive and negative generations to define an implicit
policy improvement direction, naturally incorporating reinforcement signals
into the supervised learning objective. This formulation enables training with
arbitrary black-box solvers, eliminates the need for likelihood estimation, and
requires only clean images rather than sampling trajectories for policy
optimization. DiffusionNFT is up to $25\times$ more efficient than FlowGRPO in
head-to-head comparisons, while being CFG-free. For instance, DiffusionNFT
improves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO
achieves 0.95 with over 5k steps and additional CFG employment. By leveraging
multiple reward models, DiffusionNFT significantly boosts the performance of
SD3.5-Medium in every benchmark tested.

</details>


### [279] [Network-Based Detection of Autism Spectrum Disorder Using Sustainable and Non-invasive Salivary Biomarkers](https://arxiv.org/abs/2509.16126)
*Janayna M. Fernandes,Robinson Sabino-Silva,Murillo G. Carneiro*

Main category: cs.LG

TL;DR: The paper introduces GANet, an optimized framework based on genetic algorithms and network analysis for ASD detection using salivary analysis, achieving high levels of accuracy and specificity.


<details>
  <summary>Details</summary>
Motivation: There is a need for reliable non-invasive biomarkers to enable early diagnosis of Autism Spectrum Disorder (ASD).

Method: Using ATR-FTIR spectroscopy on salivary samples paired with GANet, a genetic algorithm-based network optimization framework leveraging PageRank and Degree to extract features from spectral data.

Result: GANet performance surpasses other methods (e.g., support vector machines, linear discriminants) with 0.78 accuracy, 0.61 sensitivity, 0.90 specificity, and a 0.74 harmonic mean.

Conclusion: GANet demonstrates significant potential for non-invasive, spectral-based ASD detection and possible broader health applications.

Abstract: Autism Spectrum Disorder (ASD) lacks reliable biological markers, delaying
early diagnosis. Using 159 salivary samples analyzed by ATR-FTIR spectroscopy,
we developed GANet, a genetic algorithm-based network optimization framework
leveraging PageRank and Degree for importance-based feature characterization.
GANet systematically optimizes network structure to extract meaningful patterns
from high-dimensional spectral data. It achieved superior performance compared
to linear discriminant analysis, support vector machines, and deep learning
models, reaching 0.78 accuracy, 0.61 sensitivity, 0.90 specificity, and a 0.74
harmonic mean. These results demonstrate GANet's potential as a robust,
bio-inspired, non-invasive tool for precise ASD detection and broader
spectral-based health applications.

</details>


### [280] [Dynamic Classifier-Free Diffusion Guidance via Online Feedback](https://arxiv.org/abs/2509.16131)
*Pinelopi Papalampidi,Olivia Wiles,Ira Ktena,Aleksandar Shtedritski,Emanuele Bugliarello,Ivana Kajic,Isabela Albuquerque,Aida Nematzadeh*

Main category: cs.LG

TL;DR: Classifier-free guidance (CFG) is refined via a dynamic scheduling method to enhance text-to-image diffusion models by adapting guidance scales based on prompts.


<details>
  <summary>Details</summary>
Motivation: Static guidance scales in CFG lead to suboptimal performance in text-to-image models, as diverse prompts require varying guidance approaches.

Method: A dynamic CFG scheduling method is introduced, leveraging online feedback from evaluators like CLIP and human preference models to adjust guidance scales through greedy search at each timestep during reverse diffusion.

Result: The proposed method significantly improves text alignment, visual quality, text rendering, and reasoning capabilities. It achieves up to 53.8% human preference win-rate overall and 55.5% on text rendering-specific prompts compared to the Imagen 3 baseline.

Conclusion: Dynamic and prompt-dependent guidance scheduling is key for optimal text-to-image diffusion model performance, and the framework is both efficient and generalizable.

Abstract: Classifier-free guidance (CFG) is a cornerstone of text-to-image diffusion
models, yet its effectiveness is limited by the use of static guidance scales.
This "one-size-fits-all" approach fails to adapt to the diverse requirements of
different prompts; moreover, prior solutions like gradient-based correction or
fixed heuristic schedules introduce additional complexities and fail to
generalize. In this work, we challeng this static paradigm by introducing a
framework for dynamic CFG scheduling. Our method leverages online feedback from
a suite of general-purpose and specialized small-scale latent-space
evaluations, such as CLIP for alignment, a discriminator for fidelity and a
human preference reward model, to assess generation quality at each step of the
reverse diffusion process. Based on this feedback, we perform a greedy search
to select the optimal CFG scale for each timestep, creating a unique guidance
schedule tailored to every prompt and sample. We demonstrate the effectiveness
of our approach on both small-scale models and the state-of-the-art Imagen 3,
showing significant improvements in text alignment, visual quality, text
rendering and numerical reasoning. Notably, when compared against the default
Imagen 3 baseline, our method achieves up to 53.8% human preference win-rate
for overall preference, a figure that increases up to to 55.5% on prompts
targeting specific capabilities like text rendering. Our work establishes that
the optimal guidance schedule is inherently dynamic and prompt-dependent, and
provides an efficient and generalizable framework to achieve it.

</details>


### [281] [Spatio-temporal, multi-field deep learning of shock propagation in meso-structured media](https://arxiv.org/abs/2509.16139)
*M. Giselle Fernández-Godino,Meir H. Shachar,Kevin Korner,Jonathan L. Belof,Mukul Kumar,Jonathan Lind,William J. Schill*

Main category: cs.LG

TL;DR: The paper introduces a fast and accurate deep learning model (MSTM) for predicting shock wave dynamics in porous and architected materials relevant to applications such as asteroid deflection and fusion energy.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the difficulty of capturing complex shock wave phenomena like pore collapse and localized heating, critical for applications such as planetary defense and fusion energy.

Method: The authors present a multi-field spatio-temporal deep learning model (MSTM) that incorporates seven coupled fields and is trained on high-fidelity simulation data to predict shock wave behavior autoregressively.

Result: The model achieves speedups of about a thousand times compared to direct simulation while maintaining errors below 4% for porous materials and below 10% for lattice structures. It also preserves key integrated quantities to within 5%.

Conclusion: MSTM makes previously intractable problems feasible, providing a framework for designing materials for critical applications in planetary defense, energy, and security.

Abstract: The ability to predict how shock waves traverse porous and architected
materials is a decisive factor in planetary defense, national security, and the
race to achieve inertial fusion energy. Yet capturing pore collapse, anomalous
Hugoniot responses, and localized heating -- phenomena that can determine the
success of asteroid deflection or fusion ignition -- has remained a major
challenge despite recent advances in single-field and reduced representations.
We introduce a multi-field spatio-temporal deep learning model (MSTM) that
unifies seven coupled fields -- pressure, density, temperature, energy,
material distribution, and two velocity components -- into a single
autoregressive surrogate. Trained on high-fidelity hydrocode data, MSTM runs
about a thousand times faster than direct simulation, achieving errors below
4\% in porous materials and below 10\% in lattice structures. Unlike prior
single-field or operator-based surrogates, MSTM resolves sharp shock fronts
while preserving integrated quantities such as mass-averaged pressure and
temperature to within 5\%. This advance transforms problems once considered
intractable into tractable design studies, establishing a practical framework
for optimizing meso-structured materials in planetary impact mitigation,
inertial fusion energy, and national security.

</details>


### [282] [Automated Cyber Defense with Generalizable Graph-based Reinforcement Learning Agents](https://arxiv.org/abs/2509.16151)
*Isaiah J. King,Benjamin Bowman,H. Howie Huang*

Main category: cs.LG

TL;DR: This paper proposes a deep reinforcement learning method using attributed graphs for automated cyber defense (ACD), enabling agents to adapt to unseen network topologies and outperform state-of-the-art techniques.


<details>
  <summary>Details</summary>
Motivation: Traditional RL models for automated cyber defense fail to adapt to new network topologies due to their reliance on overfitting specific environments.

Method: ACD is framed as a context-based partially observable Markov decision problem, with observations represented using attributed graphs and relational inductive bias for reasoning.

Result: The proposed method shows significant improvement over state-of-the-art methods and allows agents to defend against unseen networks and various adversaries.

Conclusion: Representing ACD environments with attributed graphs enhances adaptability and generalization, making agents more effective in diverse and complex scenarios.

Abstract: Deep reinforcement learning (RL) is emerging as a viable strategy for
automated cyber defense (ACD). The traditional RL approach represents networks
as a list of computers in various states of safety or threat. Unfortunately,
these models are forced to overfit to specific network topologies, rendering
them ineffective when faced with even small environmental perturbations. In
this work, we frame ACD as a two-player context-based partially observable
Markov decision problem with observations represented as attributed graphs.
This approach allows our agents to reason through the lens of relational
inductive bias. Agents learn how to reason about hosts interacting with other
system entities in a more general manner, and their actions are understood as
edits to the graph representing the environment. By introducing this bias, we
will show that our agents can better reason about the states of networks and
zero-shot adapt to new ones. We show that this approach outperforms the
state-of-the-art by a wide margin, and makes our agents capable of defending
never-before-seen networks against a wide range of adversaries in a variety of
complex, and multi-agent environments.

</details>


### [283] [DIVEBATCH: Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation](https://arxiv.org/abs/2509.16173)
*Yuen Chen,Yian Wang,Hari Sundaram*

Main category: cs.LG

TL;DR: The paper introduces DiveBatch, a novel adaptive batch size SGD algorithm, to accelerate machine learning model training by dynamically adjusting batch sizes based on gradient diversity.


<details>
  <summary>Details</summary>
Motivation: Training large-scale deep neural networks is computationally expensive, and efficient methods like SGD often pose a trade-off between convergence speed, generalization, and computational efficiency.

Method: DiveBatch adapts the batch size during training using data-driven gradient diversity measures, balancing computational efficiency and generalization performance.

Result: DiveBatch achieves faster convergence speeds (1.06-5.0x improvement) on synthetic datasets and benchmarks like CiFar-10, CiFar-100, and Tiny-ImageNet, with minor trade-offs in generalization.

Conclusion: DiveBatch effectively accelerates deep network training by improving upon standard methods, offering a promising solution to large-scale training challenges with slight performance compromise.

Abstract: The goal of this paper is to accelerate the training of machine learning
models, a critical challenge since the training of large-scale deep neural
models can be computationally expensive. Stochastic gradient descent (SGD) and
its variants are widely used to train deep neural networks. In contrast to
traditional approaches that focus on tuning the learning rate, we propose a
novel adaptive batch size SGD algorithm, DiveBatch, that dynamically adjusts
the batch size. Adapting the batch size is challenging: using large batch sizes
is more efficient due to parallel computation, but small-batch training often
converges in fewer epochs and generalizes better. To address this challenge, we
introduce a data-driven adaptation based on gradient diversity, enabling
DiveBatch to maintain the generalization performance of small-batch training
while improving convergence speed and computational efficiency. Gradient
diversity has a strong theoretical justification: it emerges from the
convergence analysis of SGD. Evaluations of DiveBatch on synthetic and
CiFar-10, CiFar-100, and Tiny-ImageNet demonstrate that DiveBatch converges
significantly faster than standard SGD and AdaBatch (1.06 -- 5.0x), with a
slight trade-off in performance.

</details>


### [284] [Inverting Trojans in LLMs](https://arxiv.org/abs/2509.16203)
*Zhengxing Li,Guangmingmei Yang,Jayaram Raghuram,David J. Miller,George Kesidis*

Main category: cs.LG

TL;DR: The paper addresses challenges in detecting backdoor triggers in language models due to their discrete input space and proposes a new method involving greedy discrete search, implicit token blacklisting, and confidence-based misclassification detection.


<details>
  <summary>Details</summary>
Motivation: Challenges exist in adapting backdoor detection methods, developed for image-based AI systems, to handle language models (LLMs) due to the discrete input space and a vast number of token combinations.

Method: The authors present a three-step approach: (i) a greedy discrete token search, (ii) implicit blacklisting using cosine similarity in activation spaces, and (iii) backdoor detection by analyzing high-confidence misclassifications.

Result: The proposed method detects and successfully identifies backdoor triggers in language models, demonstrating reliability in addressing these threats.

Conclusion: The approach effectively overcomes the limitations of existing methods to handle the discrete and complex nature of LLMs' input spaces, enabling robust backdoor inversion and detection.

Abstract: While effective backdoor detection and inversion schemes have been developed
for AIs used e.g. for images, there are challenges in "porting" these methods
to LLMs. First, the LLM input space is discrete, which precludes gradient-based
search over this space, central to many backdoor inversion methods. Second,
there are ~30,000^k k-tuples to consider, k the token-length of a putative
trigger. Third, for LLMs there is the need to blacklist tokens that have strong
marginal associations with the putative target response (class) of an attack,
as such tokens give false detection signals. However, good blacklists may not
exist for some domains. We propose a LLM trigger inversion approach with three
key components: i) discrete search, with putative triggers greedily accreted,
starting from a select list of singletons; ii) implicit blacklisting, achieved
by evaluating the average cosine similarity, in activation space, between a
candidate trigger and a small clean set of samples from the putative target
class; iii) detection when a candidate trigger elicits high misclassifications,
and with unusually high decision confidence. Unlike many recent works, we
demonstrate that our approach reliably detects and successfully inverts
ground-truth backdoor trigger phrases.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [285] [Automatic layout of railroad diagrams](https://arxiv.org/abs/2509.15834)
*Shardul Chiplunkar,Clément Pit-Claudel*

Main category: cs.PL

TL;DR: This paper introduces a formal approach and practical implementation for creating railroad diagrams, solving layout challenges through a principled compiler.


<details>
  <summary>Details</summary>
Motivation: Existing railroad diagrams are mostly hand-drawn because of limited tools and lack of formal understanding of layout processes.

Method: The authors design a compiler to translate a diagram language to a layout language, incorporating optimization techniques like line wrapping, alignment, and justification.

Result: The solution enables automated conversion of grammars into layouts, showing practical results comparable to manual and other tool-created diagrams.

Conclusion: Their approach offers a structured and effective solution for generating railroad diagrams with automation and precision, making the process more accessible and scalable.

Abstract: Railroad diagrams (also called "syntax diagrams") are a common, intuitive
visualization of grammars, but limited tooling and a lack of formal attention
to their layout mostly confines them to hand-drawn documentation. We present
the first formal treatment of railroad diagram layout along with a principled,
practical implementation. We characterize the problem as compiling a *diagram
language* (specifying conceptual components and how they connect and compose)
to a *layout language* (specifying basic graphical shapes and their sizes and
positions). We then implement a compiler that performs *line wrapping* to meet
a target width, as well as vertical *alignment* and horizontal *justification*
per user-specified policies. We frame line wrapping as an optimization problem,
where we describe principled dimensions of optimality and implement
corresponding heuristics. For front-end evaluation, we show that our diagram
language is well-suited for common applications by describing how regular
expressions and Backus-Naur form can be compiled to it. For back-end
evaluation, we argue that our compiler is practical by comparing its output to
diagrams laid out by hand and by other tools.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [286] [DIPP: Discriminative Impact Point Predictor for Catching Diverse In-Flight Objects](https://arxiv.org/abs/2509.15254)
*Ngoc Huy Nguyen,Kazuki Shibata,Takamitsu Matsubara*

Main category: cs.RO

TL;DR: The paper introduces a new dataset and a method for in-flight object catching using a quadruped robot, focusing on accurate early-stage impact point prediction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of reliably predicting impact points for diverse objects under unsteady aerodynamics and to improve early-stage trajectory prediction for robotic catching.

Method: The method involves creating a dataset of 8,000 trajectories from 20 objects, and developing the Discriminative Impact Point Predictor (DIPP) with two modules: Discriminative Feature Embedding (DFE) and an Impact Point Predictor (IPP) with two variants.

Result: Experimental results indicate the proposed dataset is richer than existing ones and that the DIPP method outperforms baselines on seen and unseen objects, both in simulation and real-world experiments.

Conclusion: The study successfully improves impact point prediction and robotic catching accuracy, and demonstrates the utility of the proposed dataset and DIPP method in diverse aerodynamic conditions.

Abstract: In this study, we address the problem of in-flight object catching using a
quadruped robot with a basket. Our objective is to accurately predict the
impact point, defined as the object's landing position. This task poses two key
challenges: the absence of public datasets capturing diverse objects under
unsteady aerodynamics, which are essential for training reliable predictors;
and the difficulty of accurate early-stage impact point prediction when
trajectories appear similar across objects. To overcome these issues, we
construct a real-world dataset of 8,000 trajectories from 20 objects, providing
a foundation for advancing in-flight object catching under complex
aerodynamics. We then propose the Discriminative Impact Point Predictor (DIPP),
consisting of two modules: (i) a Discriminative Feature Embedding (DFE) that
separates trajectories by dynamics to enable early-stage discrimination and
generalization, and (ii) an Impact Point Predictor (IPP) that estimates the
impact point from these features. Two IPP variants are implemented: an Neural
Acceleration Estimator (NAE)-based method that predicts trajectories and
derives the impact point, and a Direct Point Estimator (DPE)-based method that
directly outputs it. Experimental results show that our dataset is more diverse
and complex than existing dataset, and that our method outperforms baselines on
both 15 seen and 5 unseen objects. Furthermore, we show that improved
early-stage prediction enhances catching success in simulation and demonstrate
the effectiveness of our approach through real-world experiments. The
demonstration is available at
https://sites.google.com/view/robot-catching-2025.

</details>


### [287] [GiAnt: A Bio-Inspired Hexapod for Adaptive Terrain Navigation and Object Detection](https://arxiv.org/abs/2509.15264)
*Aasfee Mosharraf Bhuiyan,Md Luban Mehda,Md. Thawhid Hasan Puspo,Jubayer Amin Pritom*

Main category: cs.RO

TL;DR: The paper introduces GiAnt, a lightweight hexapod robot inspired by ants, designed for affordability and adaptability to varied terrains using efficient locomotion and advanced features.


<details>
  <summary>Details</summary>
Motivation: The study seeks to create a cost-effective hexapod robot inspired by ants' adaptability to diverse terrains, providing outdoor applications with efficient energy use and superior terrain navigation.

Method: The robot is designed with a lightweight structure using 3D printing, laser cutting, and Single Degree of Freedom legs. It employs Arduino-based control and gait analysis for motion, integrating machine learning and image processing for object recognition.

Result: GiAnt successfully navigates uneven terrains like grass, rocks, and steep surfaces. It overcomes heights up to 8 cm and identifies 81 objects in live monitoring using advanced machine learning capabilities.

Conclusion: GiAnt demonstrates the potential for affordable hexapod robots in applications like research, exploration, and surveying, leveraging bio-inspired design for adaptability, efficiency, and ease of control.

Abstract: This paper presents the design, development and testing of GiAnt, an
affordable hexapod which is inspired by the efficient motions of ants. The
decision to model GiAnt after ants rather than other insects is rooted in ants'
natural adaptability to a variety of terrains. This bio-inspired approach gives
it a significant advantage in outdoor applications, offering terrain
flexibility along with efficient energy use. It features a lightweight
3D-printed and laser cut structure weighing 1.75 kg with dimensions of 310 mm x
200 mm x 120 mm. Its legs have been designed with a simple Single Degree of
Freedom (DOF) using a link and crank mechanism. It is great for conquering
challenging terrains such as grass, rocks, and steep surfaces. Unlike
traditional robots using four wheels for motion, its legged design gives
superior adaptability to uneven and rough surfaces. GiAnt's control system is
built on Arduino, allowing manual operation. An effective way of controlling
the legs of GiAnt was achieved by gait analysis. It can move up to 8 cm of
height easily with its advanced leg positioning system. Furthermore, equipped
with machine learning and image processing technology, it can identify 81
different objects in a live monitoring system. It represents a significant step
towards creating accessible hexapod robots for research, exploration, and
surveying, offering unique advantages in adaptability and control simplicity.

</details>


### [288] [Embodied Arena: A Comprehensive, Unified, and Evolving Evaluation Platform for Embodied AI](https://arxiv.org/abs/2509.15273)
*Fei Ni,Min Zhang,Pengyi Li,Yifu Yuan,Lingfeng Zhang,Yuecheng Liu,Peilong Han,Longxin Kou,Shaojin Ma,Jinbin Qiao,David Gamaliel Arcos Bravo,Yuening Wang,Xiao Hu,Zhanguang Zhang,Xianze Yao,Yutong Li,Zhao Zhang,Ying Wen,Ying-Cong Chen,Xiaodan Liang,Liang Lin,Bin He,Haitham Bou-Ammar,He Wang,Huazhe Xu,Jiankang Deng,Shan Luo,Shuqiang Jiang,Wei Pan,Yang Gao,Stefanos Zafeiriou,Jan Peters,Yuzheng Zhuang,Yingxue Zhang,Yan Zheng,Hongyao Tang,Jianye Hao*

Main category: cs.RO

TL;DR: Embodied Arena addresses key challenges in Embodied AI—lack of systematic understanding, evaluation standardization, and scalable data acquisition—by introducing a taxonomy, flexible benchmark integration, and automated data generation for unified assessment.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome three fundamental barriers in Embodied AI, including lack of clarity in research objectives, absence of standardized evaluations, and difficulty acquiring extensive embodied data.

Method: The authors propose the Embodied Arena, which includes a taxonomy for structuring capabilities, an evaluation system integrating 22 benchmarks, and an LLM-driven pipeline for scalable data generation.

Result: Embodied Arena integrates benchmarks from diverse domains, publishes real-time leaderboards, and highlights nine key findings to promote clarity and progression in Embodied AI research.

Conclusion: The platform provides a unified system for evaluation while enhancing research focus and scalability, accelerating advancements in Embodied AI development.

Abstract: Embodied AI development significantly lags behind large foundation models due
to three critical challenges: (1) lack of systematic understanding of core
capabilities needed for Embodied AI, making research lack clear objectives; (2)
absence of unified and standardized evaluation systems, rendering
cross-benchmark evaluation infeasible; and (3) underdeveloped automated and
scalable acquisition methods for embodied data, creating critical bottlenecks
for model scaling. To address these obstacles, we present Embodied Arena, a
comprehensive, unified, and evolving evaluation platform for Embodied AI. Our
platform establishes a systematic embodied capability taxonomy spanning three
levels (perception, reasoning, task execution), seven core capabilities, and 25
fine-grained dimensions, enabling unified evaluation with systematic research
objectives. We introduce a standardized evaluation system built upon unified
infrastructure supporting flexible integration of 22 diverse benchmarks across
three domains (2D/3D Embodied Q&A, Navigation, Task Planning) and 30+ advanced
models from 20+ worldwide institutes. Additionally, we develop a novel
LLM-driven automated generation pipeline ensuring scalable embodied evaluation
data with continuous evolution for diversity and comprehensiveness. Embodied
Arena publishes three real-time leaderboards (Embodied Q&A, Navigation, Task
Planning) with dual perspectives (benchmark view and capability view),
providing comprehensive overviews of advanced model capabilities. Especially,
we present nine findings summarized from the evaluation results on the
leaderboards of Embodied Arena. This helps to establish clear research veins
and pinpoint critical research problems, thereby driving forward progress in
the field of Embodied AI.

</details>


### [289] [Measurement and Potential Field-Based Patient Modeling for Model-Mediated Tele-ultrasound](https://arxiv.org/abs/2509.15325)
*Ryan S. Yeung,David G. Black,Septimiu E. Salcudean*

Main category: cs.RO

TL;DR: The paper introduces an improved method for teleoperated ultrasound by updating a potential field model with measured positions and forces to enhance accuracy in force rendering, which is crucial for optimizing image quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve accessibility to diagnostic medical imaging in remote areas by addressing the limitations of direct force feedback caused by communication delays in teleoperation.

Method: The authors propose a voxelized model of the patient combined with internal potential field representation, updated using a convex quadratic equation incorporating both spatial Laplace operator and measured forces.

Result: The method was tested on three volunteer patients, showing a significant reduction in force magnitude error (average 7.23 N) and force vector angle error (average 9.37°) compared to previous methods.

Conclusion: The findings demonstrate that updating the potential field model with measured forces enhances the accuracy and transparency of the tele-ultrasound system, which could improve remote medical diagnostics fundamentally.

Abstract: Teleoperated ultrasound can improve diagnostic medical imaging access for
remote communities. Having accurate force feedback is important for enabling
sonographers to apply the appropriate probe contact force to optimize
ultrasound image quality. However, large time delays in communication make
direct force feedback impractical. Prior work investigated using point
cloud-based model-mediated teleoperation and internal potential field models to
estimate contact forces and torques. We expand on this by introducing a method
to update the internal potential field model of the patient with measured
positions and forces for more transparent model-mediated tele-ultrasound. We
first generate a point cloud model of the patient's surface and transmit this
to the sonographer in a compact data structure. This is converted to a static
voxelized volume where each voxel contains a potential field value. These
values determine the forces and torques, which are rendered based on overlap
between the voxelized volume and a point shell model of the ultrasound
transducer. We solve for the potential field using a convex quadratic that
combines the spatial Laplace operator with measured forces. This was evaluated
on volunteer patients ($n=3$) by computing the accuracy of rendered forces.
Results showed the addition of measured forces to the model reduced the force
magnitude error by an average of 7.23 N and force vector angle error by an
average of 9.37$^{\circ}$ compared to using only Laplace's equation.

</details>


### [290] [Trust-Aware Embodied Bayesian Persuasion for Mixed-Autonomy](https://arxiv.org/abs/2509.15404)
*Shaoting Peng,Katherine Driggs-Campbell,Roy Dong*

Main category: cs.RO

TL;DR: This paper introduces the Trust-Aware Embodied Bayesian Persuasion (TA-EBP) framework, which enhances the safety and efficiency of autonomous vehicles (AVs) interacting with human-driven vehicles (HVs) by leveraging trust-based Bayesian persuasion.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of ensuring safe and efficient interaction between AVs and HVs, especially focusing on the shortcomings of game-theoretic models that lead to trust erosion and risky human behavior.

Method: The authors propose a TA-EBP framework that incorporates Bayesian persuasion to model interaction at traffic intersections. It introduces a trust parameter, physically meaningful action grounding, and derives theorems for minimum trust levels and optimal signal magnitudes.

Result: In mixed-autonomy traffic simulation tests, TA-EBP significantly improved HV behavior, eliminating collisions and enhancing traffic flow, outperforming baselines lacking trust awareness or communication.

Conclusion: The study offers a transparent, trust-aware framework for AV-HV interaction, which fosters trust, reduces risky behavior, and improves both safety and traffic management in mixed-autonomy scenarios.

Abstract: Safe and efficient interaction between autonomous vehicles (AVs) and
human-driven vehicles (HVs) is a critical challenge for future transportation
systems. While game-theoretic models capture how AVs influence HVs, they often
suffer from a long-term decay of influence and can be perceived as
manipulative, eroding the human's trust. This can paradoxically lead to riskier
human driving behavior over repeated interactions. In this paper, we address
this challenge by proposing the Trust-Aware Embodied Bayesian Persuasion
(TA-EBP) framework. Our work makes three key contributions: First, we apply
Bayesian persuasion to model communication at traffic intersections, offering a
transparent alternative to traditional game-theoretic models. Second, we
introduce a trust parameter to the persuasion framework, deriving a theorem for
the minimum trust level required for influence. Finally, we ground the abstract
signals of Bayesian persuasion theory into a continuous, physically meaningful
action space, deriving a second theorem for the optimal signal magnitude,
realized as an AV's forward nudge. Additionally, we validate our framework in a
mixed-autonomy traffic simulation, demonstrating that TA-EBP successfully
persuades HVs to drive more cautiously, eliminating collisions and improving
traffic flow compared to baselines that either ignore trust or lack
communication. Our work provides a transparent and non-strategic framework for
influence in human-robot interaction, enhancing both safety and efficiency.

</details>


### [291] [Sym2Real: Symbolic Dynamics with Residual Learning for Data-Efficient Adaptive Control](https://arxiv.org/abs/2509.15412)
*Easop Lee,Samuel A. Moore,Boyuan Chen*

Main category: cs.RO

TL;DR: Sym2Real introduces a data-driven framework enabling robust control of robots using minimal training data, combining symbolic regression with real-world residual learning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to achieve robust robotic control in real-world settings using minimal data while overcoming challenges of noise sensitivity and model degradation.

Method: It combines low-fidelity simulation data with real-world residual learning, leveraging shared underlying physics for adaptive robotics control.

Result: Experimental validation showcases efficient adaptation across different simulation scenarios and successful transfer to various real-world conditions.

Conclusion: The approach demonstrates data-efficient training and robust control, making it a promising strategy for real-world robotics applications.

Abstract: We present Sym2Real, a fully data-driven framework that provides a principled
way to train low-level adaptive controllers in a highly data-efficient manner.
Using only about 10 trajectories, we achieve robust control of both a quadrotor
and a racecar in the real world, without expert knowledge or simulation tuning.
Our approach achieves this data efficiency by bringing symbolic regression to
real-world robotics while addressing key challenges that prevent its direct
application, including noise sensitivity and model degradation that lead to
unsafe control. Our key observation is that the underlying physics is often
shared for a system regardless of internal or external changes. Hence, we
strategically combine low-fidelity simulation data with targeted real-world
residual learning. Through experimental validation on quadrotor and racecar
platforms, we demonstrate consistent data-efficient adaptation across six
out-of-distribution sim2sim scenarios and successful sim2real transfer across
five real-world conditions. More information and videos can be found at at
http://generalroboticslab.com/Sym2Real

</details>


### [292] [Online Slip Detection and Friction Coefficient Estimation for Autonomous Racing](https://arxiv.org/abs/2509.15423)
*Christopher Oeltjen,Carson Sobolewski,Saleh Faghfoorian,Lorant Domokos,Giancarlo Vidal,Ivan Ruchkin*

Main category: cs.RO

TL;DR: This paper introduces a simple and efficient method for real-time tire-road friction coefficient (TRFC) estimation using only IMU, LiDAR, and control actions without requiring complex models or training data.


<details>
  <summary>Details</summary>
Motivation: To improve vehicle safety, stability, and performance in scenarios like autonomous racing, where understanding the tire-road friction coefficient is critical but challenging to measure directly with standard sensors.

Method: The authors propose an online method for detecting slip events and estimating TRFC using data from IMU, LiDAR, and control commands. Slip is identified in real time by comparing commanded and actual motion, and TRFC is calculated from observed accelerations during no-slip conditions, without relying on dedicated dynamic/tire models or extensive datasets.

Result: The method achieved accurate and consistent TRFC estimations in experiments using a 1:10 scale autonomous racing car across various friction conditions. The estimated values closely matched ground-truth data.

Conclusion: The findings suggest that this approach is practical, deployable, and computationally efficient for real-time TRFC estimation and slip monitoring in autonomous driving applications.

Abstract: Accurate knowledge of the tire-road friction coefficient (TRFC) is essential
for vehicle safety, stability, and performance, especially in autonomous
racing, where vehicles often operate at the friction limit. However, TRFC
cannot be directly measured with standard sensors, and existing estimation
methods either depend on vehicle or tire models with uncertain parameters or
require large training datasets. In this paper, we present a lightweight
approach for online slip detection and TRFC estimation. Our approach relies
solely on IMU and LiDAR measurements and the control actions, without special
dynamical or tire models, parameter identification, or training data. Slip
events are detected in real time by comparing commanded and measured motions,
and the TRFC is then estimated directly from observed accelerations under
no-slip conditions. Experiments with a 1:10-scale autonomous racing car across
different friction levels demonstrate that the proposed approach achieves
accurate and consistent slip detections and friction coefficients, with results
closely matching ground-truth measurements. These findings highlight the
potential of our simple, deployable, and computationally efficient approach for
real-time slip monitoring and friction coefficient estimation in autonomous
driving.

</details>


### [293] [Implicit Kinodynamic Motion Retargeting for Human-to-humanoid Imitation Learning](https://arxiv.org/abs/2509.15443)
*Xingyu Chen,Hanyu Wu,Sikai Wu,Mingliang Zhou,Diyun Xiang,Haodong Zhang*

Main category: cs.RO

TL;DR: This paper introduces Implicit Kinodynamic Motion Retargeting (IKMR), a novel framework for efficiently converting large-scale human motion into robot-executable motion.


<details>
  <summary>Details</summary>
Motivation: Existing methods for motion retargeting rely on frame-by-frame techniques, which are not scalable for large-scale human motion data.

Method: IKMR combines pretraining of motion topology features and dual encoder-decoder architecture for kinematics, and integrates imitation learning with the retargeting network for dynamically feasible trajectories.

Result: IKMR achieves real-time large-scale motion retargeting, enabling a whole-body controller to be trained and deployed on humanoid robots in simulations and real-world scenarios.

Conclusion: Extensive experiments demonstrate that IKMR is effective and scalable, allowing humanoid robots to replicate human motion more efficiently.

Abstract: Human-to-humanoid imitation learning aims to learn a humanoid whole-body
controller from human motion. Motion retargeting is a crucial step in enabling
robots to acquire reference trajectories when exploring locomotion skills.
However, current methods focus on motion retargeting frame by frame, which
lacks scalability. Could we directly convert large-scale human motion into
robot-executable motion through a more efficient approach? To address this
issue, we propose Implicit Kinodynamic Motion Retargeting (IKMR), a novel
efficient and scalable retargeting framework that considers both kinematics and
dynamics. In kinematics, IKMR pretrains motion topology feature representation
and a dual encoder-decoder architecture to learn a motion domain mapping. In
dynamics, IKMR integrates imitation learning with the motion retargeting
network to refine motion into physically feasible trajectories. After
fine-tuning using the tracking results, IKMR can achieve large-scale physically
feasible motion retargeting in real time, and a whole-body controller could be
directly trained and deployed for tracking its retargeted trajectories. We
conduct our experiments both in the simulator and the real robot on a full-size
humanoid robot. Extensive experiments and evaluation results verify the
effectiveness of our proposed framework.

</details>


### [294] [Explainable AI-Enhanced Supervisory Control for Robust Multi-Agent Robotic Systems](https://arxiv.org/abs/2509.15491)
*Reza Pirayeshshirazinezhad,Nima Fathi*

Main category: cs.RO

TL;DR: The paper introduces an AI-enhanced control framework for multi-agent robotics that ensures safe mode switching, robust control, and interpretability across diverse environments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a safe and interpretable control framework suitable for multi-agent robotics operating in challenging and resource-constrained environments.

Method: The method integrates a timed-automata supervisor, Lyapunov-based and sliding-mode controllers, and an explainable predictor. Monte Carlo optimization is used for training.

Result: The approach was validated in spacecraft formation flying and autonomous underwater vehicle tasks, showing significant improvements in tracking error and energy efficiency compared to baselines.

Conclusion: The framework is portable, interpretable, and effective for safety-critical multi-agent robotic applications in diverse physical environments.

Abstract: We present an explainable AI-enhanced supervisory control framework for
multi-agent robotics that combines (i) a timed-automata supervisor for safe,
auditable mode switching, (ii) robust continuous control (Lyapunov-based
controller for large-angle maneuver; sliding-mode controller (SMC) with
boundary layers for precision and disturbance rejection), and (iii) an
explainable predictor that maps mission context to gains and expected
performance (energy, error). Monte Carlo-driven optimization provides the
training data, enabling transparent real-time trade-offs.
  We validated the approach in two contrasting domains, spacecraft formation
flying and autonomous underwater vehicles (AUVs). Despite different
environments (gravity/actuator bias vs. hydrodynamic drag/currents), both share
uncertain six degrees of freedom (6-DOF) rigid-body dynamics, relative motion,
and tight tracking needs, making them representative of general robotic
systems. In the space mission, the supervisory logic selects parameters that
meet mission criteria. In AUV leader-follower tests, the same SMC structure
maintains a fixed offset under stochastic currents with bounded steady error.
In spacecraft validation, the SMC controller achieved submillimeter alignment
with 21.7% lower tracking error and 81.4% lower energy consumption compared to
Proportional-Derivative PD controller baselines. At the same time, in AUV
tests, SMC maintained bounded errors under stochastic currents. These results
highlight both the portability and the interpretability of the approach for
safety-critical, resource-constrained multi-agent robotics.

</details>


### [295] [STARC: See-Through-Wall Augmented Reality Framework for Human-Robot Collaboration in Emergency Response](https://arxiv.org/abs/2509.15507)
*Shenghai Yuan,Weixiang Guo,Tianxin Hu,Yu Yang,Jinyu Chen,Rui Qian,Zhongyuan Liu,Lihua Xie*

Main category: cs.RO

TL;DR: This paper proposes STARC, a framework using LiDAR-equipped robots and responders to visualize hidden occupants and hazards in real time through see-through augmented reality.


<details>
  <summary>Details</summary>
Motivation: Emergency responders face challenges with occluded environments that obscure hazards and victims, necessitating improved situational awareness tools.

Method: STARC integrates ground robot LiDAR mapping with responder-mounted LiDAR, enabling cross-LiDAR alignment for real-time AR visualization of detected humans and hazards.

Result: Tests in simulations, labs, and field trials demonstrate accurate pose alignment, human detections, and stable overlays, affirming system reliability and robustness.

Conclusion: STARC has potential to enhance situational awareness and safety in emergency missions and will be open-sourced for broader use.

Abstract: In emergency response missions, first responders must navigate cluttered
indoor environments where occlusions block direct line-of-sight, concealing
both life-threatening hazards and victims in need of rescue. We present STARC,
a see-through AR framework for human-robot collaboration that fuses
mobile-robot mapping with responder-mounted LiDAR sensing. A ground robot
running LiDAR-inertial odometry performs large-area exploration and 3D human
detection, while helmet- or handheld-mounted LiDAR on the responder is
registered to the robot's global map via relative pose estimation. This
cross-LiDAR alignment enables consistent first-person projection of detected
humans and their point clouds - rendered in AR with low latency - into the
responder's view. By providing real-time visualization of hidden occupants and
hazards, STARC enhances situational awareness and reduces operator risk.
Experiments in simulation, lab setups, and tactical field trials confirm robust
pose alignment, reliable detections, and stable overlays, underscoring the
potential of our system for fire-fighting, disaster relief, and other
safety-critical operations. Code and design will be open-sourced upon
acceptance.

</details>


### [296] [Distribution Estimation for Global Data Association via Approximate Bayesian Inference](https://arxiv.org/abs/2509.15565)
*Yixuan Jia,Mason B. Peterson,Qingyuan Li,Yulun Tian,Jonathan P. How*

Main category: cs.RO

TL;DR: This paper introduces a Bayesian framework for ambiguous global data association in robotics, utilizing particles to represent multiple solution modes and leveraging GPU-parallelized optimization.


<details>
  <summary>Details</summary>
Motivation: Existing methods for global data association struggle in ambiguous scenarios due to reliance on single-solution approaches, which fail in highly multimodal solution spaces.

Method: The authors propose a particle-based framework for approximate Bayesian inference that evolves according to deterministic or randomized rules to explore multiple solution modes, integrated with optimization constraints and GPU-parallelization.

Result: The approach effectively captures multimodal distributions in data association problems, achieving accurate registration for point clouds and object maps in simulations and real-world tests.

Conclusion: This framework provides a robust alternative to single-solution approaches, addressing ambiguity in global data association and offering computational efficiency with GPU support.

Abstract: Global data association is an essential prerequisite for robot operation in
environments seen at different times or by different robots. Repetitive or
symmetric data creates significant challenges for existing methods, which
typically rely on maximum likelihood estimation or maximum consensus to produce
a single set of associations. However, in ambiguous scenarios, the distribution
of solutions to global data association problems is often highly multimodal,
and such single-solution approaches frequently fail. In this work, we introduce
a data association framework that leverages approximate Bayesian inference to
capture multiple solution modes to the data association problem, thereby
avoiding premature commitment to a single solution under ambiguity. Our
approach represents hypothetical solutions as particles that evolve according
to a deterministic or randomized update rule to cover the modes of the
underlying solution distribution. Furthermore, we show that our method can
incorporate optimization constraints imposed by the data association
formulation and directly benefit from GPU-parallelized optimization. Extensive
simulated and real-world experiments with highly ambiguous data show that our
method correctly estimates the distribution over transformations when
registering point clouds or object maps.

</details>


### [297] [Momentum-constrained Hybrid Heuristic Trajectory Optimization Framework with Residual-enhanced DRL for Visually Impaired Scenarios](https://arxiv.org/abs/2509.15582)
*Yuting Zeng,Zhiwen Zheng,You Zhou,JiaLing Xiao,Yongbin Yu,Manping Fan,Bo Gong,Liyong Ren*

Main category: cs.RO

TL;DR: The paper introduces a hybrid heuristic trajectory optimization framework (MHHTOF) for assistive navigation in visually impaired scenarios, leveraging trajectory sampling, optimization, and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Assistive navigation tasks for visually impaired individuals require robust, safe, and real-time trajectory planning due to their complexity and diverse challenges.

Method: The method combines heuristic trajectory sampling in the Frenet coordinate system, momentum-constrained trajectory optimization, and residual-enhanced actor-critic networks with LSTM-based temporal modeling for adaptive refinement.

Result: The proposed LSTM-ResB-PPO achieves faster training convergence, better reward outcomes, and reduced average cost, cost variance, and risks compared to baseline methods.

Conclusion: The framework successfully demonstrates enhanced robustness, safety, and feasibility for visually impaired assistive planning tasks.

Abstract: This paper proposes a momentum-constrained hybrid heuristic trajectory
optimization framework (MHHTOF) tailored for assistive navigation in visually
impaired scenarios, integrating trajectory sampling generation, optimization
and evaluation with residual-enhanced deep reinforcement learning (DRL). In the
first stage, heuristic trajectory sampling cluster (HTSC) is generated in the
Frenet coordinate system using third-order interpolation with fifth-order
polynomials and momentum-constrained trajectory optimization (MTO) constraints
to ensure smoothness and feasibility. After first stage cost evaluation, the
second stage leverages a residual-enhanced actor-critic network with LSTM-based
temporal feature modeling to adaptively refine trajectory selection in the
Cartesian coordinate system. A dual-stage cost modeling mechanism (DCMM) with
weight transfer aligns semantic priorities across stages, supporting
human-centered optimization. Experimental results demonstrate that the proposed
LSTM-ResB-PPO achieves significantly faster convergence, attaining stable
policy performance in approximately half the training iterations required by
the PPO baseline, while simultaneously enhancing both reward outcomes and
training stability. Compared to baseline method, the selected model reduces
average cost and cost variance by 30.3% and 53.3%, and lowers ego and obstacle
risks by over 77%. These findings validate the framework's effectiveness in
enhancing robustness, safety, and real-time feasibility in complex assistive
planning tasks.

</details>


### [298] [Bench-RNR: Dataset for Benchmarking Repetitive and Non-repetitive Scanning LiDAR for Infrastructure-based Vehicle Localization](https://arxiv.org/abs/2509.15583)
*Runxin Zhao,Chunxiang Wang,Hanyang Zhuang,Ming Yang*

Main category: cs.RO

TL;DR: The paper introduces a dataset for infrastructure-based vehicle localization using repetitive and non-repetitive scanning LiDARs, aiming to benchmark their performance and provide insights for selecting optimal LiDAR scanning patterns.


<details>
  <summary>Details</summary>
Motivation: Current research largely relies on repetitive scanning LiDARs despite the advantages of non-repetitive scanning LiDARs, such as reduced costs and no blind zones, for infrastructure-based vehicle localization.

Method: The authors created a dataset with 5,445 frames of point clouds from eight diverse vehicle trajectory sequences utilizing both types of LiDAR scanning systems to compare their efficiency in localization through benchmark experiments.

Result: Baseline experiments demonstrated the effectiveness of both LiDAR types in infrastructure-based vehicle localization while identifying performance differences between repetitive and non-repetitive scanning patterns.

Conclusion: The dataset, combined with the analyses, serves as a significant resource for advancing infrastructure-based perception and vehicle localization, enabling more informed choices in LiDAR scanning technologies.

Abstract: Vehicle localization using roadside LiDARs can provide centimeter-level
accuracy for cloud-controlled vehicles while simultaneously serving multiple
vehicles, enhanc-ing safety and efficiency. While most existing studies rely on
repetitive scanning LiDARs, non-repetitive scanning LiDAR offers advantages
such as eliminating blind zones and being more cost-effective. However, its
application in roadside perception and localization remains limited. To address
this, we present a dataset for infrastructure-based vehicle localization, with
data collected from both repetitive and non-repetitive scanning LiDARs, in
order to benchmark the performance of different LiDAR scanning patterns. The
dataset contains 5,445 frames of point clouds across eight vehicle trajectory
sequences, with diverse trajectory types. Our experiments establish base-lines
for infrastructure-based vehicle localization and compare the performance of
these methods using both non-repetitive and repetitive scanning LiDARs. This
work offers valuable insights for selecting the most suitable LiDAR scanning
pattern for infrastruc-ture-based vehicle localization. Our dataset is a
signifi-cant contribution to the scientific community, supporting advancements
in infrastructure-based perception and vehicle localization. The dataset and
source code are publicly available at:
https://github.com/sjtu-cyberc3/BenchRNR.

</details>


### [299] [Distributed Nash Equilibrium Seeking Algorithm in Aggregative Games for Heterogeneous Multi-Robot Systems](https://arxiv.org/abs/2509.15597)
*Yi Dong,Zhongguo Li,Sarvapali D. Ramchurn,Xiaowei Huang*

Main category: cs.RO

TL;DR: This paper introduces a distributed algorithm for multi-robot systems to achieve Nash equilibrium efficiently using shared information among neighboring robots.


<details>
  <summary>Details</summary>
Motivation: To design and validate a distributed approach for heterogeneous multi-robot systems to reach Nash equilibrium in aggregative games.

Method: Proposes a distributed optimization algorithm combined with output control laws for robots to calculate tailored references and track them.

Result: The algorithm is proven to converge and achieve efficient outcomes, validated by simulations and real-world robot experiments.

Conclusion: The approach is effective in ensuring heterogeneous multi-robot systems can collaboratively achieve Nash equilibrium with practical verification.

Abstract: This paper develops a distributed Nash Equilibrium seeking algorithm for
heterogeneous multi-robot systems. The algorithm utilises distributed
optimisation and output control to achieve the Nash equilibrium by leveraging
information shared among neighbouring robots. Specifically, we propose a
distributed optimisation algorithm that calculates the Nash equilibrium as a
tailored reference for each robot and designs output control laws for
heterogeneous multi-robot systems to track it in an aggregative game. We prove
that our algorithm is guaranteed to converge and result in efficient outcomes.
The effectiveness of our approach is demonstrated through numerical simulations
and empirical testing with physical robots.

</details>


### [300] [ORB: Operating Room Bot, Automating Operating Room Logistics through Mobile Manipulation](https://arxiv.org/abs/2509.15600)
*Jinkai Qiu,Yungjun Kim,Gaurav Sethia,Tanmay Agarwal,Siddharth Ghodasara,Zackory Erickson,Jeffrey Ichnowski*

Main category: cs.RO

TL;DR: This paper introduces the Operating Room Bot (ORB), a robotic framework aimed at automating item-level logistics tasks in hospital operating rooms using advanced software architecture for perception and motion planning.


<details>
  <summary>Details</summary>
Motivation: Address the unique challenges of automating logistics tasks in hospital operating rooms, where precision, efficiency, and sterility are critical.

Method: ORB utilizes a hierarchical behavior tree architecture integrating modules for object recognition, scene interpretation, and GPU-accelerated trajectory planning, employing technologies like YOLOv7 and cuRobo.

Result: ORB achieves an 80% success rate for OR supply retrieval and a 96% success rate for restocking operations, demonstrating its reliability.

Conclusion: ORB's success showcases its potential as an adaptable solution for automating crucial logistics tasks in hospital operating rooms, enhancing efficiency and reliability.

Abstract: Efficiently delivering items to an ongoing surgery in a hospital operating
room can be a matter of life or death. In modern hospital settings, delivery
robots have successfully transported bulk items between rooms and floors.
However, automating item-level operating room logistics presents unique
challenges in perception, efficiency, and maintaining sterility. We propose the
Operating Room Bot (ORB), a robot framework to automate logistics tasks in
hospital operating rooms (OR). ORB leverages a robust, hierarchical behavior
tree (BT) architecture to integrate diverse functionalities of object
recognition, scene interpretation, and GPU-accelerated motion planning. The
contributions of this paper include: (1) a modular software architecture
facilitating robust mobile manipulation through behavior trees; (2) a novel
real-time object recognition pipeline integrating YOLOv7, Segment Anything
Model 2 (SAM2), and Grounded DINO; (3) the adaptation of the cuRobo
parallelized trajectory optimization framework to real-time, collision-free
mobile manipulation; and (4) empirical validation demonstrating an 80% success
rate in OR supply retrieval and a 96% success rate in restocking operations.
These contributions establish ORB as a reliable and adaptable system for
autonomous OR logistics.

</details>


### [301] [PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models](https://arxiv.org/abs/2509.15607)
*Ruiqi Wang,Dezhong Zhao,Ziqin Yuan,Tianyu Shao,Guohua Chen,Dominic Kao,Sungeun Hong,Byung-Cheol Min*

Main category: cs.RO

TL;DR: This paper introduces PRIMT, a framework enhancing preference-based reinforcement learning by using foundation models for multimodal feedback and trajectory synthesis.


<details>
  <summary>Details</summary>
Motivation: Preference-based reinforcement learning often struggles with extensive human input requirements, query ambiguity, and credit assignment challenges.

Method: PRIMT employs hierarchical neuro-symbolic fusion, foresight trajectory generation, and hindsight trajectory augmentation to improve feedback reliability and reduce query ambiguity and credit assignment issues.

Result: Experimental evaluations on locomotion and manipulation tasks show PRIMT outperforms other FM-based and scripted baselines in performance.

Conclusion: By leveraging foundation models and innovative trajectory techniques, PRIMT addresses key limitations in PbRL, demonstrating its potential for teaching robots complex behaviors effectively.

Abstract: Preference-based reinforcement learning (PbRL) has emerged as a promising
paradigm for teaching robots complex behaviors without reward engineering.
However, its effectiveness is often limited by two critical challenges: the
reliance on extensive human input and the inherent difficulties in resolving
query ambiguity and credit assignment during reward learning. In this paper, we
introduce PRIMT, a PbRL framework designed to overcome these challenges by
leveraging foundation models (FMs) for multimodal synthetic feedback and
trajectory synthesis. Unlike prior approaches that rely on single-modality FM
evaluations, PRIMT employs a hierarchical neuro-symbolic fusion strategy,
integrating the complementary strengths of large language models and
vision-language models in evaluating robot behaviors for more reliable and
comprehensive feedback. PRIMT also incorporates foresight trajectory
generation, which reduces early-stage query ambiguity by warm-starting the
trajectory buffer with bootstrapped samples, and hindsight trajectory
augmentation, which enables counterfactual reasoning with a causal auxiliary
loss to improve credit assignment. We evaluate PRIMT on 2 locomotion and 6
manipulation tasks on various benchmarks, demonstrating superior performance
over FM-based and scripted baselines.

</details>


### [302] [Miniature soft robot with magnetically reprogrammable surgical functions](https://arxiv.org/abs/2509.15610)
*Chelsea Shan Xian Ng,Yu Xuan Yeoh,Nicholas Yong Wei Foo,Keerthana Radhakrishnan,Guo Zhan Lum*

Main category: cs.RO

TL;DR: This work develops a millimeter-scale soft robot with reprogrammable magnetization, enabling it to perform five surgical tasks and navigate with six degrees of freedom (DOF) in demanding environments.


<details>
  <summary>Details</summary>
Motivation: Existing miniature magnetic robots lack practicality in surgical contexts due to limited functionalities, restricted locomotion capability, or the need for very close proximity to strong external magnets.

Method: A millimeter-scale soft robot with reprogrammable magnetization is designed, capable of six-DOF movements and performing various surgical functionalities while using weak magnetic fields.

Result: This robot performs drug-dispensing, tissue cutting, gripping, storing samples, and remote heating, while achieving robust motion in complex unstructured environments via weak, uniform magnetic fields.

Conclusion: The developed robot represents a significant advancement in soft actuators and miniature robotics, paving the way for safer, minimally invasive treatments with versatile untethered robots.

Abstract: Miniature robots are untethered actuators, which have significant potential
to make existing minimally invasive surgery considerably safer and painless,
and enable unprecedented treatments because they are much smaller and dexterous
than existing surgical robots. Of the miniature robots, the magnetically
actuated ones are the most functional and dexterous. However, existing magnetic
miniature robots are currently impractical for surgery because they are either
restricted to possessing at most two on-board functionalities or having limited
five degrees-of-freedom (DOF) locomotion. Some of these actuators are also only
operational under specialized environments where actuation from strong external
magnets must be at very close proximity (< 4 cm away). Here we present a
millimeter-scale soft robot where its magnetization profile can be reprogrammed
upon command to perform five surgical functionalities: drug-dispensing, cutting
through biological tissues (simulated with gelatin), gripping, storing
(biological) samples and remote heating. By possessing full six-DOF motions,
including the sixth-DOF rotation about its net magnetic moment, our soft robot
can also roll and two-anchor crawl across challenging unstructured
environments, which are impassable by its five-DOF counterparts. Because our
actuating magnetic fields are relatively uniform and weak (at most 65 mT and
1.5 T/m), such fields can theoretically penetrate through biological tissues
harmlessly and allow our soft robot to remain controllable within the depths of
the human body. We envision that this work marks a major milestone for the
advancement of soft actuators, and towards revolutionizing minimally invasive
treatments with untethered miniature robots that have unprecedented
functionalities.

</details>


### [303] [Indoor Positioning Based on Active Radar Sensing and Passive Reflectors: Reflector Placement Optimization](https://arxiv.org/abs/2509.15613)
*Sven Hinderer,Pascal Schlachter,Zhibin Yu,Xiaofeng Wu,Bin Yang*

Main category: cs.RO

TL;DR: The paper presents a low-cost, high-accuracy indoor positioning system for autonomous robots using radar technology and optimized reflector placement.


<details>
  <summary>Details</summary>
Motivation: The authors aim to enhance indoor positioning accuracy for autonomous mobile robots while minimizing system costs.

Method: The system uses radar sensing of passive radar reflectors with a single-channel FMCW radar and incorporates a multi-objective particle swarm optimization algorithm for reflector placement.

Result: The combination of the radar and reflectors achieves high positioning accuracy, and the optimization algorithm determines effective reflector placements in complex environments.

Conclusion: The proposed approach offers an efficient and cost-effective solution for autonomous robot positioning in indoor spaces.

Abstract: We extend our work on a novel indoor positioning system (IPS) for autonomous
mobile robots (AMRs) based on radar sensing of local, passive radar reflectors.
Through the combination of simple reflectors and a single-channel frequency
modulated continuous wave (FMCW) radar, high positioning accuracy at low system
cost can be achieved. Further, a multi-objective (MO) particle swarm
optimization (PSO) algorithm is presented that optimizes the 2D placement of
radar reflectors in complex room settings.

</details>


### [304] [Omni-LIVO: Robust RGB-Colored Multi-Camera Visual-Inertial-LiDAR Odometry via Photometric Migration and ESIKF Fusion](https://arxiv.org/abs/2509.15673)
*Yinong Cao,Xin He,Yuwei Chen,Chenyang Zhang,Chengyu Pu,Bingtao Wang,Kaile Wu,Shouzheng Zhu,Fei Han,Shijie Liu,Chunlai Li,Jianyu Wang*

Main category: cs.RO

TL;DR: Omni-LIVO introduces a multi-camera LiDAR-inertial-visual odometry system to address spatial coverage and robustness limitations, improving accuracy through novel tracking and filtering techniques.


<details>
  <summary>Details</summary>
Motivation: Existing single-camera LIVO systems suffer from limited spatial coverage and degraded robustness due to the field-of-view mismatch between wide-angle LiDAR sensors and conventional cameras.

Method: Omni-LIVO uses a Cross-View direct tracking strategy for photometric consistency across non-overlapping views and extends the Error-State Iterated Kalman Filter (ESIKF) with multi-view updates and adaptive covariance weighting.

Result: The system demonstrates improved accuracy and robustness compared to state-of-the-art LIVO, LIO, and visual-inertial baselines, evaluated on public benchmarks and a custom dataset.

Conclusion: Omni-LIVO successfully bridges the field-of-view mismatch by integrating multi-camera updates and innovative tracking techniques, enhancing performance over existing odometry systems.

Abstract: Wide field-of-view (FoV) LiDAR sensors provide dense geometry across large
environments, but most existing LiDAR-inertial-visual odometry (LIVO) systems
rely on a single camera, leading to limited spatial coverage and degraded
robustness. We present Omni-LIVO, the first tightly coupled multi-camera LIVO
system that bridges the FoV mismatch between wide-angle LiDAR and conventional
cameras. Omni-LIVO introduces a Cross-View direct tracking strategy that
maintains photometric consistency across non-overlapping views, and extends the
Error-State Iterated Kalman Filter (ESIKF) with multi-view updates and adaptive
covariance weighting. The system is evaluated on public benchmarks and our
custom dataset, showing improved accuracy and robustness over state-of-the-art
LIVO, LIO, and visual-inertial baselines. Code and dataset will be released
upon publication.

</details>


### [305] [Imagination at Inference: Synthesizing In-Hand Views for Robust Visuomotor Policy Inference](https://arxiv.org/abs/2509.15717)
*Haoran Ding,Anqing Duan,Zezhou Sun,Dezhen Song,Yoshihiko Nakamura*

Main category: cs.RO

TL;DR: This paper addresses the challenge of robot manipulation in absence of in-hand cameras and achieves this via novel view synthesis using fine-tuned diffusion models.


<details>
  <summary>Details</summary>
Motivation: Visuomotor policies often depend on egocentric views, but equipping robots with in-hand cameras is challenging due to hardware constraints, complexity, and costs.

Method: The work uses a LoRA-based fine-tuning method to adapt a pretrained novel view synthesis model (ZeroNVS) to generate imagined in-hand views based on relative poses.

Result: Experiments on simulation benchmarks and real-world tasks demonstrated that synthesized views greatly improve robot performance, recovering losses from missing real in-hand cameras.

Conclusion: Synthesized in-hand views offer a scalable, hardware-light solution to enhance visuomotor policies, advancing imaginative visual reasoning in robots.

Abstract: Visual observations from different viewpoints can significantly influence the
performance of visuomotor policies in robotic manipulation. Among these,
egocentric (in-hand) views often provide crucial information for precise
control. However, in some applications, equipping robots with dedicated in-hand
cameras may pose challenges due to hardware constraints, system complexity, and
cost. In this work, we propose to endow robots with imaginative perception -
enabling them to 'imagine' in-hand observations from agent views at inference
time. We achieve this via novel view synthesis (NVS), leveraging a fine-tuned
diffusion model conditioned on the relative pose between the agent and in-hand
views cameras. Specifically, we apply LoRA-based fine-tuning to adapt a
pretrained NVS model (ZeroNVS) to the robotic manipulation domain. We evaluate
our approach on both simulation benchmarks (RoboMimic and MimicGen) and
real-world experiments using a Unitree Z1 robotic arm for a strawberry picking
task. Results show that synthesized in-hand views significantly enhance policy
inference, effectively recovering the performance drop caused by the absence of
real in-hand cameras. Our method offers a scalable and hardware-light solution
for deploying robust visuomotor policies, highlighting the potential of
imaginative visual reasoning in embodied agents.

</details>


### [306] [GP3: A 3D Geometry-Aware Policy with Multi-View Images for Robotic Manipulation](https://arxiv.org/abs/2509.15733)
*Quanhao Qian,Guoyang Zhao,Gongjie Zhang,Jiuniu Wang,Ran Xu,Junlong Gao,Deli Zhao*

Main category: cs.RO

TL;DR: GP3, a robotic manipulation policy, uses multi-view RGB inputs to create 3D scene representations without relying on depth sensors or pre-mapped environments, achieving high performance and easy real-world application.


<details>
  <summary>Details</summary>
Motivation: To improve robotic manipulation by leveraging multi-view observations for precise 3D understanding of scenes and develop a system robust to real-world settings without reliance on specialized depth sensors.

Method: GP3 integrates a spatial encoder to build dense spatial features from RGB inputs, estimates depth and camera parameters for 3D scene representation, and converts this along with language instructions into actions through a policy head.

Result: GP3 outperforms state-of-the-art methods in simulated benchmarks and adapts to real-world robots with minimal fine-tuning.

Conclusion: GP3 offers a practical, sensor-agnostic approach for advanced geometry-aware robotic manipulation, making it effective in both simulated and real-world applications.

Abstract: Effective robotic manipulation relies on a precise understanding of 3D scene
geometry, and one of the most straightforward ways to acquire such geometry is
through multi-view observations. Motivated by this, we present GP3 -- a 3D
geometry-aware robotic manipulation policy that leverages multi-view input. GP3
employs a spatial encoder to infer dense spatial features from RGB
observations, which enable the estimation of depth and camera parameters,
leading to a compact yet expressive 3D scene representation tailored for
manipulation. This representation is fused with language instructions and
translated into continuous actions via a lightweight policy head. Comprehensive
experiments demonstrate that GP3 consistently outperforms state-of-the-art
methods on simulated benchmarks. Furthermore, GP3 transfers effectively to
real-world robots without depth sensors or pre-mapped environments, requiring
only minimal fine-tuning. These results highlight GP3 as a practical,
sensor-agnostic solution for geometry-aware robotic manipulation.

</details>


### [307] [SMART: Scalable Multi-Agent Reasoning and Trajectory Planning in Dense Environments](https://arxiv.org/abs/2509.15737)
*Heye Huang,Yibin Yang,Wang Chen,Tiantian Chen,Xiaopeng Li,Sikai Chen*

Main category: cs.RO

TL;DR: The paper introduces SMART, a hierarchical framework for scalable multi-vehicle trajectory planning, showing significant efficiency and scalability improvements in dense environments.


<details>
  <summary>Details</summary>
Motivation: Trajectory planning for multiple vehicles in dense environments is challenging due to complex collision constraints and the need for real-time coordination.

Method: SMART uses a hierarchical approach combining priority-based search with distributed optimization, partitioning space into convex subproblems that are solved in parallel.

Result: Experiments demonstrate SMART's superior performance, achieving high success rates and faster runtimes compared to baselines, even with a large number of vehicles.

Conclusion: SMART offers an efficient and scalable solution to multi-vehicle trajectory planning, leveraging vehicle-to-everything communication and roadside cooperation to ensure safety and feasibility.

Abstract: Multi-vehicle trajectory planning is a non-convex problem that becomes
increasingly difficult in dense environments due to the rapid growth of
collision constraints. Efficient exploration of feasible behaviors and
resolution of tight interactions are essential for real-time, large-scale
coordination. This paper introduces SMART, Scalable Multi-Agent Reasoning and
Trajectory Planning, a hierarchical framework that combines priority-based
search with distributed optimization to achieve efficient and feasible
multi-vehicle planning. The upper layer explores diverse interaction modes
using reinforcement learning-based priority estimation and large-step hybrid A*
search, while the lower layer refines solutions via parallelizable convex
optimization. By partitioning space among neighboring vehicles and constructing
robust feasible corridors, the method decouples the joint non-convex problem
into convex subproblems solved efficiently in parallel. This design alleviates
the step-size trade-off while ensuring kinematic feasibility and collision
avoidance. Experiments show that SMART consistently outperforms baselines. On
50 m x 50 m maps, it sustains over 90% success within 1 s up to 25 vehicles,
while baselines often drop below 50%. On 100 m x 100 m maps, SMART achieves
above 95% success up to 50 vehicles and remains feasible up to 90 vehicles,
with runtimes more than an order of magnitude faster than optimization-only
approaches. Built on vehicle-to-everything communication, SMART incorporates
vehicle-infrastructure cooperation through roadside sensing and agent
coordination, improving scalability and safety. Real-world experiments further
validate this design, achieving planning times as low as 0.014 s while
preserving cooperative behaviors.

</details>


### [308] [FlyKites: Human-centric Interactive Exploration and Assistance under Limited Communication](https://arxiv.org/abs/2509.15807)
*Yuyang Zhang,Zhuoli Tian,Jinsheng Wei,Meng Guo*

Main category: cs.RO

TL;DR: FlyKites integrates distributed exploration, relay topology optimization, and human-in-the-loop execution for autonomous multi-robot systems with restricted communication during exploration in challenging environments.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges of maintaining consistent human assistance and reliable communication in autonomous multi-robot systems exploring extreme environments where traditional communication is limited.

Method: FlyKites introduces three components: distributed exploration (spread mode), relay topology optimization and robot role assignment (relay mode), and adaptive human-in-the-loop execution.

Result: Extensive simulations and hardware experiments demonstrated the efficacy of FlyKites in enhancing exploration and assistance in environments with limited communication.

Conclusion: FlyKites effectively resolves critical issues in multi-robot exploration, offering adaptive and efficient solutions to manage communication constraints and human interaction in challenging scenarios.

Abstract: Fleets of autonomous robots have been deployed for exploration of unknown
scenes for features of interest, e.g., subterranean exploration,
reconnaissance, search and rescue missions. During exploration, the robots may
encounter un-identified targets, blocked passages, interactive objects,
temporary failure, or other unexpected events, all of which require consistent
human assistance with reliable communication for a time period. This however
can be particularly challenging if the communication among the robots is
severely restricted to only close-range exchange via ad-hoc networks,
especially in extreme environments like caves and underground tunnels. This
paper presents a novel human-centric interactive exploration and assistance
framework called FlyKites, for multi-robot systems under limited communication.
It consists of three interleaved components: (I) the distributed exploration
and intermittent communication (called the "spread mode"), where the robots
collaboratively explore the environment and exchange local data among the fleet
and with the operator; (II) the simultaneous optimization of the relay
topology, the operator path, and the assignment of robots to relay roles
(called the "relay mode"), such that all requested assistance can be provided
with minimum delay; (III) the human-in-the-loop online execution, where the
robots switch between different roles and interact with the operator
adaptively. Extensive human-in-the-loop simulations and hardware experiments
are performed over numerous challenging scenes.

</details>


### [309] [Coordinated Multi-Drone Last-mile Delivery: Learning Strategies for Energy-aware and Timely Operations](https://arxiv.org/abs/2509.15830)
*Chuhao Qin,Arun Narayanan,Evangelos Pournaras*

Main category: cs.RO

TL;DR: The paper proposes a novel algorithm involving actor-critic-based multi-agent deep reinforcement learning to optimize multi-parcel delivery logistics using energy-aware drones.


<details>
  <summary>Details</summary>
Motivation: The study addresses challenges like time-sensitive deliveries, the demand for cost-efficient logistics, and energy considerations in drone-based multi-parcel delivery systems.

Method: The problem is divided into optimizing depot locations via K-means clustering, determining drone flight range through reinforcement learning, and planning delivery routes with a new optimized approach, integrating these solutions with multi-agent deep reinforcement learning.

Result: Through experiments with realistic datasets, the proposed algorithm showed exceptional performance in minimizing energy consumption, delivery delays, and execution time.

Conclusion: This work provides strategic insights for practical drone delivery systems, emphasizing economic efficiency, faster delivery times, and improved depot deployment strategies.

Abstract: Drones have recently emerged as a faster, safer, and cost-efficient way for
last-mile deliveries of parcels, particularly for urgent medical deliveries
highlighted during the pandemic. This paper addresses a new challenge of
multi-parcel delivery with a swarm of energy-aware drones, accounting for
time-sensitive customer requirements. Each drone plans an optimal multi-parcel
route within its battery-restricted flight range to minimize delivery delays
and reduce energy consumption. The problem is tackled by decomposing it into
three sub-problems: (1) optimizing depot locations and service areas using
K-means clustering; (2) determining the optimal flight range for drones through
reinforcement learning; and (3) planning and selecting multi-parcel delivery
routes via a new optimized plan selection approach. To integrate these
solutions and enhance long-term efficiency, we propose a novel algorithm
leveraging actor-critic-based multi-agent deep reinforcement learning.
Extensive experimentation using realistic delivery datasets demonstrate an
exceptional performance of the proposed algorithm. We provide new insights into
economic efficiency (minimize energy consumption), rapid operations (reduce
delivery delays and overall execution time), and strategic guidance on depot
deployment for practical logistics applications.

</details>


### [310] [High-Bandwidth Tactile-Reactive Control for Grasp Adjustment](https://arxiv.org/abs/2509.15876)
*Yonghyeon Lee,Tzu-Yuan Lin,Alexander Alexiev,Sangbae Kim*

Main category: cs.RO

TL;DR: The paper introduces a tactile-feedback grasp-adjustment algorithm that enhances grasping stability, even in scenarios with sensor noise and inaccuracies in grasp pose prediction.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of vision-only grasping systems due to calibration errors, sensor noise, and prediction inaccuracies, and to improve grasp robustness through tactile feedback.

Method: Proposes a tactile-reactive controller that refines grasping without relying on accurate grasp poses or object geometry information by using fingertip tactile sensors running at 200 Hz.

Result: Simulation studies and real-world experiments demonstrate significant improvements in grasp stability using the proposed tactile-reactive grasping framework.

Conclusion: The algorithm shows promise in refining crude, imprecise grasps and handling uncertain contact points, thereby improving overall grasp performance using tactile feedback.

Abstract: Vision-only grasping systems are fundamentally constrained by calibration
errors, sensor noise, and grasp pose prediction inaccuracies, leading to
unavoidable contact uncertainty in the final stage of grasping. High-bandwidth
tactile feedback, when paired with a well-designed tactile-reactive controller,
can significantly improve robustness in the presence of perception errors. This
paper contributes to controller design by proposing a purely tactile-feedback
grasp-adjustment algorithm. The proposed controller requires neither prior
knowledge of the object's geometry nor an accurate grasp pose, and is capable
of refining a grasp even when starting from a crude, imprecise initial
configuration and uncertain contact points. Through simulation studies and
real-world experiments on a 15-DoF arm-hand system (featuring an 8-DoF hand)
equipped with fingertip tactile sensors operating at 200 Hz, we demonstrate
that our tactile-reactive grasping framework effectively improves grasp
stability.

</details>


### [311] [Improving Robotic Manipulation with Efficient Geometry-Aware Vision Encoder](https://arxiv.org/abs/2509.15880)
*An Dinh Vuong,Minh Nhat Vu,Ian Reid*

Main category: cs.RO

TL;DR: This paper incorporates geometry-aware visual representations into imitation learning for robotics, leading to performance improvements while addressing computational challenges with a distilled encoder.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome limitations in traditional vision encoders that lack explicit 3D reasoning capabilities, hindering robotic manipulation tasks.

Method: This research integrates geometry-aware vision models like VGGT and proposes eVGGT—a distilled and efficient geometry-aware encoder—into imitation learning frameworks for robotic tasks.

Result: Using eVGGT improved success rates by up to 6.5% in robotic manipulation tasks compared to traditional vision encoders, while being 9 times faster and 5 times smaller.

Conclusion: Incorporating geometry-aware visual representations enhances imitation learning in robotics. eVGGT enables practical deployment by significantly reducing computational costs while maintaining robust 3D reasoning abilities.

Abstract: Existing RGB-based imitation learning approaches typically employ traditional
vision encoders such as ResNet or ViT, which lack explicit 3D reasoning
capabilities. Recent geometry-grounded vision models, such as
VGGT~\cite{wang2025vggt}, provide robust spatial understanding and are
promising candidates to address this limitation. This work investigates the
integration of geometry-aware visual representations into robotic manipulation.
Our results suggest that incorporating the geometry-aware vision encoder into
imitation learning frameworks, including ACT and DP, yields up to 6.5%
improvement over standard vision encoders in success rate across single- and
bi-manual manipulation tasks in both simulation and real-world settings.
Despite these benefits, most geometry-grounded models require high
computational cost, limiting their deployment in practical robotic systems. To
address this challenge, we propose eVGGT, an efficient geometry-aware encoder
distilled from VGGT. eVGGT is nearly 9 times faster and 5 times smaller than
VGGT, while preserving strong 3D reasoning capabilities. Code and pretrained
models will be released to facilitate further research in geometry-aware
robotics.

</details>


### [312] [An MPC framework for efficient navigation of mobile robots in cluttered environments](https://arxiv.org/abs/2509.15917)
*Johannes Köhler,Daniel Zhang,Raffaele Soloperto,Andrea Carron,Melanie Zeilinger*

Main category: cs.RO

TL;DR: This paper introduces an MPC framework for mobile robots in cluttered environments, ensuring collision avoidance and efficient target navigation.


<details>
  <summary>Details</summary>
Motivation: Enhance mobile robot navigation in highly cluttered environments while maintaining collision safety and dynamic target adaptability.

Method: The framework combines a finite-segment shortest path planner with finite-horizon trajectory optimization in an MPC setup.

Result: Robots efficiently navigated complex environments and reached dynamic targets within 2-3 seconds during hardware experiments.

Conclusion: The integrated approach enables successful and fast robot navigation, even in complex dynamic environments.

Abstract: We present a model predictive control (MPC) framework for efficient
navigation of mobile robots in cluttered environments. The proposed approach
integrates a finite-segment shortest path planner into the finite-horizon
trajectory optimization of the MPC. This formulation ensures convergence to
dynamically selected targets and guarantees collision avoidance, even under
general nonlinear dynamics and cluttered environments. The approach is
validated through hardware experiments on a small ground robot, where a human
operator dynamically assigns target locations. The robot successfully navigated
through complex environments and reached new targets within 2-3 seconds.

</details>


### [313] [A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning](https://arxiv.org/abs/2509.15937)
*Shaopeng Zhai,Qi Zhang,Tianyi Zhang,Fuxian Huang,Haoran Zhang,Ming Zhou,Shengzhe Zhang,Litao Liu,Sixu Lin,Jiangmiao Pang*

Main category: cs.RO

TL;DR: The paper introduces VLAC, a reward model for robotic RL with vision-language-action capabilities, drastically improving task success rates and sample efficiency in real-world operations.


<details>
  <summary>Details</summary>
Motivation: Robotic RL faces challenges like sparse rewards and inefficient exploration, limiting real-world applications. This paper aims to address these challenges using dense progress reward modeling.

Method: VLAC leverages InternVL and large-scale datasets to train on vision-language data, robotic trajectories, and human demonstrations. It offers one-shot transfer to new tasks and includes capabilities for handling irrelevant prompts and detecting stagnation.

Result: VLAC improved success rates from 30% to 90% across four real-world manipulation tasks and increased sample efficiency by 50% when human-interventions were used, reaching up to 100% success.

Conclusion: VLAC simplifies reward engineering and enhances exploration strategies, achieving significant success in real-world robotic reinforcement learning and opening pathways for broader application.

Abstract: Robotic real-world reinforcement learning (RL) with vision-language-action
(VLA) models is bottlenecked by sparse, handcrafted rewards and inefficient
exploration. We introduce VLAC, a general process reward model built upon
InternVL and trained on large scale heterogeneous datasets. Given pairwise
observations and a language goal, it outputs dense progress delta and done
signal, eliminating task-specific reward engineering, and supports one-shot
in-context transfer to unseen tasks and environments. VLAC is trained on
vision-language datasets to strengthen perception, dialogic and reasoning
capabilities, together with robot and human trajectories data that ground
action generation and progress estimation, and additionally strengthened to
reject irrelevant prompts as well as detect regression or stagnation by
constructing large numbers of negative and semantically mismatched samples.
With prompt control, a single VLAC model alternately generating reward and
action tokens, unifying critic and policy. Deployed inside an asynchronous
real-world RL loop, we layer a graded human-in-the-loop protocol (offline
demonstration replay, return and explore, human guided explore) that
accelerates exploration and stabilizes early learning. Across four distinct
real-world manipulation tasks, VLAC lifts success rates from about 30\% to
about 90\% within 200 real-world interaction episodes; incorporating
human-in-the-loop interventions yields a further 50% improvement in sample
efficiency and achieves up to 100% final success.

</details>


### [314] [Right-Side-Out: Learning Zero-Shot Sim-to-Real Garment Reversal](https://arxiv.org/abs/2509.15953)
*Chang Yu,Siyu Ma,Wenxin Du,Zeshun Zong,Han Xue,Wendi Chen,Cewu Lu,Yin Yang,Xuchen Han,Joseph Masterjohn,Alejandro Castro,Chenfanfu Jiang*

Main category: cs.RO

TL;DR: The paper presents Right-Side-Out, a framework that tackles the complex task of turning garments right-side out via task decomposition and simulation-based training, achieving a high success rate in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: The task of turning garments right-side out is complex due to its dynamic nature, rapid contact changes, and severe visual occlusion, motivating a need for an automated and efficient solution.

Method: The framework decomposes the task into two sequential steps (Drag/Fling and Insert&Pull) using depth-inferred, keypoint-parameterized bimanual primitives. Data is generated through a high-fidelity GPU-parallel simulator that models garment deformation and efficiently handles contact. Training occurs entirely in simulation without human annotations.

Result: The trained policies deploy zero-shot on real hardware and achieve a success rate of up to 81.3%, showing that the framework effectively tackles the task.

Conclusion: By leveraging task decomposition and high fidelity simulations, the framework minimizes human intervention and provides a solution for executing highly dynamic, occluded garment manipulation tasks.

Abstract: Turning garments right-side out is a challenging manipulation task: it is
highly dynamic, entails rapid contact changes, and is subject to severe visual
occlusion. We introduce Right-Side-Out, a zero-shot sim-to-real framework that
effectively solves this challenge by exploiting task structures. We decompose
the task into Drag/Fling to create and stabilize an access opening, followed by
Insert&Pull to invert the garment. Each step uses a depth-inferred,
keypoint-parameterized bimanual primitive that sharply reduces the action space
while preserving robustness. Efficient data generation is enabled by our
custom-built, high-fidelity, GPU-parallel Material Point Method (MPM) simulator
that models thin-shell deformation and provides robust and efficient contact
handling for batched rollouts. Built on the simulator, our fully automated
pipeline scales data generation by randomizing garment geometry, material
parameters, and viewpoints, producing depth, masks, and per-primitive keypoint
labels without any human annotations. With a single depth camera, policies
trained entirely in simulation deploy zero-shot on real hardware, achieving up
to 81.3% success rate. By employing task decomposition and high fidelity
simulation, our framework enables tackling highly dynamic, severely occluded
tasks without laborious human demonstrations.

</details>


### [315] [Swarm Oracle: Trustless Blockchain Agreements through Robot Swarms](https://arxiv.org/abs/2509.15956)
*Alexandre Pacheco,Hanqing Zhao,Volker Strobel,Tarik Roukny,Gregory Dudek,Andreagiovanni Reina,Marco Dorigo*

Main category: cs.RO

TL;DR: Swarm Oracle, a decentralized robot network, delivers real-world data to blockchains using a Byzantine fault-tolerant protocol.


<details>
  <summary>Details</summary>
Motivation: Existing blockchain oracles compromise autonomy, transparency, or reintroduce trust. A robust and trustless real-world data integration solution is needed.

Method: Proposes a robot swarm utilizing onboard sensors, peer-to-peer communication, and Byzantine fault-tolerant protocols for secure data consensus.

Result: Real and simulated experiments demonstrated Swarm Oracle's ability to achieve consensus in uncertain conditions and autonomously recover from attacks using a token-based reputation system.

Conclusion: Swarm Oracle ensures secure, trustless, and decentralized integration of real-world data into blockchains, overcoming current limitations.

Abstract: Blockchain consensus, rooted in the principle ``don't trust, verify'', limits
access to real-world data, which may be ambiguous or inaccessible to some
participants. Oracles address this limitation by supplying data to blockchains,
but existing solutions may reduce autonomy, transparency, or reintroduce the
need for trust. We propose Swarm Oracle: a decentralized network of autonomous
robots -- that is, a robot swarm -- that use onboard sensors and peer-to-peer
communication to collectively verify real-world data and provide it to smart
contracts on public blockchains. Swarm Oracle leverages the built-in
decentralization, fault tolerance and mobility of robot swarms, which can
flexibly adapt to meet information requests on-demand, even in remote
locations. Unlike typical cooperative robot swarms, Swarm Oracle integrates
robots from multiple stakeholders, protecting the system from single-party
biases but also introducing potential adversarial behavior. To ensure the
secure, trustless and global consensus required by blockchains, we employ a
Byzantine fault-tolerant protocol that enables robots from different
stakeholders to operate together, reaching social agreements of higher quality
than the estimates of individual robots. Through extensive experiments using
both real and simulated robots, we showcase how consensus on uncertain
environmental information can be achieved, despite several types of attacks
orchestrated by large proportions of the robots, and how a reputation system
based on blockchain tokens lets Swarm Oracle autonomously recover from faults
and attacks, a requirement for long-term operation.

</details>


### [316] [CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for Long-Tail Scenarios via Collect-and-Refine](https://arxiv.org/abs/2509.15968)
*Shiyu Fang,Yiming Cui,Haoyang Liang,Chen Lv,Peng Hang,Jian Sun*

Main category: cs.RO

TL;DR: Autonomous Driving systems struggle in rare, critical scenarios. CoReVLA, a new model, uses continual learning via data collection and refinement to handle such cases, outperforming existing methods in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the performance gap in autonomous driving systems for safety-critical long-tail scenarios that disproportionately contribute to accidents.

Method: Use a dual-stage framework: jointly fine-tune the model on QA driving datasets, deploy it in simulated environments to gather driver takeover data for refinement, and optimize via Direct Preference Optimization (DPO).

Result: The CoReVLA model achieves a Driving Score of 72.18 and a Success Rate of 50% on the Bench2Drive benchmark, surpassing state-of-the-art methods by significant margins in long-tail scenarios.

Conclusion: CoReVLA enhances autonomous driving systems by continually learning from failures and human interactions, leading to improved performance in rare safety-critical scenarios.

Abstract: Autonomous Driving (AD) systems have made notable progress, but their
performance in long-tail, safety-critical scenarios remains limited. These rare
cases contribute a disproportionate number of accidents. Vision-Language Action
(VLA) models have strong reasoning abilities and offer a potential solution,
but their effectiveness is limited by the lack of high-quality data and
inefficient learning in such conditions. To address these challenges, we
propose CoReVLA, a continual learning end-to-end autonomous driving framework
that improves the performance in long-tail scenarios through a dual-stage
process of data Collection and behavior Refinement. First, the model is jointly
fine-tuned on a mixture of open-source driving QA datasets, allowing it to
acquire a foundational understanding of driving scenarios. Next, CoReVLA is
deployed within the Cave Automatic Virtual Environment (CAVE) simulation
platform, where driver takeover data is collected from real-time interactions.
Each takeover indicates a long-tail scenario that CoReVLA fails to handle
reliably. Finally, the model is refined via Direct Preference Optimization
(DPO), allowing it to learn directly from human preferences and thereby avoid
reward hacking caused by manually designed rewards. Extensive open-loop and
closed-loop experiments demonstrate that the proposed CoReVLA model can
accurately perceive driving scenarios and make appropriate decisions. On the
Bench2Drive benchmark, CoReVLA achieves a Driving Score (DS) of 72.18 and a
Success Rate (SR) of 50%, outperforming state-of-the-art methods by 7.96 DS and
15% SR under long-tail, safety-critical scenarios. Furthermore, case studies
demonstrate the model's ability to continually improve its performance in
similar failure-prone scenarios by leveraging past takeover experiences. All
codea and preprocessed datasets are available at:
https://github.com/FanGShiYuu/CoReVLA

</details>


### [317] [Defining and Monitoring Complex Robot Activities via LLMs and Symbolic Reasoning](https://arxiv.org/abs/2509.16006)
*Francesco Argenziano,Elena Umili,Francesco Leotta,Daniele Nardi*

Main category: cs.RO

TL;DR: This paper proposes a framework that combines Large Language Models (LLMs) with automated planning, enabling natural language specification and monitoring of robotic high-level activities.


<details>
  <summary>Details</summary>
Motivation: Labor-intensive and complex activities, particularly in dynamic and unpredictable environments like industrial and agricultural settings, require automation. Current robotics lack robust solutions for human monitoring of activity progress to ensure safety and correctness.

Method: The authors introduce an architecture integrating LLMs and automated planning. This system allows humans to define high-level activities in natural language and query the execution process. They validate the approach in a precision agriculture use case with quantitative evaluations.

Result: The proposed architecture was successfully implemented and evaluated in a real-world agricultural scenario, showing promising results in handling dynamic process compositions.

Conclusion: Integrating LLMs with automated planning is effective for human-friendly interactions in robotic activity specification and monitoring, showing potential in safety-critical and complex workflows.

Abstract: Recent years have witnessed a growing interest in automating labor-intensive
and complex activities, i.e., those consisting of multiple atomic tasks, by
deploying robots in dynamic and unpredictable environments such as industrial
and agricultural settings. A key characteristic of these contexts is that
activities are not predefined: while they involve a limited set of possible
tasks, their combinations may vary depending on the situation. Moreover,
despite recent advances in robotics, the ability for humans to monitor the
progress of high-level activities - in terms of past, present, and future
actions - remains fundamental to ensure the correct execution of
safety-critical processes. In this paper, we introduce a general architecture
that integrates Large Language Models (LLMs) with automated planning, enabling
humans to specify high-level activities (also referred to as processes) using
natural language, and to monitor their execution by querying a robot. We also
present an implementation of this architecture using state-of-the-art
components and quantitatively evaluate the approach in a real-world precision
agriculture scenario.

</details>


### [318] [A Matter of Height: The Impact of a Robotic Object on Human Compliance](https://arxiv.org/abs/2509.16032)
*Michael Faber,Andrey Grishko,Julian Waksberg,David Pardo,Tomer Leivy,Yuval Hazan,Emanuel Talmansky,Benny Megidish,Hadas Erel*

Main category: cs.RO

TL;DR: This study explored how robot height affects human-robot interactions, specifically compliance with requests. Surprisingly, shorter robots elicited higher compliance from participants.


<details>
  <summary>Details</summary>
Motivation: The paper aimed to explore whether the social dynamics of human height affecting persuasiveness also applied in human-robot interactions, especially with non-humanoid robots.

Method: The study involved a mobile robotic table with adjustable height. Participants interacted with Short (95cm) or Tall (132cm) robots, which asked them to voluntarily complete a tedious questionnaire after completing a cognitive task.

Result: Participants complied more with the Short robot's request than the Tall robot, indicating that height does not influence human-robot interactions in the same way it does human-human dynamics.

Conclusion: Robot height impacts human-robot interaction differently than human-human interaction. Designers of robots must test social elements rather than assume they will work similarly as in human contexts.

Abstract: Robots come in various forms and have different characteristics that may
shape the interaction with them. In human-human interactions, height is a
characteristic that shapes human dynamics, with taller people typically
perceived as more persuasive. In this work, we aspired to evaluate if the same
impact replicates in a human-robot interaction and specifically with a highly
non-humanoid robotic object. The robot was designed with modules that could be
easily added or removed, allowing us to change its height without altering
other design features. To test the impact of the robot's height, we evaluated
participants' compliance with its request to volunteer to perform a tedious
task. In the experiment, participants performed a cognitive task on a computer,
which was framed as the main experiment. When done, they were informed that the
experiment was completed. While waiting to receive their credits, the robotic
object, designed as a mobile robotic service table, entered the room, carrying
a tablet that invited participants to complete a 300-question questionnaire
voluntarily. We compared participants' compliance in two conditions: A Short
robot composed of two modules and 95cm in height and a Tall robot consisting of
three modules and 132cm in height. Our findings revealed higher compliance with
the Short robot's request, demonstrating an opposite pattern to human dynamics.
We conclude that while height has a substantial social impact on human-robot
interactions, it follows a unique pattern of influence. Our findings suggest
that designers cannot simply adopt and implement elements from human social
dynamics to robots without testing them first.

</details>


### [319] [Learning Safety for Obstacle Avoidance via Control Barrier Functions](https://arxiv.org/abs/2509.16037)
*Shuo Liu,Zhe Huang,Calin A. Belta*

Main category: cs.RO

TL;DR: This paper introduces a method using neural networks and control barrier functions to enable efficient and safe robot navigation in cluttered environments.


<details>
  <summary>Details</summary>
Motivation: Existing control barrier function methods struggle with complex robot geometries and dynamic configurations, necessitating a solution compatible with these constraints.

Method: The method trains a residual neural network for clearance prediction, applies a Local Safety Ball concept for collision avoidance, utilizes Discrete-Time High-Order CBF constraints, and incorporates relaxation techniques into a nonlinear optimization framework.

Result: The proposed method ensures collision-free, dynamically feasible trajectories for arbitrary robot geometries with fast computation times (millisecond-level) and high prediction accuracy.

Conclusion: The framework improves on existing CBF methods by providing efficient, real-time safe navigation for robots in cluttered environments with complex geometries.

Abstract: Obstacle avoidance is central to safe navigation, especially for robots with
arbitrary and nonconvex geometries operating in cluttered environments.
Existing Control Barrier Function (CBF) approaches often rely on analytic
clearance computations, which are infeasible for complex geometries, or on
polytopic approximations, which become intractable when robot configurations
are unknown. To address these limitations, this paper trains a residual neural
network on a large dataset of robot-obstacle configurations to enable fast and
tractable clearance prediction, even at unseen configurations. The predicted
clearance defines the radius of a Local Safety Ball (LSB), which ensures
continuous-time collision-free navigation. The LSB boundary is encoded as a
Discrete-Time High-Order CBF (DHOCBF), whose constraints are incorporated into
a nonlinear optimization framework. To improve feasibility, a novel relaxation
technique is applied. The resulting framework ensure that the robot's
rigid-body motion between consecutive time steps remains collision-free,
effectively bridging discrete-time control and continuous-time safety. We show
that the proposed method handles arbitrary, including nonconvex, robot
geometries and generates collision-free, dynamically feasible trajectories in
cluttered environments. Experiments demonstrate millisecond-level solve times
and high prediction accuracy, highlighting both safety and efficiency beyond
existing CBF-based methods.

</details>


### [320] [Compose by Focus: Scene Graph-based Atomic Skills](https://arxiv.org/abs/2509.16053)
*Han Qi,Changhe Chen,Heng Yang*

Main category: cs.RO

TL;DR: The paper proposes a framework using scene graphs to enhance skill execution and compositional generalization in robots, achieving better results in complex tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges faced by generalist robots in robustly executing individual visuomotor skills during long-horizon tasks, as these skills often fail under distribution shifts caused by scene variations.

Method: The authors introduce a scene graph-based representation emphasizing task-relevant objects and relations to mitigate irrelevant variations, combine this with graph neural networks and diffusion-based imitation learning, and integrate it with a vision-language model task planner.

Result: Experiments indicate that the proposed framework yields higher success rates compared to state-of-the-art baselines, showcasing its improved robustness and generalization in both simulated and real-world manipulation tasks.

Conclusion: This framework demonstrates the potential of scene graph-based approaches in enhancing the compositional generalization and robustness of robotic systems in long-horizon tasks.

Abstract: A key requirement for generalist robots is compositional generalization - the
ability to combine atomic skills to solve complex, long-horizon tasks. While
prior work has primarily focused on synthesizing a planner that sequences
pre-learned skills, robust execution of the individual skills themselves
remains challenging, as visuomotor policies often fail under distribution
shifts induced by scene composition. To address this, we introduce a scene
graph-based representation that focuses on task-relevant objects and relations,
thereby mitigating sensitivity to irrelevant variation. Building on this idea,
we develop a scene-graph skill learning framework that integrates graph neural
networks with diffusion-based imitation learning, and further combine "focused"
scene-graph skills with a vision-language model (VLM) based task planner.
Experiments in both simulation and real-world manipulation tasks demonstrate
substantially higher success rates than state-of-the-art baselines,
highlighting improved robustness and compositional generalization in
long-horizon tasks.

</details>


### [321] [Latent Conditioned Loco-Manipulation Using Motion Priors](https://arxiv.org/abs/2509.16061)
*Maciej Stępień,Rafael Kourdis,Constant Roux,Olivier Stasse*

Main category: cs.RO

TL;DR: The paper proposes a method to train multipurpose motion policies for humanoid and quadruped robots, improving upon simple methods like Deep Reinforcement Learning by focusing on latent space control and imitation.


<details>
  <summary>Details</summary>
Motivation: Current robot control methods focus on single skills, which are inefficient for complex tasks requiring consideration of high-level goals, physical constraints, and motion styles.

Method: The authors train motion policies via imitation, incorporating latent space control for skill execution, and enhance the approach with constraints for safety and a diffusion discriminator for higher imitation quality.

Result: The techniques were validated in simulation with H1 humanoid and Solo12 quadruped, and successfully deployed on Solo12 hardware.

Conclusion: The study demonstrates the effectiveness of combining imitation learning and latent space control to achieve safe and high-quality loco-manipulation for both humanoid and quadruped robots.

Abstract: Although humanoid and quadruped robots provide a wide range of capabilities,
current control methods, such as Deep Reinforcement Learning, focus mainly on
single skills. This approach is inefficient for solving more complicated tasks
where high-level goals, physical robot limitations and desired motion style
might all need to be taken into account. A more effective approach is to first
train a multipurpose motion policy that acquires low-level skills through
imitation, while providing latent space control over skill execution. Then,
this policy can be used to efficiently solve downstream tasks. This method has
already been successful for controlling characters in computer graphics. In
this work, we apply the approach to humanoid and quadrupedal loco-manipulation
by imitating either simple synthetic motions or kinematically retargeted dog
motions. We extend the original formulation to handle constraints, ensuring
deployment safety, and use a diffusion discriminator for better imitation
quality. We verify our methods by performing loco-manipulation in simulation
for the H1 humanoid and Solo12 quadruped, as well as deploying policies on
Solo12 hardware. Videos and code are available at
https://gepetto.github.io/LaCoLoco/

</details>


### [322] [DSPv2: Improved Dense Policy for Effective and Generalizable Whole-body Mobile Manipulation](https://arxiv.org/abs/2509.16063)
*Yue Su,Chubin Zhang,Sijin Chen,Liufan Tan,Yansong Tang,Jianan Wang,Xihui Liu*

Main category: cs.RO

TL;DR: The paper introduces DSPv2, a novel policy architecture, to enhance robotic whole-body mobile manipulation by aligning 3D spatial and multi-view 2D semantic features, with superior task performance and generalization abilities.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in robotic whole-body mobile manipulation, such as processing complex observations, achieving robust generalizations, and generating coherent actions.

Method: The proposed DSPv2 architecture combines 3D spatial features with multi-view 2D semantic features for effective encoding and precise control. It extends the Dense Policy paradigm to the complex domain of whole-body manipulation.

Result: Experiments show that DSPv2 outperforms existing approaches in task performance and generalization ability within robotic manipulation tasks.

Conclusion: DSPv2 is effective in enhancing whole-body mobile manipulation by achieving precise control, coherent actions, and broad generalization, addressing key challenges in this domain.

Abstract: Learning whole-body mobile manipulation via imitation is essential for
generalizing robotic skills to diverse environments and complex tasks. However,
this goal is hindered by significant challenges, particularly in effectively
processing complex observation, achieving robust generalization, and generating
coherent actions. To address these issues, we propose DSPv2, a novel policy
architecture. DSPv2 introduces an effective encoding scheme that aligns 3D
spatial features with multi-view 2D semantic features. This fusion enables the
policy to achieve broad generalization while retaining the fine-grained
perception necessary for precise control. Furthermore, we extend the Dense
Policy paradigm to the whole-body mobile manipulation domain, demonstrating its
effectiveness in generating coherent and precise actions for the whole-body
robotic platform. Extensive experiments show that our method significantly
outperforms existing approaches in both task performance and generalization
ability. Project page is available at: https://selen-suyue.github.io/DSPv2Net/.

</details>


### [323] [I-FailSense: Towards General Robotic Failure Detection with Vision-Language Models](https://arxiv.org/abs/2509.16072)
*Clemence Grislain,Hamed Rahimi,Olivier Sigaud,Mohamed Chetouani*

Main category: cs.RO

TL;DR: The paper introduces I-FailSense, a framework that enhances failure detection in language-conditioned robotic manipulation, especially detecting semantic misalignment errors.


<details>
  <summary>Details</summary>
Motivation: To address the gap in current vision-language models where they excel at spatial reasoning and task planning but struggle to detect their own semantic misalignment failures.

Method: The authors propose a technique to generate datasets for semantic misalignment detection and introduce I-FailSense, which involves post-training a base VLM and adding lightweight FS blocks for classification at different model layers. The predictions are then aggregated using an ensemble mechanism.

Result: I-FailSense outperforms other state-of-the-art VLMs in detecting semantic misalignment errors and generalizes well to broader robotic failures, with effective zero-shot or minimal post-training transfer to new environments.

Conclusion: I-FailSense provides a robust and publicly available solution to semantic misalignment and robotic failure detection, filling a significant gap in language-conditioned manipulation systems.

Abstract: Language-conditioned robotic manipulation in open-world settings requires not
only accurate task execution but also the ability to detect failures for robust
deployment in real-world environments. Although recent advances in
vision-language models (VLMs) have significantly improved the spatial reasoning
and task-planning capabilities of robots, they remain limited in their ability
to recognize their own failures. In particular, a critical yet underexplored
challenge lies in detecting semantic misalignment errors, where the robot
executes a task that is semantically meaningful but inconsistent with the given
instruction. To address this, we propose a method for building datasets
targeting Semantic Misalignment Failures detection, from existing
language-conditioned manipulation datasets. We also present I-FailSense, an
open-source VLM framework with grounded arbitration designed specifically for
failure detection. Our approach relies on post-training a base VLM, followed by
training lightweight classification heads, called FS blocks, attached to
different internal layers of the VLM and whose predictions are aggregated using
an ensembling mechanism. Experiments show that I-FailSense outperforms
state-of-the-art VLMs, both comparable in size and larger, in detecting
semantic misalignment errors. Notably, despite being trained only on semantic
misalignment detection, I-FailSense generalizes to broader robotic failure
categories and effectively transfers to other simulation environments and
real-world with zero-shot or minimal post-training. The datasets and models are
publicly released on HuggingFace (Webpage:
https://clemgris.github.io/I-FailSense/).

</details>


### [324] [Real-Time Planning and Control with a Vortex Particle Model for Fixed-Wing UAVs in Unsteady Flows](https://arxiv.org/abs/2509.16079)
*Ashwin Gupta,Kevin Wolfe,Gino Perrotta,Joseph Moore*

Main category: cs.RO

TL;DR: This paper develops a real-time planning and control system that uses a lightweight vortex particle model to handle unsteady aerodynamic effects.


<details>
  <summary>Details</summary>
Motivation: To address the challenges aerial vehicles face due to unsteady aerodynamic effects, especially during agile maneuvers in complex environments.

Method: The method incorporates a GPU-accelerated vortex particle model combined with a sampling-based policy optimization strategy for predictive planning.

Result: Simulation and hardware experiments show enhanced performance in executing aggressive post-stall maneuvers under unsteady environmental flow disturbances.

Conclusion: Inclusion of unsteady aerodynamics modeling in real-time planning significantly improves aerial vehicle maneuverability and performance in challenging flight conditions.

Abstract: Unsteady aerodynamic effects can have a profound impact on aerial vehicle
flight performance, especially during agile maneuvers and in complex
aerodynamic environments. In this paper, we present a real-time planning and
control approach capable of reasoning about unsteady aerodynamics. Our approach
relies on a lightweight vortex particle model, parallelized to allow GPU
acceleration, and a sampling-based policy optimization strategy capable of
leveraging the vortex particle model for predictive reasoning. We demonstrate,
through both simulation and hardware experiments, that by replanning with our
unsteady aerodynamics model, we can improve the performance of aggressive
post-stall maneuvers in the presence of unsteady environmental flow
disturbances.

</details>


### [325] [Efficient Detection of Objects Near a Robot Manipulator via Miniature Time-of-Flight Sensors](https://arxiv.org/abs/2509.16122)
*Carter Sifferman,Mohit Gupta,Michael Gleicher*

Main category: cs.RO

TL;DR: The paper introduces a method for using arm-mounted low-resolution time-of-flight sensors to detect and localize objects near a robot arm, avoiding misclassifying the robot itself as an object.


<details>
  <summary>Details</summary>
Motivation: Detecting and localizing objects near a robot arm while avoiding misidentifying the arm itself in sensor measurements is crucial for safe and effective robot operation.

Method: Developed an empirical model for expected sensor measurements caused by the robot itself and designed a runtime detection system utilizing raw time-of-flight data, allowing flexible sensor placement.

Result: The method accurately detects and localizes small objects near the robot arm, with performance analyzed based on object type, location, and light conditions, while acknowledging certain measurement limitations.

Conclusion: The proposed method enhances collision avoidance and safe human-robot interaction by enabling efficient object detection and localization without self-detection by the robot.

Abstract: We provide a method for detecting and localizing objects near a robot arm
using arm-mounted miniature time-of-flight sensors. A key challenge when using
arm-mounted sensors is differentiating between the robot itself and external
objects in sensor measurements. To address this challenge, we propose a
computationally lightweight method which utilizes the raw time-of-flight
information captured by many off-the-shelf, low-resolution time-of-flight
sensor. We build an empirical model of expected sensor measurements in the
presence of the robot alone, and use this model at runtime to detect objects in
proximity to the robot. In addition to avoiding robot self-detections in common
sensor configurations, the proposed method enables extra flexibility in sensor
placement, unlocking configurations which achieve more efficient coverage of a
radius around the robot arm. Our method can detect small objects near the arm
and localize the position of objects along the length of a robot link to
reasonable precision. We evaluate the performance of the method with respect to
object type, location, and ambient light level, and identify limiting factors
on performance inherent in the measurement principle. The proposed method has
potential applications in collision avoidance and in facilitating safe
human-robot interaction.

</details>


### [326] [Reward Evolution with Graph-of-Thoughts: A Bi-Level Language Model Framework for Reinforcement Learning](https://arxiv.org/abs/2509.16136)
*Changwei Yao,Xinzi Liu,Chen Li,Marios Savvides*

Main category: cs.RO

TL;DR: The paper introduces RE-GoT, a framework combining LLMs and VLMs with graph-based reasoning to improve reward design in RL, achieving significant performance gains over existing methods.


<details>
  <summary>Details</summary>
Motivation: Reward design in reinforcement learning is a challenging process requiring expertise and refinement, with current LLM-based methods facing limitations like hallucinations and difficulties with multi-step tasks.

Method: The proposed RE-GoT framework utilizes bi-level structured graph reasoning to decompose tasks, integrated with VLMs for iterative reward refinement using visual feedback, eliminating the need for human input.

Result: RE-GoT outperforms baselines in experiments on RoboGen and ManiSkill2, improving success rates by an average of 32.25% and achieving 93.73% in success rates respectively, even surpassing expert-designed rewards.

Conclusion: The integration of LLMs, VLMs, and graph-of-thoughts reasoning in the RE-GoT framework demonstrates a scalable and effective approach for autonomous reward evolution in reinforcement learning.

Abstract: Designing effective reward functions remains a major challenge in
reinforcement learning (RL), often requiring considerable human expertise and
iterative refinement. Recent advances leverage Large Language Models (LLMs) for
automated reward design, but these approaches are limited by hallucinations,
reliance on human feedback, and challenges with handling complex, multi-step
tasks. In this work, we introduce Reward Evolution with Graph-of-Thoughts
(RE-GoT), a novel bi-level framework that enhances LLMs with structured
graph-based reasoning and integrates Visual Language Models (VLMs) for
automated rollout evaluation. RE-GoT first decomposes tasks into
text-attributed graphs, enabling comprehensive analysis and reward function
generation, and then iteratively refines rewards using visual feedback from
VLMs without human intervention. Extensive experiments on 10 RoboGen and 4
ManiSkill2 tasks demonstrate that RE-GoT consistently outperforms existing
LLM-based baselines. On RoboGen, our method improves average task success rates
by 32.25%, with notable gains on complex multi-step tasks. On ManiSkill2,
RE-GoT achieves an average success rate of 93.73% across four diverse
manipulation tasks, significantly surpassing prior LLM-based approaches and
even exceeding expert-designed rewards. Our results indicate that combining
LLMs and VLMs with graph-of-thoughts reasoning provides a scalable and
effective solution for autonomous reward evolution in RL.

</details>


### [327] [Modeling Elastic-Body Dynamics of Fish Swimming Using a Variational Framework](https://arxiv.org/abs/2509.16145)
*Zhiheng Chen,Wei Wang*

Main category: cs.RO

TL;DR: The paper presents a fish-inspired aquatic robot model based on Hamilton's principle, emphasizing accurate dynamics modeling for optimal robotic swimming design.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the need for precise and computationally efficient modeling to improve the design and control of fish-inspired aquatic robots.

Method: The study developed a full-body dynamics model using Hamilton's principle, incorporating elasticity, large deformation analysis, and fluid-structure interaction for self-propelled motion.

Result: Simulations demonstrated relationships between actuation frequency, body stiffness, swimming speed, and energy efficiency, identifying optimal values for stiffness and length.

Conclusion: Key findings contribute to understanding biological swimming and serve as a guide for designing efficient soft robotic swimmers.

Abstract: Fish-inspired aquatic robots are gaining increasing attention in research
communities due to their high swimming speeds and efficient propulsion enabled
by flexible bodies that generate undulatory motions. To support the design
optimizations and control of such systems, accurate, interpretable, and
computationally tractable modeling of the underlying swimming dynamics is
indispensable. In this letter, we present a full-body dynamics model for fish
swimming, rigorously derived from Hamilton's principle. The model captures the
continuously distributed elasticity of a deformable fish body undergoing large
deformations and incorporates fluid-structure coupling effects, enabling
self-propelled motion without prescribing kinematics. A preliminary parameter
study explores the influence of actuation frequency and body stiffness on
swimming speed and cost of transport (COT). Simulation results indicate that
swimming speed and energy efficiency exhibit opposing trends with tail-beat
frequency and that both body stiffness and body length have distinct optimal
values. These findings provide insights into biological swimming mechanisms and
inform the design of high-performance soft robotic swimmers.

</details>


### [328] [Agentic Aerial Cinematography: From Dialogue Cues to Cinematic Trajectories](https://arxiv.org/abs/2509.16176)
*Yifan Lin,Sophie Ziyu Liu,Ran Qi,George Z. Xue,Xinping Song,Chao Qin,Hugh H. -T. Liu*

Main category: cs.RO

TL;DR: ACDC introduces an autonomous aerial cinematography system using natural language prompts for drones, leveraging large language and vision foundation models.


<details>
  <summary>Details</summary>
Motivation: Current drone cinematography requires manual control and predefined intent, making it labor-intensive and inconsistent.

Method: ACDC integrates vision-language retrieval, Bayesian optimization for aesthetics, and motion planning to convert dialogue into drone trajectories.

Result: Experiments show ACDC can produce high-quality indoor UAV footage consistently, eliminating the need for robotics or cinematography expertise.

Conclusion: This paper demonstrates the capability of embodied AI to bridge dialogue inputs with real-world autonomous aerial filmmaking.

Abstract: We present Agentic Aerial Cinematography: From Dialogue Cues to Cinematic
Trajectories (ACDC), an autonomous drone cinematography system driven by
natural language communication between human directors and drones. The main
limitation of previous drone cinematography workflows is that they require
manual selection of waypoints and view angles based on predefined human intent,
which is labor-intensive and yields inconsistent performance. In this paper, we
propose employing large language models (LLMs) and vision foundation models
(VFMs) to convert free-form natural language prompts directly into executable
indoor UAV video tours. Specifically, our method comprises a vision-language
retrieval pipeline for initial waypoint selection, a preference-based Bayesian
optimization framework that refines poses using aesthetic feedback, and a
motion planner that generates safe quadrotor trajectories. We validate ACDC
through both simulation and hardware-in-the-loop experiments, demonstrating
that it robustly produces professional-quality footage across diverse indoor
scenes without requiring expertise in robotics or cinematography. These results
highlight the potential of embodied AI agents to close the loop from
open-vocabulary dialogue to real-world autonomous aerial cinematography.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [329] [Evaluating the Limitations of Local LLMs in Solving Complex Programming Challenges](https://arxiv.org/abs/2509.15283)
*Kadin Matotek,Heather Cassel,Md Amiruzzaman,Linh B. Ngo*

Main category: cs.SE

TL;DR: The study evaluates locally hosted large-language models for programming tasks, showing they lag behind proprietary models but highlighting progress in open models.


<details>
  <summary>Details</summary>
Motivation: Understand and benchmark the performance of locally hosted and open-source large-language models on complex programming tasks.

Method: Utilized an enhanced AI evaluation framework with offline capabilities to test eight open-source models on the Kattis corpus.

Result: Local models performed at half the accuracy of proprietary models such as Gemini 1.5 and ChatGPT-4.

Conclusion: Despite limitations in performance, open-source models are improving rapidly, and the evaluation framework offers a replicable workflow for organizations.

Abstract: This study examines the performance of today's open-source, locally hosted
large-language models (LLMs) in handling complex competitive programming tasks
with extended problem descriptions and contexts. Building on the original
Framework for AI-driven Code Generation Evaluation (FACE), the authors retrofit
the pipeline to work entirely offline through the Ollama runtime, collapsing
FACE's sprawling per-problem directory tree into a handful of consolidated JSON
files, and adding robust checkpointing so multi-day runs can resume after
failures. The enhanced framework generates, submits, and records solutions for
the full Kattis corpus of 3,589 problems across eight code-oriented models
ranging from 6.7-9 billion parameters. The submission results show that the
overall pass@1 accuracy is modest for the local models, with the best models
performing at approximately half the acceptance rate of the proprietary models,
Gemini 1.5 and ChatGPT-4. These findings expose a persistent gap between
private, cost-controlled LLM deployments and state-of-the-art proprietary
services, yet also highlight the rapid progress of open models and the
practical benefits of an evaluation workflow that organizations can replicate
on in-house hardware.

</details>


### [330] [LoCaL: Countering Surface Bias in Code Evaluation Metrics](https://arxiv.org/abs/2509.15397)
*Simantika Bhattacharjee Dristi,Matthew B. Dwyer*

Main category: cs.SE

TL;DR: The paper highlights the limitations in current reference-based code evaluation metrics (CEMs) due to their focus on surface-level features, introduces a new benchmark called LoCaL to address this issue, and shows these metrics perform poorly under the benchmark.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings of existing code evaluation metrics that emphasize surface rather than functional correctness, hindering progress in software engineering tasks.

Method: Critically evaluating four state-of-the-art reference-based CEMs and developing LoCaL, a benchmark consisting of 3117 code pairs with functional similarity scores calculated using differential fuzzing.

Result: LoCaL identifies significant performance degradation in current CEMs, demonstrating their vulnerability to surface bias.

Conclusion: Exposing CEMs to LoCaL-like benchmarks can aid in developing robust metrics that emphasize functional correctness over surface-level features.

Abstract: With the increasing popularity of large language models (LLMs) and LLM-based
agents, reliable and effective code evaluation metrics (CEMs) have become
crucial for progress across several software engineering tasks. While popular
benchmarks often provide test cases to assess the correctness of generated
code, crafting and executing test cases is expensive. Reference-based CEMs
provide a cheaper alternative by scoring a candidate program based on its
functional similarity to a reference. Although prior research has focused on
reporting the weak correlation between these CEMs and functional correctness,
the causes are only assumed, and plausible solutions remain unexplored. In this
work, we critically evaluate four state-of-the-art reference-based CEMs,
revealing their strong bias towards surface-level features rather than code
functionality. Despite this surface bias, current evaluation datasets for these
CEMs rarely include code pairs that are surface-similar yet functionally
dissimilar, or functionally similar yet surface-dissimilar. To mitigate this
gap, we propose LoCaL (Looks Can Lie), a CEM evaluation benchmark, with 3117
code pairs at both the method and program levels. Each pair is labeled with a
functional similarity score and aims to target regions where CEMs are likely to
perform poorly. The functional similarity scores are calculated through
differential fuzzing, which eliminates the need for predefined test cases and,
at the same time, improves the reliability of the scores by executing an order
of magnitude more tests than prior work. We find that all four CEMs show
significant performance degradation on LoCaL, compared to the baselines.
Finally, based on our findings, we draw the implication that exposing CEMs to
LoCaL-like data might facilitate the development of metrics that are robust to
surface bias.

</details>


### [331] [Brevity is the Soul of Wit: Condensing Code Changes to Improve Commit Message Generation](https://arxiv.org/abs/2509.15567)
*Hongyu Kuang,Ning Zhang,Hui Gao,Xin Zhou,Wesley K. G. Assunção,Xiaoxing Ma,Dong Shao,Guoping Rong,He Zhang*

Main category: cs.SE

TL;DR: The paper proposes a novel approach to generating concise commit messages using text templates, outperforming existing methods based on a dataset evaluation.


<details>
  <summary>Details</summary>
Motivation: Developers often neglect to write high-quality commit messages, essential for understanding code changes and aiding software maintenance.

Method: The authors use a heuristic-based tool, ChangeScribe, to derive text templates summarizing code changes, comments, and identifiers. They fine-tune CodeLlama-7B model on these templates paired with commit messages.

Result: Their method shows significant improvements in BLEU-Norm (51.7%), METEOR (78.7%), and ROUGE-L (62.5%) compared to six baselines.

Conclusion: The proposed templates enhance pre-trained language model utilization, complementing generated commit messages effectively and offering measurable improvements.

Abstract: Commit messages are valuable resources for describing why code changes are
committed to repositories in version control systems (e.g., Git). They
effectively help developers understand code changes and better perform software
maintenance tasks. Unfortunately, developers often neglect to write
high-quality commit messages in practice. Therefore, a growing body of work is
proposed to generate commit messages automatically. These works all
demonstrated that how to organize and represent code changes is vital in
generating good commit messages, including the use of fine-grained graphs or
embeddings to better represent code changes. In this study, we choose an
alternative way to condense code changes before generation, i.e., proposing
brief yet concise text templates consisting of the following three parts: (1)
summarized code changes, (2) elicited comments, and (3) emphasized code
identifiers. Specifically, we first condense code changes by using our proposed
templates with the help of a heuristic-based tool named ChangeScribe, and then
fine-tune CodeLlama-7B on the pairs of our proposed templates and corresponding
commit messages. Our proposed templates better utilize pre-trained language
models, while being naturally brief and readable to complement generated commit
messages for developers. Our evaluation based on a widely used dataset showed
that our approach can outperform six baselines in terms of BLEU-Norm, METEOR,
and ROUGE-L, with average improvements of 51.7%, 78.7%, and 62.5%,
respectively. The ablation study and human evaluation also provide further
insights into the effectiveness of our approach.

</details>


### [332] [How Far Are We? An Empirical Analysis of Current Vulnerability Localization Approaches](https://arxiv.org/abs/2509.15777)
*Haoran Xu,Zhi Chen,Junxiao Han,Xinkui Zhao,Jianwei Yin,Shuiguang Deng*

Main category: cs.SE

TL;DR: This paper introduces a two-stage framework for detecting open-source software vulnerabilities effectively, emphasizing semantic understanding and knowledge-driven approaches.


<details>
  <summary>Details</summary>
Motivation: Scalability challenges, human error, and the limitations of existing methods necessitate an improved solution for open-source software vulnerability patch detection.

Method: The proposed approach uses a two-stage framework: version-driven candidate filtering followed by large language model-based multi-round dialogue voting.

Result: The proposed method demonstrated superior performance in detecting vulnerabilities using a dataset of 750 real cases.

Conclusion: The study validates the framework's advantages over traditional methods and highlights its practical potential for vulnerability patch detection.

Abstract: Open-source software vulnerability patch detection is a critical component
for maintaining software security and ensuring software supply chain integrity.
Traditional manual detection methods face significant scalability challenges
when processing large volumes of commit histories, while being prone to human
errors and omissions. Existing automated approaches, including heuristic-based
methods and pre-trained model solutions, suffer from limited accuracy, poor
generalization capabilities, and inherent methodological constraints that
hinder their practical deployment. To address these fundamental challenges,
this paper conducts a comprehensive empirical study of existing vulnerability
patch detection methods, revealing four key insights that guide the design of
effective solutions: the critical impact of search space reduction, the
superiority of pre-trained semantic understanding over architectural
complexity, the temporal limitations of web crawling approaches, and the
advantages of knowledge-driven methods. Based on these insights, we propose a
novel two-stage framework that combines version-driven candidate filtering with
large language model-based multi-round dialogue voting to achieve accurate and
efficient vulnerability patch identification. Extensive experiments on a
dataset containing 750 real vulnerabilities demonstrate that our method
outperforms current approaches.

</details>


### [333] [Failure Modes and Effects Analysis: An Experience from the E-Bike Domain](https://arxiv.org/abs/2509.15893)
*Andrea Bombarda,Federico Conti,Marcello Minervini,Aurora Zanenga,Claudio Menghi*

Main category: cs.SE

TL;DR: This paper presents an industrial case study in the e-Bike domain, demonstrating the utility of Functional Failure Mode and Effects Analysis (FMEA) using Simulink Fault Analyzer to identify software safety breaches.


<details>
  <summary>Details</summary>
Motivation: The paper aims to validate the effectiveness of simulation-driven FMEA for identifying and assessing software faults in Cyber-Physical Systems, which is crucial for wider practical adoption in industry.

Method: The authors analyzed 13 realistic software faults in an e-Bike Cyber-Physical System using Simulink Fault Analyzer, evaluated their models through expert feedback, and assessed fault detection capabilities.

Result: The study found that 38.4% of the faults revealed unexpected effects not anticipated by engineers, which enhanced the accuracy of fault models and improved system safety analysis.

Conclusion: Simulation-driven FMEA, exemplified by Simulink Fault Analyzer, proves effective in identifying and analyzing software faults. It offers valuable insights for engineers and safety analysts by improving models and uncovering new safety risks.

Abstract: Software failures can have catastrophic and costly consequences. Functional
Failure Mode and Effects Analysis (FMEA) is a standard technique used within
Cyber-Physical Systems (CPS) to identify software failures and assess their
consequences. Simulation-driven approaches have recently been shown to be
effective in supporting FMEA. However, industries need evidence of the
effectiveness of these approaches to increase practical adoption. This
industrial paper presents our experience with using FMEA to analyze the safety
of a CPS from the e-Bike domain. We used Simulink Fault Analyzer, an industrial
tool that supports engineers with FMEA. We identified 13 realistic faults,
modeled them, and analyzed their effects. We sought expert feedback to analyze
the appropriateness of our models and the effectiveness of the faults in
detecting safety breaches. Our results reveal that for the faults we
identified, our models were accurate or contained minor imprecision that we
subsequently corrected. They also confirm that FMEA helps engineers improve
their models. Specifically, the output provided by the simulation-driven
support for 38.4% (5 out of 13) of the faults did not match the engineers'
expectations, helping them discover unexpected effects of the faults. We
present a thorough discussion of our results and ten lessons learned. Our
findings are useful for software engineers who work as Simulink engineers, use
the Simulink Fault Analyzer, or work as safety analysts.

</details>


### [334] [LeakageDetector 2.0: Analyzing Data Leakage in Jupyter-Driven Machine Learning Pipelines](https://arxiv.org/abs/2509.15971)
*Owen Truong,Terrence Zhang,Arnav Marchareddy,Ryan Lee,Jeffery Busold,Michael Socas,Eman Abdullah AlOmar*

Main category: cs.SE

TL;DR: The paper introduces a VS Code extension, LeakageDetector, to detect and correct Data Leakage issues in ML models.


<details>
  <summary>Details</summary>
Motivation: To improve code quality in ML development by addressing the problem of Data Leakage that misleads performance evaluation.

Method: Developed a VS Code extension, LeakageDetector, to identify specific leakage types in Jupyter notebooks and included two correction mechanisms: manual quick fixes and LLM-guided solutions.

Result: LeakageDetector successfully detects and corrects issues like Overlap, Preprocessing, and Multi-test leakage.

Conclusion: LeakageDetector enhances ML pipeline practices and assists developers in maintaining cleaner code by addressing Data Leakage problems effectively.

Abstract: In software development environments, code quality is crucial. This study
aims to assist Machine Learning (ML) engineers in enhancing their code by
identifying and correcting Data Leakage issues within their models. Data
Leakage occurs when information from the test dataset is inadvertently included
in the training data when preparing a data science model, resulting in
misleading performance evaluations. ML developers must carefully separate their
data into training, evaluation, and test sets to avoid introducing Data Leakage
into their code. In this paper, we develop a new Visual Studio Code (VS Code)
extension, called LeakageDetector, that detects Data Leakage, mainly Overlap,
Preprocessing and Multi-test leakage, from Jupyter Notebook files. Beyond
detection, we included two correction mechanisms: a conventional approach,
known as a quick fix, which manually fixes the leakage, and an LLM-driven
approach that guides ML developers toward best practices for building ML
pipelines.

</details>


### [335] [Software Development Aspects of Integrating Linear Algebra Libraries](https://arxiv.org/abs/2509.16081)
*Marcel Koch,Tobias Ribizel,Pratik Nayak,Fritz Göbel,Gregor Olenik,Terry Cojean*

Main category: cs.SE

TL;DR: The paper examines the impact and benefits of adopting the Ginkgo library for optimized sparse numerical linear algebra in simulation software across diverse application domains.


<details>
  <summary>Details</summary>
Motivation: Simulation applications often rely on external libraries for specialized tasks in unfamiliar domains. Ginkgo offers optimized support for sparse numerical linear algebra, which is crucial for improving simulation performance.

Method: The study analyzes Ginkgo’s integration into simulation software in multiple domains (CFD, power grid simulation, electro-cardiophysiology), focusing on its impact from a software engineering perspective.

Result: Ginkgo integration leads to enhanced simulation performance and better transition to modern systems while highlighting sustainable software development practices.

Conclusion: Ginkgo demonstrates practical advantages for diverse scientific applications, driving software sustainability and performance improvements in numerical linear algebra.

Abstract: Many scientific discoveries are made through, or aided by, the use of
simulation software. These sophisticated software applications are not built
from the ground up, instead they rely on smaller parts for specific use cases,
usually from domains unfamiliar to the application scientists. The software
library Ginkgo is one of these building blocks to handle sparse numerical
linear algebra on different platforms. By using Ginkgo, applications are able
to ease the transition to modern systems, and speed up their simulations
through faster numerical linear algebra routines. This paper discusses the
challenges and benefits for application software in adopting Ginkgo. It will
present examples from different domains, such as CFD, power grid simulation, as
well as electro-cardiophysiology. For these cases, the impact of the
integrations on the application code is discussed from a software engineering
standpoint, and in particular, the approaches taken by Ginkgo and the
applications to enable sustainable software development are highlighted.

</details>


### [336] [When Bugs Linger: A Study of Anomalous Resolution Time Outliers and Their Themes](https://arxiv.org/abs/2509.16140)
*Avinash Patil*

Main category: cs.SE

TL;DR: The study analyzes anomalies in bug resolution times across seven open-source projects using statistical and clustering methods to find trends and common themes.


<details>
  <summary>Details</summary>
Motivation: To address delays in bug resolution and identify process inefficiencies or complex issues.

Method: Identified anomalies in bug resolution times using Z-score and IQR; used TF-IDF and KMeans clustering for thematic analysis.

Result: Consistent patterns were found, with anomalies clustering around test failures, enhancement requests, and UI issues.

Conclusion: The study offers actionable insights for maintainers to improve prioritization and resolution of persistent bugs.

Abstract: Efficient bug resolution is critical for maintaining software quality and
user satisfaction. However, specific bug reports experience unusually long
resolution times, which may indicate underlying process inefficiencies or
complex issues. This study presents a comprehensive analysis of bug resolution
anomalies across seven prominent open-source repositories: Cassandra, Firefox,
Hadoop, HBase, SeaMonkey, Spark, and Thunderbird. Utilizing statistical methods
such as Z-score and Interquartile Range (IQR), we identify anomalies in bug
resolution durations. To understand the thematic nature of these anomalies, we
apply Term Frequency-Inverse Document Frequency (TF-IDF) for textual feature
extraction and KMeans clustering to group similar bug summaries. Our findings
reveal consistent patterns across projects, with anomalies often clustering
around test failures, enhancement requests, and user interface issues. This
approach provides actionable insights for project maintainers to prioritize and
effectively address long-standing bugs.

</details>


### [337] [MatchFixAgent: Language-Agnostic Autonomous Repository-Level Code Translation Validation and Repair](https://arxiv.org/abs/2509.16187)
*Ali Reza Ibrahimzada,Brandon Paulsen,Reyhaneh Jabbarvand,Joey Dodds,Daniel Kroening*

Main category: cs.SE

TL;DR: MatchFixAgent is a large language model-based framework designed for validating and repairing code translations across programming languages (PLs) using a multi-agent architecture, outperforming prior methods in accuracy and repair capabilities.


<details>
  <summary>Details</summary>
Motivation: Validating and repairing code translation is vital to ensure functional equivalence, yet existing tools struggle to generalize across many PLs and depend on limited test suites.

Method: MatchFixAgent uses a multi-agent architecture where semantic analysis is divided into sub-tasks, tests are generated and executed, bugs are repaired, and a verdict agent provides a final decision on equivalence.

Result: MatchFixAgent achieved 99.2% (in)equivalence verdicts for tested translations and surpassed prior systems in both accuracy (correcting validation errors 60.7% of the time) and repair effectiveness (fixing 50.6% compared to 18.5%).

Conclusion: MatchFixAgent provides a robust and adaptable solution for validating and repairing code translations across multiple PL pairs, outperforming existing methodologies in generality and precision.

Abstract: Code translation transforms source code from one programming language (PL) to
another. Validating the functional equivalence of translation and repairing, if
necessary, are critical steps in code translation. Existing automated
validation and repair approaches struggle to generalize to many PLs due to high
engineering overhead, and they rely on existing and often inadequate test
suites, which results in false claims of equivalence and ineffective
translation repair. We develop MatchFixAgent, a large language model
(LLM)-based, PL-agnostic framework for equivalence validation and repair of
translations. MatchFixAgent features a multi-agent architecture that divides
equivalence validation into several sub-tasks to ensure thorough and consistent
semantic analysis of the translation. Then it feeds this analysis to test agent
to write and execute tests. Upon observing a test failure, the repair agent
attempts to fix the translation bug. The final (in)equivalence decision is made
by the verdict agent, considering semantic analyses and test execution results.
  We compare MatchFixAgent's validation and repair results with four
repository-level code translation techniques. We use 2,219 translation pairs
from their artifacts, which cover 6 PL pairs, and are collected from 24 GitHub
projects totaling over 900K lines of code. Our results demonstrate that
MatchFixAgent produces (in)equivalence verdicts for 99.2% of translation pairs,
with the same equivalence validation result as prior work on 72.8% of them.
When MatchFixAgent's result disagrees with prior work, we find that 60.7% of
the time MatchFixAgent's result is actually correct. In addition, we show that
MatchFixAgent can repair 50.6% of inequivalent translation, compared to prior
work's 18.5%. This demonstrates that MatchFixAgent is far more adaptable to
many PL pairs than prior work, while producing highly accurate validation
results.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [338] [Incorporating Visual Cortical Lateral Connection Properties into CNN: Recurrent Activation and Excitatory-Inhibitory Separation](https://arxiv.org/abs/2509.15460)
*Jin Hyun Park,Cheng Zhang,Yoonsuck Choe*

Main category: q-bio.NC

TL;DR: This paper integrates lateral connections into standard Convolutional Neural Networks (CNNs), addressing a key feature of the mammalian visual system missing in modern CNN architectures.


<details>
  <summary>Details</summary>
Motivation: Current CNN architectures lack lateral (horizontal) connections analogous to those found within the mammalian visual system, limiting their alignment and understanding of biological visual computation.

Method: The authors introduce lateral connections in CNNs using recurrent activation with weight sharing and develop a custom loss function for separating excitatory and inhibitory weights.

Result: Integrating lateral connections enhanced classification accuracy and revealed activation and connection properties resembling those of the biological visual system.

Conclusion: The study bridges the gap between CNN architecture and the mammalian visual system, offering insights into biological visual computation principles while improving CNN performance.

Abstract: The original Convolutional Neural Networks (CNNs) and their modern updates
such as the ResNet are heavily inspired by the mammalian visual system. These
models include afferent connections (retina and LGN to the visual cortex) and
long-range projections (connections across different visual cortical areas).
However, in the mammalian visual system, there are connections within each
visual cortical area, known as lateral (or horizontal) connections. These would
roughly correspond to connections within CNN feature maps, and this important
architectural feature is missing in current CNN models. In this paper, we
present how such lateral connections can be modeled within the standard CNN
framework, and test its benefits and analyze its emergent properties in
relation to the biological visual system. We will focus on two main
architectural features of lateral connections: (1) recurrent activation and (2)
separation of excitatory and inhibitory connections. We show that recurrent CNN
using weight sharing is equivalent to lateral connections, and propose a custom
loss function to separate excitatory and inhibitory weights. The addition of
these two leads to increased classification accuracy, and importantly, the
activation properties and connection properties of the resulting model show
properties similar to those observed in the biological visual system. We expect
our approach to help align CNN closer to its biological counterpart and better
understand the principles of visual cortical computation.

</details>


### [339] [Overcoming Output Dimension Collapse: How Sparsity Enables Zero-shot Brain-to-Image Reconstruction at Small Data Scales](https://arxiv.org/abs/2509.15832)
*Kenya Otsuka,Yoshihiro Nagano,Yukiyasu Kamitani*

Main category: q-bio.NC

TL;DR: The paper analyzes brain-to-image reconstruction, focusing on zero-shot prediction with limited data. Two models are studied, highlighting the challenges of the naive linear regression and the advantages of sparse linear regression.


<details>
  <summary>Details</summary>
Motivation: To understand theoretical guidelines for efficiently and accurately reconstructing visual experiences from brain activity, especially under limited training data for zero-shot prediction.

Method: The paper characterizes the behavior of two models—naive linear regression and sparse linear regression—under varying data scales. Mathematical formulas link prediction error with data scale and problem parameters.

Result: The naive linear regression model suffers from output dimension collapse at small data scales, while sparse linear regression demonstrates accurate zero-shot prediction by effectively leveraging sparsity.

Conclusion: Sparse linear regression is theoretically advantageous for zero-shot brain-to-image reconstruction, providing a guideline for avoiding errors and benefiting from variable selection.

Abstract: Advances in brain-to-image reconstruction are enabling us to externalize the
subjective visual experiences encoded in the brain as images. Achieving such
reconstruction with limited training data requires generalization beyond the
training set, a task known as zero-shot prediction. Despite its importance, we
still lack theoretical guidelines for achieving efficient and accurate
reconstruction. In this paper, we provide a theoretical analysis of two widely
used models for translating brain activity to latent image features. We define
the data scale as the ratio of the number of training samples to the latent
feature dimensionality and characterize how each model behaves across data
scales. We first show that the naive linear regression model, which uses a
shared set of input variables for all outputs, suffers from "output dimension
collapse" at small data scales, restricting generalization beyond the training
data. We then mathematically characterize the prediction error of the sparse
linear regression model by deriving formulas linking prediction error with data
scale and other problem parameters. Leveraging the sparsity of the
brain-to-feature mapping, this approach enables accurate zero-shot prediction
even at small data scales without trapping in output dimension collapse. Our
results provide a theoretical guideline for achieving zero-shot reconstruction
and highlight the benefits of variable selection in brain-to-image
reconstruction.

</details>


### [340] [A Hypothesis-First Framework for Mechanistic Model Evaluation and Selection in Neuroimaging](https://arxiv.org/abs/2509.16070)
*Dominic Boutet,Sylvain Baillet*

Main category: q-bio.NC

TL;DR: The paper presents a framework to evaluate mechanistic neuroimaging models by testing mechanistic hypotheses using feature generalization constraints, avoiding reliance on expert-level methodologies.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between statistical and bio-realistic modeling in neuroimaging by devising a framework that evaluates models rigorously without requiring specialized expertise.

Method: The approach evaluates mechanistic models by constraining them to reproduce broader or distinct features and comparing the resulting expected outputs to empirical statistical models, penalizing mis-specified or overly complex hypotheses.

Result: The framework successfully identifies and rejects incorrect mechanistic hypotheses, penalizes over-parameterization, and retains valid model specifications in synthetic experiments involving Wilson-Cowan dynamics.

Conclusion: This hypothesis-driven framework offers a practical means of deploying mechanistic models in neuroimaging, filling a critical methodological gap and avoiding the need for expert-level skills.

Abstract: Neuroimaging provides rich measurements of brain structure and neural
activity, but turning these data into mechanistic insight remains difficult.
Statistical models quantify associations without much considerations for how
they arise, whereas bio-realistic models directly embody candidate mechanisms
but remain hard to deploy rigorously without specialized training. We present a
framework that recasts modeling choices as testable mechanistic hypotheses and
supplies a simple protocol for rejecting inappropriate model specifications,
such as under-/over-parameterization or invalid simplifying assumptions, based
on predefined criteria before any parameter inference. The key idea is expected
model behavior under feature generalization constraints: instead of judging a
model solely by how well it fits a specific target feature of interest Y at an
optimal parameter set, we evaluate the model's expected Y output when the model
is constrained to reproduce a broader, or distinct, feature Z over the entire
parameter space. We then assess whether a mirror statistical model, derived
from the model's expected Y outputs, to the empirical statistical model using
standard statistics. In synthetic experiments with known ground truth (Wilson
Cowan dynamics), the framework correctly rejects mis-specified hypotheses,
penalizes unnecessary degrees of freedom, and preserves valid specifications.
This provides a practical, hypothesis-first route to using mechanistic models
for neuroimaging without requiring expert-level methodology.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [341] [SETrLUSI: Stochastic Ensemble Multi-Source Transfer Learning Using Statistical Invariant](https://arxiv.org/abs/2509.15593)
*Chunna Li,Yiwei Song,Yuanhai Shao*

Main category: stat.ML

TL;DR: This paper introduces SETrLUSI, a multi-source transfer learning framework that effectively integrates diverse knowledge using Statistical Invariant techniques, resulting in better convergence and performance.


<details>
  <summary>Details</summary>
Motivation: Traditional transfer learning methods often focus on a single type of knowledge from all domains, limiting their ability to leverage diverse and domain-specific knowledge.

Method: The paper proposes Stochastic Ensemble Multi-Source Transfer Learning Using Statistical Invariant (SETrLUSI), which employs Statistical Invariant (SI) techniques, stochastic SI selection, proportional source domain sampling, and target domain bootstrapping.

Result: Experiments demonstrate that SETrLUSI achieves good convergence, superior performance, and reduced training time compared to existing methods.

Conclusion: SETrLUSI proves to be an effective and efficient framework for multi-source transfer learning, utilizing diverse knowledge while enhancing stability and reducing computational costs.

Abstract: In transfer learning, a source domain often carries diverse knowledge, and
different domains usually emphasize different types of knowledge. Different
from handling only a single type of knowledge from all domains in traditional
transfer learning methods, we introduce an ensemble learning framework with a
weak mode of convergence in the form of Statistical Invariant (SI) for
multi-source transfer learning, formulated as Stochastic Ensemble Multi-Source
Transfer Learning Using Statistical Invariant (SETrLUSI). The proposed SI
extracts and integrates various types of knowledge from both source and target
domains, which not only effectively utilizes diverse knowledge but also
accelerates the convergence process. Further, SETrLUSI incorporates stochastic
SI selection, proportional source domain sampling, and target domain
bootstrapping, which improves training efficiency while enhancing model
stability. Experiments show that SETrLUSI has good convergence and outperforms
related methods with a lower time cost.

</details>


### [342] [Interpretable Network-assisted Random Forest+](https://arxiv.org/abs/2509.15611)
*Tiffany M. Tang,Elizaveta Levina,Ji Zhu*

Main category: stat.ML

TL;DR: The paper introduces RF+, a new family of network-assisted models that extends random forests, balancing high prediction accuracy and interpretability. It addresses challenges in leveraging network-based data dependencies in machine learning.


<details>
  <summary>Details</summary>
Motivation: To resolve the trade-off between high prediction accuracy and interpretability in network-data-based machine learning models, addressing both challenges and opportunities inherent in network dependencies.

Method: Developed RF+, a generalization of random forests, enhanced with network-assisted capabilities. It includes tools for interpretation such as global/local feature importance and network contribution measures.

Result: RF+ achieves competitive prediction accuracy while maintaining interpretability through feature importance analysis and network influence quantification.

Conclusion: RF+ broadens the usability of network-assisted machine learning, combining prediction performance and transparency, making it suitable for problems requiring interpretability.

Abstract: Machine learning algorithms often assume that training samples are
independent. When data points are connected by a network, the induced
dependency between samples is both a challenge, reducing effective sample size,
and an opportunity to improve prediction by leveraging information from network
neighbors. Multiple methods taking advantage of this opportunity are now
available, but many, including graph neural networks, are not easily
interpretable, limiting their usefulness for understanding how a model makes
its predictions. Others, such as network-assisted linear regression, are
interpretable but often yield substantially worse prediction performance. We
bridge this gap by proposing a family of flexible network-assisted models built
upon a generalization of random forests (RF+), which achieves
highly-competitive prediction accuracy and can be interpreted through feature
importance measures. In particular, we develop a suite of interpretation tools
that enable practitioners to not only identify important features that drive
model predictions, but also quantify the importance of the network contribution
to prediction. Importantly, we provide both global and local importance
measures as well as sample influence measures to assess the impact of a given
observation. This suite of tools broadens the scope and applicability of
network-assisted machine learning for high-impact problems where
interpretability and transparency are essential.

</details>


### [343] [Phase Transition for Stochastic Block Model with more than $\sqrt{n}$ Communities](https://arxiv.org/abs/2509.15822)
*Alexandra Carpentier,Christophe Giraud,Nicolas Verzelen*

Main category: stat.ML

TL;DR: This paper provides evidence supporting Chin et al.'s (2025) conjecture regarding a new recovery threshold for Stochastic Block Models (SBMs) when the number of communities $K \geq \sqrt{n}$. It extends their findings beyond a sparse regime.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing whether community recovery in Stochastic Block Models (SBMs) remains feasible below the classical Kesten-Stigum (KS) threshold when the number of communities $K$ becomes very large (specifically $K \geq \sqrt{n}$).

Method: The paper uses mathematical proofs to validate the conjecture; it demonstrates the failure of low-degree polynomials below the new threshold proposed by Chin et al. (2025), and shows that community recovery is possible in polynomial time above this threshold by counting clique occurrences in the observed graph.

Result: The paper confirms that low-degree polynomials fail below Chin et al.'s newly proposed threshold for community recovery in SBMs, and recovery is feasible above this threshold even in some moderately sparse regimes.

Conclusion: The findings validate the new threshold postulated by Chin et al. (2025) for SBMs with $K \geq \sqrt{n}$, showing that community recovery is achievable in polynomial time under certain conditions and extending the boundaries of recovery feasibility beyond the KS threshold in specific regimes.

Abstract: Predictions from statistical physics postulate that recovery of the
communities in Stochastic Block Model (SBM) is possible in polynomial time
above, and only above, the Kesten-Stigum (KS) threshold. This conjecture has
given rise to a rich literature, proving that non-trivial community recovery is
indeed possible in SBM above the KS threshold, as long as the number $K$ of
communities remains smaller than $\sqrt{n}$, where $n$ is the number of nodes
in the observed graph. Failure of low-degree polynomials below the KS threshold
was also proven when $K=o(\sqrt{n})$.
  When $K\geq \sqrt{n}$, Chin et al.(2025) recently prove that, in a sparse
regime, community recovery in polynomial time is possible below the KS
threshold by counting non-backtracking paths. This breakthrough result lead
them to postulate a new threshold for the many communities regime $K\geq
\sqrt{n}$. In this work, we provide evidences that confirm their conjecture for
$K\geq \sqrt{n}$:
  1- We prove that, for any density of the graph, low-degree polynomials fail
to recover communities below the threshold postulated by Chin et al.(2025);
  2- We prove that community recovery is possible in polynomial time above the
postulated threshold, not only in the sparse regime of~Chin et al., but also in
some (but not all) moderately sparse regimes by essentially counting clique
occurence in the observed graph.

</details>


### [344] [Model-free algorithms for fast node clustering in SBM type graphs and application to social role inference in animals](https://arxiv.org/abs/2509.15989)
*Bertrand Cloez,Adrien Cotil,Jean-Baptiste Menassol,Nicolas Verzelen*

Main category: stat.ML

TL;DR: The paper proposes new model-free algorithms for graph clustering and parameter inference in stochastic block models, extending the Lloyd algorithm with consistency guarantees and achieving faster performance.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient and accurate algorithms in community detection under the stochastic block model framework, particularly for graphs with general edge weight distributions.

Method: The authors developed a family of model-free algorithms inspired by Lloyd's k-means clustering method, ensuring consistency under certain conditions and tested through numerical experiments and empirical network data.

Result: The proposed methods demonstrated faster computations and lower estimation errors compared to state-of-the-art techniques, validated by empirical data.

Conclusion: The novel algorithms are practical, effective, and computationally efficient for clustering and inference in stochastic block models, showing broad applicability.

Abstract: We propose a novel family of model-free algorithms for node clustering and
parameter inference in graphs generated from the Stochastic Block Model (SBM),
a fundamental framework in community detection. Drawing inspiration from the
Lloyd algorithm for the $k$-means problem, our approach extends to SBMs with
general edge weight distributions. We establish the consistency of our
estimator under a natural identifiability condition. Through extensive
numerical experiments, we benchmark our methods against state-of-the-art
techniques, demonstrating significantly faster computation times with the lower
order of estimation error. Finally, we validate the practical relevance of our
algorithms by applying them to empirical network data from behavioral ecology.

</details>


### [345] [What is a good matching of probability measures? A counterfactual lens on transport maps](https://arxiv.org/abs/2509.16027)
*Lucas De Lara,Luca Ganassali*

Main category: stat.ML

TL;DR: The paper analyzes deterministic transport maps connecting probability measures, comparing three types of maps: cyclically monotone, quantile-preserving, and triangular monotone. It establishes conditions for their equivalence and links statistical transport methods to causal model-based counterfactual reasoning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to clarify the diverse constructions and interpretations of transport maps in statistical and causal inference, addressing the non-identifiability problem in deterministic transport coupling.

Method: Three concepts of transport maps are compared through rigorous conditions and equivalence proofs. Counterfactual reasoning is formalized within structural causal models to align fixed marginal transports with causal assumptions.

Result: Necessary and sufficient conditions for equivalence among the three types of transport maps are given. Additionally, causal graphs and structural equations are described to show when statistical and causal transport maps align.

Conclusion: This work enhances the theoretical framework for understanding transport maps and connects them to causal reasoning, fostering stronger ties between statistical transport and causal inference domains.

Abstract: Coupling probability measures lies at the core of many problems in statistics
and machine learning, from domain adaptation to transfer learning and causal
inference. Yet, even when restricted to deterministic transports, such
couplings are not identifiable: two atomless marginals admit infinitely many
transport maps. The common recourse to optimal transport, motivated by cost
minimization and cyclical monotonicity, obscures the fact that several distinct
notions of multivariate monotone matchings coexist. In this work, we first
carry a comparative analysis of three constructions of transport maps:
cyclically monotone, quantile-preserving and triangular monotone maps. We
establish necessary and sufficient conditions for their equivalence, thereby
clarifying their respective structural properties. In parallel, we formulate
counterfactual reasoning within the framework of structural causal models as a
problem of selecting transport maps between fixed marginals, which makes
explicit the role of untestable assumptions in counterfactual reasoning. Then,
we are able to connect these two perspectives by identifying conditions on
causal graphs and structural equations under which counterfactual maps coincide
with classical statistical transports. In this way, we delineate the
circumstances in which causal assumptions support the use of a specific
structure of transport map. Taken together, our results aim to enrich the
theoretical understanding of families of transport maps and to clarify their
possible causal interpretations. We hope this work contributes to establishing
new bridges between statistical transport and causal inference.

</details>


### [346] [A more efficient method for large-sample model-free feature screening via multi-armed bandits](https://arxiv.org/abs/2509.16085)
*Xiaxue Ouyang,Xinlai Kang,Mengyu Li,Zhenxing Dou,Jun Yu,Cheng Meng*

Main category: stat.ML

TL;DR: This paper introduces the CR-SIS and BanditCR-SIS methods for efficient feature screening in ultrahigh-dimensional data analysis, achieving lower computational costs and improved detection of nonlinear relationships.


<details>
  <summary>Details</summary>
Motivation: Existing feature screening methods struggle with computational challenges in large sample sizes and fail to effectively detect nonlinear relationships.

Method: The authors propose CR-SIS, a rank-based screening method using Chatterjee's rank correlation, and its computationally optimized version BanditCR-SIS, inspired by multi-armed bandit (MAB) problem formulation.

Result: Experimental studies on synthetic and real-world datasets demonstrate the superior performance of CR-SIS and BanditCR-SIS compared to traditional methods, with reduced computational time and improved feature detection.

Conclusion: The proposed methods achieve efficient and effective model-free feature screening with theoretical guarantees, addressing the limitations of existing approaches.

Abstract: We consider the model-free feature screening in large-scale
ultrahigh-dimensional data analysis. Existing feature screening methods often
face substantial computational challenges when dealing with large sample sizes.
To alleviate the computational burden, we propose a rank-based model-free sure
independence screening method (CR-SIS) and its efficient variant, BanditCR-SIS.
The CR-SIS method, based on Chatterjee's rank correlation, is as
straightforward to implement as the sure independence screening (SIS) method
based on Pearson correlation introduced by Fan and Lv(2008), but it is
significantly more powerful in detecting nonlinear relationships between
variables. Motivated by the multi-armed bandit (MAB) problem, we reformulate
the feature screening procedure to significantly reduce the computational
complexity of CR-SIS. For a predictor matrix of size n \times p, the
computational cost of CR-SIS is O(nlog(n)p), while BanditCR-SIS reduces this to
O(\sqrt(n)log(n)p + nlog(n)). Theoretically, we establish the sure screening
property for both CR-SIS and BanditCR-SIS under mild regularity conditions.
Furthermore, we demonstrate the effectiveness of our methods through extensive
experimental studies on both synthetic and real-world datasets. The results
highlight their superior performance compared to classical screening methods,
requiring significantly less computational time.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [347] [Kernel Model Validation: How To Do It, And Why You Should Care](https://arxiv.org/abs/2509.15244)
*Carlo Graziani,Marieme Ngom*

Main category: stat.ME

TL;DR: This paper explores the proper calibration of uncertainty in Gaussian Process (GP) models, focusing on its impact on Targeted Adaptive Design (TAD) and proposing covariance kernel validation procedures.


<details>
  <summary>Details</summary>
Motivation: To highlight the issues in the calibration of uncertainty in Gaussian Process models and how such failures impact applications like optimization algorithms.

Method: The authors propose a formal covariance kernel validation procedure leveraging the multivariate normal nature of GP predictions to ensure probabilistic calibration.

Result: The paper provides illustrative examples demonstrating GP regression misspecification in 1D models, with discussions on implications for higher dimensions.

Conclusion: Proper probabilistic calibration of GP uncertainties is critical for meaningful uncertainty quantification and model reliability in applications such as optimization.

Abstract: Gaussian Process (GP) models are popular tools in uncertainty quantification
(UQ) because they purport to furnish functional uncertainty estimates that can
be used to represent model uncertainty. It is often difficult to state with
precision what probabilistic interpretation attaches to such an uncertainty,
and in what way is it calibrated. Without such a calibration statement, the
value of such uncertainty estimates is quite limited and qualitative. We
motivate the importance of proper probabilistic calibration of GP predictions
by describing how GP predictive calibration failures can cause degraded
convergence properties in a target optimization algorithm called Targeted
Adaptive Design (TAD). We discuss the interpretation of GP-generated
uncertainty intervals in UQ, and how one may learn to trust them, through a
formal procedure for covariance kernel validation that exploits the
multivariate normal nature of GP predictions. We give simple examples of GP
regression misspecified 1-dimensional models, and discuss the situation with
respect to higher-dimensional models.

</details>


### [348] [Beyond the Average: Distributional Causal Inference under Imperfect Compliance](https://arxiv.org/abs/2509.15594)
*Undral Byambadalai,Tomu Hirata,Tatsushi Oka,Shota Yasui*

Main category: stat.ME

TL;DR: This paper develops a robust estimation method for distributional treatment effects using instrumental variables in randomized experiments with imperfect compliance.


<details>
  <summary>Details</summary>
Motivation: The study addresses challenges in randomized experiments where participants don't comply with assigned treatments, necessitating robust methods to estimate treatment effects for compliers.

Method: The authors present a regression-adjusted estimator based on a distribution regression framework using Neyman-orthogonal moment conditions, applicable to various outcome types and randomization schemes.

Result: The proposed estimator achieves semiparametric efficiency bound and performs well in simulations and a real-world application to the Oregon Health Insurance Experiment.

Conclusion: The method is effective, robust, and adaptable, offering a practical tool for analyzing treatment effects under imperfect compliance and diverse experimental conditions.

Abstract: We study the estimation of distributional treatment effects in randomized
experiments with imperfect compliance. When participants do not adhere to their
assigned treatments, we leverage treatment assignment as an instrumental
variable to identify the local distributional treatment effect-the difference
in outcome distributions between treatment and control groups for the
subpopulation of compliers. We propose a regression-adjusted estimator based on
a distribution regression framework with Neyman-orthogonal moment conditions,
enabling robustness and flexibility with high-dimensional covariates. Our
approach accommodates continuous, discrete, and mixed discrete-continuous
outcomes, and applies under a broad class of covariate-adaptive randomization
schemes, including stratified block designs and simple random sampling. We
derive the estimator's asymptotic distribution and show that it achieves the
semiparametric efficiency bound. Simulation results demonstrate favorable
finite-sample performance, and we demonstrate the method's practical relevance
in an application to the Oregon Health Insurance Experiment.

</details>


### [349] [Transfer learning under latent space model](https://arxiv.org/abs/2509.15797)
*Kuangnan Fang,Ruixuan Qin,Xinyan Fan*

Main category: stat.ME

TL;DR: This paper proposes a transfer learning method to improve latent variable estimation in networks, addressing challenges with high-dimensional latent spaces.


<details>
  <summary>Details</summary>
Motivation: Accurate estimation of latent variables in network analysis is crucial for tasks like link prediction, yet challenging due to the high number of parameters involved.

Method: A two-stage transfer learning algorithm using transferable source networks is introduced. It includes projected gradient descent algorithms for estimation, conditions for identification, and a detection algorithm for finding suitable source networks.

Result: Theoretical properties of the estimators are established, and their effectiveness is demonstrated through simulations and real dataset analyses.

Conclusion: The proposed method enhances latent variable estimation by leveraging source networks, offering theoretical robustness and practical utility.

Abstract: Latent space model plays a crucial role in network analysis, and accurate
estimation of latent variables is essential for downstream tasks such as link
prediction. However, the large number of parameters to be estimated presents a
challenge, especially when the latent space dimension is not exceptionally
small. In this paper, we propose a transfer learning method that leverages
information from networks with latent variables similar to those in the target
network, thereby improving the estimation accuracy for the target. Given
transferable source networks, we introduce a two-stage transfer learning
algorithm that accommodates differences in node numbers between source and
target networks. In each stage, we derive sufficient identification conditions
and design tailored projected gradient descent algorithms for estimation.
Theoretical properties of the resulting estimators are established. When the
transferable networks are unknown, a detection algorithm is introduced to
identify suitable source networks. Simulation studies and analyses of two real
datasets demonstrate the effectiveness of the proposed methods.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [350] [(SP)$^2$-Net: A Neural Spatial Spectrum Method for DOA Estimation](https://arxiv.org/abs/2509.15475)
*Lioz Berman,Sharon Gannot,Tom Tirer*

Main category: eess.SP

TL;DR: This paper introduces a deep learning model (SP$^2$-Net) to improve the estimation of signal directions (DOAs) from a single snapshot, outperforming traditional beamforming methods.


<details>
  <summary>Details</summary>
Motivation: Classical methods for DOA estimation, such as the Bartlett beamformer, have limitations in resolution and accuracy, especially when the array aperture is constrained, and no multiple snapshots are available.

Method: A novel deep neural network architecture and training strategy are proposed, where the model takes measurements and a hypothesis angle as input, and generates a spatial spectrum using the capabilities of a wider array.

Result: The proposed model (SP$^2$-Net) achieves higher resolution and performance compared to the standard Bartlett beamformer and sparsity-based approaches.

Conclusion: Deep learning enables more accurate and high-resolution DOA estimation from single snapshots, offering significant advancements over traditional methods.

Abstract: We consider the problem of estimating the directions of arrival (DOAs) of
multiple sources from a single snapshot of an antenna array, a task with many
practical applications. In such settings, the classical Bartlett beamformer is
commonly used, as maximum likelihood estimation becomes impractical when the
number of sources is unknown or large, and spectral methods based on the sample
covariance are not applicable due to the lack of multiple snapshots. However,
the accuracy and resolution of the Bartlett beamformer are fundamentally
limited by the array aperture. In this paper, we propose a deep learning
technique, comprising a novel architecture and training strategy, for
generating a high-resolution spatial spectrum from a single snapshot.
Specifically, we train a deep neural network that takes the measurements and a
hypothesis angle as input and learns to output a score consistent with the
capabilities of a much wider array. At inference time, a heatmap can be
produced by scanning an arbitrary set of angles. We demonstrate the advantages
of our trained model, named (SP)$^2$-Net, over the Bartlett beamformer and
sparsity-based DOA estimation methods.

</details>


### [351] [MoE-CE: Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework](https://arxiv.org/abs/2509.15964)
*Tianyu Li,Yan Xin,Jianzhong,Zhang*

Main category: eess.SP

TL;DR: The paper introduces MoE-CE, a mixture-of-experts framework to improve channel estimation in dynamic wireless environments, outperforming traditional deep learning methods.


<details>
  <summary>Details</summary>
Motivation: Reliable channel estimation is essential for communication in diverse wireless conditions, but traditional deep learning models struggle to generalize effectively in multitask and zero-shot scenarios.

Method: The proposed method, MoE-CE, uses multiple expert subnetworks tailored to distinct channel characteristics and a learned router to dynamically select experts, optimizing generalization while being model-agnostic.

Result: Experiments showed that MoE-CE consistently delivers better performance and adaptability compared to conventional deep learning approaches across varied channel conditions.

Conclusion: MoE-CE is an effective and efficient framework for channel estimation in dynamic environments, addressing the limitations of generalization in existing deep learning methods.

Abstract: Reliable channel estimation (CE) is fundamental for robust communication in
dynamic wireless environments, where models must generalize across varying
conditions such as signal-to-noise ratios (SNRs), the number of resource blocks
(RBs), and channel profiles. Traditional deep learning (DL)-based methods
struggle to generalize effectively across such diverse settings, particularly
under multitask and zero-shot scenarios. In this work, we propose MoE-CE, a
flexible mixture-of-experts (MoE) framework designed to enhance the
generalization capability of DL-based CE methods. MoE-CE provides an
appropriate inductive bias by leveraging multiple expert subnetworks, each
specialized in distinct channel characteristics, and a learned router that
dynamically selects the most relevant experts per input. This architecture
enhances model capacity and adaptability without a proportional rise in
computational cost while being agnostic to the choice of the backbone model and
the learning algorithm. Through extensive experiments on synthetic datasets
generated under diverse SNRs, RB numbers, and channel profiles, including
multitask and zero-shot evaluations, we demonstrate that MoE-CE consistently
outperforms conventional DL approaches, achieving significant performance gains
while maintaining efficiency.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [352] [All-Electric Heavy-Duty Robotic Manipulator: Actuator Configuration Optimization and Sensorless Control](https://arxiv.org/abs/2509.15778)
*Mohammad Bahari,Amir Hossein Barjini,Pauli Mustalahti,Jouni Mattila*

Main category: eess.SY

TL;DR: The paper introduces a framework integrating modeling, optimization, and sensorless control for heavy-duty robotic manipulators driven by electromechanical linear actuators (EMLAs), incorporating advanced methods like genetic algorithms and neural networks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve efficiency, accuracy, and control of heavy-duty robotic manipulators, addressing challenges in trajectory planning, actuator optimization, and sensorless force/velocity control.

Method: The framework combines mathematical modeling, trajectory mapping with safety measures, multi-objective optimization using NSGA-II, deep learning for efficiency prediction, and physics-informed surrogate models for refinement, and hierarchical control for voltages.

Result: Experimental validation on a one-degree-of-freedom EMLA testbed shows reliable trajectory tracking and sensorless control performance under variable loads.

Conclusion: The integrated approach proves successful in optimizing actuator design and control for robotic manipulators, offering a scalable, efficient solution suitable for heavy-duty applications.

Abstract: This paper presents a unified framework that integrates modeling,
optimization, and sensorless control of an all-electric heavy-duty robotic
manipulator (HDRM) driven by electromechanical linear actuators (EMLAs). An
EMLA model is formulated to capture motor electromechanics and
direction-dependent transmission efficiencies, while a mathematical model of
the HDRM, incorporating both kinematics and dynamics, is established to
generate joint-space motion profiles for prescribed TCP trajectories. A
safety-ensured trajectory generator, tailored to this model, maps Cartesian
goals to joint space while enforcing joint-limit and velocity margins. Based on
the resulting force and velocity demands, a multi-objective Non-dominated
Sorting Genetic Algorithm II (NSGA-II) is employed to select the optimal EMLA
configuration. To accelerate this optimization, a deep neural network, trained
with EMLA parameters, is embedded in the optimization process to predict
steady-state actuator efficiency from trajectory profiles. For the chosen EMLA
design, a physics-informed Kriging surrogate, anchored to the analytic model
and refined with experimental data, learns residuals of EMLA outputs to support
force and velocity sensorless control. The actuator model is further embedded
in a hierarchical virtual decomposition control (VDC) framework that outputs
voltage commands. Experimental validation on a one-degree-of-freedom EMLA
testbed confirms accurate trajectory tracking and effective sensorless control
under varying loads.

</details>


### [353] [Hierarchical Reinforcement Learning with Low-Level MPC for Multi-Agent Control](https://arxiv.org/abs/2509.15799)
*Max Studt,Georg Schildbach*

Main category: eess.SY

TL;DR: This paper presents a hierarchical method combining reinforcement learning (RL) for tactical decision-making with Model Predictive Control (MPC) for safe, low-level execution, improving reward, safety, and consistency in multi-agent control.


<details>
  <summary>Details</summary>
Motivation: Learning-based control faces challenges in sample efficiency and reliability, while model-based approaches struggle with generalization in dynamic environments. A need exists for methods that balance tactical and operational control in constraint-rich multi-agent systems.

Method: The authors propose a hierarchical framework combining RL for high-level decision-making (selecting regions of interest) and MPC for ensuring dynamically feasible and safe motions. This synergy addresses weaknesses in pure end-to-end learning and model-based methods.

Result: Experimental results on a predator-prey benchmark demonstrate superior performance in reward, safety, and consistency compared to both end-to-end learning and shielding-based RL approaches.

Conclusion: The integration of structured RL with model-based MPC enables safer and more effective control in complex environments, showcasing its viability for multi-agent systems.

Abstract: Achieving safe and coordinated behavior in dynamic, constraint-rich
environments remains a major challenge for learning-based control. Pure
end-to-end learning often suffers from poor sample efficiency and limited
reliability, while model-based methods depend on predefined references and
struggle to generalize. We propose a hierarchical framework that combines
tactical decision-making via reinforcement learning (RL) with low-level
execution through Model Predictive Control (MPC). For the case of multi-agent
systems this means that high-level policies select abstract targets from
structured regions of interest (ROIs), while MPC ensures dynamically feasible
and safe motion. Tested on a predator-prey benchmark, our approach outperforms
end-to-end and shielding-based RL baselines in terms of reward, safety, and
consistency, underscoring the benefits of combining structured learning with
model-based control.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [354] [VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency](https://arxiv.org/abs/2509.15969)
*Nikita Torgashov,Gustav Eje Henter,Gabriel Skantze*

Main category: eess.AS

TL;DR: VoXtream is a zero-shot streaming text-to-speech system offering real-time output and minimal delay of 102 ms, achieving competitive quality despite training on a mid-scale corpus.


<details>
  <summary>Details</summary>
Motivation: Develop a real-time TTS system capable of speaking from the first word with minimal latency, while maintaining competitive quality.

Method: VoXtream utilizes three transformers: incremental phoneme transformer, temporal transformer for semantic and duration prediction, and depth transformer for acoustic token generation using a monotonic alignment and dynamic look-ahead scheme.

Result: VoXtream delivers the lowest delay (102 ms on GPU) among streaming TTS systems, with performance matching or exceeding larger models despite being trained on a smaller 9k-hour corpus.

Conclusion: VoXtream provides an innovative approach to real-time TTS systems, improving initial delay and achieving high-quality outputs with mid-scale training resources.

Abstract: We present VoXtream, a fully autoregressive, zero-shot streaming
text-to-speech (TTS) system for real-time use that begins speaking from the
first word. VoXtream directly maps incoming phonemes to audio tokens using a
monotonic alignment scheme and a dynamic look-ahead that does not delay onset.
Built around an incremental phoneme transformer, a temporal transformer
predicting semantic and duration tokens, and a depth transformer producing
acoustic tokens, VoXtream achieves, to our knowledge, the lowest initial delay
among publicly available streaming TTS: 102 ms on GPU. Despite being trained on
a mid-scale 9k-hour corpus, it matches or surpasses larger baselines on several
metrics, while delivering competitive quality in both output- and
full-streaming settings. Demo and code are available at
https://herimor.github.io/voxtream.

</details>


### [355] [Breathing and Semantic Pause Detection and Exertion-Level Classification in Post-Exercise Speech](https://arxiv.org/abs/2509.15473)
*Yuyu Wang,Wuyue Xia,Huaxiu Yao,Jingping Nie*

Main category: eess.AS

TL;DR: The paper introduces a systematic approach to identify post-exercise speech pauses, distinguish their types, and classify exertion levels using various models.


<details>
  <summary>Details</summary>
Motivation: Existing works lack systematic methods to detect and differentiate pause types in post-exercise speech, despite their value in assessing physiological states.

Method: The study employs deep learning models (e.g., GRU, CNN-LSTM), acoustic features (e.g., MFCC, MFB), and Wav2Vec2 representations, evaluated through single feature, feature fusion, and two-stage setups.

Result: Detection accuracy reached up to 89% for semantic, 55% for breathing, 86% for combined pauses, and exertion-level classification achieved a notable 90.5%.

Conclusion: The research significantly enhances pause detection and exertion-level classification methodologies, surpassing prior efforts in accuracy and offering robust insights.

Abstract: Post-exercise speech contains rich physiological and linguistic cues, often
marked by semantic pauses, breathing pauses, and combined breathing-semantic
pauses. Detecting these events enables assessment of recovery rate, lung
function, and exertion-related abnormalities. However, existing works on
identifying and distinguishing different types of pauses in this context are
limited. In this work, building on a recently released dataset with
synchronized audio and respiration signals, we provide systematic annotations
of pause types. Using these annotations, we systematically conduct exploratory
breathing and semantic pause detection and exertion-level classification across
deep learning models (GRU, 1D CNN-LSTM, AlexNet, VGG16), acoustic features
(MFCC, MFB), and layer-stratified Wav2Vec2 representations. We evaluate three
setups-single feature, feature fusion, and a two-stage detection-classification
cascade-under both classification and regression formulations. Results show
per-type detection accuracy up to 89$\%$ for semantic, 55$\%$ for breathing,
86$\%$ for combined pauses, and 73$\%$overall, while exertion-level
classification achieves 90.5$\%$ accuracy, outperformin prior work.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [356] [CLASS: A Controller-Centric Layout Synthesizer for Dynamic Quantum Circuits](https://arxiv.org/abs/2509.15742)
*Yu Chen,Yilun Zhao,Bing Li,He Li,Mengdi Wang,Yinhe Han,Ying Wang*

Main category: quant-ph

TL;DR: This paper introduces CLASS, a novel layout synthesis technique for quantum computing that prioritizes minimizing inter-controller communication latency rather than focusing solely on reducing circuit depth.


<details>
  <summary>Details</summary>
Motivation: Traditional LSQC methods focus on circuit depth reduction but fail to address challenges posed by inter-controller communication latency in Distributed Quantum Circuits (DQC).

Method: CLASS uses a two-stage framework: hypergraph-based modeling and heuristic-based graph partitioning to optimize inter-controller communication in quantum systems.

Result: CLASS achieves up to 100% reduction in communication latency with an average increase of only 2.10% in additional operations.

Conclusion: CLASS represents a shift to a controller-centric approach in layout synthesis, making it efficient for DQC systems by balancing communication latency with manageable computational overhead.

Abstract: Layout Synthesis for Quantum Computing (LSQC) is a critical component of
quantum design tools. Traditional LSQC studies primarily focus on optimizing
for reduced circuit depth by adopting a device-centric design methodology.
However, these approaches overlook the impact of classical processing and
communication time, thereby being insufficient for Dynamic Quantum Circuits
(DQC).
  To address this, we introduce CLASS, a controller-centric layout synthesizer
designed to reduce inter-controller communication latency in a distributed
control system. It consists of a two-stage framework featuring a
hypergraph-based modeling and a heuristic-based graph partitioning algorithm.
Evaluations demonstrate that CLASS effectively reduces communication latency by
up to 100% with only a 2.10% average increase in the number of additional
operations.

</details>


### [357] [Quantum Generative Adversarial Autoencoders: Learning latent representations for quantum data generation](https://arxiv.org/abs/2509.16186)
*Naipunnya Raj,Rajiv Sangle,Avinash Singh,Krishna Kumar Sabapathy*

Main category: quant-ph

TL;DR: This paper proposes the Quantum Generative Adversarial Autoencoder (QGAA), combining quantum autoencoders and generative adversarial networks to generate quantum data.


<details>
  <summary>Details</summary>
Motivation: Develop methods to effectively generate quantum states, relevant for quantum chemistry and machine learning applications.

Method: The QGAA consists of a Quantum Autoencoder for state compression and a Quantum GAN to learn its latent space, enabling state generation.

Result: Generated pure entangled states and molecular ground states with low energy error (0.02 Ha for H$_2$ and 0.06 Ha for LiH) in simulations up to 6 qubits.

Conclusion: QGAA demonstrates efficient quantum state generation and paves the way for applications in quantum chemistry and quantum machine learning.

Abstract: In this work, we introduce the Quantum Generative Adversarial Autoencoder
(QGAA), a quantum model for generation of quantum data. The QGAA consists of
two components: (a) Quantum Autoencoder (QAE) to compress quantum states, and
(b) Quantum Generative Adversarial Network (QGAN) to learn the latent space of
the trained QAE. This approach imparts the QAE with generative capabilities.
The utility of QGAA is demonstrated in two representative scenarios: (a)
generation of pure entangled states, and (b) generation of parameterized
molecular ground states for H$_2$ and LiH. The average errors in the energies
estimated by the trained QGAA are 0.02 Ha for H$_2$ and 0.06 Ha for LiH in
simulations upto 6 qubits. These results illustrate the potential of QGAA for
quantum state generation, quantum chemistry, and near-term quantum machine
learning applications.

</details>


### [358] [Neural Architecture Search Algorithms for Quantum Autoencoders](https://arxiv.org/abs/2509.15451)
*Ankit Kulshrestha,Xiaoyuan Liu,Hayato Ushijima-Mwesigwa,Ilya Safro*

Main category: quant-ph

TL;DR: The paper introduces a method to automate quantum circuit design using two Quantum-NAS algorithms inspired by Neural Architecture Search, showcasing their utility in quantum data compression tasks.


<details>
  <summary>Details</summary>
Motivation: Current quantum circuit design relies heavily on manual effort and is unsustainable for complex future algorithms, leading to inefficiencies and biases.

Method: The authors propose and develop two Quantum-NAS algorithms tailored to automate circuit design, using quantum data compression as a test application.

Result: The Quantum-NAS algorithms developed outperform traditional baselines in tasks like quantum data denoising and data compression.

Conclusion: Automated Quantum-NAS methods offer a promising avenue for efficiently designing quantum circuits while reducing manual intervention and improving task-specific circuit performance.

Abstract: The design of quantum circuits is currently driven by the specific objectives
of the quantum algorithm in question. This approach thus relies on a
significant manual effort by the quantum algorithm designer to design an
appropriate circuit for the task. However this approach cannot scale to more
complex quantum algorithms in the future without exponentially increasing the
circuit design effort and introducing unwanted inductive biases. Motivated by
this observation, we propose to automate the process of cicuit design by
drawing inspiration from Neural Architecture Search (NAS). In this work, we
propose two Quantum-NAS algorithms that aim to find efficient circuits given a
particular quantum task. We choose quantum data compression as our driver
quantum task and demonstrate the performance of our algorithms by finding
efficient autoencoder designs that outperform baselines on three different
tasks - quantum data denoising, classical data compression and pure quantum
data compression. Our results indicate that quantum NAS algorithms can
significantly alleviate the manual effort while delivering performant quantum
circuits for any given task.

</details>


### [359] [AI Methods for Permutation Circuit Synthesis Across Generic Topologies](https://arxiv.org/abs/2509.16020)
*Victor Villar,Juan Cruz-Benito,Ismael Faro,David Kremer*

Main category: quant-ph

TL;DR: The paper explores using reinforcement learning to create a universal model for the synthesis of permutation circuits on various topologies, bypassing the need for topology-specific training.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and limitations of topology-specific AI models or classical heuristics in the synthesis of permutation circuits.

Method: Uses reinforcement learning (RL) with a foundational model trained on a generic rectangular lattice, combined with masking mechanisms for dynamic topology subset selection.

Result: The proposed model outperforms classical heuristics, matches specialized AI models on known topologies, and performs well even on unseen topologies. It can also be fine-tuned for specific topologies.

Conclusion: This approach enables a practical, versatile, and efficient synthesis system for diverse circuit topologies, useful for transpilation workflows.

Abstract: This paper investigates artificial intelligence (AI) methodologies for the
synthesis and transpilation of permutation circuits across generic topologies.
Our approach uses Reinforcement Learning (RL) techniques to achieve
near-optimal synthesis of permutation circuits up to 25 qubits. Rather than
developing specialized models for individual topologies, we train a
foundational model on a generic rectangular lattice, and employ masking
mechanisms to dynamically select subsets of topologies during the synthesis.
This enables the synthesis of permutation circuits on any topology that can be
embedded within the rectangular lattice, without the need to re-train the
model. In this paper we show results for 5x5 lattice and compare them to
previous AI topology-oriented models and classical methods, showing that they
outperform classical heuristics, and match previous specialized AI models, and
performs synthesis even for topologies that were not seen during training. We
further show that the model can be fine tuned to strengthen the performance for
selected topologies of interest. This methodology allows a single trained model
to efficiently synthesize circuits across diverse topologies, allowing its
practical integration into transpilation workflows.

</details>


### [360] [Triplet Loss Based Quantum Encoding for Class Separability](https://arxiv.org/abs/2509.15705)
*Marco Mordacci,Mahul Pandey,Paolo Santini,Michele Amoretti*

Main category: quant-ph

TL;DR: A novel encoding scheme enhances variational quantum classifiers by producing separable clusters in Hilbert space for classification tasks.


<details>
  <summary>Details</summary>
Motivation: To improve the performance of variational quantum classifiers by addressing the challenges posed by complex datasets like images.

Method: The scheme uses a specialized encoding circuit trained with a triplet loss function to optimize class separability in Hilbert space.

Result: Benchmarked on MNIST and MedMNIST datasets, achieving better performance than amplitude encoding with reduced circuit depth.

Conclusion: The proposed encoding effectively boosts classification performance and demonstrates potential for practical quantum machine learning applications.

Abstract: An efficient and data-driven encoding scheme is proposed to enhance the
performance of variational quantum classifiers. This encoding is specially
designed for complex datasets like images and seeks to help the classification
task by producing input states that form well-separated clusters in the Hilbert
space according to their classification labels. The encoding circuit is trained
using a triplet loss function inspired by classical facial recognition
algorithms, and class separability is measured via average trace distances
between the encoded density matrices. Benchmark tests performed on various
binary classification tasks on MNIST and MedMNIST datasets demonstrate
considerable improvement over amplitude encoding with the same VQC structure
while requiring a much lower circuit depth.

</details>


### [361] [Impact of Single Rotations and Entanglement Topologies in Quantum Neural Networks](https://arxiv.org/abs/2509.15722)
*Marco Mordacci,Michele Amoretti*

Main category: quant-ph

TL;DR: This study investigates the performance of different Variational Quantum Circuits (VQCs) focusing on circuit structure, gate choices, and their effectiveness across various quantum machine learning tasks.


<details>
  <summary>Details</summary>
Motivation: To identify the optimal construction strategy for Quantum Neural Networks by analyzing how circuit design affects performance in tasks such as generating probability distributions, image generation, and image classification.

Method: The study compares two types of circuits: one with alternating rotation and entanglement layers and another similar circuit with an added rotation layer at the end. It evaluates these circuits using various combinations of rotation sequences and four entanglement topologies: linear, circular, pairwise, and full.

Result: Results show correlations between circuit expressibility and entanglement capability with their effectiveness on quantum tasks, providing insights into the impact of design choices on VQC performance.

Conclusion: The analysis concludes that circuit design, especially in terms of rotation layers and entanglement topologies, significantly impacts quantum machine learning performance, aiding in the strategic construction of Quantum Neural Networks.

Abstract: In this work, an analysis of the performance of different Variational Quantum
Circuits is presented, investigating how it changes with respect to
entanglement topology, adopted gates, and Quantum Machine Learning tasks to be
performed. The objective of the analysis is to identify the optimal way to
construct circuits for Quantum Neural Networks. In the presented experiments,
two types of circuits are used: one with alternating layers of rotations and
entanglement, and the other, similar to the first one, but with an additional
final layer of rotations. As rotation layers, all combinations of one and two
rotation sequences are considered. Four different entanglement topologies are
compared: linear, circular, pairwise, and full. Different tasks are considered,
namely the generation of probability distributions and images, and image
classification. Achieved results are correlated with the expressibility and
entanglement capability of the different circuits to understand how these
features affect performance.

</details>


### [362] [Training Variational Quantum Circuits Using Particle Swarm Optimization](https://arxiv.org/abs/2509.15726)
*Marco Mordacci,Michele Amoretti*

Main category: quant-ph

TL;DR: The study applies Particle Swarm Optimization (PSO) to train Variational Quantum Circuits (VQCs) and demonstrates that it can achieve comparable or better image classification results compared to gradient descent methods, even with fewer quantum gates used.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the barren plateaus problem in gradient-based optimization for Variational Quantum Circuits (VQCs) by exploring Particle Swarm Optimization (PSO), which is less prone to this issue.

Method: PSO, inspired by the behavior of bird swarms, was employed to train VQCs by optimizing gate selection, target qubits, and rotation angles. The algorithm was tested across multiple medical image datasets using a restricted gate set: Rx, Ry, Rz, and CNOT.

Result: The study found that the PSO-trained VQCs achieved classification accuracy comparable to or better than predefined VQCs optimized with gradient descent, and it accomplished this with a reduced number of quantum gates.

Conclusion: PSO provides an effective alternative to gradient descent for training VQCs, overcoming challenges like barren plateaus and achieving high performance with optimized quantum resources.

Abstract: In this work, the Particle Swarm Optimization (PSO) algorithm has been used
to train various Variational Quantum Circuits (VQCs). This approach is
motivated by the fact that commonly used gradient-based optimization methods
can suffer from the barren plateaus problem. PSO is a stochastic optimization
technique inspired by the collective behavior of a swarm of birds. The
dimension of the swarm, the number of iterations of the algorithm, and the
number of trainable parameters can be set. In this study, PSO has been used to
train the entire structure of VQCs, allowing it to select which quantum gates
to apply, the target qubits, and the rotation angle, in case a rotation is
chosen. The algorithm is restricted to choosing from four types of gates: Rx,
Ry, Rz, and CNOT. The proposed optimization approach has been tested on various
datasets of the MedMNIST, which is a collection of biomedical image datasets
designed for image classification tasks. Performance has been compared with the
results achieved by classical stochastic gradient descent applied to a
predefined VQC. The results show that the PSO can achieve comparable or even
better classification accuracy across multiple datasets, despite the PSO using
a lower number of quantum gates than the VQC used with gradient descent
optimization.

</details>


### [363] [Quantum Enhanced Anomaly Detection for ADS-B Data using Hybrid Deep Learning](https://arxiv.org/abs/2509.15991)
*Rani Naaman,Felipe Gohring de Magalhaes,Jean-Yves Ouattara,Gabriela Nicolescu*

Main category: quant-ph

TL;DR: This paper explores a quantum-classical approach using a hybrid quantum neural network for anomaly detection in ADS-B data, demonstrating accuracy comparable to traditional models.


<details>
  <summary>Details</summary>
Motivation: The paper pursues using quantum machine learning to leverage quantum properties like superposition and entanglement for addressing high-dimensional data challenges in anomaly detection.

Method: A Hybrid-Fully Connected Quantum Neural Network (H-FQNN) is developed, tested with various loss functions, and applied to an ADS-B dataset for detecting anomalies. Performance is compared against traditional neural network models.

Result: The H-FQNN achieved anomaly detection accuracies between 90.17% and 94.05%, comparable to traditional models that scored between 91.50% and 93.37%.

Conclusion: Quantum-classical hybrid models are competitive in anomaly detection, validating the integration of quantum computing with machine learning for practical applications.

Abstract: The emerging field of Quantum Machine Learning (QML) has shown promising
advantages in accelerating processing speed and effectively handling the high
dimensionality associated with complex datasets. Quantum Computing (QC) enables
more efficient data manipulation through the quantum properties of
superposition and entanglement. In this paper, we present a novel approach
combining quantum and classical machine learning techniques to explore the
impact of quantum properties for anomaly detection in Automatic Dependent
Surveillance-Broadcast (ADS-B) data. We compare the performance of a
Hybrid-Fully Connected Quantum Neural Network (H-FQNN) with different loss
functions and use a publicly available ADS-B dataset to evaluate the
performance. The results demonstrate competitive performance in detecting
anomalies, with accuracies ranging from 90.17% to 94.05%, comparable to the
performance of a traditional Fully Connected Neural Network (FNN) model, which
achieved accuracies between 91.50% and 93.37%.

</details>


### [364] [Quantum Reinforcement Learning with Dynamic-Circuit Qubit Reuse and Grover-Based Trajectory Optimization](https://arxiv.org/abs/2509.16002)
*Thet Htar Su,Shaswot Shresthamali,Masaaki Kondo*

Main category: quant-ph

TL;DR: This paper introduces a fully quantum reinforcement learning framework integrating quantum Markov decision processes, dynamic circuit-based qubit reuse, and Grover's algorithm to optimize trajectories while minimizing qubit requirements.


<details>
  <summary>Details</summary>
Motivation: To address scalability and practical implementation challenges in quantum reinforcement learning by fully leveraging quantum mechanics to eliminate classical subroutines and reduce resource requirements.

Method: The method combines quantum Markov decision processes, mid-circuit measurement and qubit resetting to reuse qubits efficiently, quantum arithmetic for trajectory returns, and Grover's algorithm to identify the optimal policy. It achieves a qubit reduction from 7*T to 7 for T time steps while maintaining integrity.

Result: Simulations demonstrated a 66% reduction in qubit usage compared to traditional static designs, while preserving trajectory fidelity. Experimental validation on IBM Heron-class quantum hardware confirmed the feasibility of the approach under current quantum hardware constraints.

Conclusion: The proposed framework enhances the scalability and practicality of quantum reinforcement learning for large-scale sequential decision-making, demonstrating it is operable under noisy intermediate-scale quantum conditions.

Abstract: A fully quantum reinforcement learning framework is developed that integrates
a quantum Markov decision process, dynamic circuit-based qubit reuse, and
Grover's algorithm for trajectory optimization. The framework encodes states,
actions, rewards, and transitions entirely within the quantum domain, enabling
parallel exploration of state-action sequences through superposition and
eliminating classical subroutines. Dynamic circuit operations, including
mid-circuit measurement and reset, allow reuse of the same physical qubits
across multiple agent-environment interactions, reducing qubit requirements
from 7*T to 7 for T time steps while preserving logical continuity. Quantum
arithmetic computes trajectory returns, and Grover's search is applied to the
superposition of these evaluated trajectories to amplify the probability of
measuring those with the highest return, thereby accelerating the
identification of the optimal policy. Simulations demonstrate that the
dynamic-circuit-based implementation preserves trajectory fidelity while
reducing qubit usage by 66 percent relative to the static design. Experimental
deployment on IBM Heron-class quantum hardware confirms that the framework
operates within the constraints of current quantum processors and validates the
feasibility of fully quantum multi-step reinforcement learning under noisy
intermediate-scale quantum conditions. This framework advances the scalability
and practical application of quantum reinforcement learning for large-scale
sequential decision-making tasks.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [365] [Subset Selection for Stratified Sampling in Online Controlled Experiments](https://arxiv.org/abs/2509.15576)
*Haru Momozu,Yuki Uehara,Naoki Nishimura,Koya Ohashi,Deddy Jobson,Yilin Li,Phuong Dinh,Noriyoshi Sukegawa,Yuichi Takano*

Main category: stat.CO

TL;DR: The paper introduces an efficient subset selection method for stratified sampling that enhances the sensitivity of online controlled experiments (A/B testing) by reducing variance using stratification variables.


<details>
  <summary>Details</summary>
Motivation: Online controlled experiments, such as A/B testing, need improved statistical power to make reliable marketing decisions. Stratified sampling traditionally helps, but effective selection of stratification variables for variance reduction remains a challenge.

Method: The paper proposes an algorithm that iteratively selects stratification variables by simulating stratified sampling processes to identify effective variables for variance reduction. Computational complexity is also evaluated.

Result: Synthetic and real-world experiments showed that the proposed method performs better than other variance reduction techniques, particularly in cases where multiple variables are correlated with outcome variables.

Conclusion: The method improves sensitivity in controlled experiments, enabling more accurate marketing impact estimations through reliable stratified sampling processes.

Abstract: Online controlled experiments, also known as A/B testing, are the digital
equivalent of randomized controlled trials for estimating the impact of
marketing campaigns on website visitors. Stratified sampling is a traditional
technique for variance reduction to improve the sensitivity (or statistical
power) of controlled experiments; this technique first divides the population
into strata (homogeneous subgroups) based on stratification variables and then
draws samples from each stratum to avoid sampling bias. To enhance the
estimation accuracy of stratified sampling, we focus on the problem of
selecting a subset of stratification variables that are effective in variance
reduction. We design an efficient algorithm that selects stratification
variables one by one by simulating a series of stratified sampling processes.
We also estimate the computational complexity of our subset selection
algorithm. Computational experiments using synthetic and real-world datasets
demonstrate that our method can outperform other variance reduction techniques
especially when multiple variables have a certain correlation with the outcome
variable. Our subset selection method for stratified sampling can improve the
sensitivity of online controlled experiments, thus enabling more reliable
marketing decisions.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [366] [Generating Plans for Belief-Desire-Intention (BDI) Agents Using Alternating-Time Temporal Logic (ATL)](https://arxiv.org/abs/2509.15238)
*Dylan Léveillé*

Main category: cs.MA

TL;DR: The paper discusses a tool for automatically generating BDI agent plans using Alternating-Time Temporal Logic (ATL) to facilitate collaboration or competition in multi-agent environments.


<details>
  <summary>Details</summary>
Motivation: Manual plan generation for BDI agents is effort-intensive and often not applicable to multi-agent systems, necessitating automated and cooperative solutions.

Method: The paper leverages Alternating-Time Temporal Logic (ATL) to develop a tool that can automatically create plans for BDI agents, emphasizing inter-agent cooperation or competition.

Result: The tool was tested on a collaborative game scenario, demonstrating its ability to create effective plans that enable agents to achieve shared goals.

Conclusion: The proposed ATL-based tool successfully alleviates manual effort and supports plan generation in multi-agent BDI systems, highlighting its practical effectiveness in cooperative contexts.

Abstract: Belief-Desire-Intention (BDI) is a framework for modelling agents based on
their beliefs, desires, and intentions. Plans are a central component of BDI
agents, and define sequences of actions that an agent must undertake to achieve
a certain goal. Existing approaches to plan generation often require
significant manual effort, and are mainly focused on single-agent systems. As a
result, in this work, we have developed a tool that automatically generates BDI
plans using Alternating-Time Temporal Logic (ATL). By using ATL, the plans
generated accommodate for possible competition or cooperation between the
agents in the system. We demonstrate the effectiveness of the tool by
generating plans for an illustrative game that requires agent collaboration to
achieve a shared goal. We show that the generated plans allow the agents to
successfully attain this goal.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [367] [Hornet Node and the Hornet DSL: A Minimal, Executable Specification for Bitcoin Consensus](https://arxiv.org/abs/2509.15754)
*Toby Sharp*

Main category: cs.CR

TL;DR: The paper introduces a compact, executable specification and a domain-specific language (Hornet DSL) for Bitcoin's consensus rules, facilitating formal verification, education, and experimentation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create a formal specification for Bitcoin's consensus rules to verify them across different implementations and reduce consensus bugs, ultimately enhancing decentralization.

Method: The authors propose a declarative C++ specification and the Hornet DSL to encode Bitcoin’s rules unambiguously, paired with the spec-driven Hornet Node implemented for modularity and clarity.

Result: Hornet Node synchronizes Bitcoin's mainnet quickly, while its design allows for formal reasoning, consensus code generation, and adversarial testing.

Conclusion: Hornet DSL and Hornet Node together provide a feasible approach to a precise and executable Bitcoin consensus specification, beneficial for educational, experimental, and formal verification purposes.

Abstract: Bitcoin's consensus rules are encoded in the implementation of its reference
client: "The code is the spec." Yet this code is unsuitable for formal
verification due to side effects, mutable state, concurrency, and legacy
design. A standalone formal specification would enable verification both across
versions of the reference client and against new client implementations,
strengthening decentralization by reducing the risk of consensus-splitting
bugs. Yet such a specification has long been considered intractable given the
complexity of Bitcoin's consensus logic. We demonstrate a compact, executable,
declarative C++ specification of Bitcoin consensus rules that syncs mainnet to
tip in a few hours on a single thread. We also introduce the Hornet
Domain-Specific Language (DSL) specifically designed to encode these rules
unambiguously for execution, enabling formal reasoning, consensus code
generation, and AI-driven adversarial testing. Our spec-driven client Hornet
Node offers a modern and modular complement to the reference client. Its clear,
idiomatic style makes it suitable for education, while its performance makes it
ideal for experimentation. We highlight architectural contributions such as its
layered design, efficient data structures, and strong separation of concerns,
supported by production-quality code examples. We argue that Hornet Node and
Hornet DSL together provide the first credible path toward a pure, formal,
executable specification of Bitcoin consensus.

</details>


### [368] [How Exclusive are Ethereum Transactions? Evidence from non-winning blocks](https://arxiv.org/abs/2509.16052)
*Vabuk Pahari,Andrea Canidio*

Main category: cs.CR

TL;DR: The paper investigates the prevalence and economic impact of exclusive transactions in Ethereum blocks, finding that such transactions dominate builder revenues.


<details>
  <summary>Details</summary>
Motivation: The study seeks to understand the dynamics and economic implications of exclusive and private transactions in Ethereum blockchain blocks.

Method: Analysis of 15,097 proposed Ethereum blockchain blocks within an 8-minute window, classifying transactions as exclusive or private, and examining transaction logs.

Result: Exclusive transactions account for 84% of fees in winning blocks, with only 7% from relationships tied to single builders. At least 77.2% of fees come from exclusive strategies even when duplicates are considered.

Conclusion: Exclusive transactions are the primary contributors to builder revenues, emphasizing their economic significance in Ethereum's ecosystem.

Abstract: We analyze 15,097 blocks proposed for inclusion in Ethereum's blockchain over
an 8-minute window on December 3, 2024, during which 38 blocks were added to
the chain. We classify transactions as exclusive -- present only in blocks from
a single builder -- or private -- absent from the public mempool but included
in blocks from multiple builders. We find that exclusive transactions account
for 84% of the total fees paid by transactions in winning blocks. Furthermore,
we show that exclusivity cannot be fully explained by exclusive relationships
between senders and builders: about 7% of all exclusive transactions included
on-chain, by value, come from senders who route exclusively to a single
builder. Analyzing transaction logs shows that some exclusive transactions are
duplicates or variations of the same strategy, but even accounting for that,
the share of the total fees paid by transactions in winning blocks is at least
77.2%. Taken together, our findings highlight that exclusive transactions are
the dominant source of builder revenues.

</details>


### [369] [An Adversarial Robust Behavior Sequence Anomaly Detection Approach Based on Critical Behavior Unit Learning](https://arxiv.org/abs/2509.15756)
*Dongyang Zhan,Kai Tan,Lin Ye,Xiangzhan Yu,Hongli Zhang,Zheng He*

Main category: cs.CR

TL;DR: The paper introduces a multilevel deep learning-based anomaly detection method focusing on behavior units to improve the robustness of malware classification against adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Malware classifiers based on sequential deep learning are vulnerable to adversarial samples that alter behavior sequences to mislead detection.

Method: The method extracts semantic-rich behavior units and analyzes their contexts using a multilevel deep learning model to detect anomalies, addressing perturbation attacks and supporting varied behavior logs.

Result: Experimental results demonstrate that the proposed method outperforms other approaches, showing superior resistance to obfuscation attacks on both API and syscall logs.

Conclusion: The approach enhances adversarial robustness, offering improved detection accuracy and robustness against local and large-scale perturbations in malware behavior sequences.

Abstract: Sequential deep learning models (e.g., RNN and LSTM) can learn the sequence
features of software behaviors, such as API or syscall sequences. However,
recent studies have shown that these deep learning-based approaches are
vulnerable to adversarial samples. Attackers can use adversarial samples to
change the sequential characteristics of behavior sequences and mislead malware
classifiers. In this paper, an adversarial robustness anomaly detection method
based on the analysis of behavior units is proposed to overcome this problem.
We extract related behaviors that usually perform a behavior intention as a
behavior unit, which contains the representative semantic information of local
behaviors and can be used to improve the robustness of behavior analysis. By
learning the overall semantics of each behavior unit and the contextual
relationships among behavior units based on a multilevel deep learning model,
our approach can mitigate perturbation attacks that target local and
large-scale behaviors. In addition, our approach can be applied to both
low-level and high-level behavior logs (e.g., API and syscall logs). The
experimental results show that our approach outperforms all the compared
methods, which indicates that our approach has better performance against
obfuscation attacks.

</details>


### [370] [Hybrid Deep Learning-Federated Learning Powered Intrusion Detection System for IoT/5G Advanced Edge Computing Network](https://arxiv.org/abs/2509.15555)
*Rasil Baidar,Sasa Maric,Robert Abbas*

Main category: cs.CR

TL;DR: A privacy-preserving federated learning-based intrusion detection system, combining CNN, BiLSTM, and autoencoder, achieves high accuracy and efficiency on IoT and 5G-Advanced devices, particularly for edge deployments.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of IoT and 5G-Advanced applications has significantly increased security vulnerabilities such as DDoS, malware, and zero-day attacks, necessitating advanced intrusion detection solutions.

Method: The researchers fused CNN, BiLSTM, and autoencoder within a federated learning framework to capture local, cross-feature interactions and reconstruction-based anomaly sensitivity, enabling training on edge devices without sharing raw data.

Result: The proposed model achieved an AUC of 99.59% and an F1 score of 97.36% on UNSW-NB15 dataset with balanced error rates. Inference time was 0.0476 ms per sample, meeting the stringent <10 ms URLLC requirement for IoT applications.

Conclusion: This study presents an effective, explainable, and drift-tolerant intrusion detection system suitable for scalable and privacy-compliant IoT security within 5G-Advanced environments.

Abstract: The exponential expansion of IoT and 5G-Advanced applications has enlarged
the attack surface for DDoS, malware, and zero-day intrusions. We propose an
intrusion detection system that fuses a convolutional neural network (CNN), a
bidirectional LSTM (BiLSTM), and an autoencoder (AE) bottleneck within a
privacy-preserving federated learning (FL) framework. The CNN-BiLSTM branch
captures local and gated cross-feature interactions, while the AE emphasizes
reconstruction-based anomaly sensitivity. Training occurs across edge devices
without sharing raw data. On UNSW-NB15 (binary), the fused model attains AUC
99.59 percent and F1 97.36 percent; confusion-matrix analysis shows balanced
error rates with high precision and recall. Average inference time is
approximately 0.0476 ms per sample on our test hardware, which is well within
the less than 10 ms URLLC budget, supporting edge deployment. We also discuss
explainability, drift tolerance, and FL considerations for compliant, scalable
5G-Advanced IoT security.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [371] [Deep Gaussian Process-based Cost-Aware Batch Bayesian Optimization for Complex Materials Design Campaigns](https://arxiv.org/abs/2509.14408)
*Sk Md Ahnaf Akif Alvi,Brent Vela,Vahid Attari,Jan Janssen,Danny Perez,Douglas Allaire,Raymundo Arroyave*

Main category: cond-mat.mtrl-sci

TL;DR: The paper proposes a deep Gaussian process-based Bayesian optimization strategy that integrates cost-awareness and parallel candidate evaluation for efficient materials discovery, demonstrated on high-entropy alloys.


<details>
  <summary>Details</summary>
Motivation: The need to optimize vast, nonlinear design spaces in materials discovery while minimizing resource allocation challenges.

Method: A cost-aware, batch Bayesian optimization framework employing deep Gaussian process surrogates and a heterotopic querying strategy to balance exploration and exploitation.

Result: The proposed framework efficiently converged to optimal formulations in fewer iterations and with better cost awareness compared to conventional methods.

Conclusion: The study demonstrates the potential of deep Gaussian process models and cost-sensitive strategies in streamlining materials discovery campaigns.

Abstract: The accelerating pace and expanding scope of materials discovery demand
optimization frameworks that efficiently navigate vast, nonlinear design spaces
while judiciously allocating limited evaluation resources. We present a
cost-aware, batch Bayesian optimization scheme powered by deep Gaussian process
(DGP) surrogates and a heterotopic querying strategy. Our DGP surrogate, formed
by stacking GP layers, models complex hierarchical relationships among
high-dimensional compositional features and captures correlations across
multiple target properties, propagating uncertainty through successive layers.
We integrate evaluation cost into an upper-confidence-bound acquisition
extension, which, together with heterotopic querying, proposes small batches of
candidates in parallel, balancing exploration of under-characterized regions
with exploitation of high-mean, low-variance predictions across correlated
properties. Applied to refractory high-entropy alloys for high-temperature
applications, our framework converges to optimal formulations in fewer
iterations with cost-aware queries than conventional GP-based BO, highlighting
the value of deep, uncertainty-aware, cost-sensitive strategies in materials
campaigns.

</details>


### [372] [An Equivariant Graph Network for Interpretable Nanoporous Materials Design](https://arxiv.org/abs/2509.15908)
*Zhenhao Zhou,Salman Bin Kashif,Dawei Feng,Jin-Hu Dou,Kaihang Shi,Tao Deng,Zhenpeng Yao*

Main category: cond-mat.mtrl-sci

TL;DR: The paper introduces a machine learning model that predicts material properties in nanoporous materials with high accuracy, interpretability, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning models for nanoporous materials fail to balance interpretability and fidelity, making it hard to link crystal geometry to properties.

Method: The authors propose a 3D periodic space sampling approach to break down large nanoporous structures into local geometrical sites for property prediction and site-wise analysis.

Result: The method demonstrates state-of-the-art accuracy and efficiency in predicting properties like gas storage, separation, and electrical conduction, while allowing the identification of key performance-driving sites.

Conclusion: The study offers a novel, interpretable, and symmetry-aware framework for designing nanoporous materials, with potential extensions to other material types.

Abstract: Nanoporous materials hold promise for diverse sustainable applications, yet
their vast chemical space poses challenges for efficient design. Machine
learning offers a compelling pathway to accelerate the exploration, but
existing models lack either interpretability or fidelity for elucidating the
correlation between crystal geometry and property. Here, we report a
three-dimensional periodic space sampling method that decomposes large
nanoporous structures into local geometrical sites for combined property
prediction and site-wise contribution quantification. Trained with a
constructed database and retrieved datasets, our model achieves
state-of-the-art accuracy and data efficiency for property prediction on gas
storage, separation, and electrical conduction. Meanwhile, this approach
enables the interpretation of the prediction and allows for accurate
identification of significant local sites for targeted properties. Through
identifying transferable high-performance sites across diverse nanoporous
frameworks, our model paves the way for interpretable, symmetry-aware
nanoporous materials design, which is extensible to other materials, like
molecular crystals and beyond.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [373] [Emotion-Aware Speech Generation with Character-Specific Voices for Comics](https://arxiv.org/abs/2509.15253)
*Zhiwen Qian,Jinhua Liang,Huan Zhang*

Main category: cs.SD

TL;DR: This paper proposes an automated system to create character-specific, emotion-aware speech from comics, enhancing the reading experience.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the comic reading experience by introducing automated voiceovers that align with a character's dialogue and emotional state.

Method: The method combines image processing for character detection and emotion recognition, a large language model for dialogue attribution and context-aware emotion analysis, and a text-to-speech model for speech synthesis with tailored voice profiles.

Result: The system allows transforming full comic volumes into speech aligned with characters’ dialogues and emotions through integrated technologies.

Conclusion: The paper demonstrates the feasibility of creating tools for generating interactive and immersive voiceovers for comics, paving the way for innovative reading experiences.

Abstract: This paper presents an end-to-end pipeline for generating character-specific,
emotion-aware speech from comics. The proposed system takes full comic volumes
as input and produces speech aligned with each character's dialogue and
emotional state. An image processing module performs character detection, text
recognition, and emotion intensity recognition. A large language model performs
dialogue attribution and emotion analysis by integrating visual information
with the evolving plot context. Speech is synthesized through a text-to-speech
model with distinct voice profiles tailored to each character and emotion. This
work enables automated voiceover generation for comics, offering a step toward
interactive and immersive comic reading experience.

</details>


### [374] [Impact of Phonetics on Speaker Identity in Adversarial Voice Attack](https://arxiv.org/abs/2509.15437)
*Daniyal Kabir Dar,Qiben Yan,Li Xiao,Arun Ross*

Main category: cs.SD

TL;DR: Adversarial audio modifications can cause transcription errors and speaker identity drift, impacting ASR and speaker verification systems.


<details>
  <summary>Details</summary>
Motivation: Address the vulnerabilities of ASR and speaker verification systems to subtle adversarial audio perturbations.

Method: Analyze phonetic impacts of adversarial audio using DeepSpeech, evaluating transcription errors and speaker identity drift across diverse target phrases.

Result: Adversarial audio leads to vowel centralization, consonant substitutions, transcription errors, and identity drift in speaker embeddings.

Conclusion: Phonetic-aware defenses are crucial for improving the robustness of ASR and speaker verification systems.

Abstract: Adversarial perturbations in speech pose a serious threat to automatic speech
recognition (ASR) and speaker verification by introducing subtle waveform
modifications that remain imperceptible to humans but can significantly alter
system outputs. While targeted attacks on end-to-end ASR models have been
widely studied, the phonetic basis of these perturbations and their effect on
speaker identity remain underexplored. In this work, we analyze adversarial
audio at the phonetic level and show that perturbations exploit systematic
confusions such as vowel centralization and consonant substitutions. These
distortions not only mislead transcription but also degrade phonetic cues
critical for speaker verification, leading to identity drift. Using DeepSpeech
as our ASR target, we generate targeted adversarial examples and evaluate their
impact on speaker embeddings across genuine and impostor samples. Results
across 16 phonetically diverse target phrases demonstrate that adversarial
audio induces both transcription errors and identity drift, highlighting the
need for phonetic-aware defenses to ensure the robustness of ASR and speaker
recognition systems.

</details>


### [375] [SightSound-R1: Cross-Modal Reasoning Distillation from Vision to Audio Language Models](https://arxiv.org/abs/2509.15661)
*Qiaolin Wang,Xilin Jiang,Linyang He,Junkai Wu,Nima Mesgarani*

Main category: cs.SD

TL;DR: The paper presents SightSound-R1, a framework to improve audio-language model reasoning by distilling vision-language model capabilities using audio-visual data.


<details>
  <summary>Details</summary>
Motivation: Large audio-language models struggle with reasoning in complex soundscapes, which lags behind vision-language models due to insufficient chain-of-thought audio data.

Method: The SightSound-R1 framework includes test-time scaling for generating audio-focused reasoning chains, audio-grounded filtering to remove hallucinations, and supervised distillation with policy optimization.

Result: SightSound-R1 enhances reasoning in audio-language models, outperforming baseline methods on both test datasets and unseen auditory scenarios.

Conclusion: Vision reasoning can be effectively transferred to audio models, leveraging audio-visual data for scalability and performance improvement.

Abstract: While large audio-language models (LALMs) have demonstrated state-of-the-art
audio understanding, their reasoning capability in complex soundscapes still
falls behind large vision-language models (LVLMs). Compared to the visual
domain, one bottleneck is the lack of large-scale chain-of-thought audio data
to teach LALM stepwise reasoning. To circumvent this data and modality gap, we
present SightSound-R1, a cross-modal distillation framework that transfers
advanced reasoning from a stronger LVLM teacher to a weaker LALM student on the
same audio-visual question answering (AVQA) dataset. SightSound-R1 consists of
three core steps: (i) test-time scaling to generate audio-focused chains of
thought (CoT) from an LVLM teacher, (ii) audio-grounded validation to filter
hallucinations, and (iii) a distillation pipeline with supervised fine-tuning
(SFT) followed by Group Relative Policy Optimization (GRPO) for the LALM
student. Results show that SightSound-R1 improves LALM reasoning performance
both in the in-domain AVQA test set as well as in unseen auditory scenes and
questions, outperforming both pretrained and label-only distilled baselines.
Thus, we conclude that vision reasoning can be effectively transferred to audio
models and scaled with abundant audio-visual data.

</details>


### [376] [Contrastive Learning with Spectrum Information Augmentation in Abnormal Sound Detection](https://arxiv.org/abs/2509.15570)
*Xinxin Meng,Jiangtao Guo,Yunxiang Zhang,Shun Huang*

Main category: cs.SD

TL;DR: The paper proposes a data augmentation method focusing on high-frequency audio information to enhance unsupervised anomaly sound detection.


<details>
  <summary>Details</summary>
Motivation: Despite the effectiveness of outlier exposure methods in anomaly sound detection, there's a need to improve the model's learning of the normal data distribution space using biological and data insights.

Method: The method enhances contrastive learning by augmenting the dataset with high-frequency audio information, encouraging the model to focus on low-frequency data that indicates normal machine operation.

Result: The proposed method outperformed other contrastive learning approaches on the DCASE 2020 Task 2 dataset and demonstrated generalizability on the DCASE 2022 Task 2 dataset.

Conclusion: Focusing the model on low-frequency information by amplifying high-frequency components in contrastive learning improves anomaly detection and generalizes well across different datasets.

Abstract: The outlier exposure method is an effective approach to address the
unsupervised anomaly sound detection problem. The key focus of this method is
how to make the model learn the distribution space of normal data. Based on
biological perception and data analysis, it is found that anomalous audio and
noise often have higher frequencies. Therefore, we propose a data augmentation
method for high-frequency information in contrastive learning. This enables the
model to pay more attention to the low-frequency information of the audio,
which represents the normal operational mode of the machine. We evaluated the
proposed method on the DCASE 2020 Task 2. The results showed that our method
outperformed other contrastive learning methods used on this dataset. We also
evaluated the generalizability of our method on the DCASE 2022 Task 2 dataset.

</details>


### [377] [Direct Simultaneous Translation Activation for Large Audio-Language Models](https://arxiv.org/abs/2509.15692)
*Pei Zhang,Yiming Wang,Jialong Tang,Baosong Yang,Rui Wang,Derek F. Wong,Fei Huang*

Main category: cs.SD

TL;DR: The paper proposes a strategy called SimulSA to activate simultaneous speech-to-text translation (Simul-S2TT) capabilities in large audio-language models (LALMs) using just 1% simultaneous data, avoiding architectural changes.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of enabling Simul-S2TT capabilities in LALMs without requiring architectural modifications or decoding strategy changes.

Method: SimulSA uses random truncation of speech and generates partially aligned translations to create simultaneous data, which is integrated into offline SFT data during pretraining to prepare models for simultaneous tasks.

Result: Experimental results show that incorporating only about 1% of simultaneous data is sufficient to activate Simul-S2TT abilities in LALMs, achieving effective real-time translation.

Conclusion: The SimulSA strategy bridges the gap between offline pretraining and simultaneous translation inference, enabling real-time translation efficiently and without additional architectural complexities.

Abstract: Simultaneous speech-to-text translation (Simul-S2TT) aims to translate speech
into target text in real time, outputting translations while receiving source
speech input, rather than waiting for the entire utterance to be spoken.
Simul-S2TT research often modifies model architectures to implement read-write
strategies. However, with the rise of large audio-language models (LALMs), a
key challenge is how to directly activate Simul-S2TT capabilities in base
models without additional architectural changes. In this paper, we introduce
{\bf Simul}taneous {\bf S}elf-{\bf A}ugmentation ({\bf SimulSA}), a strategy
that utilizes LALMs' inherent capabilities to obtain simultaneous data by
randomly truncating speech and constructing partially aligned translation. By
incorporating them into offline SFT data, SimulSA effectively bridges the
distribution gap between offline translation during pretraining and
simultaneous translation during inference. Experimental results demonstrate
that augmenting only about {\bf 1\%} of the simultaneous data, compared to the
full offline SFT data, can significantly activate LALMs' Simul-S2TT
capabilities without modifications to model architecture or decoding strategy.

</details>


### [378] [TISDiSS: A Training-Time and Inference-Time Scalable Framework for Discriminative Source Separation](https://arxiv.org/abs/2509.15666)
*Yongsheng Feng,Yuetonghui Xu,Jiehui Luo,Hongjia Liu,Xiaobing Li,Feng Yu,Wei Li*

Main category: cs.SD

TL;DR: The paper introduces TISDiSS, a scalable framework for source separation with reduced parameters and flexible inference, achieving state-of-the-art results on speech separation benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve source separation performance while reducing the dependency on increasingly large networks, which lead to higher training and deployment costs.

Method: Proposes TISDiSS, a unified framework integrating early-split multi-loss supervision, shared-parameter design, and dynamic inference repetitions to achieve adjustable speed-performance trade-offs.

Result: Achieved state-of-the-art performance in speech separation with fewer parameters, supporting low-latency applications through optimized training and design.

Conclusion: TISDiSS is a practical, scalable solution for adaptive source separation, balancing performance and computational efficiency.

Abstract: Source separation is a fundamental task in speech, music, and audio
processing, and it also provides cleaner and larger data for training
generative models. However, improving separation performance in practice often
depends on increasingly large networks, inflating training and deployment
costs. Motivated by recent advances in inference-time scaling for generative
modeling, we propose Training-Time and Inference-Time Scalable Discriminative
Source Separation (TISDiSS), a unified framework that integrates early-split
multi-loss supervision, shared-parameter design, and dynamic inference
repetitions. TISDiSS enables flexible speed-performance trade-offs by adjusting
inference depth without retraining additional models. We further provide
systematic analyses of architectural and training choices and show that
training with more inference repetitions improves shallow-inference
performance, benefiting low-latency applications. Experiments on standard
speech separation benchmarks demonstrate state-of-the-art performance with a
reduced parameter count, establishing TISDiSS as a scalable and practical
framework for adaptive source separation.

</details>


### [379] [Compose Yourself: Average-Velocity Flow Matching for One-Step Speech Enhancement](https://arxiv.org/abs/2509.15952)
*Gang Yang,Yue Lei,Wenxin Tai,Jin Wu,Jia Chen,Ting Zhong,Fan Zhou*

Main category: cs.SD

TL;DR: This paper proposes COSE, a one-step method for speech enhancement (SE) that is faster and more computationally efficient than previous models, without losing quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of multi-step diffusion and flow matching models for SE, including high computational cost and vulnerability to discretization errors.

Method: The proposed COSE framework uses a velocity composition identity to compute average velocity efficiently, reducing the high training overhead seen in MeanFlow while maintaining consistency and enhancement quality.

Result: COSE achieves up to 5x faster sampling and 40% lower training costs compared to traditional methods, with no compromise in speech quality.

Conclusion: COSE provides a computationally efficient, high-quality solution for SE, making it a significant advancement in the development of one-step generative modeling frameworks.

Abstract: Diffusion and flow matching (FM) models have achieved remarkable progress in
speech enhancement (SE), yet their dependence on multi-step generation is
computationally expensive and vulnerable to discretization errors. Recent
advances in one-step generative modeling, particularly MeanFlow, provide a
promising alternative by reformulating dynamics through average velocity
fields. In this work, we present COSE, a one-step FM framework tailored for SE.
To address the high training overhead of Jacobian-vector product (JVP)
computations in MeanFlow, we introduce a velocity composition identity to
compute average velocity efficiently, eliminating expensive computation while
preserving theoretical consistency and achieving competitive enhancement
quality. Extensive experiments on standard benchmarks show that COSE delivers
up to 5x faster sampling and reduces training cost by 40%, all without
compromising speech quality. Code is available at
https://github.com/ICDM-UESTC/COSE.

</details>


### [380] [Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data](https://arxiv.org/abs/2509.15389)
*Youngwon Choi,Jaeyoon Jung,Hyeonyu Kim,Huu-Kim Nguyen,Hwayeon Kim*

Main category: cs.SD

TL;DR: The paper explores fine-tuning schemes for Large Audio Language Models (LALMs) under limited speech data and shows that minimal speech data with curriculum learning can significantly improve performance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of exploration into fine-tuning LALMs for spoken language understanding in scenarios with limited paired speech-label data.

Method: The study systematically compares fine-tuning schemes such as text-only, direct mixing, and curriculum learning, using both abundant text-label and limited speech-label data.

Result: LALMs achieve competitive performance using text-only fine-tuning, with significant improvements by adding 2-5% speech data. Curriculum learning proves highly effective under limited data scenarios.

Conclusion: LALMs exhibit strong generalization and can be effectively fine-tuned with minimal speech data for SLU, offering practical guidance for adapting LALMs under constrained data conditions.

Abstract: Large Audio Language Models (LALMs) have emerged as powerful tools for
speech-related tasks but remain underexplored for fine-tuning, especially with
limited speech data. To bridge this gap, we systematically examine how
different fine-tuning schemes including text-only, direct mixing, and
curriculum learning affect spoken language understanding (SLU), focusing on
scenarios where text-label pairs are abundant while paired speech-label data
are limited. Results show that LALMs already achieve competitive performance
with text-only fine-tuning, highlighting their strong generalization ability.
Adding even small amounts of speech data (2-5%) yields substantial further
gains, with curriculum learning particularly effective under scarce data. In
cross-lingual SLU, combining source-language speech data with target-language
text and minimal target-language speech data enables effective adaptation.
Overall, this study provides practical insights into the LALM fine-tuning under
realistic data constraints.

</details>


### [381] [Fed-PISA: Federated Voice Cloning via Personalized Identity-Style Adaptation](https://arxiv.org/abs/2509.16010)
*Qi Wang,Shituo Ma,Guoxin Yu,Hanyang Peng,Yue Yu*

Main category: cs.SD

TL;DR: Fed-PISA is a Federated Learning framework for voice cloning that reduces communication costs and enhances personalization by introducing Low-Rank Adaptation and collaborative filtering-inspired aggregation.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of high communication costs and loss of stylistic diversity in federated learning-based voice cloning models.

Method: Fed-PISA uses a disentangled Low-Rank Adaptation (ID-LoRA for local timbre retention and style-LoRA for minimal server transmission) and introduces an aggregation inspired by collaborative filtering to tailor custom models for stylistically similar peers.

Result: Fed-PISA demonstrated improved style expressivity, naturalness, and speaker similarity over standard federated baselines with minimal communication overhead.

Conclusion: Fed-PISA offers an effective and efficient solution for personalized voice cloning in Federated Learning by balancing privacy, communication efficiency, and stylistic heterogeneity.

Abstract: Voice cloning for Text-to-Speech (TTS) aims to generate expressive and
personalized speech from text using limited data from a target speaker.
Federated Learning (FL) offers a collaborative and privacy-preserving framework
for this task, but existing approaches suffer from high communication costs and
tend to suppress stylistic heterogeneity, resulting in insufficient
personalization. To address these issues, we propose Fed-PISA, which stands for
Federated Personalized Identity-Style Adaptation. To minimize communication
costs, Fed-PISA introduces a disentangled Low-Rank Adaptation (LoRA) mechanism:
the speaker's timbre is retained locally through a private ID-LoRA, while only
a lightweight style-LoRA is transmitted to the server, thereby minimizing
parameter exchange. To harness heterogeneity, our aggregation method, inspired
by collaborative filtering, is introduced to create custom models for each
client by learning from stylistically similar peers. Experiments show that
Fed-PISA improves style expressivity, naturalness, and speaker similarity,
outperforming standard federated baselines with minimal communication costs.

</details>


### [382] [FocalCodec-Stream: Streaming Low-Bitrate Speech Coding via Causal Distillation](https://arxiv.org/abs/2509.16195)
*Luca Della Libera,Cem Subakan,Mirco Ravanelli*

Main category: cs.SD

TL;DR: FocalCodec-Stream is a neural audio codec designed for real-time applications, offering low-latency, high-quality speech compression at rates between 0.55 and 0.80 kbps.


<details>
  <summary>Details</summary>
Motivation: To address the lack of streamable neural audio codecs for real-time applications, despite recent advancements in low-bitrate reconstruction and downstream task representation.

Method: FocalCodec-Stream employs multi-stage causal distillation of WavLM and incorporates architectural innovations like a lightweight refiner module to balance quality with latency.

Result: The codec outperforms other streamable codecs in terms of quality, semantic preservation, and efficiency at similar bitrates.

Conclusion: FocalCodec-Stream achieves an effective trade-off between reconstruction quality, real-time latency, and computational efficiency, proving its usability for generative audio pipelines.

Abstract: Neural audio codecs are a fundamental component of modern generative audio
pipelines. Although recent codecs achieve strong low-bitrate reconstruction and
provide powerful representations for downstream tasks, most are non-streamable,
limiting their use in real-time applications. We present FocalCodec-Stream, a
hybrid codec based on focal modulation that compresses speech into a single
binary codebook at 0.55 - 0.80 kbps with a theoretical latency of 80 ms. Our
approach combines multi-stage causal distillation of WavLM with targeted
architectural improvements, including a lightweight refiner module that
enhances quality under latency constraints. Experiments show that
FocalCodec-Stream outperforms existing streamable codecs at comparable
bitrates, while preserving both semantic and acoustic information. The result
is a favorable trade-off between reconstruction quality, downstream task
performance, latency, and efficiency. Code and checkpoints will be released at
https://github.com/lucadellalib/focalcodec.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [383] [Collective Voice: Recovered-Peer Support Mediated by An LLM-Based Chatbot for Eating Disorder Recovery](https://arxiv.org/abs/2509.15289)
*Ryuhaerang Choi,Taehan Kim,Subin Park,Seohyeon Yoo,Jennifer G. Kim,Sung-Ju Lee*

Main category: cs.HC

TL;DR: This study designed RecoveryTeller, a chatbot with a recovered-peer persona to support people with eating disorders (EDs), and compared it with a lay-mentor chatbot, finding that RecoveryTeller offered stronger emotional resonance but required complementary use.


<details>
  <summary>Details</summary>
Motivation: To address the limited availability of peer-involved ED recovery programs and mitigate the relapse risks for recovered peers, by exploring a chatbot's potential to replicate the support of peer recovery narratives.

Method: A chatbot named RecoveryTeller with a recovered-peer persona was developed, and its emotional and epistemic impacts were examined in a 20-day cross-over deployment study with 26 ED participants, comparing it to a lay-mentor chatbot.

Result: RecoveryTeller evoked stronger emotional resonance than the lay-mentor persona chatbot; however, tensions between emotional and epistemic trust meant participants found the two personas complementary rather than interchangeable.

Conclusion: RecoveryTeller demonstrated the potential benefits of peer-style chatbot personas for ED recovery support while highlighting the need for complementary chatbot designs that balance emotional and epistemic support.

Abstract: Peer recovery narratives provide unique benefits beyond professional or lay
mentoring by fostering hope and sustained recovery in eating disorder (ED)
contexts. Yet, such support is limited by the scarcity of peer-involved
programs and potential drawbacks on recovered peers, including relapse risk. To
address this, we designed RecoveryTeller, a chatbot adopting a recovered-peer
persona that portrays itself as someone recovered from an ED. We examined
whether such a persona can reproduce the support affordances of peer recovery
narratives. We compared RecoveryTeller with a lay-mentor persona chatbot
offering similar guidance but without a recovery background. We conducted a
20-day cross-over deployment study with 26 ED participants, each using both
chatbots for 10 days. RecoveryTeller elicited stronger emotional resonance than
a lay-mentor chatbot, yet tensions between emotional and epistemic trust led
participants to view the two personas as complementary rather than substitutes.
We provide design implications for mental health chatbot persona design.

</details>


### [384] [Where Do I 'Add the Egg'?: Exploring Agency and Ownership in AI Creative Co-Writing Systems](https://arxiv.org/abs/2509.15440)
*Dashiel Carrera,Jeb Thomas-Mitchell,Daniel Wigdor*

Main category: cs.HC

TL;DR: This paper investigates how different interface metaphors in AI co-writing systems affect writer perceptions of control, agency, and authorship.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address challenges related to agency and ownership in AI creative co-writing, which hinder adoption.

Method: Three AI co-writing systems with identical functionality but different interface metaphors (agentic, tool-like, magical) were developed and tested through interviews with 18 writers.

Result: Findings revealed a taxonomy of agency and ownership types and highlighted how metaphors influence writers' perceptions of control and conceptual contributions.

Conclusion: Interface metaphors significantly shape user expectations and authorship conceptions. Recommendations for designing AI co-writing systems are provided, emphasizing metaphor-driven experiences.

Abstract: AI co-writing systems challenge long held ideals about agency and ownership
in the creative process, thereby hindering widespread adoption. In order to
address this, we investigate conceptions of agency and ownership in AI creative
co-writing. Drawing on insights from a review of commercial systems, we
developed three co-writing systems with identical functionality but distinct
interface metaphors: agentic, tool-like, and magical. Through interviews with
professional and non-professional writers (n = 18), we explored how these
metaphors influenced participants' sense of control and authorship. Our
analysis resulted in a taxonomy of agency and ownership subtypes and underscore
how tool-like metaphors shift writers' expected points of control while agentic
metaphors foreground conceptual contributions. We argue that interface
metaphors not only guide expectations of control but also frame conceptions of
authorship. We conclude with recommendations for the design of AI co-writing
systems, emphasizing how metaphor shapes user experience and creative practice.

</details>


### [385] [Subject Matter Expertise vs Professional Management in Collective Sequential Decision Making](https://arxiv.org/abs/2509.15263)
*David Shoresh,Yonatan Loewenstein*

Main category: cs.HC

TL;DR: The paper investigates whether companies should choose a subject matter expert (internal candidate) or a professional manager (external candidate) by simulating decision-making processes using chess as a model system, finding that professional managers outperform expert managers.


<details>
  <summary>Details</summary>
Motivation: The paper aims to quantitatively and objectively address the debate over whether companies should prioritize promoting internal subject matter experts or hiring external professional managers as leaders for optimal decision-making.

Method: The method involves modeling decision-making through a chess-based experiment where a team of chess players suggests moves, and a manager (either an expert or a RL-trained system) makes the final decision.

Result: The study finds that professional managers trained via reinforcement learning outperform subject matter experts, even those with high skill levels, while only acquiring minimal knowledge of chess.

Conclusion: Subject matter expertise adds limited value beyond a minimal threshold for team synergy, and professional managers trained with RL yield superior performance, lending support to the notion of hiring professional managers for leadership roles.

Abstract: Your company's CEO is retiring. You search for a successor. You can promote
an employee from the company familiar with the company's operations, or recruit
an external professional manager. Who should you prefer? It has not been clear
how to address this question, the "subject matter expertise vs. professional
manager debate", quantitatively and objectively. We note that a company's
success depends on long sequences of interdependent decisions, with
often-opposing recommendations of diverse board members. To model this task in
a controlled environment, we utilize chess - a complex, sequential game with
interdependent decisions which allows for quantitative analysis of performance
and expertise (since the states, actions and game outcomes are well-defined).
The availability of chess engines differing in style and expertise, allows
scalable experimentation. We considered a team of (computer) chess players. At
each turn, team members recommend a move and a manager chooses a
recommendation. We compared the performance of two manager types. For manager
as "subject matter expert", we used another (computer) chess player that
assesses the recommendations of the team members based on its own chess
expertise. We examined the performance of such managers at different strength
levels. To model a "professional manager", we used Reinforcement Learning (RL)
to train a network that identifies the board positions in which different team
members have relative advantage, without any pretraining in chess. We further
examined this network to see if any chess knowledge is acquired implicitly. We
found that subject matter expertise beyond a minimal threshold does not
significantly contribute to team synergy. Moreover, performance of a RL-trained
"professional" manager significantly exceeds that of even the best "expert"
managers, while acquiring only limited understanding of chess.

</details>


### [386] [Explainable AI for Maritime Autonomous Surface Ships (MASS): Adaptive Interfaces and Trustworthy Human-AI Collaboration](https://arxiv.org/abs/2509.15959)
*Zhuoyue Zhang,Haitong Xu*

Main category: cs.HC

TL;DR: This paper reviews 100 studies to analyze transparency for Maritime Autonomous Surface Ships (MASS) focusing on situation awareness, human factors, and interface design. It explores modes of operation, human unsafe actions, transparency features, and design strategies.


<details>
  <summary>Details</summary>
Motivation: To address challenges of unsafe human-automation interactions and opaque decision-making in Maritime Autonomous Surface Ships, facilitating safe and trusted navigation.

Method: Synthesizing findings from 100 studies, mapping operational modes, analyzing human-unsafe control actions, summarizing transparency strategies, and proposing regulatory implications.

Result: Key transparency features, such as decision rationales and uncertainty indicators, improve trust and understanding. Identified unsafe control points and outlined design strategies for improving interfaces, training, and regulation.

Conclusion: Adopt an adaptive transparency framework, integrating operator state estimation and explainable decision support to reduce cognitive overload and enhance safer navigation for maritime autonomy.

Abstract: Autonomous navigation in maritime domains is accelerating alongside advances
in artificial intelligence, sensing, and connectivity. Opaque decision-making
and poorly calibrated human-automation interaction remain key barriers to safe
adoption. This article synthesizes 100 studies on automation transparency for
Maritime Autonomous Surface Ships (MASS) spanning situation awareness (SA),
human factors, interface design, and regulation. We (i) map the
Guidance-Navigation-Control stack to shore-based operational modes -- remote
supervision (RSM) and remote control (RCM) -- and identify where human unsafe
control actions (Human-UCAs) concentrate in handover and emergency loops; (ii)
summarize evidence that transparency features (decision rationales,
alternatives, confidence/uncertainty, and rule-compliance indicators) improve
understanding and support trust calibration, though reliability and
predictability often dominate trust; (iii) distill design strategies for
transparency at three layers: sensor/SA acquisition and fusion, HMI/eHMI
presentation (textual/graphical overlays, color coding, conversational and
immersive UIs), and engineer-facing processes (resilient interaction design,
validation, and standardization). We integrate methods for Human-UCA
identification (STPA-Cog + IDAC), quantitative trust/SA assessment, and
operator workload monitoring, and outline regulatory and rule-based
implications including COLREGs formalization and route exchange. We conclude
with an adaptive transparency framework that couples operator state estimation
with explainable decision support to reduce cognitive overload and improve
takeover timeliness. The review highlights actionable figure-of-merit displays
(e.g., CPA/TCPA risk bars, robustness heatmaps), transparent model outputs
(rule traceability, confidence), and training pipelines (HIL/MIL, simulation)
as near-term levers for safer MASS operations.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [387] [Query-Efficient Locally Private Hypothesis Selection via the Scheffe Graph](https://arxiv.org/abs/2509.16180)
*Gautam Kamath,Alireza F. Pour,Matthew Regehr,David P. Woodruff*

Main category: cs.DS

TL;DR: The paper introduces an improved algorithm for hypothesis selection under local differential privacy, reducing query complexity from $Ω(k^2)$ to $\tilde{O}(k^{3/2})$.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inefficiency in query complexity and the need for multiple interactive rounds in previous hypothesis selection algorithms under local differential privacy.

Method: The authors developed an algorithm leveraging a novel structure called the Scheffé graph to efficiently identify the closest probability distribution in $Q$ to $p$.

Result: The algorithm achieves local differential privacy while reducing query complexity to $\tilde{O}(k^{3/2})$, improving over prior methods which required $Ω(k^2)$ queries.

Conclusion: The new approach represents a significant improvement in query complexity for hypothesis selection tasks while maintaining privacy standards, and introduces potentially versatile tools like the Scheffé graph.

Abstract: We propose an algorithm with improved query-complexity for the problem of
hypothesis selection under local differential privacy constraints. Given a set
of $k$ probability distributions $Q$, we describe an algorithm that satisfies
local differential privacy, performs $\tilde{O}(k^{3/2})$ non-adaptive queries
to individuals who each have samples from a probability distribution $p$, and
outputs a probability distribution from the set $Q$ which is nearly the closest
to $p$. Previous algorithms required either $\Omega(k^2)$ queries or many
rounds of interactive queries.
  Technically, we introduce a new object we dub the Scheff\'e graph, which
captures structure of the differences between distributions in $Q$, and may be
of more broad interest for hypothesis selection tasks.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [388] [Geometric Integration for Neural Control Variates](https://arxiv.org/abs/2509.15538)
*Daniel Meister,Takahiro Harada*

Main category: cs.GR

TL;DR: The paper investigates using multilayer perceptrons (MLPs) with piecewise linear activation for Monte Carlo variance reduction, proposing an integration method based on domain subdivision.


<details>
  <summary>Details</summary>
Motivation: Monte Carlo methods can be inefficient due to high variance. The authors aim to explore the use of neural networks as control variates to reduce this variance.

Method: The study focuses on leveraging the analytical integration of MLPs with continuous piecewise linear activation. They propose an integration domain subdivision technique using computational geometry in 2D.

Result: The authors show that MLPs, combined with their proposed integration method, can effectively serve as control variates in light transport simulation.

Conclusion: A novel method combining MLPs and domain subdivision supports the practical use of neural networks for variance reduction in Monte Carlo methods.

Abstract: Control variates are a variance-reduction technique for Monte Carlo
integration. The principle involves approximating the integrand by a function
that can be analytically integrated, and integrating using the Monte Carlo
method only the residual difference between the integrand and the
approximation, to obtain an unbiased estimate. Neural networks are universal
approximators that could potentially be used as a control variate. However, the
challenge lies in the analytic integration, which is not possible in general.
In this manuscript, we study one of the simplest neural network models, the
multilayered perceptron (MLP) with continuous piecewise linear activation
functions, and its possible analytic integration. We propose an integration
method based on integration domain subdivision, employing techniques from
computational geometry to solve this problem in 2D. We demonstrate that an MLP
can be used as a control variate in combination with our integration method,
showing applications in the light transport simulation.

</details>


### [389] [ChannelFlow-Tools: A Standardized Dataset Creation Pipeline for 3D Obstructed Channel Flows](https://arxiv.org/abs/2509.15236)
*Shubham Kavane,Kajol Kulkarni,Harald Koestler*

Main category: cs.GR

TL;DR: ChannelFlow-Tools is a framework that simplifies and standardizes the process of generating ML-ready inputs for 3D obstructed channel flows.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the difficulty of repeatable and standardized datasets for machine learning applications in computational fluid dynamics (CFD) with obstructed flows.

Method: The proposed framework integrates CAD generation, HPC solver orchestration, voxelization, and geometry checks into a unified configuration-based pipeline managed by Hydra/OmegaConf.

Result: The framework demonstrated reproducibility through creating 10,000+ scenes spanning varied Reynolds numbers and successfully supported surrogate models and ML training evaluation.

Conclusion: ChannelFlow-Tools provides a configurable pathway from CAD to standardized ML-ready outputs, addressing reproducibility issues in CFD-related machine learning datasets.

Abstract: We present ChannelFlow-Tools, a configuration-driven framework that
standardizes the end-to-end path from programmatic CAD solid generation to
ML-ready inputs and targets for 3D obstructed channel flows. The toolchain
integrates geometry synthesis with feasibility checks, signed distance field
(SDF) voxelization, automated solver orchestration on HPC (waLBerla LBM), and
Cartesian resampling to co-registered multi-resolution tensors. A single
Hydra/OmegaConf configuration governs all stages, enabling deterministic
reproduction and controlled ablations. As a case study, we generate 10k+ scenes
spanning Re=100-15000 with diverse shapes and poses. An end-to-end evaluation
of storage trade-offs directly from the emitted artifacts, a minimal 3D U-Net
at 128x32x32, and example surrogate models with dataset size illustrate that
the standardized representations support reproducible ML training.
ChannelFlow-Tools turns one-off dataset creation into a reproducible,
configurable pipeline for CFD surrogate modeling.

</details>


### [390] [GenCAD-3D: CAD Program Generation using Multimodal Latent Space Alignment and Synthetic Dataset Balancing](https://arxiv.org/abs/2509.15246)
*Nomi Yu,Md Ferdous Alam,A. John Hart,Faez Ahmed*

Main category: cs.GR

TL;DR: The paper introduces GenCAD-3D, a framework utilizing multimodal generative techniques combined with dataset augmentation (SynthBal) to improve CAD program generation from nonparametric data.


<details>
  <summary>Details</summary>
Motivation: The authors aim to simplify and automate the generation of CAD programs from nonparametric data like point clouds and meshes, addressing the challenge of requiring extensive manual intervention and overcoming limitations of small and imbalanced datasets.

Method: The paper proposes GenCAD-3D with contrastive learning to align CAD and geometry embeddings and latent diffusion models for CAD sequence generation. It also introduces SynthBal, a synthetic data augmentation strategy to balance datasets and improve complex CAD representation.

Result: Experiments demonstrate that SynthBal improves reconstruction accuracy, reduces invalid CAD models, and enhances generation performance for complex geometries, achieving better results than existing benchmarks.

Conclusion: The work advances reverse engineering and automation in engineering design processes, with plans to publicly release datasets and code for further research and collaboration.

Abstract: CAD programs, structured as parametric sequences of commands that compile
into precise 3D geometries, are fundamental to accurate and efficient
engineering design processes. Generating these programs from nonparametric data
such as point clouds and meshes remains a crucial yet challenging task,
typically requiring extensive manual intervention. Current deep generative
models aimed at automating CAD generation are significantly limited by
imbalanced and insufficiently large datasets, particularly those lacking
representation for complex CAD programs. To address this, we introduce
GenCAD-3D, a multimodal generative framework utilizing contrastive learning for
aligning latent embeddings between CAD and geometric encoders, combined with
latent diffusion models for CAD sequence generation and retrieval.
Additionally, we present SynthBal, a synthetic data augmentation strategy
specifically designed to balance and expand datasets, notably enhancing
representation of complex CAD geometries. Our experiments show that SynthBal
significantly boosts reconstruction accuracy, reduces the generation of invalid
CAD models, and markedly improves performance on high-complexity geometries,
surpassing existing benchmarks. These advancements hold substantial
implications for streamlining reverse engineering and enhancing automation in
engineering design. We will publicly release our datasets and code, including a
set of 51 3D-printed and laser-scanned parts on our project site.

</details>


### [391] [Causal Reasoning Elicits Controllable 3D Scene Generation](https://arxiv.org/abs/2509.15249)
*Shen Chen,Ruiyu Zhao,Jiale Zhou,Zongkai Wu,Jenq-Neng Hwang,Lei Li*

Main category: cs.GR

TL;DR: CausalStruct integrates causal reasoning to improve 3D scene generation's ability in handling logical dependencies and physical constraints, ensuring realistic and textually guided layouts.


<details>
  <summary>Details</summary>
Motivation: Current 3D scene generation struggles with modeling complex dependencies and adapting to dynamic, realistic scenarios.

Method: CausalStruct uses large language models to build causal graphs for objects and their attributes, and employs PID controllers along with physics-driven constraints to refine 3D scene layouts based on text or images.

Result: CausalStruct enhances the logical coherence, spatial interactions, and adaptability of generated 3D scenes in experiments.

Conclusion: By embedding causal reasoning into 3D scene generation, CausalStruct creates more realistic and dynamically adaptive 3D environments.

Abstract: Existing 3D scene generation methods often struggle to model the complex
logical dependencies and physical constraints between objects, limiting their
ability to adapt to dynamic and realistic environments. We propose
CausalStruct, a novel framework that embeds causal reasoning into 3D scene
generation. Utilizing large language models (LLMs), We construct causal graphs
where nodes represent objects and attributes, while edges encode causal
dependencies and physical constraints. CausalStruct iteratively refines the
scene layout by enforcing causal order to determine the placement order of
objects and applies causal intervention to adjust the spatial configuration
according to physics-driven constraints, ensuring consistency with textual
descriptions and real-world dynamics. The refined scene causal graph informs
subsequent optimization steps, employing a
Proportional-Integral-Derivative(PID) controller to iteratively tune object
scales and positions. Our method uses text or images to guide object placement
and layout in 3D scenes, with 3D Gaussian Splatting and Score Distillation
Sampling improving shape accuracy and rendering stability. Extensive
experiments show that CausalStruct generates 3D scenes with enhanced logical
coherence, realistic spatial interactions, and robust adaptability.

</details>


### [392] [MoAngelo: Motion-Aware Neural Surface Reconstruction for Dynamic Scenes](https://arxiv.org/abs/2509.15892)
*Mohamed Ebbed,Zorah Lähner*

Main category: cs.GR

TL;DR: The paper introduces a novel framework to achieve high-quality dynamic 3D reconstructions from multi-view videos by extending the static 3D reconstruction method NeuralAngelo.


<details>
  <summary>Details</summary>
Motivation: Current methods for dynamic scene reconstruction either produce noisy mesh results in novel-view synthesis or overly smooth meshes due to the problem's ill-posedness.

Method: The approach builds on NeuralAngelo to create a high-quality template scene from the first frame and optimizes deformation fields for tracking and refining geometry through the temporal sequence.

Result: The proposed method demonstrates superior reconstruction accuracy over state-of-the-art approaches, tested on the ActorsHQ dataset.

Conclusion: The novel method achieves more detailed and accurate dynamic 3D reconstructions, addressing issues like occlusion and topology changes.

Abstract: Dynamic scene reconstruction from multi-view videos remains a fundamental
challenge in computer vision. While recent neural surface reconstruction
methods have achieved remarkable results in static 3D reconstruction, extending
these approaches with comparable quality for dynamic scenes introduces
significant computational and representational challenges. Existing dynamic
methods focus on novel-view synthesis, therefore, their extracted meshes tend
to be noisy. Even approaches aiming for geometric fidelity often result in too
smooth meshes due to the ill-posedness of the problem. We present a novel
framework for highly detailed dynamic reconstruction that extends the static 3D
reconstruction method NeuralAngelo to work in dynamic settings. To that end, we
start with a high-quality template scene reconstruction from the initial frame
using NeuralAngelo, and then jointly optimize deformation fields that track the
template and refine it based on the temporal sequence. This flexible template
allows updating the geometry to include changes that cannot be modeled with the
deformation field, for instance occluded parts or the changes in the topology.
We show superior reconstruction accuracy in comparison to previous
state-of-the-art methods on the ActorsHQ dataset.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [393] [A CARLA-based Simulation of Electrically Driven Forklifts](https://arxiv.org/abs/2509.15909)
*David Claus,Christiane Thielemann,Hans-Georg Stark*

Main category: cs.CE

TL;DR: This paper uses the open-source tool CARLA to simulate and visualize the operation of an electric forklift fleet in a 3D warehouse scenario. It incorporates realistic tasks, localization data playback, and energy consumption analysis.


<details>
  <summary>Details</summary>
Motivation: To leverage CARLA, a known automotive simulation tool, for novel applications in warehouse logistics simulation and analysis.

Method: The authors use CARLA to create a 3D outdoor warehouse model, simulate forklift tasks like pick-and-place, optimize pathfinding, visualize historic movement data, and simulate energy consumption using a physical battery model.

Result: The simulation demonstrated forklift movements, localization data playback, SOC predictions, and supported analyses of traffic density and optimal charging station placement in a warehouse context.

Conclusion: This approach showcases CARLA's versatility for intralogistics simulation, offering advanced visualization and decision-making tools for fleet operations in warehouses.

Abstract: This paper presents the simulation of the operation of an electric forklift
fleet within an intralogistics scenario. For this purpose, the open source
simulation tool CARLA is used; according to our knowledge this is a novel
approach in the context of logistics simulation. First, CARLA is used to
generate and visualize a realistic 3D outdoor warehouse scenario, incorporating
a number of randomly moving forklifts. In a next step, intralogistics transport
tasks, such as pick-and-place, are simulated for the forklift fleet, including
shortest-path finding. Furthermore, the capability to play back localization
data, previously recorded from a ''real'' forklift fleet, is demonstrated.This
play back is done in the original recreated environment, thereby enabling the
visualization of the forklifts movements. Finally, the energy consumption of
the forklift trucks is simulated by integrating a physical battery model that
generates the state of charge (SOC) of each truck as a function of load and
activity. To demonstrate the wide range of possible applications for the CARLA
simulation platform, we describe two use cases. The first deals with the
problem of detecting regions with critically high traffic densities, the second
with optimal placement of charging stations for the forklift trucks. Both use
cases are calculated for an exemplary warehouse model.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [394] [Recent Advancements in Microscopy Image Enhancement using Deep Learning: A Survey](https://arxiv.org/abs/2509.15363)
*Debasish Dutta,Neeharika Sonowal,Risheraj Barauh,Deepjyoti Chetia,Sanjib Kr Kalita*

Main category: eess.IV

TL;DR: The paper surveys deep learning methods for microscopy image enhancement, focusing on super-resolution, reconstruction, and denoising.


<details>
  <summary>Details</summary>
Motivation: To understand the evolution, applications, challenges, and future directions of microscopy image enhancement using deep learning.

Method: The paper provides a state-of-the-art review of deep learning applications in microscopy enhancement, categorizing them into key domains: super-resolution, reconstruction, and denoising.

Result: Discusses recent trends and practical utility of deep learning methods in these domains.

Conclusion: Highlights the pivotal role of deep learning in advancing microscopy image enhancement and outlines future challenges and directions.

Abstract: Microscopy image enhancement plays a pivotal role in understanding the
details of biological cells and materials at microscopic scales. In recent
years, there has been a significant rise in the advancement of microscopy image
enhancement, specifically with the help of deep learning methods. This survey
paper aims to provide a snapshot of this rapidly growing state-of-the-art
method, focusing on its evolution, applications, challenges, and future
directions. The core discussions take place around the key domains of
microscopy image enhancement of super-resolution, reconstruction, and
denoising, with each domain explored in terms of its current trends and their
practical utility of deep learning.

</details>


### [395] [Analysis Plug-and-Play Methods for Imaging Inverse Problems](https://arxiv.org/abs/2509.15422)
*Edward P. Chandler,Shirin Shoushtari,Brendt Wohlberg,Ulugbek S. Kamilov*

Main category: eess.IV

TL;DR: The paper introduces an alternative to Standard Plug-and-Play (PnP) methods, proposing a framework where Gaussian denoisers operate in the gradient domain of images for solving imaging inverse problems.


<details>
  <summary>Details</summary>
Motivation: Plug-and-Play (PnP) methods are widely used for solving imaging inverse problems by applying Gaussian denoisers directly in the image domain. The paper seeks to explore an alternative approach by using the prior in a transformed domain, such as image gradients.

Method: The authors train a Gaussian denoiser to operate in the gradient domain of images instead of the image itself. Two novel algorithms (APnP-HQS and APnP-ADMM) are proposed to facilitate this gradient-domain prior for image reconstruction.

Result: The analysis PnP algorithms were evaluated for tasks like image deblurring and super-resolution, showing comparable performance to traditional image-domain PnP algorithms.

Conclusion: Operating with priors in the gradient domain is a feasible and effective alternative to image-domain PnP. It extends classical total variation (TV) regularization into a learned TV regularization for solving imaging problems.

Abstract: Plug-and-Play Priors (PnP) is a popular framework for solving imaging inverse
problems by integrating learned priors in the form of denoisers trained to
remove Gaussian noise from images. In standard PnP methods, the denoiser is
applied directly in the image domain, serving as an implicit prior on natural
images. This paper considers an alternative analysis formulation of PnP, in
which the prior is imposed on a transformed representation of the image, such
as its gradient. Specifically, we train a Gaussian denoiser to operate in the
gradient domain, rather than on the image itself. Conceptually, this is an
extension of total variation (TV) regularization to learned TV regularization.
To incorporate this gradient-domain prior in image reconstruction algorithms,
we develop two analysis PnP algorithms based on half-quadratic splitting
(APnP-HQS) and the alternating direction method of multipliers (APnP-ADMM). We
evaluate our approach on image deblurring and super-resolution, demonstrating
that the analysis formulation achieves performance comparable to image-domain
PnP algorithms.

</details>


### [396] [Prostate Capsule Segmentation from Micro-Ultrasound Images using Adaptive Focal Loss](https://arxiv.org/abs/2509.15595)
*Kaniz Fatema,Vaibhav Thakur,Emad A. Mohammed*

Main category: eess.IV

TL;DR: The study introduces an adaptive focal loss function for deep learning-based prostate capsule segmentation in micro-ultrasound images, outperforming existing methods by addressing ambiguous boundaries and annotation variability.


<details>
  <summary>Details</summary>
Motivation: Challenges in prostate capsule segmentation arise due to its ambiguous boundaries and variability in expert annotations, which hinders accurate visualization and cancer detection.

Method: The paper proposes integrating a baseline focal loss function with an adaptive mechanism that dynamically emphasizes regions based on their difficulty and annotation inconsistencies.

Result: The adaptive focal loss function achieves a mean dice coefficient of 0.940 and a mean Hausdorff distance of 1.949 mm, demonstrating enhanced segmentation accuracy.

Conclusion: Deploying advanced loss functions significantly improves deep learning model performance, aiding prostate cancer diagnosis and treatment optimization.

Abstract: Micro-ultrasound (micro-US) is a promising imaging technique for cancer
detection and computer-assisted visualization. This study investigates prostate
capsule segmentation using deep learning techniques from micro-US images,
addressing the challenges posed by the ambiguous boundaries of the prostate
capsule. Existing methods often struggle in such cases, motivating the
development of a tailored approach. This study introduces an adaptive focal
loss function that dynamically emphasizes both hard and easy regions, taking
into account their respective difficulty levels and annotation variability. The
proposed methodology has two primary strategies: integrating a standard focal
loss function as a baseline to design an adaptive focal loss function for
proper prostate capsule segmentation. The focal loss baseline provides a robust
foundation, incorporating class balancing and focusing on examples that are
difficult to classify. The adaptive focal loss offers additional flexibility,
addressing the fuzzy region of the prostate capsule and annotation variability
by dilating the hard regions identified through discrepancies between expert
and non-expert annotations. The proposed method dynamically adjusts the
segmentation model's weights better to identify the fuzzy regions of the
prostate capsule. The proposed adaptive focal loss function demonstrates
superior performance, achieving a mean dice coefficient (DSC) of 0.940 and a
mean Hausdorff distance (HD) of 1.949 mm in the testing dataset. These results
highlight the effectiveness of integrating advanced loss functions and adaptive
techniques into deep learning models. This enhances the accuracy of prostate
capsule segmentation in micro-US images, offering the potential to improve
clinical decision-making in prostate cancer diagnosis and treatment planning.

</details>


### [397] [Uncertainty-Gated Deformable Network for Breast Tumor Segmentation in MR Images](https://arxiv.org/abs/2509.15758)
*Yue Zhang,Jiahua Dong,Chengtao Peng,Qiuli Wang,Dan Song,Guiduo Duan*

Main category: eess.IV

TL;DR: This paper introduces a method combining CNNs and Transformers for breast tumor segmentation on MRI by addressing challenges of irregular shapes and feature integration.


<details>
  <summary>Details</summary>
Motivation: Existing segmentation methods struggle with irregular tumor shapes and balancing local and global feature integration in breast MRI imaging.

Method: The approach integrates deformable CNN and Transformer modules with a novel Uncertainty-Gated Enhancing Module (U-GEM) to adaptively exchange features based on pixel-wise uncertainty and improves boundary delineation using a Boundary-sensitive Deep Supervision Loss.

Result: The method outperforms state-of-the-art approaches with superior segmentation accuracy on two clinical breast MRI datasets.

Conclusion: This technique has strong clinical potential, providing an effective solution for precise breast tumor segmentation in MRI.

Abstract: Accurate segmentation of breast tumors in magnetic resonance images (MRI) is
essential for breast cancer diagnosis, yet existing methods face challenges in
capturing irregular tumor shapes and effectively integrating local and global
features. To address these limitations, we propose an uncertainty-gated
deformable network to leverage the complementary information from CNN and
Transformers. Specifically, we incorporates deformable feature modeling into
both convolution and attention modules, enabling adaptive receptive fields for
irregular tumor contours. We also design an Uncertainty-Gated Enhancing Module
(U-GEM) to selectively exchange complementary features between CNN and
Transformer based on pixel-wise uncertainty, enhancing both local and global
representations. Additionally, a Boundary-sensitive Deep Supervision Loss is
introduced to further improve tumor boundary delineation. Comprehensive
experiments on two clinical breast MRI datasets demonstrate that our method
achieves superior segmentation performance compared with state-of-the-art
methods, highlighting its clinical potential for accurate breast tumor
delineation.

</details>


### [398] [DPC-QA Net: A No-Reference Dual-Stream Perceptual and Cellular Quality Assessment Network for Histopathology Images](https://arxiv.org/abs/2509.15802)
*Qijun Yang,Boyang Wang,Hujun Yin*

Main category: eess.IV

TL;DR: DPC-QA Net, a no-reference dual-stream network, enhances whole slide imaging (WSI) quality evaluation by addressing staining artefacts, defocus, and cellular degradations with >92% accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the problem of common image quality issues such as staining artefacts, defocus, and cellular degradations in WSI, which impact pathological analysis.

Method: The paper introduces DPC-QA Net, which uses a dual-stream network combining wavelet-based global difference perception and cellular quality assessment through an Aggr-RWKV module. It integrates cross-attention fusion and multi-term losses to align perceptual and cellular metrics.

Result: DPC-QA Net achieves >92% accuracy in detecting quality issues across datasets. It surpasses state-of-the-art methods in NR-IQA tasks and strongly correlates predicted quality with improved cell recognition metrics in downstream tasks.

Conclusion: DPC-QA Net proves effective for practical WSI quality pre-screening, enhancing computational pathology workflows by correlating quality predictions to cell recognition and usability scores.

Abstract: Reliable whole slide imaging (WSI) hinges on image quality,yet staining
artefacts, defocus, and cellular degradations are common. We present DPC-QA
Net, a no-reference dual-stream network that couples wavelet-based global
difference perception with cellular quality assessment from nuclear and
membrane embeddings via an Aggr-RWKV module. Cross-attention fusion and
multi-term losses align perceptual and cellular cues. Across different
datasets, our model detects staining, membrane, and nuclear issues with >92%
accuracy and aligns well with usability scores; on LIVEC and KonIQ it
outperforms state-of-the-art NR-IQA. A downstream study further shows strong
positive correlations between predicted quality and cell recognition accuracy
(e.g., nuclei PQ/Dice, membrane boundary F-score), enabling practical
pre-screening of WSI regions for computational pathology.

</details>


### [399] [QWD-GAN: Quality-aware Wavelet-driven GAN for Unsupervised Medical Microscopy Images Denoising](https://arxiv.org/abs/2509.15814)
*Qijun Yang,Yating Huang,Lintao Xiang,Hujun Yin*

Main category: eess.IV

TL;DR: The paper proposes QWD-GAN, an unsupervised GAN-based method for denoising biomedical microscopy images, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To improve biomedical image denoising by addressing challenges like image detail preservation, algorithmic efficiency, and clinical interpretability.

Method: A GAN-based approach using a wavelet-driven multi-scale generator and a dual-branch discriminator incorporating feature maps and original features.

Result: Experimental validation shows notable performance, excelling in preserving high-frequency details across multiple microscopy datasets.

Conclusion: QWD-GAN offers a quality-aware, efficient denoising framework compatible with various GAN architectures.

Abstract: Image denoising plays a critical role in biomedical and microscopy imaging,
especially when acquiring wide-field fluorescence-stained images. This task
faces challenges in multiple fronts, including limitations in image acquisition
conditions, complex noise types, algorithm adaptability, and clinical
application demands. Although many deep learning-based denoising techniques
have demonstrated promising results, further improvements are needed in
preserving image details, enhancing algorithmic efficiency, and increasing
clinical interpretability. We propose an unsupervised image denoising method
based on a Generative Adversarial Network (GAN) architecture. The approach
introduces a multi-scale adaptive generator based on the Wavelet Transform and
a dual-branch discriminator that integrates difference perception feature maps
with original features. Experimental results on multiple biomedical microscopy
image datasets show that the proposed model achieves state-of-the-art denoising
performance, particularly excelling in the preservation of high-frequency
information. Furthermore, the dual-branch discriminator is seamlessly
compatible with various GAN frameworks. The proposed quality-aware,
wavelet-driven GAN denoising model is termed as QWD-GAN.

</details>


### [400] [The Missing Piece: A Case for Pre-Training in 3D Medical Object Detection](https://arxiv.org/abs/2509.15947)
*Katharina Eckstein,Constantin Ulrich,Michael Baumgartner,Jessica Kächele,Dimitrios Bounias,Tassilo Wald,Ralf Floca,Klaus H. Maier-Hein*

Main category: eess.IV

TL;DR: This paper explores pre-training methods for 3D medical object detection, highlighting the benefits of reconstruction-based self-supervised methods.


<details>
  <summary>Details</summary>
Motivation: To improve 3D medical object detection, an essential tool for accurate computer-aided diagnosis, using large-scale pre-training approaches.

Method: The study systematically integrates and evaluates existing pre-training methods (CNNs, Transformers) on state-of-the-art architectures, examining the impact of supervised, contrastive, and self-supervised methods.

Result: Pre-training consistently enhances detection across various tasks. Reconstruction-based self-supervised approaches outperform supervised methods, while contrastive pre-training shows limited benefits.

Conclusion: Reconstruction-based self-supervised pre-training is the most effective approach for boosting 3D medical object detection performance in comparison to other methods.

Abstract: Large-scale pre-training holds the promise to advance 3D medical object
detection, a crucial component of accurate computer-aided diagnosis. Yet, it
remains underexplored compared to segmentation, where pre-training has already
demonstrated significant benefits. Existing pre-training approaches for 3D
object detection rely on 2D medical data or natural image pre-training, failing
to fully leverage 3D volumetric information. In this work, we present the first
systematic study of how existing pre-training methods can be integrated into
state-of-the-art detection architectures, covering both CNNs and Transformers.
Our results show that pre-training consistently improves detection performance
across various tasks and datasets. Notably, reconstruction-based
self-supervised pre-training outperforms supervised pre-training, while
contrastive pre-training provides no clear benefit for 3D medical object
detection. Our code is publicly available at:
https://github.com/MIC-DKFZ/nnDetection-finetuning.

</details>


### [401] [SLaM-DiMM: Shared Latent Modeling for Diffusion Based Missing Modality Synthesis in MRI](https://arxiv.org/abs/2509.16019)
*Bhavesh Sandbhor,Bheeshm Sharma,Balamurugan Palaniappan*

Main category: eess.IV

TL;DR: The paper introduces SLaM-DiMM, a diffusion model-based method to generate missing MRI modalities, ensuring anatomical and structural coherence.


<details>
  <summary>Details</summary>
Motivation: MRI modalities provide complementary information crucial for understanding brain anatomy, but clinical constraints can lead to missing modalities. Solving this challenge can improve image analysis and downstream tasks.

Method: The framework employs diffusion models to synthesize missing MRI modalities while integrating a coherence enhancement mechanism to preserve structural consistency.

Result: SLaM-DiMM demonstrated superior performance in generating anatomically plausible and structurally consistent MRI scans on the BraTS-Lighthouse-2025 dataset.

Conclusion: The proposed approach effectively addresses missing modality generation, advancing medical image analysis applications and offering reproducibility through publicly available code.

Abstract: Brain MRI scans are often found in four modalities, consisting of T1-weighted
with and without contrast enhancement (T1ce and T1w), T2-weighted imaging
(T2w), and Flair. Leveraging complementary information from these different
modalities enables models to learn richer, more discriminative features for
understanding brain anatomy, which could be used in downstream tasks such as
anomaly detection. However, in clinical practice, not all MRI modalities are
always available due to various reasons. This makes missing modality generation
a critical challenge in medical image analysis. In this paper, we propose
SLaM-DiMM, a novel missing modality generation framework that harnesses the
power of diffusion models to synthesize any of the four target MRI modalities
from other available modalities. Our approach not only generates high-fidelity
images but also ensures structural coherence across the depth of the volume
through a dedicated coherence enhancement mechanism. Qualitative and
quantitative evaluations on the BraTS-Lighthouse-2025 Challenge dataset
demonstrate the effectiveness of the proposed approach in synthesizing
anatomically plausible and structurally consistent results. Code is available
at https://github.com/BheeshmSharma/SLaM-DiMM-MICCAI-BraTS-Challenge-2025.

</details>


### [402] [FMD-TransUNet: Abdominal Multi-Organ Segmentation Based on Frequency Domain Multi-Axis Representation Learning and Dual Attention Mechanisms](https://arxiv.org/abs/2509.16044)
*Fang Lu,Jingyu Xu,Qinxiu Sun,Qiong Lou*

Main category: eess.IV

TL;DR: This paper introduces FMD-TransUNet, a novel framework for precise abdominal multi-organ segmentation that integrates frequency-domain features and attention mechanisms, achieving superior performance over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing segmentation methods struggle with small, irregular, or anatomically complex organs and often overlook the frequency-domain's potential for complementing spatial-domain analysis.

Method: The paper proposes FMD-TransUNet, which incorporates the Multi-axis External Weight Block (MEWB) for capturing frequency-domain features and an enhanced dual attention module (DA+) for better feature fusion in the TransUNet framework.

Result: Experimental results on the Synapse dataset show FMD-TransUNet achieves an average DSC of 81.32% and HD of 16.35 mm across eight abdominal organs, outperforming state-of-the-art methods.

Conclusion: FMD-TransUNet effectively improves the accuracy of abdominal multi-organ segmentation by addressing limitations in existing methods and leveraging complementary spatial and frequency-domain information.

Abstract: Accurate abdominal multi-organ segmentation is critical for clinical
applications. Although numerous deep learning-based automatic segmentation
methods have been developed, they still struggle to segment small, irregular,
or anatomically complex organs. Moreover, most current methods focus on
spatial-domain analysis, often overlooking the synergistic potential of
frequency-domain representations. To address these limitations, we propose a
novel framework named FMD-TransUNet for precise abdominal multi-organ
segmentation. It innovatively integrates the Multi-axis External Weight Block
(MEWB) and the improved dual attention module (DA+) into the TransUNet
framework. The MEWB extracts multi-axis frequency-domain features to capture
both global anatomical structures and local boundary details, providing
complementary information to spatial-domain representations. The DA+ block
utilizes depthwise separable convolutions and incorporates spatial and channel
attention mechanisms to enhance feature fusion, reduce redundant information,
and narrow the semantic gap between the encoder and decoder. Experimental
validation on the Synapse dataset shows that FMD-TransUNet outperforms other
recent state-of-the-art methods, achieving an average DSC of 81.32\% and a HD
of 16.35 mm across eight abdominal organs. Compared to the baseline model, the
average DSC increased by 3.84\%, and the average HD decreased by 15.34 mm.
These results demonstrate the effectiveness of FMD-TransUNet in improving the
accuracy of abdominal multi-organ segmentation.

</details>


### [403] [PRISM: Probabilistic and Robust Inverse Solver with Measurement-Conditioned Diffusion Prior for Blind Inverse Problems](https://arxiv.org/abs/2509.16106)
*Yuanyun Hu,Evan Bell,Guijin Wang,Yu Sun*

Main category: eess.IV

TL;DR: The paper introduces PRISM, a novel diffusion-based method for addressing blind inverse problems like image deblurring, showcasing its superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based inverse solvers require complete knowledge of the forward operator, limiting their effectiveness for blind inverse problems.

Method: PRISM uses a measurement-conditioned diffusion model, integrated into a theoretically principled posterior sampling framework, to solve blind inverse problems.

Result: Experiments in blind image deblurring demonstrate that PRISM outperforms current state-of-the-art methods in both image and blur kernel recovery.

Conclusion: PRISM provides a robust and probabilistic solution for blind inverse problems, offering technical advancements over existing methods.

Abstract: Diffusion models are now commonly used to solve inverse problems in
computational imaging. However, most diffusion-based inverse solvers require
complete knowledge of the forward operator to be used. In this work, we
introduce a novel probabilistic and robust inverse solver with
measurement-conditioned diffusion prior (PRISM) to effectively address blind
inverse problems. PRISM offers a technical advancement over current methods by
incorporating a powerful measurement-conditioned diffusion model into a
theoretically principled posterior sampling scheme. Experiments on blind image
deblurring validate the effectiveness of the proposed method, demonstrating the
superior performance of PRISM over state-of-the-art baselines in both image and
blur kernel recovery.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [404] [Learning Analytics from Spoken Discussion Dialogs in Flipped Classroom](https://arxiv.org/abs/2301.12399)
*Hang Su,Borislav Dzodzo,Changlun Li,Danyang Zhao,Hao Geng,Yunxiang Li,Sidharth Jaggi,Helen Meng*

Main category: cs.CY

TL;DR: The study explores the use of spoken discussion analytics in flipped classrooms to predict group learning outcomes using machine learning.


<details>
  <summary>Details</summary>
Motivation: Investigate group learning processes and outcomes via analytics of spoken discussions in flipped classrooms.

Method: Recorded and transcribed in-class discussions, extracted features using tools, analyzed statistical indicators, and applied machine learning for outcome prediction.

Result: Achieved a prediction accuracy of 78.9% for group learning outcomes categorized as High, Mid, or Low.

Conclusion: Demonstrates the feasibility of automatic prediction of learning outcomes from discussion dialogs in flipped classrooms.

Abstract: The flipped classroom is a new pedagogical strategy that has been gaining
increasing importance recently. Spoken discussion dialog commonly occurs in
flipped classroom, which embeds rich information indicating processes and
progression of students' learning. This study focuses on learning analytics
from spoken discussion dialog in the flipped classroom, which aims to collect
and analyze the discussion dialogs in flipped classroom in order to get to know
group learning processes and outcomes. We have recently transformed a course
using the flipped classroom strategy, where students watched video-recorded
lectures at home prior to group-based problem-solving discussions in class. The
in-class group discussions were recorded throughout the semester and then
transcribed manually. After features are extracted from the dialogs by multiple
tools and customized processing techniques, we performed statistical analyses
to explore the indicators that are related to the group learning outcomes from
face-to-face discussion dialogs in the flipped classroom. Then, machine
learning algorithms are applied to the indicators in order to predict the group
learning outcome as High, Mid or Low. The best prediction accuracy reaches
78.9%, which demonstrates the feasibility of achieving automatic learning
outcome prediction from group discussion dialog in flipped classroom.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [405] [Efficient and Versatile Model for Multilingual Information Retrieval of Islamic Text: Development and Deployment in Real-World Scenarios](https://arxiv.org/abs/2509.15380)
*Vera Pavlova,Mohammed Makhlouf*

Main category: cs.IR

TL;DR: This paper develops an ad-hoc multilingual information retrieval system designed for the Islamic domain using the Quranic corpus and evaluates various training methods to optimize model performance.


<details>
  <summary>Details</summary>
Motivation: Standard MLIR approaches are limited in real-world applicability, so the study aims to bridge this gap, especially for the Islamic domain using a unique dataset (the Quranic multilingual corpus).

Method: The study developed 11 retrieval models using monolingual, cross-lingual, translate-train-all, and a new mixed training methodology that combines monolingual and cross-lingual learning for assessment.

Result: The mixed training approach achieved strong, scenario-agnostic performance on an in-domain dataset, outperforming other methods.

Conclusion: The findings highlight the potential of cost-efficient and lightweight models for practical, multilingual information retrieval, particularly in the Islamic domain.

Abstract: Despite recent advancements in Multilingual Information Retrieval (MLIR), a
significant gap remains between research and practical deployment. Many studies
assess MLIR performance in isolated settings, limiting their applicability to
real-world scenarios. In this work, we leverage the unique characteristics of
the Quranic multilingual corpus to examine the optimal strategies to develop an
ad-hoc IR system for the Islamic domain that is designed to satisfy users'
information needs in multiple languages. We prepared eleven retrieval models
employing four training approaches: monolingual, cross-lingual,
translate-train-all, and a novel mixed method combining cross-lingual and
monolingual techniques. Evaluation on an in-domain dataset demonstrates that
the mixed approach achieves promising results across diverse retrieval
scenarios. Furthermore, we provide a detailed analysis of how different
training configurations affect the embedding space and their implications for
multilingual retrieval effectiveness. Finally, we discuss deployment
considerations, emphasizing the cost-efficiency of deploying a single
versatile, lightweight model for real-world MLIR applications.

</details>


### [406] [Dual-Mode Visual System for Brain-Computer Interfaces: Integrating SSVEP and P300 Responses](https://arxiv.org/abs/2509.15439)
*Ekgari Kasawala,Surej Mouli*

Main category: cs.IR

TL;DR: This paper introduces an LED-based dual stimulation system for brain-computer interface (BCI) applications, integrating SSVEP and P300 paradigms, achieving 86.25% accuracy and an ITR of 42.08 bpm.


<details>
  <summary>Details</summary>
Motivation: Current BCI systems utilizing SSVEP and P300 paradigms typically depend on LCD-based visual stimulation, which is limiting for practical applications. The study seeks to address these limitations with a novel LED-based system.

Method: An LED device with four stimulation frequencies (7 Hz, 8 Hz, 9 Hz, 10 Hz) was developed. Real-time feature extraction was performed using FFT amplitude analysis and P300 peak detection to classify user intent. The system's precision and accuracy were validated instrumentally and via computational methods.

Result: The system demonstrated high precision of stimulation frequencies (error margin: 0.15%-0.20%), accurately identified user-directed controls, and achieved 86.25% mean classification accuracy with an ITR of 42.08 bpm.

Conclusion: The proposed LED-based hybrid BCI system successfully integrates SSVEP and P300 paradigms, offering improved practical deployment potential and high-level performance in task recognition and information transfer.

Abstract: In brain-computer interface (BCI) systems, steady-state visual evoked
potentials (SSVEP) and P300 responses have achieved widespread implementation
owing to their superior information transfer rates (ITR) and minimal training
requirements. These neurophysiological signals have exhibited robust efficacy
and versatility in external device control, demonstrating enhanced precision
and scalability. However, conventional implementations predominantly utilise
liquid crystal display (LCD)-based visual stimulation paradigms, which present
limitations in practical deployment scenarios. This investigation presents the
development and evaluation of a novel light-emitting diode (LED)-based dual
stimulation apparatus designed to enhance SSVEP classification accuracy through
the integration of both SSVEP and P300 paradigms. The system employs four
distinct frequencies, 7 Hz, 8 Hz, 9 Hz, and 10 Hz, corresponding to forward,
backward, right, and left directional controls, respectively. Oscilloscopic
verification confirmed the precision of these stimulation frequencies.
Real-time feature extraction was accomplished through the concurrent analysis
of maximum Fast Fourier Transform (FFT) amplitude and P300 peak detection to
ascertain user intent. Directional control was determined by the frequency
exhibiting maximal amplitude characteristics. The visual stimulation hardware
demonstrated minimal frequency deviation, with error differentials ranging from
0.15%to 0.20%across all frequencies. The implemented signal processing
algorithm successfully discriminated all four stimulus frequencies whilst
correlating them with their respective P300 event markers. Classification
accuracy was evaluated based on correct task intention recognition. The
proposed hybrid system achieved a mean classification accuracy of 86.25%,
coupled with an average ITR of 42.08 bits per minute (bpm).

</details>


### [407] [CFDA & CLIP at TREC iKAT 2025: Enhancing Personalized Conversational Search via Query Reformulation and Rank Fusion](https://arxiv.org/abs/2509.15588)
*Yu-Cheng Chang,Guan-Wei Yeo,Quah Eugene,Fan-Jie Shih,Yuan-Ching Kuo,Tsung-En Yu,Hung-Chun Hsu,Ming-Feng Tsai,Chuan-Ju Wang*

Main category: cs.IR

TL;DR: The paper addresses strategies for the TREC Interactive Knowledge Assistance Track (iKAT) in 2025, focusing on query rewriting and retrieval fusion to enhance system robustness and efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve robustness, efficiency, and accuracy in interactive and offline tasks of iKAT under real-time and controlled settings.

Method: The study employed Best-of-N selection and Reciprocal Rank Fusion (RRF) strategies for query rewriting and retrieval fusion in interactive and offline submission tasks.

Result: Results demonstrate improvements in robustness with trade-offs observed between system effectiveness and efficiency.

Conclusion: Reranking and retrieval fusion strategies effectively balanced robustness and efficiency across tasks, though trade-offs remain significant.

Abstract: The 2025 TREC Interactive Knowledge Assistance Track (iKAT) featured both
interactive and offline submission tasks. The former requires systems to
operate under real-time constraints, making robustness and efficiency as
important as accuracy, while the latter enables controlled evaluation of
passage ranking and response generation with pre-defined datasets. To address
this, we explored query rewriting and retrieval fusion as core strategies. We
built our pipelines around Best-of-$N$ selection and Reciprocal Rank Fusion
(RRF) strategies to handle different submission tasks. Results show that
reranking and fusion improve robustness while revealing trade-offs between
effectiveness and efficiency across both tasks.

</details>


### [408] [Chunk Knowledge Generation Model for Enhanced Information Retrieval: A Multi-task Learning Approach](https://arxiv.org/abs/2509.15658)
*Jisu Kim,Jinhee Park,Changhyun Jeon,Jungwoo Choi,Keonwoo Kim,Minji Hong,Sehyun Kim*

Main category: cs.IR

TL;DR: The study proposes using a "Chunk Knowledge Generation Model" to improve information retrieval efficiency and accuracy by dividing documents into chunks and generating semantic data like titles and questions in parallel.


<details>
  <summary>Details</summary>
Motivation: Existing query and document expansion methods face challenges such as increased preprocessing costs, larger index sizes, and unreliable generated content, prompting the need for alternatives.

Method: The proposed method employs a T5-based multi-task learning model to generate titles, candidate questions, and extract keywords simultaneously through a single encoding and dual decoding structure.

Result: The model demonstrated 95.41% accuracy at Top@10 in retrieval, outperforming existing document chunk-level retrieval systems in efficiency and reliability.

Conclusion: This approach improves large-scale information retrieval accuracy and efficiency while providing a structured and practical alternative for enhancing retrieval pipelines.

Abstract: Traditional query expansion techniques for addressing vocabulary mismatch
problems in information retrieval are context-sensitive and may lead to
performance degradation. As an alternative, document expansion research has
gained attention, but existing methods such as Doc2Query have limitations
including excessive preprocessing costs, increased index size, and reliability
issues with generated content. To mitigate these problems and seek more
structured and efficient alternatives, this study proposes a method that
divides documents into chunk units and generates textual data for each chunk to
simultaneously improve retrieval efficiency and accuracy. The proposed "Chunk
Knowledge Generation Model" adopts a T5-based multi-task learning structure
that simultaneously generates titles and candidate questions from each document
chunk while extracting keywords from user queries. This approach maximizes
computational efficiency by generating and extracting three types of semantic
information in parallel through a single encoding and two decoding processes.
The generated data is utilized as additional information in the retrieval
system. GPT-based evaluation on 305 query-document pairs showed that retrieval
using the proposed model achieved 95.41% accuracy at Top@10, demonstrating
superior performance compared to document chunk-level retrieval. This study
contributes by proposing an approach that simultaneously generates titles and
candidate questions from document chunks for application in retrieval
pipelines, and provides empirical evidence applicable to large-scale
information retrieval systems by demonstrating improved retrieval accuracy
through qualitative evaluation.

</details>


### [409] [Optimizing Product Deduplication in E-Commerce with Multimodal Embeddings](https://arxiv.org/abs/2509.15858)
*Aysenur Kulunk,Berk Taskin,M. Furkan Eseoglu,H. Bahadir Sahin*

Main category: cs.IR

TL;DR: The paper presents a scalable multimodal product deduplication system for e-commerce, using BERT and MaskedAutoEncoders for feature extraction, achieving high precision in identifying duplicate listings.


<details>
  <summary>Details</summary>
Motivation: Duplicate product listings in e-commerce marketplaces lead to consumer confusion, reduced platform trust, and increased operational costs, necessitating efficient deduplication methods.

Method: The approach combines domain-specific BERT-based text models with MaskedAutoEncoders for image representation and dimensionality reduction, alongside a decider model integrated with Milvus for optimized similarity searches.

Result: The system achieves a macro-average F1-score of 0.90, outperforming third-party solutions (F1-score of 0.83) while handling over 200 million items efficiently using 100GB RAM.

Conclusion: This multimodal approach demonstrates the effectiveness of domain-specific and advanced machine learning techniques for addressing the challenges of duplicate detection in large-scale e-commerce platforms.

Abstract: In large scale e-commerce marketplaces, duplicate product listings frequently
cause consumer confusion and operational inefficiencies, degrading trust on the
platform and increasing costs. Traditional keyword-based search methodologies
falter in accurately identifying duplicates due to their reliance on exact
textual matches, neglecting semantic similarities inherent in product titles.
To address these challenges, we introduce a scalable, multimodal product
deduplication designed specifically for the e-commerce domain. Our approach
employs a domain-specific text model grounded in BERT architecture in
conjunction with MaskedAutoEncoders for image representations. Both of these
architectures are augmented with dimensionality reduction techniques to produce
compact 128-dimensional embeddings without significant information loss.
Complementing this, we also developed a novel decider model that leverages both
text and image vectors. By integrating these feature extraction mechanisms with
Milvus, an optimized vector database, our system can facilitate efficient and
high-precision similarity searches across extensive product catalogs exceeding
200 million items with just 100GB of system RAM consumption. Empirical
evaluations demonstrate that our matching system achieves a macro-average F1
score of 0.90, outperforming third-party solutions which attain an F1 score of
0.83. Our findings show the potential of combining domain-specific adaptations
with state-of-the-art machine learning techniques to mitigate duplicate
listings in large-scale e-commerce environments.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [410] [Diversity of Structured Domains via k-Kemeny Scores](https://arxiv.org/abs/2509.15812)
*Piotr Faliszewski,Krzysztof Sornat,Stanisław Szufa,Tomasz Wąs*

Main category: cs.GT

TL;DR: The k-Kemeny problem involves reducing ordinal election rankings to at most k different rankings through minimal swaps, studied across structured domains.


<details>
  <summary>Details</summary>
Motivation: To address the complexity of the k-Kemeny problem in structured domains and measure the diversity of these domains.

Method: Analyzing the k-Kemeny problem in single-peaked, single-crossing, group-separable, and Euclidean domains, and evaluating their diversity.

Result: The problem is intractable under most domains studied, even when k=2.

Conclusion: Structured domains differ in diversity, with complexity arising in simplifying ordinal rankings within these domains.

Abstract: In the k-Kemeny problem, we are given an ordinal election, i.e., a collection
of votes ranking the candidates from best to worst, and we seek the smallest
number of swaps of adjacent candidates that ensure that the election has at
most k different rankings. We study this problem for a number of structured
domains, including the single-peaked, single-crossing, group-separable, and
Euclidean ones. We obtain two kinds of results: (1) We show that k-Kemeny
remains intractable under most of these domains, even for k=2, and (2) we use
k-Kemeny to rank these domains in terms of their diversity.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [411] [Training thermodynamic computers by gradient descent](https://arxiv.org/abs/2509.15324)
*Stephen Whitelam*

Main category: cond-mat.stat-mech

TL;DR: This paper demonstrates how thermodynamic computers can use gradient descent to perform computations, achieving significant energy efficiency compared to digital computers.


<details>
  <summary>Details</summary>
Motivation: The paper aims to bridge the fields of thermodynamic computing and machine learning by developing a training methodology that enables thermodynamic computers to perform specific computations efficiently.

Method: The process involves simulating a thermodynamic computer digitally, adjusting its parameters via gradient descent, and designing idealized trajectories that mimic trained neural networks performing the desired computation.

Result: The method was successfully applied to an image classification task, and the thermodynamic implementation showed energy savings exceeding seven orders of magnitude compared to its digital counterpart.

Conclusion: Gradient descent proves effective for training thermodynamic computers, positioning them as energy-efficient alternatives for tasks traditionally handled by digital machines.

Abstract: We show how to adjust the parameters of a thermodynamic computer by gradient
descent in order to perform a desired computation at a specified observation
time. Within a digital simulation of a thermodynamic computer, training
proceeds by maximizing the probability with which the computer would generate
an idealized dynamical trajectory. The idealized trajectory is designed to
reproduce the activations of a neural network trained to perform the desired
computation. This teacher-student scheme results in a thermodynamic computer
whose finite-time dynamics enacts a computation analogous to that of the neural
network. The parameters identified in this way can be implemented in the
hardware realization of the thermodynamic computer, which will perform the
desired computation automatically, driven by thermal noise. We demonstrate the
method on a standard image-classification task, and estimate the thermodynamic
advantage -- the ratio of energy costs of the digital and thermodynamic
implementations -- to exceed seven orders of magnitude. Our results establish
gradient descent as a viable training method for thermodynamic computing,
enabling application of the core methodology of machine learning to this
emerging field.

</details>


<div id='physics.atom-ph'></div>

# physics.atom-ph [[Back]](#toc)

### [412] [Accelerating Atomic Fine Structure Determination with Graph Reinforcement Learning](https://arxiv.org/abs/2509.16184)
*M. Ding,V. -A. Darvariu,A. N. Ryabtsev,N. Hawes,J. C. Pickering*

Main category: physics.atom-ph

TL;DR: This paper proposes automating atomic fine structure level energy determination using graph reinforcement learning. Results show significant time efficiency and accuracy for Co II and Nd II-III.


<details>
  <summary>Details</summary>
Motivation: The demand for accurate atomic data from plasma diagnostics in astronomy and fusion science exceeds the current capability of manual analysis involving thousands of atomic spectra.

Method: The authors employ graph reinforcement learning to automate the determination of fine structure level energies, using historical human decision-based reward functions.

Result: The automated approach calculates hundreds of level energies within hours, achieving accuracy comparable to published values: 95% for Co II and 54-87% for Nd II-III.

Conclusion: The AI-based automation significantly enhances efficiency and provides a scalable solution to meet growing demands for atomic spectral analysis data.

Abstract: Atomic data determined by analysis of observed atomic spectra are essential
for plasma diagnostics. For each low-ionisation open d- and f-subshell atomic
species, around $10^3$ fine structure level energies can be determined through
years of analysis of $10^4$ observable spectral lines. We propose the
automation of this task by casting the analysis procedure as a Markov decision
process and solving it by graph reinforcement learning using reward functions
learned on historical human decisions. In our evaluations on existing spectral
line lists and theoretical calculations for Co II and Nd II-III, hundreds of
level energies were computed within hours, agreeing with published values in
95% of cases for Co II and 54-87% for Nd II-III. As the current efficiency in
atomic fine structure determination struggles to meet growing atomic data
demands from astronomy and fusion science, our new artificial intelligence
approach sets the stage for closing this gap.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [413] [Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents](https://arxiv.org/abs/2509.15233)
*Xueqiao Zhang,Chao Zhang,Jingtao Xu,Yifan Zhu,Xin Shi,Yi Yang,Yawei Luo*

Main category: cs.MM

TL;DR: The paper introduces dynamic role profiles for role-playing agents by leveraging video data, aiming to enhance perceptual and interactive capabilities.


<details>
  <summary>Details</summary>
Motivation: Improve RPAs by integrating dynamic perceptual abilities, which existing static role-based models lack.

Method: Develop a dataset (Role-playing-Video60k) and combine video-based dynamic profiles with static role profiles using temporal frame sampling and LLM integration.

Result: The framework improves RPAs' responsiveness across eight evaluation metrics, showcasing effectiveness of dynamic profiles.

Conclusion: Dynamic role profiles are critical for advancing RPAs in creating interactive experiences and improving response quality.

Abstract: Role-playing agents (RPAs) have attracted growing interest for their ability
to simulate immersive and interactive characters. However, existing approaches
primarily focus on static role profiles, overlooking the dynamic perceptual
abilities inherent to humans. To bridge this gap, we introduce the concept of
dynamic role profiles by incorporating video modality into RPAs. To support
this, we construct Role-playing-Video60k, a large-scale, high-quality dataset
comprising 60k videos and 700k corresponding dialogues. Based on this dataset,
we develop a comprehensive RPA framework that combines adaptive temporal
sampling with both dynamic and static role profile representations.
Specifically, the dynamic profile is created by adaptively sampling video
frames and feeding them to the LLM in temporal order, while the static profile
consists of (1) character dialogues from training videos during fine-tuning,
and (2) a summary context from the input video during inference. This joint
integration enables RPAs to generate greater responses. Furthermore, we propose
a robust evaluation method covering eight metrics. Experimental results
demonstrate the effectiveness of our framework, highlighting the importance of
dynamic role profiles in developing RPAs.

</details>


### [414] [Copycat vs. Original: Multi-modal Pretraining and Variable Importance in Box-office Prediction](https://arxiv.org/abs/2509.15277)
*Qin Chao,Eunsoo Kim,Boyang Li*

Main category: cs.MM

TL;DR: This paper creates a multimodal neural network to improve box-office prediction and analyzes the role of 'copycat movies' in revenue generation.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the high-risk nature of the movie industry by developing automated tools to aid box-office revenue prediction and inform decision-making.

Method: It uses a multimodal neural network that integrates crowdsourced descriptive keywords with visual information from movie posters to improve prediction accuracy.

Result: The model achieves a 14.5% reduction in box-office prediction error and finds that 'copycat movies' can generate revenue, although their benefit diminishes with content oversaturation.

Conclusion: The research advances deep learning applications in the movie industry and offers insights on balancing the commercial viability of similar movie themes.

Abstract: The movie industry is associated with an elevated level of risk, which
necessitates the use of automated tools to predict box-office revenue and
facilitate human decision-making. In this study, we build a sophisticated
multimodal neural network that predicts box offices by grounding crowdsourced
descriptive keywords of each movie in the visual information of the movie
posters, thereby enhancing the learned keyword representations, resulting in a
substantial reduction of 14.5% in box-office prediction error. The advanced
revenue prediction model enables the analysis of the commercial viability of
"copycat movies," or movies with substantial similarity to successful movies
released recently. We do so by computing the influence of copycat features in
box-office prediction. We find a positive relationship between copycat status
and movie revenue. However, this effect diminishes when the number of similar
movies and the similarity of their content increase. Overall, our work develops
sophisticated deep learning tools for studying the movie industry and provides
valuable business insight.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [415] [DeepMech: A Machine Learning Framework for Chemical Reaction Mechanism Prediction](https://arxiv.org/abs/2509.15872)
*Manajit Das,Ajnabiul Hoque,Mayank Baranwal,Raghavan B. Sunoj*

Main category: physics.chem-ph

TL;DR: DeepMech is a graph-based deep learning framework that predicts chemical reaction mechanisms with high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: The challenge of predicting detailed chemical reaction mechanisms traditionally requires costly experiments or quantum computations, and current deep learning methods have limitations such as ignoring intermediates and generating unrealistic mechanisms.

Method: The authors developed DeepMech, a graph-based deep learning framework with attention mechanisms guided by templates of mechanistic operations (TMOps) and trained on a curated dataset of ~30K chemical reaction mechanisms.

Result: DeepMech achieves 98.98% accuracy in predicting elementary steps and 95.94% accuracy in full CRM tasks, performs well on out-of-distribution and side/byproduct predictions, and reconstructs complex biomolecular pathways.

Conclusion: DeepMech combines high accuracy, interpretability, and robustness, making it promising for understanding and designing chemical reaction pathways.

Abstract: Prediction of complete step-by-step chemical reaction mechanisms (CRMs)
remains a major challenge. Whereas the traditional approaches in CRM tasks rely
on expert-driven experiments or costly quantum chemical computations,
contemporary deep learning (DL) alternatives ignore key intermediates and
mechanistic steps and often suffer from hallucinations. We present DeepMech, an
interpretable graph-based DL framework employing atom- and bond-level
attention, guided by generalized templates of mechanistic operations (TMOps),
to generate CRMs. Trained on our curated ReactMech dataset (~30K CRMs with 100K
atom-mapped and mass-balanced elementary steps), DeepMech achieves
98.98+/-0.12% accuracy in predicting elementary steps and 95.94+/-0.21% in
complete CRM tasks, besides maintaining high fidelity even in
out-of-distribution scenarios as well as in predicting side and/or byproducts.
Extension to multistep CRMs relevant to prebiotic chemistry, demonstrates the
ability of DeepMech in effectively reconstructing pathways from simple
primordial substrates to complex biomolecules such as serine and aldopentose.
Attention analysis identifies reactive atoms/bonds in line with chemical
intuition, rendering our model interpretable and suitable for reaction design.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [416] [The (Short-Term) Effects of Large Language Models on Unemployment and Earnings](https://arxiv.org/abs/2509.15510)
*Danqing Chen,Carina Kane,Austin Kozlowski,Nadav Kunievsky,James A. Evans*

Main category: econ.GN

TL;DR: The paper evaluates how Large Language Model (LLM) exposure impacts labor market outcomes, finding earnings increases for exposed occupations without affecting unemployment.


<details>
  <summary>Details</summary>
Motivation: To understand the labor market consequences of rapid LLM adoption following ChatGPT's introduction, especially in terms of earnings and employment shifts.

Method: The study employs a Synthetic Difference in Differences approach to analyze earnings and unemployment across occupations with varying exposure to LLM technologies.

Result: Occupations highly exposed to LLMs experienced increased earnings with no significant change in unemployment rates post-ChatGPT.

Conclusion: Initial economic adjustments to LLMs are reflected through higher earnings rather than workforce reallocation, assuaging fears of widespread job displacement.

Abstract: Large Language Models have spread rapidly since the release of ChatGPT in
late 2022, accompanied by claims of major productivity gains but also concerns
about job displacement. This paper examines the short-run labor market effects
of LLM adoption by comparing earnings and unemployment across occupations with
differing levels of exposure to these technologies. Using a Synthetic
Difference in Differences approach, we estimate the impact of LLM exposure on
earnings and unemployment. Our findings show that workers in highly exposed
occupations experienced earnings increases following ChatGPT's introduction,
while unemployment rates remained unchanged. These results suggest that initial
labor market adjustments to LLMs operate primarily through earnings rather than
worker reallocation.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [417] [A Flow-rate-conserving CNN-based Domain Decomposition Method for Blood Flow Simulations](https://arxiv.org/abs/2509.15900)
*Simon Klaes,Axel Klawonn,Natalie Kubicki,Martin Lanser,Kengo Nakajima,Takashi Shimokawabe,Janine Weber*

Main category: math.NA

TL;DR: This study develops CNN-based surrogate models combined with a domain decomposition method to predict blood flow in stenosed arteries, incorporating a physics-aware approach for reliability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to efficiently predict blood flow with non-Newtonian viscosity in stenosed arteries, which is complex due to varying shapes, lengths, and inflow conditions.

Method: The paper employs a CNN-based universal subdomain solver (USDS) within an alternating Schwarz domain decomposition framework, trained on fixed geometry data and incorporating physics principles like flow rate conservation.

Result: The study demonstrates that a physics-aware USDS model outperforms purely data-driven approaches, achieving better subdomain solutions, maintaining flow accuracy, and improving global convergence reliability.

Conclusion: A physics-integrated approach to surrogate modeling provides significant advantages in terms of predictive accuracy and convergence reliability in simulations of complex arterial blood flow environments.

Abstract: This work aims to predict blood flow with non-Newtonian viscosity in stenosed
arteries using convolutional neural network (CNN) surrogate models. An
alternating Schwarz domain decomposition method is proposed which uses
CNN-based subdomain solvers. A universal subdomain solver (USDS) is trained on
a single, fixed geometry and then applied for each subdomain solve in the
Schwarz method. Results for two-dimensional stenotic arteries of varying shape
and length for different inflow conditions are presented and statistically
evaluated. One key finding, when using a limited amount of training data, is
the need to implement a USDS which preserves some of the physics, as, in our
case, flow rate conservation. A physics-aware approach outperforms purely
data-driven USDS, delivering improved subdomain solutions and preventing
overshooting or undershooting of the global solution during the Schwarz
iterations, thereby leading to more reliable convergence.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [418] [ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow Matching](https://arxiv.org/abs/2509.15942)
*Graham Clyne,Guillaume Couairon,Guillaume Gastineau,Claire Monteleoni,Anastase Charantonis*

Main category: physics.ao-ph

TL;DR: ArchesClimate, a deep learning-based climate model emulator, addresses the computational expense of generating ensembles for climate projections by providing stable and consistent simulations over decadal timescales. It aims to be interchangeable with traditional models.


<details>
  <summary>Details</summary>
Motivation: Climate projections suffer from expensive computational costs due to the reliance on traditional climate models and ensembles of simulations to quantify uncertainties.

Method: ArchesClimate adapts the flow matching approach from ArchesWeatherGen, training on decadal hindcasts of the IPSL-CM6A-LR climate model at a spatial resolution of about 2.5x1.25 degrees.

Result: The emulator generates stable and physically consistent climate states over up to 10 years, with interchangeable outputs compared to the IPSL climate model for key climate variables.

Conclusion: ArchesClimate demonstrates the potential of deep learning in reducing the computational cost of climate simulations while ensuring stable, reliable, and interchangeable outputs compared to traditional models.

Abstract: Climate projections have uncertainties related to components of the climate
system and their interactions. A typical approach to quantifying these
uncertainties is to use climate models to create ensembles of repeated
simulations under different initial conditions. Due to the complexity of these
simulations, generating such ensembles of projections is computationally
expensive. In this work, we present ArchesClimate, a deep learning-based
climate model emulator that aims to reduce this cost. ArchesClimate is trained
on decadal hindcasts of the IPSL-CM6A-LR climate model at a spatial resolution
of approximately 2.5x1.25 degrees. We train a flow matching model following
ArchesWeatherGen, which we adapt to predict near-term climate. Once trained,
the model generates states at a one-month lead time and can be used to
auto-regressively emulate climate model simulations of any length. We show that
for up to 10 years, these generations are stable and physically consistent. We
also show that for several important climate variables, ArchesClimate generates
simulations that are interchangeable with the IPSL model. This work suggests
that climate model emulators could significantly reduce the cost of climate
model simulations.

</details>
