{"id": "2601.15598", "pdf": "https://arxiv.org/pdf/2601.15598", "abs": "https://arxiv.org/abs/2601.15598", "authors": ["Boxuan Zhang", "Jiaxin Wang", "Zhen Xu", "Kuan Tao"], "title": "Ternary Spiking Neural Networks Enhanced by Complemented Neurons and Membrane Potential Aggregation", "categories": ["cs.NE"], "comment": "Submission in progress. Code is available at https://github.com/ZBX05/Enhanced-TernarySNN", "summary": "Spiking Neural Networks (SNNs) are promising energy-efficient models and powerful framworks of modeling neuron dynamics. However, existing binary spiking neurons exhibit limited biological plausibilities and low information capacity. Recently developed ternary spiking neuron possesses higher consistency with biological principles (i.e. excitation-inhibition balance mechanism). Despite of this, the ternary spiking neuron suffers from defects including iterative information loss, temporal gradient vanishing and irregular distributions of membrane potentials. To address these issues, we propose Complemented Ternary Spiking Neuron (CTSN), a novel ternary spiking neuron model that incorporates an learnable complemental term to store information from historical inputs. CTSN effectively improves the deficiencies of ternary spiking neuron, while the embedded learnable factors enable CTSN to adaptively adjust neuron dynamics, providing strong neural heterogeneity. Furthermore, based on the temporal evolution features of ternary spiking neurons' membrane potential distributions, we propose the Temporal Membrane Potential Regularization (TMPR) training method. TMPR introduces time-varying regularization strategy utilizing membrane potentials, furhter enhancing the training process by creating extra backpropagation paths. We validate our methods through extensive experiments on various datasets, demonstrating remarkable performance advances.", "AI": {"tldr": "This paper introduces Complemented Ternary Spiking Neuron (CTSN) and Temporal Membrane Potential Regularization (TMPR) to improve ternary spiking neurons' biological plausibility, training efficiency, and performance.", "motivation": "Spiking Neural Networks (SNNs) are energy-efficient but binary neurons have limitations in biological plausibility and information capacity. Recent ternary neurons align better biologically but face issues like iterative information loss and gradient vanishing.", "method": "To overcome these issues, the authors propose CTSN, which adds a learnable complementary term to store past information and adapt neuron dynamics automatically. They also introduce TMPR, a training method to utilize temporal membrane potential changes for better backpropagation.", "result": "The proposed methods show significant performance improvements in experiments conducted on multiple datasets, demonstrating the effectiveness of CTSN and TMPR.", "conclusion": "CTSN enhances ternary spiking neurons' capabilities and biological alignment while TMPR improves training, collectively advancing SNN performance and adaptability."}}
{"id": "2601.16032", "pdf": "https://arxiv.org/pdf/2601.16032", "abs": "https://arxiv.org/abs/2601.16032", "authors": ["Yifan Zhu", "Yekai Pan", "Chen Ding"], "title": "Sawtooth Wavefront Reordering: Enhanced CuTile FlashAttention on NVIDIA GB10", "categories": ["cs.PF", "cs.AI", "cs.LG", "cs.OS"], "comment": null, "summary": "High-performance attention kernels are essential for Large Language Models. This paper presents analysis of CuTile-based Flash Attention memory behavior and a technique to improve its cache performance. In particular, our analysis on the NVIDIA GB10 (Grace Blackwell) identifies the main cause of L2 cache miss. Leveraging this insight, we introduce a new programming technique called Sawtooth Wavefront Reordering that reduces L2 misses. We validate it in both CUDA and CuTile, observing 50\\% or greater reduction in L2 misses and up to 60\\% increase in throughput on GB10.", "AI": {"tldr": "The paper introduces the Sawtooth Wavefront Reordering technique to enhance cache performance in Flash Attention kernels for Large Language Models, resulting in reduced L2 misses and increased throughput.", "motivation": "Improve cache performance for high-performance attention kernels used in Large Language Models on NVIDIA GB10 hardware.", "method": "Analyzed cache behavior in CuTile-based Flash Attention and introduced Sawtooth Wavefront Reordering technique.", "result": "Sawtooth Wavefront Reordering reduces L2 cache misses by 50% and boosts throughput by up to 60% on NVIDIA GB10.", "conclusion": "The technique significantly enhances cache efficiency and throughput for Flash Attention on NVIDIA GB10, contributing to better performance for Large Language Models."}}
{"id": "2601.15738", "pdf": "https://arxiv.org/pdf/2601.15738", "abs": "https://arxiv.org/abs/2601.15738", "authors": ["Junhao Qiu", "Haoyang Zhuang", "Fei Liu", "Jianjun Liu", "Qingfu Zhang"], "title": "LLM-Assisted Automatic Dispatching Rule Design for Dynamic Flexible Assembly Flow Shop Scheduling", "categories": ["cs.NE"], "comment": null, "summary": "Dynamic multi-product delivery environments demand rapid coordination of part completion and product-level kitting within hybrid processing and assembly systems to satisfy strict hierarchical supply constraints. The flexible assembly flow shop scheduling problem formally defines dependencies for multi-stage kitting, yet dynamic variants make designing integrated scheduling rules under multi-level time coupling highly challenging. Existing automated heuristic design methods, particularly genetic programming constrained to fixed terminal symbol sets, struggle to capture and leverage dynamic uncertainties and hierarchical dependency information under transient decision states. This study develops an LLM-assisted Dynamic Rule Design framework (LLM4DRD) that automatically evolves integrated online scheduling rules adapted to scheduling features. Firstly, multi-stage processing and assembly supply decisions are transformed into feasible directed edge orderings based on heterogeneous graph. Then, an elite knowledge guided initialization embeds advanced design expertise into initial rules to enhance initial quality. Additionally, a dual-expert mechanism is introduced in which LLM-A evolutionary code to generate candidate rules and LLM-S conducts scheduling evaluation, while dynamic feature-fitting rule evolution combined with hybrid evaluation enables continuous improvement and extracts adaptive rules with strong generalization capability. A series of experiments are conducted to validate the effectiveness of the method. The average tardiness of LLM4DRD is 3.17-12.39% higher than state-of-the-art methods in 20 practical instances used for training and testing, respectively. In 24 scenarios with different resource configurations, order loads, and disturbance levels totaling 480 instances, it achieves 11.10% higher performance than the second best competitor, exhibiting excellent robustness.", "AI": {"tldr": "The study develops a framework called LLM4DRD to optimize scheduling in dynamic multi-product delivery systems, using an LLM-based mechanism for adaptive rule evolution and better generalization capabilities.", "motivation": "To address challenges in dynamic scheduling environments, particularly the inability of existing heuristic design methods to effectively handle hierarchical dependencies and transient decision states.", "method": "The paper introduces LLM4DRD which employs a dual-expert mechanism for rule generation and evaluation, utilizing advanced initialization, graph-based modeling, and hybrid evaluation for adaptive scheduling rule evolution.", "result": "LLM4DRD demonstrates superior performance, achieving up to 12.39% and 11.10% higher effectiveness compared to state-of-the-art methods in training/testing and diverse testing scenarios, respectively.", "conclusion": "The framework provides adaptable and robust scheduling solutions for complex delivery environments, showcasing improved performance and generalization."}}
{"id": "2509.02811", "pdf": "https://arxiv.org/pdf/2509.02811", "abs": "https://arxiv.org/abs/2509.02811", "authors": ["Alessandro Traspadini", "Michele Zorzi", "Marco Giordani"], "title": "Performance Evaluation of LoRa for IoT Applications in Non-Terrestrial Networks via ns-3", "categories": ["cs.NI", "cs.PF"], "comment": "6 pages, 4 figures, 2 tables. Accepted for publication in the 2025 IEEE Global Communications Conference (GLOBECOM) \\c{opyright}2025 IEEE. A. Traspadini, M. Zorzi, and M. Giordani \"Performance Evaluation of LoRa for IoT Applications in Non-Terrestrial Networks via ns-3,\" in Proc. IEEE Global Communications Conference (GLOBECOM), 2025", "summary": "The integration of Internet of Things (IoT) and Non-Terrestrial Networks (NTNs) has emerged as a key paradigm to provide connectivity for sensors and actuators via satellite gateways in remote areas where terrestrial infrastructure is limited or unavailable. Among other Low-Power Wide-Area Network (LPWAN) technologies for IoT, Long Range (LoRa) holds great potential given its long range, energy efficiency, and flexibility. In this paper, we explore the feasibility and performance of LoRa to support large-scale IoT connectivity through Low Earth Orbit (LEO) satellite gateways. To do so, we developed a new ns3-LoRa-NTN simulation module, which integrates and extends the ns3-LoRa and ns3-NTN modules, to enable full-stack end-to-end simulation of satellite communication in LoRa networks. Our results, given in terms of average data rate and Packet Reception Ratio (PRR), confirm that LoRa can effectively support direct communication from the ground to LEO satellites, but network optimization is required to mitigate collision probability when end nodes use the same Spreading Factors (SFs) over long distances.", "AI": {"tldr": "The paper investigates LoRa's capability to support IoT connectivity via LEO satellite gateways, using simulations to confirm feasibility but highlighting optimization needs.", "motivation": "To establish robust IoT connectivity in remote areas lacking terrestrial infrastructure through the integration of IoT technologies and NTNs.", "method": "Developed ns3-LoRa-NTN simulation module for end-to-end analysis of LoRa-based satellite communication performance.", "result": "LoRa demonstrated feasibility for direct ground-to-LEO satellite communication but with collision issues when sharing Spreading Factors over long distances.", "conclusion": "LoRa is a promising candidate for NTN IoT connectivity, though network adjustments are crucial for optimal performance."}}
{"id": "2601.16096", "pdf": "https://arxiv.org/pdf/2601.16096", "abs": "https://arxiv.org/abs/2601.16096", "authors": ["Hyunsoo Kim", "Ehsan Pajouheshgar", "Sabine S\u00fcsstrunk", "Wenzel Jakob", "Jinah Park"], "title": "Neural Particle Automata: Learning Self-Organizing Particle Dynamics", "categories": ["cs.NE", "cs.CV"], "comment": "15 pages, 15 figures", "summary": "We introduce Neural Particle Automata (NPA), a Lagrangian generalization of Neural Cellular Automata (NCA) from static lattices to dynamic particle systems. Unlike classical Eulerian NCA where cells are pinned to pixels or voxels, NPA model each cell as a particle with a continuous position and internal state, both updated by a shared, learnable neural rule. This particle-based formulation yields clear individuation of cells, allows heterogeneous dynamics, and concentrates computation only on regions where activity is present. At the same time, particle systems pose challenges: neighborhoods are dynamic, and a naive implementation of local interactions scale quadratically with the number of particles. We address these challenges by replacing grid-based neighborhood perception with differentiable Smoothed Particle Hydrodynamics (SPH) operators backed by memory-efficient, CUDA-accelerated kernels, enabling scalable end-to-end training. Across tasks including morphogenesis, point-cloud classification, and particle-based texture synthesis, we show that NPA retain key NCA behaviors such as robustness and self-regeneration, while enabling new behaviors specific to particle systems. Together, these results position NPA as a compact neural model for learning self-organizing particle dynamics.", "AI": {"tldr": "Neural Particle Automata (NPA) generalize Neural Cellular Automata (NCA) to dynamic particle systems, addressing computational scaling issues and enabling self-organizing behaviors in learning particle dynamics.", "motivation": "The paper seeks to extend Neural Cellular Automata (NCA) from static lattices to dynamic particle systems to overcome limitations in classical Eulerian NCA, enabling individualized cells, heterogeneous dynamics, and efficient computation in active regions.", "method": "The authors introduce NPA, a particle-based model where each cell functions as a particle with continuous position and internal state, updated via a shared neural rule. Neighborhood interactions are facilitated efficiently using CUDA-accelerated Smoothed Particle Hydrodynamics (SPH) operators.", "result": "NPA demonstrates robustness, self-regeneration, and the capability to handle tasks like morphogenesis, point-cloud classification, and particle-based texture synthesis, while retaining key NCA behaviors and introducing new ones for particle systems.", "conclusion": "NPA establishes itself as a compact neural model for learning self-organizing dynamics of particle systems, advancing the utility and scalability of neural automata in dynamic environments."}}
{"id": "2601.15298", "pdf": "https://arxiv.org/pdf/2601.15298", "abs": "https://arxiv.org/abs/2601.15298", "authors": ["Anantha Sharma"], "title": "Embedding Retrofitting: Data Engineering for better RAG", "categories": ["cs.CL", "cs.AI", "cs.PF"], "comment": "16 pages, 11 figures, 7 tables", "summary": "Embedding retrofitting adjusts pre-trained word vectors using knowledge graph constraints to improve domain-specific retrieval. However, the effectiveness of retrofitting depends critically on knowledge graph quality, which in turn depends on text preprocessing. This paper presents a data engineering framework that addresses data quality degradation from annotation artifacts in real-world corpora.\n  The analysis shows that hashtag annotations inflate knowledge graph density, leading to creating spurious edges that corrupt the retrofitting objective. On noisy graphs, all retrofitting techniques produce statistically significant degradation ($-3.5\\%$ to $-5.2\\%$, $p<0.05$). After preprocessing, \\acrshort{ewma} retrofitting achieves $+6.2\\%$ improvement ($p=0.0348$) with benefits concentrated in quantitative synthesis questions ($+33.8\\%$ average). The gap between clean and noisy preprocessing (10\\%+ swing) exceeds the gap between algorithms (3\\%), establishing preprocessing quality as the primary determinant of retrofitting success.", "AI": {"tldr": "The paper proposes a data engineering framework for reducing annotation artifacts in real-world corpora to improve retrofitting of word embeddings with knowledge graphs.", "motivation": "The study aims to address the critical issue of annotation artifacts inflating knowledge graph density, which compromises the retrofitting objective for domain-specific retrieval.", "method": "The paper analyzes the effect of hashtag annotations on knowledge graph quality and provides preprocessing steps to reduce noise that disrupts retrofitting.", "result": "Preprocessing significantly mitigates graph noise, enabling improved retrofitting techniques such as EWMA to achieve up to +6.2% improvement, with notable success on quantitative synthesis tasks (+33.8% average improvement).", "conclusion": "Preprocessing quality, not algorithm selection, is the key factor for successful knowledge graph-based retrofitting, emphasizing the importance of data engineering for enhanced performance."}}
{"id": "2601.15807", "pdf": "https://arxiv.org/pdf/2601.15807", "abs": "https://arxiv.org/abs/2601.15807", "authors": ["Tobias Boege", "Antony Della Vecchia", "Marina Garrote-L\u00f3pez", "Benjamin Hollering"], "title": "Algebraic Statistics in OSCAR", "categories": ["stat.CO", "cs.NE", "math.AC", "math.ST"], "comment": null, "summary": "We introduce the AlgebraicStatistics section of the OSCAR computer algebra system. We give an overview of its extensible design and highlight its features including serialization of data types for sharing results and creating databases, and state-of-the-art implicitization algorithms.", "AI": {"tldr": "The paper introduces AlgebraicStatistics in OSCAR, emphasizing design and features.", "motivation": "To enhance computational tools for algebraic statistics.", "method": "Developing extensible design in AlgebraicStatistics within OSCAR.", "result": "Features include serialization of data types and advanced implicitization algorithms.", "conclusion": "AlgebraicStatistics improves result sharing, database creation, and computation in algebraic statistics."}}
{"id": "2601.15476", "pdf": "https://arxiv.org/pdf/2601.15476", "abs": "https://arxiv.org/abs/2601.15476", "authors": ["Alex Dantart"], "title": "Reliability by design: quantifying and eliminating fabrication risk in LLMs. From generative to consultative AI: a comparative analysis in the legal domain and lessons for high-stakes knowledge bases", "categories": ["cs.AI", "cs.PF"], "comment": null, "summary": "This paper examines how to make large language models reliable for high-stakes legal work by reducing hallucinations. It distinguishes three AI paradigms: (1) standalone generative models (\"creative oracle\"), (2) basic retrieval-augmented systems (\"expert archivist\"), and (3) an advanced, end-to-end optimized RAG system (\"rigorous archivist\"). The authors introduce two reliability metrics -False Citation Rate (FCR) and Fabricated Fact Rate (FFR)- and evaluate 2,700 judicial-style answers from 12 LLMs across 75 legal tasks using expert, double-blind review. Results show that standalone models are unsuitable for professional use (FCR above 30%), while basic RAG greatly reduces errors but still leaves notable misgrounding. Advanced RAG, using techniques such as embedding fine-tuning, re-ranking, and self-correction, reduces fabrication to negligible levels (below 0.2%). The study concludes that trustworthy legal AI requires rigor-focused, retrieval-based architectures emphasizing verification and traceability, and provides an evaluation framework applicable to other high-risk domains.", "AI": {"tldr": "The paper analyzes reducing hallucinations in LLM-based legal work, comparing standalone models, basic RAG systems, and advanced RAG systems, emphasizing rigorous, retrieval-based AI for reliability.", "motivation": "To create reliable AI tools for high-stakes legal tasks by addressing the issue of hallucinations in large language models.", "method": "The authors distinguish three AI paradigms, introduce two reliability metrics (FCR, FFR), and evaluate responses from 12 LLMs across 75 legal tasks using expert, double-blind review.", "result": "Standalone generative models are unreliable (FCR above 30%), basic RAG systems improve reliability, and advanced RAG systems reduce fabricated outputs to negligible levels (below 0.2%).", "conclusion": "Trustworthy legal AI requires advanced, verification-focused retrieval architectures and provides a framework for evaluating reliability in other high-risk domains."}}
{"id": "2601.15528", "pdf": "https://arxiv.org/pdf/2601.15528", "abs": "https://arxiv.org/abs/2601.15528", "authors": ["Jiazhu Xie", "Bowen Li", "Heyu Fu", "Chong Gao", "Ziqi Xu", "Fengling Han"], "title": "Securing LLM-as-a-Service for Small Businesses: An Industry Case Study of a Distributed Chatbot Deployment Platform", "categories": ["cs.DC", "cs.CR"], "comment": "Accepted by AISC 2026", "summary": "Large Language Model (LLM)-based question-answering systems offer significant potential for automating customer support and internal knowledge access in small businesses, yet their practical deployment remains challenging due to infrastructure costs, engineering complexity, and security risks, particularly in retrieval-augmented generation (RAG)-based settings. This paper presents an industry case study of an open-source, multi-tenant platform that enables small businesses to deploy customised LLM-based support chatbots via a no-code workflow. The platform is built on distributed, lightweight k3s clusters spanning heterogeneous, low-cost machines and interconnected through an encrypted overlay network, enabling cost-efficient resource pooling while enforcing container-based isolation and per-tenant data access controls. In addition, the platform integrates practical, platform-level defences against prompt injection attacks in RAG-based chatbots, translating insights from recent prompt injection research into deployable security mechanisms without requiring model retraining or enterprise-scale infrastructure. We evaluate the proposed platform through a real-world e-commerce deployment, demonstrating that secure and efficient LLM-based chatbot services can be achieved under realistic cost, operational, and security constraints faced by small businesses.", "AI": {"tldr": "This paper introduces a secure, low-cost platform for small businesses to deploy customized LLM-based chatbots via a no-code workflow, with features addressing infrastructure and security challenges.", "motivation": "Small businesses face challenges such as high costs, engineering complexity, and security risks when deploying LLM-based systems, particularly in RAG scenarios.", "method": "The paper proposes a distributed, no-code platform built on lightweight k3s clusters interconnected through encrypted overlays, ensuring cost efficiency, isolation, and secure data access controls.", "result": "A real-world e-commerce use case demonstrated that this platform effectively delivers secure and efficient LLM-based chatbot services within constraints like cost and security.", "conclusion": "The study concludes that the presented platform enables small businesses to easily and securely deploy LLM-based chatbots, tackling key practical barriers in resource and security management."}}
{"id": "2601.15710", "pdf": "https://arxiv.org/pdf/2601.15710", "abs": "https://arxiv.org/abs/2601.15710", "authors": ["Jiahao Zhang", "Zifan He", "Nicholas Fraser", "Michaela Blott", "Yizhou Sun", "Jason Cong"], "title": "FlexLLM: Composable HLS Library for Flexible Hybrid LLM Accelerator Design", "categories": ["cs.AR", "cs.AI", "cs.LG"], "comment": null, "summary": "We present FlexLLM, a composable High-Level Synthesis (HLS) library for rapid development of domain-specific LLM accelerators. FlexLLM exposes key architectural degrees of freedom for stage-customized inference, enabling hybrid designs that tailor temporal reuse and spatial dataflow differently for prefill and decode, and provides a comprehensive quantization suite to support accurate low-bit deployment. Using FlexLLM, we build a complete inference system for the Llama-3.2 1B model in under two months with only 1K lines of code. The system includes: (1) a stage-customized accelerator with hardware-efficient quantization (12.68 WikiText-2 PPL) surpassing SpinQuant baseline, and (2) a Hierarchical Memory Transformer (HMT) plug-in for efficient long-context processing. On the AMD U280 FPGA at 16nm, the accelerator achieves 1.29$\\times$ end-to-end speedup, 1.64$\\times$ higher decode throughput, and 3.14$\\times$ better energy efficiency than an NVIDIA A100 GPU (7nm) running BF16 inference; projected results on the V80 FPGA at 7nm reach 4.71$\\times$, 6.55$\\times$, and 4.13$\\times$, respectively. In long-context scenarios, integrating the HMT plug-in reduces prefill latency by 23.23$\\times$ and extends the context window by 64$\\times$, delivering 1.10$\\times$/4.86$\\times$ lower end-to-end latency and 5.21$\\times$/6.27$\\times$ higher energy efficiency on the U280/V80 compared to the A100 baseline. FlexLLM thus bridges algorithmic innovation in LLM inference and high-performance accelerators with minimal manual effort.", "AI": {"tldr": "The paper introduces FlexLLM, an HLS library enabling rapid development of efficient accelerators for LLM inference with significant hardware and energy improvements.", "motivation": "To address the need for domain-specific accelerators for LLMs that balance rapid development and high-performance inference, especially in scenarios requiring hybrid designs and efficient quantization.", "method": "They developed FlexLLM, an HLS library providing customization for temporal reuse and spatial dataflow optimization, a quantization toolkit, and support for efficient long-context LLM processing.", "result": "The FlexLLM-based system achieves significant improvements in speed, throughput, and energy efficiency compared to traditional GPU solutions. For example, it achieves up to 6.55\u00d7 decode throughput and 6.27\u00d7 energy efficiency on an FPGA compared to the NVIDIA A100 GPU.", "conclusion": "FlexLLM provides a streamlined way to prototype and deploy high-performance accelerators for LLMs quickly, bridging the gap between advanced algorithmic innovations and hardware efficiency."}}
{"id": "2601.15305", "pdf": "https://arxiv.org/pdf/2601.15305", "abs": "https://arxiv.org/abs/2601.15305", "authors": ["Alfred Shen", "Aaron Shen"], "title": "Gated Sparse Attention: Combining Computational Efficiency with Training Stability for Long-Context Language Models", "categories": ["cs.AI"], "comment": "15 pages, 1 figure, attention mechanism, sparse attention, gating, long-context", "summary": "The computational burden of attention in long-context language models has motivated two largely independent lines of work: sparse attention mechanisms that reduce complexity by attending to selected tokens, and gated attention variants that improve training sta-bility while mitigating the attention sink phenomenon. We observe that these approaches address complementary weaknesses and propose Gated Sparse Attention (GSA), an architecture that realizes the benefits of both. GSA incorporates a gated lightning indexer with sigmoid activations that produce bounded, interpretable selection scores, an adaptive sparsity controller that modulates the number of attended tokens based on local uncertainty, and dual gating at the value and output stages. We establish theoretical foundations for the approach, including complexity analysis, expressiveness results, and convergence guarantees. In experiments with 1.7B parameter models trained on 400B tokens, GSA matches the efficiency of sparse-only baselines (12-16x speedup at 128K context) while achieving the quality gains associated with gated attention: perplexity improves from 6.03 to 5.70, RULER scores at 128K context nearly double, and attention to the first token, a proxy for attention sinks, drops from 47% to under 4%. Training stability improves markedly, with loss spikes reduced by 98%.", "AI": {"tldr": "This paper proposes Gated Sparse Attention (GSA), which integrates sparse and gated attention techniques for long-context language models, achieving significant efficiency and accuracy improvements.", "motivation": "To address the computational burden in long-context language models while maintaining training stability and mitigating the attention sink phenomenon.", "method": "The authors introduce GSA, a novel architecture combining a gated lightning indexer, adaptive sparsity controller, and dual gating mechanisms. The method includes theoretical analysis and efficiency-focused design.", "result": "GSA achieves a 12-16x speedup (128K context), improves perplexity (6.03 -> 5.70), doubles RULER scores, reduces attention sink (from 47% to under 4%), and decreases training loss spikes by 98%.", "conclusion": "GSA successfully combines sparsity and gating mechanisms to optimize long-context language models, improving both computational efficiency and performance metrics."}}
{"id": "2601.15455", "pdf": "https://arxiv.org/pdf/2601.15455", "abs": "https://arxiv.org/abs/2601.15455", "authors": ["Patrycja Balik", "Szymon J\u0119dras", "Piotr Polesiuk"], "title": "Remarks on Algebraic Reconstruction of Types and Effects", "categories": ["cs.PL"], "comment": null, "summary": "In their 1991 paper \"Algebraic Reconstruction of Types and Effects,\" Pierre Jouvelot and David Gifford presented a type-and-effect reconstruction algorithm based on an algebraic structure of effects. Their work is considered a milestone in the development of type-and-effect systems, and has inspired numerous subsequent works in the area of static analysis. However, unlike the later research it spawned, the original algorithm considered a language with higher-rank polymorphism, a feature which is challenging to implement correctly. In this note, we identify subtle bugs related to variable binding in their approach to this feature. We revisit their type system and reconstruction algorithm, and describe the discovered issues.", "AI": {"tldr": "The paper revisits Jouvelot and Gifford's 1991 type-and-effect reconstruction algorithm, identifying bugs in its handling of higher-rank polymorphism and proposing corrections.", "motivation": "The authors aimed to revisit a seminal 1991 paper by Jouvelot and Gifford, addressing bugs in its approach to handling higher-rank polymorphism in type-and-effect reconstruction.", "method": "The authors analyzed the original algorithm's treatment of variable binding, identified bugs, and reassessed the type system as well as reconstruction algorithm.", "result": "Issues related to variable binding in the original type-and-effect reconstruction algorithm were discovered, highlighting specific flaws in handling higher-rank polymorphism.", "conclusion": "The paper suggests corrections to the original algorithm, improving its robustness and correctness in handling higher-rank polymorphism."}}
{"id": "2601.15313", "pdf": "https://arxiv.org/pdf/2601.15313", "abs": "https://arxiv.org/abs/2601.15313", "authors": ["Matt Beton", "Simran Chana"], "title": "Mind the Gap: Why Neural Memory Fails Under Semantic Density", "categories": ["q-bio.NC", "cs.AI"], "comment": "24 Pages, 5 Figures", "summary": "The brain solves a problem that current AI architectures struggle to manage: storing specific episodic facts without corrupting general semantic knowledge. Neuroscience explains this through Complementary Learning Systems theory - a fast hippocampal system for episodic storage using pattern-separated representations, and a slow neocortical system for extracting statistical regularities. Current AI systems lack this separation, attempting both functions through neural weights alone. We identify the 'Stability Gap' in online neural memory: fast-weight mechanisms that write facts into shared continuous parameters collapse to near-random accuracy within tens of semantically related facts. Through semantic density (rho), we show collapse occurs with as few as N=5 facts at high density (rho > 0.6) or N ~ 20-75 at moderate density - a phenomenon we formalise as the Orthogonality Constraint. This failure persists even with perfect attention and unlimited context, arising from write-time interference when storage and retrieval share the same substrate. We also identify schema drift and version ambiguity as primary failure modes in production systems, observing 40-70% schema consistency and 0-100% clean correction rates. Context-based memory incurs 30-300% cost premium over selective retrieval. We propose Knowledge Objects (KOs): discrete, typed memory units with controlled vocabularies and explicit version chains. Paired with neural weights, KOs enable a true complementary learning architecture, suggesting reliable AI memory may require this bicameral design.", "AI": {"tldr": "The paper discusses the 'Stability Gap' issue in AI memory systems, where neural weights fail to effectively store episodic facts while preserving semantic knowledge, and introduces Knowledge Objects to mitigate these issues.", "motivation": "Current AI systems struggle to simultaneously store episodic facts and semantic knowledge effectively without interference, unlike the human brain's complementary learning systems.", "method": "The authors analyze AI memory systems using semantic density modeling and introduce the Orthogonality Constraint to explain memory collapse. They propose Knowledge Objects as an alternate memory architecture.", "result": "They identify key issues such as write-time interference, schema drift, and the inefficiencies of context-based memory. Results show collapse with high and moderate semantic densities, affecting accuracy and cost-efficiency.", "conclusion": "The proposed bicameral architecture combining neural networks with Knowledge Objects could enhance AI memory reliability and emulate the brain's complementary learning process."}}
{"id": "2601.15296", "pdf": "https://arxiv.org/pdf/2601.15296", "abs": "https://arxiv.org/abs/2601.15296", "authors": ["Longxuan Wei", "Yubo Zhang", "Zijiao Zhang", "Zhihu Wang", "Shiwan Zhao", "Tianyu Huang", "Huiting Zhao", "Chenfei Liu", "Shenao Zhang", "Junchi Yan"], "title": "Entropy-Tree: Tree-Based Decoding with Entropy-Guided Exploration", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models achieve strong reasoning performance, yet existing decoding strategies either explore blindly (random sampling) or redundantly (independent multi-sampling). We propose Entropy-Tree, a tree-based decoding method that exploits entropy as a signal for branching decisions--expanding the search tree only at positions where the model exhibits genuine uncertainty. Entropy-Tree shows superior accuracy and calibration in reasoning tasks: it achieves better pass@k than Multi-chain across multiple models and datasets, and its predictive entropy demonstrates better AUROC compared to several traditional metrics. Entropy-Tree unifies efficient structured exploration and reliable uncertainty estimation within a single decoding procedure.", "AI": {"tldr": "The paper introduces Entropy-Tree, a decoding strategy for large language models that leverages model uncertainty, improving reasoning accuracy and uncertainty estimation efficiency.", "motivation": "Existing decoding methods in language models either lack direction or explore redundantly, leading to inefficiencies in reasoning tasks. The paper aims to optimize exploration and uncertainty estimation.", "method": "Entropy-Tree utilizes entropy to guide tree-based decoding, expanding search trees based on genuine uncertainty in the model's predictions.", "result": "Entropy-Tree surpasses Multi-chain in pass@k metrics across various models and datasets, and its predictive entropy achieves better AUROC than traditional metrics.", "conclusion": "Entropy-Tree effectively integrates structured exploration and reliable uncertainty estimation, improving reasoning performance in language models."}}
{"id": "2601.15366", "pdf": "https://arxiv.org/pdf/2601.15366", "abs": "https://arxiv.org/abs/2601.15366", "authors": ["Christina Thrainer"], "title": "AI-Based Culvert-Sewer Inspection", "categories": ["cs.CV"], "comment": "Masters thesis, University of Technology Graz, 2025", "summary": "Culverts and sewer pipes are critical components of drainage systems, and their failure can lead to serious risks to public safety and the environment. In this thesis, we explore methods to improve automated defect segmentation in culverts and sewer pipes. Collecting and annotating data in this field is cumbersome and requires domain knowledge. Having a large dataset for structural defect detection is therefore not feasible. Our proposed methods are tested under conditions with limited annotated data to demonstrate applicability to real-world scenarios. Overall, this thesis proposes three methods to significantly enhance defect segmentation and handle data scarcity. This can be addressed either by enhancing the training data or by adjusting a models architecture.\n  First, we evaluate preprocessing strategies, including traditional data augmentation and dynamic label injection. These techniques significantly improve segmentation performance, increasing both Intersection over Union (IoU) and F1 score. Second, we introduce FORTRESS, a novel architecture that combines depthwise separable convolutions, adaptive Kolmogorov-Arnold Networks (KAN), and multi-scale attention mechanisms. FORTRESS achieves state-of-the-art performance on the culvert sewer pipe defect dataset, while significantly reducing the number of trainable parameters, as well as its computational cost. Finally, we investigate few-shot semantic segmentation and its applicability to defect detection. Few-shot learning aims to train models with only limited data available. By employing a bidirectional prototypical network with attention mechanisms, the model achieves richer feature representations and achieves satisfactory results across evaluation metrics.", "AI": {"tldr": "The paper develops methods to improve defect segmentation in culverts and sewer pipes with limited data, using preprocessing, novel architectures, and few-shot learning.", "motivation": "The motivation is the difficulty of collecting and annotating data for structural defect detection in drainage systems, as well as the risks posed by culvert and sewer pipe failures.", "method": "Three approaches were proposed: 1) Data preprocessing using augmentation and dynamic label injection; 2) Developing a novel architecture called FORTRESS that leverages advanced techniques like multi-scale attention and adaptive networks; and 3) Analyzing few-shot learning using bidirectional prototypical networks with attention.", "result": "The proposed methods achieved improved defect segmentation performance, such as higher IoU and F1 scores, reduced computational requirements, and effective predictions with limited data.", "conclusion": "The thesis offers practical solutions to enhance defect detection in drainage systems, addressing challenges of data scarcity and computational efficiency while achieving state-of-the-art results."}}
{"id": "2601.15360", "pdf": "https://arxiv.org/pdf/2601.15360", "abs": "https://arxiv.org/abs/2601.15360", "authors": ["Eichi Uehara"], "title": "Robust X-Learner: Breaking the Curse of Imbalance and Heavy Tails via Robust Cross-Imputation", "categories": ["stat.ML", "cs.LG", "econ.EM", "stat.ME"], "comment": "17 pages, 4 figures, 4 tables", "summary": "Estimating Heterogeneous Treatment Effects (HTE) in industrial applications such as AdTech and healthcare presents a dual challenge: extreme class imbalance and heavy-tailed outcome distributions. While the X-Learner framework effectively addresses imbalance through cross-imputation, we demonstrate that it is fundamentally vulnerable to \"Outlier Smearing\" when reliant on Mean Squared Error (MSE) minimization. In this failure mode, the bias from a few extreme observations (\"whales\") in the minority group is propagated to the entire majority group during the imputation step, corrupting the estimated treatment effect structure. To resolve this, we propose the Robust X-Learner (RX-Learner). This framework integrates a redescending \u03b3-divergence objective -- structurally equivalent to the Welsch loss under Gaussian assumptions -- into the gradient boosting machinery. We further stabilize the non-convex optimization using a Proxy Hessian strategy grounded in Majorization-Minimization (MM) principles. Empirical evaluation on a semi-synthetic Criteo Uplift dataset demonstrates that the RX-Learner reduces the Precision in Estimation of Heterogeneous Effect (PEHE) metric by 98.6% compared to the standard X-Learner, effectively decoupling the stable \"Core\" population from the volatile \"Periphery\".", "AI": {"tldr": "The paper addresses challenges in estimating heterogeneous treatment effects in industrial applications by proposing a robust solution to counteract outlier bias.", "motivation": "Address challenges of class imbalance and heavy-tailed outcome distributions in estimating heterogeneous treatment effects.", "method": "Introduced the RX-Learner integrating \u03b3-divergence objective and stabilization through a Proxy Hessian strategy.", "result": "Empirical evaluation shows RX-Learner reduces estimation error (PEHE metric) by 98.6% compared to X-Learner.", "conclusion": "RX-Learner successfully mitigates bias from outliers, providing more accurate and stable treatment effect estimations."}}
{"id": "2601.15309", "pdf": "https://arxiv.org/pdf/2601.15309", "abs": "https://arxiv.org/abs/2601.15309", "authors": ["Jiaxin Xu", "Chao Zhang", "Raymond H. Cuijpers", "Wijnand A. IJsselsteijn"], "title": "Designing Persuasive Social Robots for Health Behavior Change: A Systematic Review of Behavior Change Strategies and Evaluation Methods", "categories": ["cs.RO", "cs.HC"], "comment": "Accepted to HRI 2026", "summary": "Social robots are increasingly applied as health behavior change interventions, yet actionable knowledge to guide their design and evaluation remains limited. This systematic review synthesizes (1) the behavior change strategies used in existing HRI studies employing social robots to promote health behavior change, and (2) the evaluation methods applied to assess behavior change outcomes. Relevant literature was identified through systematic database searches and hand searches. Analysis of 39 studies revealed four overarching categories of behavior change strategies: coaching strategies, counseling strategies, social influence strategies, and persuasion-enhancing strategies. These strategies highlight the unique affordances of social robots as behavior change interventions and offer valuable design heuristics. The review also identified key characteristics of current evaluation practices, including study designs, settings, durations, and outcome measures, on the basis of which we propose several directions for future HRI research.", "AI": {"tldr": "This review examines the behavior change strategies and evaluation methods in studies using social robots for promoting health behavior change. It identifies four strategy categories and proposes directions for future research.", "motivation": "The motivation is to address the limited actionable knowledge for designing and evaluating social robots as health behavior change interventions.", "method": "Conducted a systematic review using database searches and hand searches, analyzing 39 studies involving social robots.", "result": "Four categories of behavior change strategies were uncovered: coaching, counseling, social influence, and persuasion-enhancing strategies. Evaluation characteristics and gaps were also identified.", "conclusion": "Social robots have unique capacities for facilitating health behavior change. Future HRI research should improve on current designs and evaluations for more effective interventions."}}
{"id": "2601.15335", "pdf": "https://arxiv.org/pdf/2601.15335", "abs": "https://arxiv.org/abs/2601.15335", "authors": ["Yi Zhai", "Dian Shen", "Junzhou Luo", "Bin Yang"], "title": "ToolCaching: Towards Efficient Caching for LLM Tool-calling", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have revolutionized web applications, enabling intelligent search, recommendation, and assistant services with natural language interfaces. Tool-calling extends LLMs with the ability to interact with external APIs, greatly enhancing their practical utility. While prior research has improved tool-calling performance by adopting traditional computer systems techniques, such as parallel and asynchronous execution, the challenge of redundant or repeated tool-calling requests remains largely unaddressed. Caching is a classic solution to this problem, but applying it to LLM tool-calling introduces new difficulties due to heterogeneous request semantics, dynamic workloads, and varying freshness requirements, which render conventional cache policies ineffective. To address these issues, we propose ToolCaching, an efficient feature-driven and adaptive caching framework for LLM tool-calling systems. ToolCaching systematically integrates semantic and system-level features to evaluate request cacheability and estimate caching value. At its core, the VAAC algorithm integrates bandit-based admission with value-driven, multi-factor eviction, jointly accounting for request frequency, recency, and caching value. Extensive experiments on synthetic and public tool-calling workloads demonstrate that ToolCaching with VAAC achieves up to 11% higher cache hit ratios and 34% lower latency compared to standard policies, effectively accelerating LLM tool-calling in practical applications.", "AI": {"tldr": "The paper introduces ToolCaching, a framework to enhance large language model (LLM) tool-calling by tackling redundant tool-calling with adaptive caching, improving efficiency and reducing latency.", "motivation": "While LLM tool-calling has enhanced application utilities, challenges like redundant tool-calling requests remain unresolved, necessitating an advanced caching solution to address request variability and freshness.", "method": "The authors propose ToolCaching, which combines semantic and system-level features to assess cacheability. The core VAAC algorithm uses bandit-based admission and multi-factor eviction to optimize caching by accounting for various request attributes.", "result": "ToolCaching demonstrated up to an 11% improvement in cache hit ratios and a 34% reduction in latency during experiments with synthetic and public workloads.", "conclusion": "ToolCaching effectively addresses the challenges of caching in LLM tool-calling by using adaptive, feature-driven strategies, significantly improving efficiency and performance in practical applications."}}
{"id": "2601.15333", "pdf": "https://arxiv.org/pdf/2601.15333", "abs": "https://arxiv.org/abs/2601.15333", "authors": ["Xuanning Hu", "Anchen Li", "Qianli Xing", "Jinglong Ji", "Hao Tuo", "Bo Yang"], "title": "Empowering LLMs for Structure-Based Drug Design via Exploration-Augmented Latent Inference", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Large Language Models (LLMs) possess strong representation and reasoning capabilities, but their application to structure-based drug design (SBDD) is limited by insufficient understanding of protein structures and unpredictable molecular generation. To address these challenges, we propose Exploration-Augmented Latent Inference for LLMs (ELILLM), a framework that reinterprets the LLM generation process as an encoding, latent space exploration, and decoding workflow. ELILLM explicitly explores portions of the design problem beyond the model's current knowledge while using a decoding module to handle familiar regions, generating chemically valid and synthetically reasonable molecules. In our implementation, Bayesian optimization guides the systematic exploration of latent embeddings, and a position-aware surrogate model efficiently predicts binding affinity distributions to inform the search. Knowledge-guided decoding further reduces randomness and effectively imposes chemical validity constraints. We demonstrate ELILLM on the CrossDocked2020 benchmark, showing strong controlled exploration and high binding affinity scores compared with seven baseline methods. These results demonstrate that ELILLM can effectively enhance LLMs capabilities for SBDD.", "AI": {"tldr": "This paper introduces ELILLM, a framework enhancing Large Language Models (LLMs) for structure-based drug design (SBDD) by utilizing a latent space exploration mechanism, achieving better molecule generation and binding affinity.", "motivation": "LLMs have limitations in comprehending protein structures and generating predictable molecules for SBDD.", "method": "The authors propose ELILLM, which transforms the LLM generation process into encoding, latent space exploration (guided by Bayesian optimization), and decoding using a knowledge-guided module.", "result": "ELILLM achieved high binding affinity scores and outperformed seven baseline methods on the CrossDocked2020 benchmark.", "conclusion": "ELILLM enhances the ability of LLMs to tackle complex SBDD problems through systematic exploration and informed molecule generation strategies."}}
{"id": "2601.16118", "pdf": "https://arxiv.org/pdf/2601.16118", "abs": "https://arxiv.org/abs/2601.16118", "authors": ["Marco Ronzani", "Cristina Silvano"], "title": "A Case for Hypergraphs to Model and Map SNNs on Neuromorphic Hardware", "categories": ["cs.AR", "cs.NE"], "comment": null, "summary": "Executing Spiking Neural Networks (SNNs) on neuromorphic hardware poses the problem of mapping neurons to cores. SNNs operate by propagating spikes between neurons that form a graph through synapses. Neuromorphic hardware mimics them through a network-on-chip, transmitting spikes, and a mesh of cores, each managing several neurons. Its operational cost is tied to spike movement and active cores. A mapping comprises two tasks: partitioning the SNN's graph to fit inside cores and placement of each partition on the hardware mesh. Both are NP-hard problems, and as SNNs and hardware scale towards billions of neurons, they become increasingly difficult to tackle effectively. In this work, we propose to raise the abstraction of SNNs from graphs to hypergraphs, redesigning mapping techniques accordingly. The resulting model faithfully captures the replication of spikes inside cores by exposing the notion of hyperedge co-membership between neurons. We further show that the overlap and locality of hyperedges strongly correlate with high-quality mappings, making these properties instrumental in devising mapping algorithms. By exploiting them directly, grouping neurons through shared hyperedges, communication traffic and hardware resource usage can be reduced be yond what just contracting individual connections attains. To substantiate this insight, we consider several partitioning and placement algorithms, some newly devised, others adapted from literature, and compare them over progressively larger and bio-plausible SNNs. Our results show that hypergraph based techniques can achieve better mappings than the state-of-the-art at several execution time regimes. Based on these observations, we identify a promising selection of algorithms to achieve effective mappings at any scale.", "AI": {"tldr": "The paper proposes using hypergraphs instead of traditional graphs for mapping Spiking Neural Networks (SNNs) to neuromorphic hardware, achieving improved performance metrics.", "motivation": "The challenge lies in mapping massive Spiking Neural Networks (SNNs) onto neuromorphic hardware efficiently, as scaling complicates partitioning and placement due to operational costs and NP-hard problems.", "method": "The authors elevate SNN abstractions to hypergraphs, redesign mapping methods, and introduce algorithms exploiting hypergraph properties like hyperedge co-membership and locality.", "result": "Experimental comparisons showed that hypergraph-based techniques outperformed state-of-the-art graph-based methods for mapping, including across larger and bio-realistic SNNs.", "conclusion": "Hypergraph-based approaches are demonstrated to be superior in reducing communication traffic and resource usage, offering scalable solutions for mapping SNNs."}}
{"id": "2601.15924", "pdf": "https://arxiv.org/pdf/2601.15924", "abs": "https://arxiv.org/abs/2601.15924", "authors": ["Brainard Philemon Jagati", "Jitendra Tembhurne", "Harsh Goud", "Rudra Pratap Singh", "Chandrashekhar Meshram"], "title": "Class Confidence Aware Reweighting for Long Tailed Learning", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.PF"], "comment": "9 pages, 3 figures, IEEE Transaction on Neural Networks and Learning Systems (Submitted)", "summary": "Deep neural network models degrade significantly in the long-tailed data distribution, with the overall training data dominated by a small set of classes in the head, and the tail classes obtaining less training examples. Addressing the imbalance in the classes, attention in the related literature was given mainly to the adjustments carried out in the decision space in terms of either corrections performed at the logit level in order to compensate class-prior bias, with the least attention to the optimization process resulting from the adjustments introduced through the differences in the confidences among the samples. In the current study, we present the design of a class and confidence-aware re-weighting scheme for long-tailed learning. This scheme is purely based upon the loss level and has a complementary nature to the existing methods performing the adjustment of the logits. In the practical implementation stage of the proposed scheme, we use an \u03a9(p_t, f_c) function. This function enables the modulation of the contribution towards the training task based upon the confidence value of the prediction, as well as the relative frequency of the corresponding class. Our observations in the experiments are corroborated by significant experimental results performed on the CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets under various values of imbalance factors that clearly authenticate the theoretical discussions above.", "AI": {"tldr": "A new re-weighting scheme is introduced for deep learning in long-tailed data distributions, focusing on class and confidence-aware adjustments rather than logit-level corrections.", "motivation": "To address the degradation of deep neural network performance caused by imbalanced data distributions where tail classes have fewer training examples.", "method": "Developed a loss-level re-weighting scheme, combining class frequency and confidence modulation using the \u03a9(p_t, f_c) function.", "result": "Experimental results on CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets demonstrate significant improvement in long-tailed learning performance.", "conclusion": "The proposed scheme effectively complements existing logit-based adjustments, improving model performance in long-tailed data distributions."}}
{"id": "2601.15633", "pdf": "https://arxiv.org/pdf/2601.15633", "abs": "https://arxiv.org/abs/2601.15633", "authors": ["Enzo Meneses", "Hugo Bec", "Crist\u00f3bal A. Navarroa", "Beno\u00eet Crespin", "Felipe A. Quezada", "Nancy Hitschfeld", "Heinich Porro", "Maxime Maria"], "title": "Advancing RT Core-Accelerated Fixed-Radius Nearest Neighbor Search", "categories": ["cs.DC"], "comment": "Journal submission", "summary": "In this work we introduce three ideas that can further improve particle FRNN physics simulations running on RT Cores; i) a real-time update/rebuild ratio optimizer for the bounding volume hierarchy (BVH) structure, ii) a new RT core use, with two variants, that eliminates the need of a neighbor list and iii) a technique that enables RT cores for FRNN with periodic boundary conditions (BC). Experimental evaluation using the Lennard-Jones FRNN interaction model as a case study shows that the proposed update/rebuild ratio optimizer is capable of adapting to the different dynamics that emerge during a simulation, leading to a RT core pipeline up to $\\sim 3.4\\times$ faster than with other known approaches to manage the BVH. In terms of simulation step performance, the proposed variants can significantly improve the speedup and EE of the base RT core idea; from $\\sim1.3\\times$ at small radius to $\\sim2.0\\times$ for log normal radius distributions. Furthermore, the proposed variants manage to simulate cases that would otherwise not fit in memory because of the use of neighbor lists, such as clusters of particles with log normal radius distribution. The proposed RT Core technique to support periodic BC is indeed effective as it does not introduce any significant penalty in performance. In terms of scaling, the proposed methods scale both their performance and EE across GPU generations. Throughout the experimental evaluation, we also identify the simulation cases were regular GPU computation should still be preferred, contributing to the understanding of the strengths and limitations of RT cores.", "AI": {"tldr": "This paper proposes new optimizations for particle FRNN physics simulations on RT cores.", "motivation": "Current methods for particle FRNN physics simulations face performance and memory challenges, particularly with BVH structures and periodic boundary conditions.", "method": "The authors introduce three innovations: a real-time BVH update/rebuild optimizer, novel RT core usage eliminating neighbor lists, and a technique enabling RT cores for periodic boundary conditions.", "result": "The authors achieved up to 3.4x improvement compared to existing BVH methods and significant performance and energy efficiency gains in simulations, handling cases previously limited by memory.", "conclusion": "The advancements in RT core-based FRNN simulations are effective, scalable across GPU generations, and help identify scenarios where standard GPU computation remains preferable."}}
{"id": "2601.15306", "pdf": "https://arxiv.org/pdf/2601.15306", "abs": "https://arxiv.org/abs/2601.15306", "authors": ["Ethan Zhang"], "title": "Uncovering Latent Bias in LLM-Based Emergency Department Triage Through Proxy Variables", "categories": ["cs.AI"], "comment": "15 pages, 3 figures", "summary": "Recent advances in large language models (LLMs) have enabled their integration into clinical decision-making; however, hidden biases against patients across racial, social, economic, and clinical backgrounds persist. In this study, we investigate bias in LLM-based medical AI systems applied to emergency department (ED) triage. We employ 32 patient-level proxy variables, each represented by paired positive and negative qualifiers, and evaluate their effects using both public (MIMIC-IV-ED Demo, MIMIC-IV Demo) and restricted-access credentialed (MIMIC-IV-ED and MIMIC-IV) datasets as appropriate~\\cite{mimiciv_ed_demo,mimiciv_ed,mimiciv}. Our results reveal discriminatory behavior mediated through proxy variables in ED triage scenarios, as well as a systematic tendency for LLMs to modify perceived patient severity when specific tokens appear in the input context, regardless of whether they are framed positively or negatively. These findings indicate that AI systems is still imperfectly trained on noisy, sometimes non-causal signals that do not reliably reflect true patient acuity. Consequently, more needs to be done to ensure the safe and responsible deployment of AI technologies in clinical settings.", "AI": {"tldr": "The paper investigates bias in large language model (LLM)-based medical AI for emergency department triage, finding discriminatory behavior influenced by non-reliable patient variables.", "motivation": "To evaluate and address hidden biases in LLM-based clinical AI systems, specifically in emergency department triage.", "method": "The study uses 32 proxy variables with paired positive and negative qualifiers, analyzing their effects with public and restricted-access datasets like MIMIC-IV-ED and MIMIC-IV.", "result": "The study reveals discriminatory behavior in the AI systems, with LLMs modifying patient severity perception based on input context, regardless of framing.", "conclusion": "AI clinical models remain sensitive to noisy, non-causal data, highlighting the need for improvements before safe clinical implementation."}}
{"id": "2601.16008", "pdf": "https://arxiv.org/pdf/2601.16008", "abs": "https://arxiv.org/abs/2601.16008", "authors": ["Federico Bruzzone", "Walter Cazzola", "Luca Favini"], "title": "Prioritizing Configuration Relevance via Compiler-Based Refined Feature Ranking", "categories": ["cs.PL"], "comment": "29 pages 4 figures", "summary": "Modern programming languages, most notably Rust, offer advanced linguistic constructs for building highly configurable software systems as aggregation of features -- identified by a configuration. However, they pose substantial challenges for program analysis, optimization, and testing, as the combinatorial explosion of configurations often makes exhaustive exploration infeasible. In this manuscript, we present the first compiler-based method for prioritizing configurations. Our approach consists of four main steps: 1. extracting a tailored intermediate representation from the Rust compiler, 2. constructing two complementary graph-based data structures, 3. using centrality measures to rank features, and 4. refining the ranking by considering the extent of code they impact. A fixed number of most relevant configurations are generated based on the achieved feature ranking. The validity of the generated configurations is guaranteed by using a SAT solver that takes a representation of this graph in conjunctive normal form. We formalized this approach and implemented it in a prototype, RustyEx, by instrumenting the Rust compiler. An empirical evaluation on higher-ranked open source Rust projects shows that RustyEx efficiently generates user-specified sets of configurations within bounded resources, while ensuring soundness by construction. The results demonstrate that centrality-guided configuration prioritization enables effective and practical exploration of large configuration spaces, paving the way for future research in configuration-aware analysis and optimization.", "AI": {"tldr": "The paper introduces RustyEx, a compiler-based tool for prioritizing configurations in Rust software for analysis and optimization, ensuring sound, efficient exploration of large configuration spaces.", "motivation": "The motivation is to address the challenges posed by combinatorial explosion in configurations within software systems, making exhaustive exploration infeasible for analysis, optimization, and testing.", "method": "The method involves constructing a graph-based system, ranking configurations using centrality measures, and refining rankings based on code impact, finalized using a SAT solver for valid configuration generation. The tool, RustyEx, was implemented into the Rust compiler.", "result": "RustyEx efficiently prioritized configurations in open-source Rust projects, generating configuration sets within limited resources and ensuring soundness.", "conclusion": "Centrality-guided configuration prioritization proves to be effective, practical, and paves the way for better configuration-aware analysis, optimization, and research."}}
{"id": "2601.15314", "pdf": "https://arxiv.org/pdf/2601.15314", "abs": "https://arxiv.org/abs/2601.15314", "authors": ["Lalit Kumar Shukla"], "title": "Beyond the Einstein-Bohr Debate: Cognitive Complementarity and the Emergence of Quantum Intuition", "categories": ["q-bio.NC", "cs.AI", "quant-ph"], "comment": "This interdisciplinary work bridges quantum foundations and cognitive science, proposing a formal extension of complementarity into cognitive reasoning and introducing the testable construct of quantum intuition", "summary": "Recent high-precision experimental confirmations of quantum complementarity have revitalized foundational debates about measurement, description, and realism. This article argues that complementarity is most productively interpreted as an epistemic principle--constraining what can be simultaneously accessed and represented--rather than as an ontological claim about quantum reality. Reexamining the Einstein-Bohr debate through this lens reveals a persistent tension between descriptive completeness and contextual meaning, a tension experiments clarify but do not dissolve. Building on this analysis, we introduce cognitive complementarity as a structural principle governing reasoning under non-classical uncertainty, where mutually constraining representations cannot be jointly optimized. Within this framework, we propose quantum intuition as a testable cognitive capacity: the ability to sustain representational plurality, regulate commitment timing, and resolve perspective-incompatibilities in a context-sensitive manner. Formulated as a naturalistic construct grounded in shared informational constraints, quantum intuition offers a principled bridge between quantum measurement theory and cognition. This work reframes the historical debate, extends epistemic lessons from quantum foundations into cognitive science, and outlines empirical pathways for studying decision-making in contexts of irreducible uncertainty.", "AI": {"tldr": "Discusses interpreting quantum complementarity as an epistemic principle, introduces cognitive complementarity and quantum intuition to connect quantum theory with human cognition.", "motivation": "The paper is motivated by renewed interest in quantum complementarity and its implications for measurement, description, and realism, alongside a desire to extend its epistemic lessons to cognitive science.", "method": "Reexamines historical debates, particularly the Einstein-Bohr discussion, to explore epistemic interpretations and introduces cognitive complementarity as a reasoning framework.", "result": "Identifies quantum intuition as a cognitive capacity for managing representational plurality and irreducible uncertainty while linking quantum theory insights to decision-making processes.", "conclusion": "The study reinterprets foundational quantum debates, bridges quantum principles with human cognition, and suggests empirical approaches to studying decision-making under uncertainty."}}
{"id": "2601.15297", "pdf": "https://arxiv.org/pdf/2601.15297", "abs": "https://arxiv.org/abs/2601.15297", "authors": ["Edward Ajayi"], "title": "AfriEconQA: A Benchmark Dataset for African Economic Analysis based on World Bank Reports", "categories": ["cs.CL"], "comment": null, "summary": "We introduce AfriEconQA, a specialized benchmark dataset for African economic analysis grounded in a comprehensive corpus of 236 World Bank reports. The task of AfriEconQA is to answer complex economic queries that require high-precision numerical reasoning and temporal disambiguation from specialized institutional documents. The dataset consists of 8,937 curated QA instances, rigorously filtered from a pool of 10018 synthetic questions to ensure high-quality evidence-answer alignment. Each instance is composed of: (1) a question requiring reasoning over economic indicators, (2) the corresponding evidence retrieved from the corpus, (3) a verified ground-truth answer, and (4) source metadata (e.g., URL and publication date) to ensure temporal provenance. AfriEconQA is the first benchmark focused specifically on African economic analysis, providing a unique challenge for Information Retrieval (IR) systems, as the data is largely absent from the pretraining corpora of current Large Language Models (LLMs). We operationalize this dataset through an 11-experiment matrix, benchmarking a zero-shot baseline (GPT-5 Mini) against RAG configurations using GPT-4o and Qwen 32B across five distinct embedding and ranking strategies. Our results demonstrate a severe parametric knowledge gap, where zero-shot models fail to answer over 90 percent of queries, and even state-of-the-art RAG pipelines struggle to achieve high precision. This confirms AfriEconQA as a robust and challenging benchmark for the next generation of domain-specific IR and RAG systems. The AfriEconQA dataset and code will be made publicly available upon publication.", "AI": {"tldr": "This study introduces AfriEconQA, a benchmark dataset for African economic analysis, consisting of 8,937 high-quality QA instances based on World Bank reports. Through rigorous experiments, it identifies challenges for current IR and RAG systems and highlights a knowledge gap in LLMs for domain-specific queries.", "motivation": "To address the lack of datasets explicitly focused on African economic analysis and to provide a resource for testing and improving domain-specific IR systems and RAG capabilities.", "method": "The dataset was curated from a corpus of 236 World Bank reports, generating 8,937 high-quality QA instances through evidence alignment and filtering. Various models, including GPT-4o and Qwen 32B, were evaluated through an 11-experiment matrix.", "result": "Zero-shot models like GPT-5 Mini failed over 90% of queries, while advanced RAG pipelines struggled with precision, revealing a knowledge gap for domain-specific IR systems. AfriEconQA is confirmed as a challenging benchmark.", "conclusion": "AfriEconQA provides a robust challenge for improving domain-specific IR and RAG systems while exposing the limitations of current LLMs in handling specialized economic queries, with resources to be made accessible upon publication."}}
{"id": "2601.15406", "pdf": "https://arxiv.org/pdf/2601.15406", "abs": "https://arxiv.org/abs/2601.15406", "authors": ["Hatef Otroshi Shahreza", "Anjith George", "S\u00e9bastien Marcel"], "title": "Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems.", "AI": {"tldr": "The paper evaluates the suitability of multimodal large language models (MLLMs) for heterogeneous face recognition (HFR) and finds significant performance gaps compared to traditional systems.", "motivation": "To assess the feasibility and performance of MLLMs in biometric tasks, particularly in challenging cross-modality (e.g., VIS-NIR) face recognition scenarios.", "method": "Systematically benchmarked state-of-the-art MLLMs in HFR tasks across various modalities. Evaluation was conducted using biometric protocols and metrics such as Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR).", "result": "MLLMs exhibit significant performance gaps compared to classical face recognition systems, particularly in cross-spectral settings like VIS-NIR and VIS-THERMAL.", "conclusion": "MLLMs are currently not suitable substitutes for established face recognition techniques, emphasizing the need for rigorous biometric evaluations before deployment."}}
{"id": "2601.15363", "pdf": "https://arxiv.org/pdf/2601.15363", "abs": "https://arxiv.org/abs/2601.15363", "authors": ["Jason Bohne", "Ieva Petrulionyte", "Michael Arbel", "Julien Mairal", "Pawe\u0142 Polak"], "title": "Non-Stationary Functional Bilevel Optimization", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Functional bilevel optimization (FBO) provides a powerful framework for hierarchical learning in function spaces, yet current methods are limited to static offline settings and perform suboptimally in online, non-stationary scenarios. We propose SmoothFBO, the first algorithm for non-stationary FBO with both theoretical guarantees and practical scalability. SmoothFBO introduces a time-smoothed stochastic hypergradient estimator that reduces variance through a window parameter, enabling stable outer-loop updates with sublinear regret. Importantly, the classical parametric bilevel case is a special reduction of our framework, making SmoothFBO a natural extension to online, non-stationary settings. Empirically, SmoothFBO consistently outperforms existing FBO methods in non-stationary hyperparameter optimization and model-based reinforcement learning, demonstrating its practical effectiveness. Together, these results establish SmoothFBO as a general, theoretically grounded, and practically viable foundation for bilevel optimization in online, non-stationary scenarios.", "AI": {"tldr": "SmoothFBO, a novel algorithm for functional bilevel optimization (FBO), addresses challenges in online and non-stationary scenarios, ensuring stable and effective solutions.", "motivation": "Current functional bilevel optimization methods struggle with non-stationary, online scenarios, necessitating an improved framework with theoretical guarantees and scalability.", "method": "SmoothFBO introduces a time-smoothed stochastic hypergradient estimator to reduce variance and enables stable outer-loop updates with sublinear regret.", "result": "Empirical results show SmoothFBO outperforms existing FBO methods in areas like hyperparameter optimization and model-based reinforcement learning.", "conclusion": "SmoothFBO provides a theoretically grounded, scalable, and practically effective approach for online, non-stationary bilevel optimization tasks."}}
{"id": "2601.15349", "pdf": "https://arxiv.org/pdf/2601.15349", "abs": "https://arxiv.org/abs/2601.15349", "authors": ["Jiaqing Chang", "Song Gao", "Chaowei Dong", "zhaobang Li", "Yang Liu"], "title": "Preparation and Motion Study of Magnetically Driven Micro Soft Robot Mimicking the Cownose Ray", "categories": ["cs.RO", "eess.SY"], "comment": "These experiments lay an important foundation for the study of tether-free control of underwater micro-soft robots. Furthermore, this research provides important references for the fields of biomimetic robots and magnetically controlled micro-soft robots", "summary": "In narrow, unstructured underwater environments such as environmental monitoring and minimally invasive medical procedures, micro soft robots exhibit unique advantages due to their flexible movement capabilities and small size. At the same time, applying bionic technology to the structural design of micro soft robots can significantly improve their swimming performance. However, limited by their miniaturization, these robots are difficult to power internally and usually adopt a wireless power supply method. This study designs and fabricates a magnetically responsive, cownose ray-inspired micro soft robot based on the swimming principle of the cownose ray. The robot is made of a certain proportion of NdFeB and PDMS. Then, a three-dimensional Helmholtz coil is used to generate an oscillating harmonic magnetic field to conduct swimming experiments on the robot, exploring the influence of magnetic field parameters on the robot's swimming performance. The experimental results show that the swimming speed is the fastest at B = 5 mT and f = 11 Hz, reaching 5.25 mm/s, which is about 0.5 body lengths per second. In addition, by adjusting the current direction and frequency of the coil, the robot can perform different swimming modes such as straight swimming, turning swimming, and directional swimming. By employing a stepwise adjustment method, the impact of response errors on the robot's trajectory can be effectively reduced. This study demonstrates a method for magnetically driven micro soft robots, laying a foundation for the application of wireless-driven robots in underwater narrow spaces.", "AI": {"tldr": "A cownose ray-inspired magnetically-driven micro soft robot demonstrates efficient swimming and controlled movement in narrow underwater environments.", "motivation": "To enhance the swimming performance of micro soft robots in unstructured underwater environments by incorporating bio-inspired design and addressing power supply challenges.", "method": "Developed a cownose ray-inspired micro soft robot using a combination of NdFeB and PDMS. Conducted experiments using a three-dimensional Helmholtz coil to create an oscillating harmonic magnetic field for wireless propulsion and control.", "result": "The robot achieved a maximum swimming speed of 5.25 mm/s (approximately 0.5 body lengths per second) at an optimal magnetic field of B = 5 mT and f = 11 Hz. It demonstrated straight, turning, and directional swimming modes with minimized trajectory response errors.", "conclusion": "This study highlights the feasibility of using magnetically responsive, bio-inspired micro soft robots powered wirelessly, paving the way for their effective deployment in narrow underwater environments."}}
{"id": "2601.15339", "pdf": "https://arxiv.org/pdf/2601.15339", "abs": "https://arxiv.org/abs/2601.15339", "authors": ["Jayant Havare", "Ashish Mittal", "Srikanth Tamilselvam", "Ganesh Ramakrishnan"], "title": "Lost in Transcription: How Speech-to-Text Errors Derail Code Understanding", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Code understanding is a foundational capability in software engineering tools and developer workflows. However, most existing systems are designed for English-speaking users interacting via keyboards, which limits accessibility in multilingual and voice-first settings, particularly in regions like India. Voice-based interfaces offer a more inclusive modality, but spoken queries involving code present unique challenges due to the presence of non-standard English usage, domain-specific vocabulary, and custom identifiers such as variable and function names, often combined with code-mixed expressions. In this work, we develop a multilingual speech-driven framework for code understanding that accepts spoken queries in a user native language, transcribes them using Automatic Speech Recognition (ASR), applies code-aware ASR output refinement using Large Language Models (LLMs), and interfaces with code models to perform tasks such as code question answering and code retrieval through benchmarks such as CodeSearchNet, CoRNStack, and CodeQA. Focusing on four widely spoken Indic languages and English, we systematically characterize how transcription errors impact downstream task performance. We also identified key failure modes in ASR for code and demonstrated that LLM-guided refinement significantly improves performance across both transcription and code understanding stages. Our findings underscore the need for code-sensitive adaptations in speech interfaces and offer a practical solution for building robust, multilingual voice-driven programming tools.", "AI": {"tldr": "The paper addresses challenges in voice-based code understanding for multilingual users, proposing a framework leveraging ASR, LLM-guided refinement, and code models.", "motivation": "To make code understanding tools accessible to multilingual and voice-first users, especially in regions like India.", "method": "The framework accepts spoken queries, refines ASR transcriptions using LLMs, and integrates with code models for tasks such as code question answering and retrieval.", "result": "The approach improves transcription accuracy and downstream code understanding tasks, especially for Indic languages and English.", "conclusion": "Speech interfaces must be adapted for code-sensitive contexts, and the proposed framework offers a practical solution for robust multilingual programming tools."}}
{"id": "2601.15337", "pdf": "https://arxiv.org/pdf/2601.15337", "abs": "https://arxiv.org/abs/2601.15337", "authors": ["Shourya Jain", "Paras Chopra"], "title": "Language Models Entangle Language and Culture", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted at LM4UC Workshop at AAAI'26, Submitted to ACL 2026. 17 pages, 7 figures", "summary": "Users should not be systemically disadvantaged by the language they use for interacting with LLMs; i.e. users across languages should get responses of similar quality irrespective of language used. In this work, we create a set of real-world open-ended questions based on our analysis of the WildChat dataset and use it to evaluate whether responses vary by language, specifically, whether answer quality depends on the language used to query the model. We also investigate how language and culture are entangled in LLMs such that choice of language changes the cultural information and context used in the response by using LLM-as-a-Judge to identify the cultural context present in responses. To further investigate this, we evaluate LLMs on a translated subset of the CulturalBench benchmark across multiple languages. Our evaluations reveal that LLMs consistently provide lower quality answers to open-ended questions in low resource languages. We find that language significantly impacts the cultural context used by the model. This difference in context impacts the quality of the downstream answer.", "AI": {"tldr": "The paper investigates whether the quality of responses from LLMs differs across languages, focusing on lower quality answers in low-resource languages and the cultural context in responses.", "motivation": "To ensure fairness and inclusivity in LLM interactions by analyzing how language choice impacts the response quality and cultural context used by the models.", "method": "The authors evaluate LLMs using open-ended questions from the WildChat dataset and CulturalBench benchmark, analyzing responses across multiple languages to study quality disparities and cultural biases.", "result": "The analysis reveals that LLMs provide lower-quality answers in low-resource languages and that cultural context varies based on the language used, impacting the quality of answers.", "conclusion": "LLMs need improvements to provide equitable response quality across languages and mitigate cultural biases dependent on language."}}
{"id": "2601.16073", "pdf": "https://arxiv.org/pdf/2601.16073", "abs": "https://arxiv.org/abs/2601.16073", "authors": ["Hanwen Zhang", "Qiaojin Shen", "Yuxi Liu", "Yuesheng Zhu", "Guibo Luo"], "title": "DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models", "categories": ["cs.CV", "cs.DC"], "comment": null, "summary": "Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.", "AI": {"tldr": "The paper introduces DSFedMed, a dual-scale federated learning framework addressing computational challenges in foundation models for medical image segmentation.", "motivation": "Foundation Models show strong generalization across vision tasks but face challenges in federated settings due to high resource demands.", "method": "A dual-scale federated framework enabling mutual knowledge distillation between a foundation model and lightweight client models is proposed. It uses generated high-quality medical images and a learnability-guided sample selection strategy.", "result": "On five medical imaging segmentation datasets, DSFedMed improves Dice score accuracy by 2% while reducing communication and inference costs by nearly 90%.", "conclusion": "DSFedMed provides efficiency and scalability for federated medical imaging tasks, addressing deployment challenges of foundation models."}}
{"id": "2601.15307", "pdf": "https://arxiv.org/pdf/2601.15307", "abs": "https://arxiv.org/abs/2601.15307", "authors": ["Guo-Biao Zhang", "Ding-Yuan Liu", "Da-Yi Wu", "Tian Lan", "Heyan Huang", "Zhijing Wu", "Xian-Ling Mao"], "title": "DeepSurvey-Bench: Evaluating Academic Value of Automatically Generated Scientific Survey", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The rapid development of automated scientific survey generation technology has made it increasingly important to establish a comprehensive benchmark to evaluate the quality of generated surveys.Nearly all existing evaluation benchmarks rely on flawed selection criteria such as citation counts and structural coherence to select human-written surveys as the ground truth survey datasets, and then use surface-level metrics such as structural quality and reference relevance to evaluate generated surveys.However, these benchmarks have two key issues: (1) the ground truth survey datasets are unreliable because of a lack academic dimension annotations; (2) the evaluation metrics only focus on the surface quality of the survey such as logical coherence. Both issues lead to existing benchmarks cannot assess to evaluate their deep \"academic value\", such as the core research objectives and the critical analysis of different studies. To address the above problems, we propose DeepSurvey-Bench, a novel benchmark designed to comprehensively evaluate the academic value of generated surveys. Specifically, our benchmark propose a comprehensive academic value evaluation criteria covering three dimensions: informational value, scholarly communication value, and research guidance value. Based on this criteria, we construct a reliable dataset with academic value annotations, and evaluate the deep academic value of the generated surveys. Extensive experimental results demonstrate that our benchmark is highly consistent with human performance in assessing the academic value of generated surveys.", "AI": {"tldr": "This paper introduces DeepSurvey-Bench, a benchmark designed to evaluate the academic value of generated surveys.", "motivation": "Current benchmarks for evaluating automatically generated surveys rely on unreliable ground truth datasets and surface-level metrics, failing to assess deep academic value.", "method": "The authors propose new evaluation criteria encompassing informational value, scholarly communication value, and research guidance value. They also construct a dataset annotated with academic value and assess generated surveys accordingly.", "result": "DeepSurvey-Bench aligns closely with human evaluations in assessing the academic value of generated surveys, demonstrating its reliability and comprehensive nature.", "conclusion": "DeepSurvey-Bench effectively addresses limitations in existing benchmarks and enables a more thorough evaluation of the academic significance of automated surveys."}}
{"id": "2601.15294", "pdf": "https://arxiv.org/pdf/2601.15294", "abs": "https://arxiv.org/abs/2601.15294", "authors": ["Elif Uskuplu", "Lawrence S. Moss", "Valeria de Paiva"], "title": "KnowTeX: Visualizing Mathematical Dependencies", "categories": ["cs.HC", "cs.IR", "cs.PL"], "comment": null, "summary": "Mathematical knowledge exists in many forms, ranging from informal textbooks and lecture notes to large formal proof libraries, yet moving between these representations remains difficult. Informal texts hide dependencies, while formal systems expose every detail in ways that are not always human-readable. Dependency graphs offer a middle ground by making visible the structure of results, definitions, and proofs. We present KnowTeX, a standalone, user-friendly tool that extends the ideas of Lean's Blueprints, enabling the visualization of conceptual dependencies directly from LaTeX sources. Using a simple \"uses\" command, KnowTeX extracts relationships among statements and generates previewable graphs in DOT and TikZ formats. Applied to mathematical texts, such graphs clarify core results, support education and formalization, and provide a resource for aligning informal and formal mathematical representations. We argue that dependency graphs should become a standard feature of mathematical writing, benefiting both human readers and automated systems.", "AI": {"tldr": "KnowTeX is a tool facilitating visualization of conceptual dependencies from LaTeX sources by creating dependency graphs, aiding mathematical understanding and representation alignment.", "motivation": "Mathematical texts are either too informal, hiding dependencies, or overly formal, exposing excessive details. This creates challenges in bridging the gap between informal and formal knowledge representations.", "method": "KnowTeX extracts relationships using a simple 'uses' command from LaTeX sources and generates dependency graphs in formats such as DOT and TikZ.", "result": "KnowTeX improves understanding of mathematical texts by clarifying dependencies, aiding education and formalization, and aligning informal and formal representations.", "conclusion": "Dependency graphs should be standardized in mathematical writing to benefit humans and automated systems alike."}}
{"id": "2601.15319", "pdf": "https://arxiv.org/pdf/2601.15319", "abs": "https://arxiv.org/abs/2601.15319", "authors": ["Francesco Chiappone", "Davide Marocco", "Nicola Milano"], "title": "Large Language Models as Simulative Agents for Neurodivergent Adult Psychometric Profiles", "categories": ["q-bio.NC", "cs.AI"], "comment": null, "summary": "Adult neurodivergence, including Attention-Deficit/Hyperactivity Disorder (ADHD), high-functioning Autism Spectrum Disorder (ASD), and Cognitive Disengagement Syndrome (CDS), is marked by substantial symptom overlap that limits the discriminant sensitivity of standard psychometric instruments. While recent work suggests that Large Language Models (LLMs) can simulate human psychometric responses from qualitative data, it remains unclear whether they can accurately and stably model neurodevelopmental traits rather than broad personality characteristics. This study examines whether LLMs can generate psychometric responses that approximate those of real individuals when grounded in a structured qualitative interview, and whether such simulations are sensitive to variations in trait intensity. Twenty-six adults completed a 29-item open-ended interview and four standardized self-report measures (ASRS, BAARS-IV, AQ, RAADS-R). Two LLMs (GPT-4o and Qwen3-235B-A22B) were prompted to infer an individual psychological profile from interview content and then respond to each questionnaire in-role. Accuracy, reliability, and sensitivity were assessed using group-level comparisons, error metrics, exact-match scoring, and a randomized baseline. Both models outperformed random responses across instruments, with GPT-4o showing higher accuracy and reproducibility. Simulated responses closely matched human data for ASRS, BAARS-IV, and RAADS-R, while the AQ revealed subscale-specific limitations, particularly in Attention to Detail. Overall, the findings indicate that interview-grounded LLMs can produce coherent and above-chance simulations of neurodevelopmental traits, supporting their potential use as synthetic participants in early-stage psychometric research, while highlighting clear domain-specific constraints.", "AI": {"tldr": "This study evaluates whether large language models (LLMs) can accurately simulate psychometric responses related to adult neurodivergence and examines their reliability and sensitivity.", "motivation": "The study addresses the challenge of overlapping symptoms in adult neurodivergence and explores the potential of LLMs to model neurodevelopmental traits through structured qualitative data, overcoming limitations in current psychometric tools.", "method": "Researchers used structured qualitative interviews and standardized self-report measures to compare the psychometric responses of 26 participants to those generated by two LLMs (GPT-4o and Qwen3-235B-A22B). Accuracy, reliability, and sensitivity were analyzed using group comparisons and error metrics.", "result": "Both LLMs surpassed random baselines, with GPT-4o achieving greater accuracy and reliability. Simulated responses were consistent for most measures, though some limitations were noted in the AQ's Attention to Detail subscale.", "conclusion": "LLMs, grounded in structured interviews, are capable of generating meaningful simulations of neurodevelopmental traits, demonstrating promise for psychometric research but with certain domain-specific limitations."}}
{"id": "2601.15408", "pdf": "https://arxiv.org/pdf/2601.15408", "abs": "https://arxiv.org/abs/2601.15408", "authors": ["Pablo Messina", "Andr\u00e9s Villa", "Juan Le\u00f3n Alc\u00e1zar", "Karen S\u00e1nchez", "Carlos Hinojosa", "Denis Parra", "\u00c1lvaro Soto", "Bernard Ghanem"], "title": "CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "31 pages, 7 figures, submitted to CVPR 2026 (under review)", "summary": "Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure", "AI": {"tldr": "CURE enhances radiology report generation by improving grounding accuracy and reducing hallucinations, using a curriculum learning framework without extra data.", "motivation": "To address the challenges of poor visual grounding and factual inconsistency in radiology report generation by existing medical vision-language models.", "method": "The paper introduces CURE, a curriculum learning framework that fine-tunes multimodal instructional models on phrase grounding, grounded report generation, and anatomy-grounded report generation. Dynamic sampling based on model performance is employed to focus on challenging samples.", "result": "CURE demonstrates significant improvements: a +0.37 IoU increase in grounding accuracy, +0.188 CXRFEScore boost in report quality, and an 18.6% reduction in hallucinations.", "conclusion": "CURE provides a data-efficient approach to enhance visual-text alignment and reliability in radiology report generation, showcasing its value in medical AI applications."}}
{"id": "2601.15500", "pdf": "https://arxiv.org/pdf/2601.15500", "abs": "https://arxiv.org/abs/2601.15500", "authors": ["Saptarshi Roy", "Alessandro Rinaldo", "Purnamrita Sarkar"], "title": "Low-Dimensional Adaptation of Rectified Flow: A New Perspective through the Lens of Diffusion and Stochastic Localization", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST"], "comment": "32 pages, 7 figures", "summary": "In recent years, Rectified flow (RF) has gained considerable popularity largely due to its generation efficiency and state-of-the-art performance. In this paper, we investigate the degree to which RF automatically adapts to the intrinsic low dimensionality of the support of the target distribution to accelerate sampling. We show that, using a carefully designed choice of the time-discretization scheme and with sufficiently accurate drift estimates, the RF sampler enjoys an iteration complexity of order $O(k/\\varepsilon)$ (up to log factors), where $\\varepsilon$ is the precision in total variation distance and $k$ is the intrinsic dimension of\n  the target distribution. In addition, we show that the denoising diffusion probabilistic model (DDPM) procedure is equivalent to a stochastic version of RF by establishing a novel connection between these processes and stochastic localization. Building on this connection, we further design a stochastic RF sampler that also adapts to the low-dimensionality of the target distribution under milder requirements on the accuracy of the drift estimates, and also with a specific time schedule. We illustrate with simulations on the synthetic data and text-to-image data experiments the improved performance of the proposed samplers implementing the newly designed time-discretization schedules.", "AI": {"tldr": "The paper explores Rectified Flow (RF) and its adaption to low-dimensional target distributions, showcasing improved sampling performance and a new stochastic RF sampler.", "motivation": "To analyze RF's ability to adapt to low dimensionality for efficient sampling and to bridge connections between RF, DDPM, and stochastic localization.", "method": "The study develops time-discretization schemes, derives complexity bounds, and formulates a novel stochastic RF sampler based on stochastic localization.", "result": "The proposed samplers, with optimized time-discretization schedules, display better performance in synthetic and text-to-image data experiments.", "conclusion": "RF, when combined with appropriate time-discretization and stochastic methods, enhances sampling efficiency, especially for low-dimensional target distributions."}}
{"id": "2601.15419", "pdf": "https://arxiv.org/pdf/2601.15419", "abs": "https://arxiv.org/abs/2601.15419", "authors": ["Yashuai Yan", "Dongheui Lee"], "title": "Learning a Unified Latent Space for Cross-Embodiment Robot Control", "categories": ["cs.RO"], "comment": null, "summary": "We present a scalable framework for cross-embodiment humanoid robot control by learning a shared latent representation that unifies motion across humans and diverse humanoid platforms, including single-arm, dual-arm, and legged humanoid robots. Our method proceeds in two stages: first, we construct a decoupled latent space that captures localized motion patterns across different body parts using contrastive learning, enabling accurate and flexible motion retargeting even across robots with diverse morphologies. To enhance alignment between embodiments, we introduce tailored similarity metrics that combine joint rotation and end-effector positioning for critical segments, such as arms. Then, we train a goal-conditioned control policy directly within this latent space using only human data. Leveraging a conditional variational autoencoder, our policy learns to predict latent space displacements guided by intended goal directions. We show that the trained policy can be directly deployed on multiple robots without any adaptation. Furthermore, our method supports the efficient addition of new robots to the latent space by learning only a lightweight, robot-specific embedding layer. The learned latent policies can also be directly applied to the new robots. Experimental results demonstrate that our approach enables robust, scalable, and embodiment-agnostic robot control across a wide range of humanoid platforms.", "AI": {"tldr": "The paper proposes a framework for scalable humanoid robot control using shared latent representation unifying motion across various humanoid platforms while requiring minimal adaptation for new embodiments.", "motivation": "Developing a unified approach for humanoid robot control that enables seamless motion retargeting across robots with diverse morphologies.", "method": "The method utilizes a two-staged approach: contrastive learning for creating a decoupled latent space capturing localized motion patterns and training a goal-conditioned control policy within this space using human data.", "result": "Experimental results demonstrate robustness, scalability, and adaptability of the method across diverse humanoid robots with effective motion retargeting and direct deployment.", "conclusion": "The framework facilitates embodiment-agnostic control, scalable deployment, and easy integration of new robots, advancing scalable cross-embodiment humanoid robot control."}}
{"id": "2601.15352", "pdf": "https://arxiv.org/pdf/2601.15352", "abs": "https://arxiv.org/abs/2601.15352", "authors": ["Adeyemi Adeseye", "Aisvarya Adeseye"], "title": "A Prompt-Based Framework for Loop Vulnerability Detection Using Local LLMs", "categories": ["cs.SE"], "comment": "Accepted and Waiting to be published ICAI'25: 27th International Conference on Artificial Intelligence https://american-cse.org/csce2025/conferences-ICAI", "summary": "Loop vulnerabilities are one major risky construct in software development. They can easily lead to infinite loops or executions, exhaust resources, or introduce logical errors that degrade performance and compromise security. The problem are often undetected by traditional static analyzers because such tools rely on syntactic patterns, which makes them struggle to detect semantic flaws. Consequently, Large Language Models (LLMs) offer new potential for vulnerability detection because of their ability to understand code contextually. Moreover, local LLMs unlike commercial ones like ChatGPT or Gemini addresses issues such as privacy, latency, and dependency concerns by facilitating efficient offline analysis. Consequently, this study proposes a prompt-based framework that utilize local LLMs for the detection of loop vulnerabilities within Python 3.7+ code. The framework targets three categories of loop-related issues, such as control and logic errors, security risks inside loops, and resource management inefficiencies. A generalized and structured prompt-based framework was designed and tested with two locally deployed LLMs (LLaMA 3.2; 3B and Phi 3.5; 4B) by guiding their behavior via iterative prompting. The designed prompt-based framework included key safeguarding features such as language-specific awareness, code-aware grounding, version sensitivity, and hallucination prevention. The LLM results were validated against a manually established baseline truth, and the results indicate that Phi outperforms LLaMA in precision, recall, and F1-score. The findings emphasize the importance of designing effective prompts for local LLMs to perform secure and accurate code vulnerability analysis.", "AI": {"tldr": "The paper addresses loop vulnerabilities in Python code and proposes a prompt-based framework using local LLMs for detecting these issues, testing them on LLaMA and Phi models.", "motivation": "To address undetected loop vulnerabilities in code due to limitations of traditional static analyzers, and explore the role of local LLMs in ensuring efficient, private, and reliable offline analysis.", "method": "A generalized, structured prompt-based framework is developed for local LLMs (LLaMA 3.2; Phi 3.5) using iterative prompting. It focuses on three loop-related issues and features safeguards like code-aware grounding and hallucination prevention.", "result": "The Phi model outperformed the LLaMA model in precision, recall, and F1-score when detecting loop vulnerabilities.", "conclusion": "The study highlights the value of designing effective prompts for local LLMs to enhance secure code vulnerability analysis and detection."}}
{"id": "2601.15370", "pdf": "https://arxiv.org/pdf/2601.15370", "abs": "https://arxiv.org/abs/2601.15370", "authors": ["Maciej Kilian", "Oleg Mkrtchyan", "Luke Zettlemoyer", "Akshat Shrivastava", "Armen Aghajanyan"], "title": "Improving MoE Compute Efficiency by Composing Weight and Data Sparsity", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mixture-of-Experts layers achieve compute efficiency through weight sparsity: each token activates only a subset of experts. Data sparsity, where each expert processes only a subset of tokens, offers a complementary axis. Expert-choice routing implements data sparsity directly but violates causality in autoregressive models, creating train-inference mismatch. We recover data sparsity within causal token-choice MoE by leveraging zero-compute (null) experts within the routing pool. When a token routes to null experts, those slots consume no compute. The standard load balancing objective trains the model to uniformly use all experts (real and null) therefore creating data sparsity in expectation without the causality violations. We evaluate on vision-language model training, where data heterogeneity is pronounced: vision encoders produce many low-information tokens while text tokens are denser. At matched expected FLOPs, composing weight and data sparsity yields a more compute-efficient frontier than weight sparsity alone, with gains in training loss and downstream performance. The model learns implicit modality-aware allocation, routing vision tokens to null experts more aggressively than text, without explicit modality routing.", "AI": {"tldr": "The paper introduces causal token-choice Mixture-of-Experts (MoE) by leveraging null experts for efficient compute, without train-inference mismatches in autoregressive models.", "motivation": "Address inefficiencies and causality violations in autoregressive models' token routing processes during training and inference.", "method": "Develop a causal token-choice Mixture-of-Experts routing approach that incorporates null experts to create complementary data sparsity while maintaining expected compute efficiency.", "result": "The proposed method improves training loss, downstream performance, and achieves better compute efficiency by leveraging implicit modality-aware allocation during vision-language model training.", "conclusion": "Integrating data and weight sparsity within routing effectively enhances model performance without explicit modality-based routing. The approach improves computational efficiency while handling data heterogeneity such as sparse vision tokens."}}
{"id": "2601.16169", "pdf": "https://arxiv.org/pdf/2601.16169", "abs": "https://arxiv.org/abs/2601.16169", "authors": ["Robert Walkup", "Juha J\u00e4ykk\u00e4", "Igor Pasichnyk", "Zachary Streeter", "Kasia \u015awirydowicz", "Mikko Tukiainen", "Yasuko Eckert", "Luke Bertels", "Daniel Claudino", "Peter Groszkowski", "Travis S. Humble", "Constantinos Evangelinos", "Javier Robledo-Moreno", "William Kirby", "Antonio Mezzacapo", "Antonio C\u00f3rcoles", "Seetharami Seelam"], "title": "Scaling Sample-Based Quantum Diagonalization on GPU-Accelerated Systems using OpenMP Offload", "categories": ["cs.ET", "cs.DC"], "comment": "12 pages", "summary": "Hybrid quantum-HPC algorithms advance research by delegating complex tasks to quantum processors and using HPC systems to orchestrate workflows and complementary computations. Sample-based quantum diagonalization (SQD) is a hybrid quantum-HPC method in which information from a molecular Hamiltonian is encoded into a quantum circuit for evaluation on a quantum computer. A set of measurements on the quantum computer yields electronic configurations that are filtered on the classical computer, which also performs diagonalization on the selected subspace and identifies configurations to be carried over to the next step in an iterative process. Diagonalization is the most demanding task for the classical computer. Previous studies used the Fugaku supercomputer and a highly scalable diagonalization code designed for CPUs. In this work, we describe our efforts to enable efficient scalable and portable diagonalization on heterogeneous systems using GPUs as the main compute engines based on the previous work.\n  GPUs provide massive on-device thread-level parallelism that is well aligned with the algorithms used for diagonalization. We focus on the computation of ground-state energies and wavefunctions using the Davidson algorithm with a selected set of electron configurations. We describe the offload strategy, code transformations, and data-movement, with examples of measurements on the Frontier supercomputer and five other GPU accelerated systems. Our measurements show that GPUs provide an outstanding performance boost of order 100x on a per-node basis. This dramatically expedites the diagonalization step-essential for extracting ground and excited state energies-bringing the classical processing time down from hours to minutes.", "AI": {"tldr": "The paper explores efficient diagonalization using GPUs in hybrid quantum-HPC methods, achieving substantial performance gains and reducing processing times dramatically.", "motivation": "To optimize the diagonalization step in hybrid quantum-HPC methods, which is computationally demanding, by utilizing the computational power of GPUs.", "method": "The work implements GPU-based acceleration for the diagonalization task using the Davidson algorithm, with code transformations, data movement strategies, and testing on multiple GPU-accelerated systems.", "result": "Experiments show a 100x performance improvement on GPUs per node, significantly reducing classical diagonalization step durations.", "conclusion": "GPU acceleration effectively boosts diagonalization tasks in hybrid quantum-HPC systems, making computations more efficient and quicker."}}
{"id": "2601.15311", "pdf": "https://arxiv.org/pdf/2601.15311", "abs": "https://arxiv.org/abs/2601.15311", "authors": ["Mustafa Arslan"], "title": "Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the \"Lost in the Middle\" phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily \"Flat RAG\" architectures relying on vector databases, treat memory as an unstructured bag of embeddings. This approach fails to capture the hierarchical and temporal structure of long-horizon interactions, leading to \"Vector Haze\", the retrieval of disjointed facts lacking episodic continuity. We propose Aeon, a Neuro-Symbolic Cognitive Operating System that redefines memory not as a static store, but as a managed OS resource. Aeon structures memory into a Memory Palace (a spatial index implemented via Atlas, a SIMD-accelerated Page-Clustered Vector Index that combines small-world graph navigation with B+ Tree-style disk locality to minimize read amplification) and a Trace (a neuro-symbolic episodic graph). We introduce the Semantic Lookaside Buffer (SLB), a predictive caching mechanism that exploits conversational locality to achieve sub-millisecond retrieval latencies. Benchmarks demonstrate that Aeon achieves < 1ms retrieval latency on conversational workloads while ensuring state consistency via a zero-copy C++/Python bridge, effectively enabling persistent, structured memory for autonomous agents.", "AI": {"tldr": "The paper introduces Aeon, a system addressing limitations in Large Language Models (LLMs) such as computational inefficiency and memory structure, offering advanced memory management for improved long-term context understanding.", "motivation": "To overcome the high computational costs of self-attention in LLMs and the degradation of reasoning across expanded context windows, while addressing limitations in current memory architectures (e.g., lack of structure and continuity in Flat RAG systems).", "method": "Aeon redefines memory as a managed resource using two key components: a Memory Palace (a spatially indexed memory system) and a Trace (an episodic graph for neuro-symbolic continuity). It implements a Semantic Lookaside Buffer (SLB) for caching and ensures fast, consistent memory retrieval through a specialized indexing approach.", "result": "Aeon achieves sub-millisecond retrieval latencies on conversational tasks and provides state consistency, enabling structured and persistent memory systems for LLM-based autonomous agents.", "conclusion": "Aeon alleviates memory-related inefficiencies in LLMs by offering structured and accessible long-term memory, providing a significant improvement over existing vector-based memory architectures and facilitating better autonomous agent performance."}}
{"id": "2601.15320", "pdf": "https://arxiv.org/pdf/2601.15320", "abs": "https://arxiv.org/abs/2601.15320", "authors": ["Takao Inou\u00e9"], "title": "On Brain as a Mathematical Manifold: Neural Manifolds, Sheaf Semantics, and Leibnizian Harmony", "categories": ["q-bio.NC"], "comment": "17 pages with 2 figures; a sheaf-theoretic model of neural integration and pathology, with an application to the interpretation of Leibnizian harmony", "summary": "We present a mathematical and philosophical framework in which brain function is modeled using sheaf theory over neural state spaces. Local neural or cognitive functions are represented as sections of a sheaf, while global coherence corresponds to the existence of global sections. Brain pathologies are interpreted as obstructions to such global integration and are classified using tools from sheaf cohomology. The framework builds on the neural manifold program in contemporary neuroscience and on standard results in sheaf theory, and is further interpreted through a Leibnizian lens \\cite{Churchland2012, Leibniz1714, MacLaneMoerdijk, Perich2025}. This paper is intended as a conceptual and formal proposal rather than a complete empirical theory.", "AI": {"tldr": "The paper proposes a theoretical framework using sheaf theory to model brain functions, linking neural coherence and pathologies to mathematical structures.", "motivation": "The paper aims to bridge neuroscience and advanced mathematical tools to better understand global coherence in brain function and brain pathologies.", "method": "Sheaf theory is applied to neural state spaces, representing local functions as sheaf sections and global coherence as global sections, with obstructions linked to pathological states.", "result": "The paper defines a conceptual framework integrating neuroscience with sheaf cohomology to theoretically interpret brain coherence and pathologies.", "conclusion": "The work serves as a theoretical exploration, presenting a novel mathematical perspective on neural and cognitive functions rather than offering direct empirical validation."}}
{"id": "2601.15299", "pdf": "https://arxiv.org/pdf/2601.15299", "abs": "https://arxiv.org/abs/2601.15299", "authors": ["Yash Sharma"], "title": "MALTopic: Multi-Agent LLM Topic Modeling Framework", "categories": ["cs.CL", "cs.IR", "cs.MA"], "comment": "6 pages. Published in 2025 IEEE World AI-IoT Congress. \\c{opyright} 2025 IEEE. Project code and data available at: https://github.com/yash91sharma/MALTopic", "summary": "Topic modeling is a crucial technique for extracting latent themes from unstructured text data, particularly valuable in analyzing survey responses. However, traditional methods often only consider free-text responses and do not natively incorporate structured or categorical survey responses for topic modeling. And they produce abstract topics, requiring extensive human interpretation. To address these limitations, we propose the Multi-Agent LLM Topic Modeling Framework (MALTopic). This framework decomposes topic modeling into specialized tasks executed by individual LLM agents: an enrichment agent leverages structured data to enhance textual responses, a topic modeling agent extracts latent themes, and a deduplication agent refines the results. Comparative analysis on a survey dataset demonstrates that MALTopic significantly improves topic coherence, diversity, and interpretability compared to LDA and BERTopic. By integrating structured data and employing a multi-agent approach, MALTopic generates human-readable topics with enhanced contextual relevance, offering a more effective solution for analyzing complex survey data.", "AI": {"tldr": "The paper presents MALTopic, a Multi-Agent LLM Topic Modeling Framework, aimed at improving topic modeling by incorporating structured survey data and specialized agents for better results.", "motivation": "Traditional topic modeling methods fail to incorporate structured data and produce less interpretable topics, necessitating more effective and human-readable solutions.", "method": "A multi-agent framework is proposed where agents focus on specific tasks: an enrichment agent integrates structured data, a topic modeling agent extracts latent themes, and a deduplication agent refines the topics.", "result": "MALTopic outperforms traditional methods (LDA and BERTopic) in terms of topic coherence, diversity, and human interpretability.", "conclusion": "By integrating structured data and a multi-agent approach, MALTopic effectively addresses traditional topic modeling limitations, making it a superior tool for analyzing complex survey data."}}
{"id": "2601.15416", "pdf": "https://arxiv.org/pdf/2601.15416", "abs": "https://arxiv.org/abs/2601.15416", "authors": ["Cuong Tran Van", "Trong-Thang Pham", "Ngoc-Son Nguyen", "Duy Minh Ho Nguyen", "Ngan Le"], "title": "DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": "Published with J2C Certification in Transactions on Machine Learning Research (TMLR)", "summary": "Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings.", "AI": {"tldr": "DuFal proposes a novel dual-path architecture for improving sparse-view Cone-Beam CT reconstruction, with an emphasis on recovering high-frequency anatomical details.", "motivation": "To address the challenge of reconstructing fine-grained anatomical details in sparse-view Cone-Beam CT, which traditional CNN-based methods struggle with due to their low-frequency learning bias.", "method": "Introduces the DuFal framework that integrates frequency-domain and spatial-domain processing using a dual-path architecture. It employs High-Local Factorized Fourier Neural Operators and reduces parameter counts through a Spectral-Channel Factorization scheme. Cross-attention frequency fusion is utilized to integrate features.", "result": "Experimental validation on LUNA16 and ToothFairy datasets shows a significant improvement in preserving high-frequency features compared to state-of-the-art methods in sparse-view settings.", "conclusion": "DuFal effectively addresses the limitations of existing methods in sparse-view CT reconstruction, enhancing the ability to recover high-frequency anatomical structures with improved computational efficiency."}}
{"id": "2601.16070", "pdf": "https://arxiv.org/pdf/2601.16070", "abs": "https://arxiv.org/abs/2601.16070", "authors": ["Jingfu Peng", "Yuhong Yang"], "title": "On damage of interpolation to adversarial robustness in regression", "categories": ["stat.ML", "cs.LG", "math.ST"], "comment": null, "summary": "Deep neural networks (DNNs) typically involve a large number of parameters and are trained to achieve zero or near-zero training error. Despite such interpolation, they often exhibit strong generalization performance on unseen data, a phenomenon that has motivated extensive theoretical investigations. Comforting results show that interpolation indeed may not affect the minimax rate of convergence under the squared error loss. In the mean time, DNNs are well known to be highly vulnerable to adversarial perturbations in future inputs. A natural question then arises: Can interpolation also escape from suboptimal performance under a future $X$-attack? In this paper, we investigate the adversarial robustness of interpolating estimators in a framework of nonparametric regression. A finding is that interpolating estimators must be suboptimal even under a subtle future $X$-attack, and achieving perfect fitting can substantially damage their robustness. An interesting phenomenon in the high interpolation regime, which we term the curse of simple size, is also revealed and discussed. Numerical experiments support our theoretical findings.", "AI": {"tldr": "While deep neural networks (DNNs) achieve strong generalization, their susceptibility to adversarial attacks is examined. Interpolating estimators in regression are found suboptimal under adversarial setups.", "motivation": "Understanding why DNNs generalize well despite interpolation and exploring their robustness against future adversarial attacks.", "method": "Theoretical investigation into the adversarial properties of interpolating estimators within nonparametric regression. Empirical experiments validate findings.", "result": "Interpolating estimators are inherently suboptimal under adversarial $X$-attacks, and their robustness is compromised with perfect fitting in high interpolation scenarios.", "conclusion": "Achieving perfect interpolation in DNNs may lead to poor adversarial robustness, and a curse of sample size exacerbates this issue in the high interpolation regime."}}
{"id": "2601.15459", "pdf": "https://arxiv.org/pdf/2601.15459", "abs": "https://arxiv.org/abs/2601.15459", "authors": ["Sarvin Ghiasi", "Majid Roshanfar", "Jake Barralet", "Liane S. Feldman", "Amir Hooshiar"], "title": "Neural Collision Detection for Multi-arm Laparoscopy Surgical Robots Through Learning-from-Simulation", "categories": ["cs.RO"], "comment": null, "summary": "This study presents an integrated framework for enhancing the safety and operational efficiency of robotic arms in laparoscopic surgery by addressing key challenges in collision detection and minimum distance estimation. By combining analytical modeling, real-time simulation, and machine learning, the framework offers a robust solution for ensuring safe robotic operations. An analytical model was developed to estimate the minimum distances between robotic arms based on their joint configurations, offering precise theoretical calculations that serve as both a validation tool and a benchmark. To complement this, a 3D simulation environment was created to model two 7-DOF Kinova robotic arms, generating a diverse dataset of configurations for collision detection and distance estimation. Using these insights, a deep neural network model was trained with joint actuators of robot arms and relative positions as inputs, achieving a mean absolute error of 282.2 mm and an R-squared value of 0.85. The close alignment between predicted and actual distances highlights the network's accuracy and its ability to generalize spatial relationships. This work demonstrates the effectiveness of combining analytical precision with machine learning algorithms to enhance the precision and reliability of robotic systems.", "AI": {"tldr": "The paper introduces a framework integrating analytical modeling, simulation, and machine learning to improve collision detection and distance estimation for robotic arms in laparoscopic surgery.", "motivation": "The motivation is to address safety challenges and improve operational efficiency in robotic laparoscopic surgeries, particularly in collision detection and minimum distance estimation.", "method": "The method involves creating an analytical model for theoretical calculations, a 3D simulation environment for generating datasets, and training a deep neural network to predict spatial relationships of robotic arms.", "result": "The trained deep neural network achieved a mean absolute error of 282.2 mm and an R-squared value of 0.85, showing close alignment between predicted and actual distances.", "conclusion": "The study concludes that the integration of analytical precision and machine learning enhances robotic safety and reliability in laparoscopic surgery environments."}}
{"id": "2601.15493", "pdf": "https://arxiv.org/pdf/2601.15493", "abs": "https://arxiv.org/abs/2601.15493", "authors": ["M M Abid Naziri", "Shinhae Kim", "Feiran Qin", "Marcelo d'Amorim", "Saikat Dutta"], "title": "Testing Deep Learning Libraries via Neurosymbolic Constraint Learning", "categories": ["cs.SE"], "comment": null, "summary": "Deep Learning (DL) libraries (e.g., PyTorch) are popular in AI development. These libraries are complex and contain bugs. Researchers have proposed various bug-finding techniques for such libraries. Yet, there is much room for improvement. A key challenge in testing DL libraries is the lack of API specifications. Prior testing approaches often inaccurately model the input specifications of DL APIs, resulting in missed valid inputs that could reveal bugs or false alarms due to invalid inputs.\n  To address this challenge, we develop Centaur -- the first neurosymbolic technique to test DL library APIs using dynamically learned input constraints. Centaur leverages the key idea that formal API constraints can be learned from a small number of automatically generated seed inputs, and that the learned constraints can be solved using SMT solvers to generate valid and diverse test inputs.\n  We develop a novel grammar that represents first-order logic formulae over API parameters and expresses tensor-related properties (e.g., shape, data types) as well as relational properties between parameters. We use the grammar to guide a Large Language Model (LLM) to enumerate syntactically correct candidate rules, validated using seed inputs. Further, we develop a custom refinement strategy to prune the set of learned rules to eliminate spurious or redundant rules. We use the learned constraints to systematically generate valid and diverse inputs by integrating SMT-based solving with randomized sampling.\n  We evaluate Centaur for testing PyTorch and TensorFlow. Our results show that Centaur's constraints have a recall of 94.0% and a precision of 94.0% on average. In terms of coverage, Centaur covers 203, 150, and 9,608 more branches than TitanFuzz, ACETest and Pathfinder, respectively. Using Centaur, we also detect 26 new bugs in PyTorch and TensorFlow, 18 of which are confirmed.", "AI": {"tldr": "This paper introduces Centaur, a neurosymbolic approach to test deep learning library APIs using dynamically learned input constraints, significantly improving bug detection in PyTorch and TensorFlow.", "motivation": "Testing deep learning libraries is challenging due to the absence of clear API specifications, leading to missed or false bug reports. Existing techniques provide suboptimal results, necessitating a more effective solution.", "method": "Centaur dynamically learns API input constraints by leveraging first-order logic grammar, LLM-guided rule generation, and SMT solvers. It refines learned rules and generates diverse valid inputs for testing library APIs.", "result": "Centaur achieves 94% recall and precision in its constraints, covers significantly more code branches than existing tools, and discovers 26 new bugs in PyTorch and TensorFlow, 18 confirmed.", "conclusion": "Centaur offers a robust testing framework, enhancing bug detection and improving coverage in deep learning libraries using learned API constraints."}}
{"id": "2601.15380", "pdf": "https://arxiv.org/pdf/2601.15380", "abs": "https://arxiv.org/abs/2601.15380", "authors": ["Elon Litman", "Gabe Guo"], "title": "You Need Better Attention Priors", "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": null, "summary": "We generalize the attention mechanism by viewing it through the lens of Entropic Optimal Transport, revealing that standard attention corresponds to a transport problem regularized by an implicit uniform prior. We introduce Generalized Optimal transport Attention with Trainable priors (GOAT), a new attention mechanism that replaces this naive assumption with a learnable, continuous prior. This prior maintains full compatibility with optimized kernels such as FlashAttention. GOAT also provides an EOT-based explanation of attention sinks and materializes a solution for them, avoiding the representational trade-offs of standard attention. Finally, by absorbing spatial information into the core attention computation, GOAT learns an extrapolatable prior that combines the flexibility of learned positional embeddings with the length generalization of fixed encodings.", "AI": {"tldr": "This paper introduces an enhanced attention mechanism called GOAT by generalizing attention using Entropic Optimal Transport and trainable priors, improving flexibility and representation.", "motivation": "The motivation is to address the limitations of standard attention mechanisms with fixed or implicit assumptions, such as uniform priors and lack of spatial information integration.", "method": "The authors propose GOAT, which replaces standard attention's implicit uniform prior with a learnable, continuous prior while integrating spatial information and maintaining compatibility with optimized kernels.", "result": "GOAT provides a solution for attention sinks, enables more advanced representations, and enhances extrapolation capabilities compared to standard attention mechanisms.", "conclusion": "GOAT improves attention mechanisms by introducing learnable priors and better spatial integration, enhancing its applicability and generalization over traditional approaches."}}
{"id": "2601.15316", "pdf": "https://arxiv.org/pdf/2601.15316", "abs": "https://arxiv.org/abs/2601.15316", "authors": ["Wei Ai", "Yilong Tan", "Yuntao Shou", "Tao Meng", "Haowen Chen", "Zhixiong He", "Keqin Li"], "title": "The Paradigm Shift: A Comprehensive Survey on Large Vision Language Models for Multimodal Fake News Detection", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "In recent years, the rapid evolution of large vision-language models (LVLMs) has driven a paradigm shift in multimodal fake news detection (MFND), transforming it from traditional feature-engineering approaches to unified, end-to-end multimodal reasoning frameworks. Early methods primarily relied on shallow fusion techniques to capture correlations between text and images, but they struggled with high-level semantic understanding and complex cross-modal interactions. The emergence of LVLMs has fundamentally changed this landscape by enabling joint modeling of vision and language with powerful representation learning, thereby enhancing the ability to detect misinformation that leverages both textual narratives and visual content. Despite these advances, the field lacks a systematic survey that traces this transition and consolidates recent developments. To address this gap, this paper provides a comprehensive review of MFND through the lens of LVLMs. We first present a historical perspective, mapping the evolution from conventional multimodal detection pipelines to foundation model-driven paradigms. Next, we establish a structured taxonomy covering model architectures, datasets, and performance benchmarks. Furthermore, we analyze the remaining technical challenges, including interpretability, temporal reasoning, and domain generalization. Finally, we outline future research directions to guide the next stage of this paradigm shift. To the best of our knowledge, this is the first comprehensive survey to systematically document and analyze the transformative role of LVLMs in combating multimodal fake news. The summary of existing methods mentioned is in our Github: \\href{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}.", "AI": {"tldr": "The paper surveys the application of large vision-language models (LVLMs) in multimodal fake news detection (MFND), documenting its evolution, structured taxonomy, challenges, and future research directions.", "motivation": "The motivation is to address the gap in systematic studies regarding the transformative impact of LVLMs on multimodal fake news detection, offering a consolidated resource and tracing the paradigm shift in the field.", "method": "The paper provides a comprehensive review through historical analysis, taxonomy of models, datasets, benchmarks, and exploration of technical challenges in MFND utilizing LVLMs.", "result": "The paper documents the evolution of MFND frameworks, discusses the advancements in LVLM-driven approaches, organizes related resources, and highlights existing challenges such as interpretability, temporal reasoning, and domain generalization.", "conclusion": "This paper concludes that LVLMs have significantly enhanced MFND capabilities and provides directions for overcoming current challenges to further advance this paradigm shift."}}
{"id": "2601.15321", "pdf": "https://arxiv.org/pdf/2601.15321", "abs": "https://arxiv.org/abs/2601.15321", "authors": ["Sayan Saha"], "title": "Analysis of the Ventriloquism Aftereffect Using Network Theory Techniques", "categories": ["q-bio.NC", "cs.HC"], "comment": null, "summary": "Ventriloquism After-Effect is the phenomenon where sustained exposure to the ventriloquist illusion causes a change in unisensory auditory localization towards the location where the visual stimulus was present. We investigate the recalibration in EEG networks that causes this change and the track the timeline of changes in the auditory processing pathway. Our results obtained using network analysis, non-stationary time series analysis and multivariate pattern classification show that recalibration takes place early in the auditory processing pathway and the after-effect decays with time after exposure to the illusion.", "AI": {"tldr": "This paper discusses the Ventriloquism After-Effect and explores how auditory processing recalibrates in response to visual stimuli. Changes are tracked using EEG data analysis.", "motivation": "Investigating how the human auditory system undergoes recalibration when exposed to ventriloquism illusion, which has implications for understanding sensory interactions.", "method": "The study utilized network analysis, non-stationary time series analysis, and multivariate pattern classification on EEG data to track changes in the auditory processing pathway after exposure to the illusion.", "result": "Recalibration occurs early in the auditory pathway, with the Ventriloquism After-Effect decaying over time post-exposure.", "conclusion": "The study provides evidence for early-stage recalibration in auditory processing due to visual stimuli and emphasizes the transient nature of the Ventriloquism After-Effect."}}
{"id": "2601.15300", "pdf": "https://arxiv.org/pdf/2601.15300", "abs": "https://arxiv.org/abs/2601.15300", "authors": ["Weiwei Wang", "Jiyong Min", "Weijie Zou"], "title": "Intelligence Degradation in Long-Context LLMs: Critical Threshold Determination via Natural Length Distribution Analysis", "categories": ["cs.CL"], "comment": "29 pages", "summary": "Large Language Models (LLMs) exhibit catastrophic performance degradation when processing contexts approaching certain critical thresholds, even when information remains relevant. This intelligence degradation-defined as over 30% drop in task performance-severely limits long-context applications. This degradation shows a common pattern: models maintain strong performance up to a critical threshold, then collapse catastrophically. We term this shallow long-context adaptation-models adapt for short to medium contexts but fail beyond critical thresholds. This paper presents three contributions: (1) Natural Length Distribution Analysis: We use each sample's natural token length without truncation or padding, providing stronger causal evidence that degradation results from context length itself. (2) Critical Threshold Determination: Through experiments on a mixed dataset (1,000 samples covering 5%-95% of context length), we identify the critical threshold for Qwen2.5-7B at 40-50% of maximum context length, where F1 scores drop from 0.55-0.56 to 0.3 (45.5% degradation), using five-method cross-validation. (3) Unified Framework: We consolidate shallow adaptation, explaining degradation patterns and providing a foundation for mitigation strategies. This work provides the first systematic characterization of intelligence degradation in open-source Qwen models, offering practical guidance for deploying LLMs in long-context scenarios.", "AI": {"tldr": "The paper investigates the catastrophic performance degradation in long-context tasks faced by large language models (LLMs) due to shallow long-context adaptation, focusing on Qwen2.5-7B as a case study.", "motivation": "The motivation behind this study is to address the critical issue of performance degradation in LLMs when processing contexts nearing length thresholds, which limits their effectiveness in long-context applications.", "method": "The authors analyze the natural distribution of token lengths, determine the critical context length thresholds through a mixed dataset and cross-validation, and propose a unified framework explaining shallow long-context adaptation.", "result": "The study finds that for Qwen2.5-7B, the performance critically drops by 45.5% when reaching 40-50% of the model's maximum context length, as evidenced by a reduction in F1 scores.", "conclusion": "The research systematically characterizes shallow long-context adaptation in Qwen models, offering insights and guidance for improving LLM performance in long-context scenarios."}}
{"id": "2601.15453", "pdf": "https://arxiv.org/pdf/2601.15453", "abs": "https://arxiv.org/abs/2601.15453", "authors": ["Morteza Poudineh", "Marc Lalonde"], "title": "DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection", "categories": ["cs.CV"], "comment": "8 pages", "summary": "Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.", "AI": {"tldr": "The paper introduces a deviation-guided prompt learning framework for few-normal shot anomaly detection, leveraging vision-language models and statistical scoring for improved anomaly localization and interpretability.", "motivation": "Few-normal shot anomaly detection is challenging due to limited normal data and the diverse types of potential defects in images.", "method": "The method integrates vision-language models with deviation-based statistical scoring using learnable prompt contexts and deviation loss with a Top-K Multiple Instance Learning (MIL) strategy.", "result": "Experiments on MVTecAD and VISA benchmarks show superior pixel-level detection performance over existing methods like PromptAD, with additional validation through ablation studies.", "conclusion": "The proposed framework enhances anomaly detection by combining semantic and statistical modeling, achieving better localization, detection accuracy, and interpretability."}}
{"id": "2601.16120", "pdf": "https://arxiv.org/pdf/2601.16120", "abs": "https://arxiv.org/abs/2601.16120", "authors": ["Zhengchi Ma", "Anru R. Zhang"], "title": "Synthetic Augmentation in Imbalanced Learning: When It Helps, When It Hurts, and How Much to Add", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Imbalanced classification, where one class is observed far less frequently than the other, often causes standard training procedures to prioritize the majority class and perform poorly on rare but important cases. A classic and widely used remedy is to augment the minority class with synthetic examples, but two basic questions remain under-resolved: when does synthetic augmentation actually help, and how many synthetic samples should be generated?\n  We develop a unified statistical framework for synthetic augmentation in imbalanced learning, studying models trained on imbalanced data augmented with synthetic minority samples and evaluated under the balanced population risk. Our theory shows that synthetic data is not always beneficial. In a ``local symmetry\" regime, imbalance is not the dominant source of error near the balanced optimum, so adding synthetic samples cannot improve learning rates and can even degrade performance by amplifying generator mismatch. When augmentation can help (a ``local asymmetry\" regime), the optimal synthetic size depends on generator accuracy and on whether the generator's residual mismatch is directionally aligned with the intrinsic majority-minority shift. This structure can make the best synthetic size deviate from naive full balancing, sometimes by a small refinement and sometimes substantially when generator bias is systematic. Practically, we recommend Validation-Tuned Synthetic Size (VTSS): select the synthetic size by minimizing balanced validation loss over a range centered near the fully balanced baseline, while allowing meaningful departures when the data indicate them. Simulations and a real sepsis prediction study support the theory and illustrate when synthetic augmentation helps, when it cannot, and how to tune its quantity effectively.", "AI": {"tldr": "The paper investigates synthetic data augmentation in imbalanced classification and develops a statistical framework to determine when and how much synthetic data is beneficial.", "motivation": "To address the challenge of poor model performance on minority classes in imbalanced classification, and to better understand the conditions under which synthetic data augmentation is effective.", "method": "The authors provide a unified statistical theory framing the effect of synthetic minority data, introducing the concepts of 'local symmetry' and 'local asymmetry' and proposing a Validation-Tuned Synthetic Size (VTSS) method.", "result": "The study shows synthetic data doesn't always help, as its effectiveness depends on factors such as generator accuracy and the alignment of residual mismatch with majority-minority shifts. Simulations and a real study validate and illustrate these findings.", "conclusion": "Synthetic augmentation efficacy varies based on context. Using VTSS to optimize synthetic data size can help improve learning performance, but context-specific understanding is required to determine its utility or limitations."}}
{"id": "2601.15486", "pdf": "https://arxiv.org/pdf/2601.15486", "abs": "https://arxiv.org/abs/2601.15486", "authors": ["Javier N. Ramos-Silva", "Peter J. Burke"], "title": "A Universal Large Language Model -- Drone Command and Control Interface", "categories": ["cs.RO"], "comment": null, "summary": "The use of artificial intelligence (AI) for drone control can have a transformative impact on drone capabilities, especially when real world information can be integrated with drone sensing, command, and control, part of a growing field of physical AI. Large language models (LLMs) can be advantageous if trained at scale on general knowledge, but especially and in particular when the training data includes information such as detailed map geography topology of the entire planet, as well as the ability to access real time situational data such as weather. However, challenges remain in the interface between drones and LLMs in general, with each application requiring a tedious, labor intensive effort to connect the LLM trained knowledge to drone command and control. Here, we solve that problem, using an interface strategy that is LLM agnostic and drone agnostic, providing the first universal, versatile, comprehensive and easy to use drone control interface. We do this using the new model context protocol (MCP) standard, an open standard that provides a universal way for AI systems to access external data, tools, and services. We develop and deploy a cloud based Linux machine hosting an MCP server that supports the Mavlink protocol, an ubiquitous drone control language used almost universally by millions of drones including Ardupilot and PX4 framework.We demonstrate flight control of a real unmanned aerial vehicle. In further testing, we demonstrate extensive flight planning and control capability in a simulated drone, integrated with a Google Maps MCP server for up to date, real time navigation information. This demonstrates a universal approach to integration of LLMs with drone command and control, a paradigm that leverages and exploits virtually all of modern AI industry with drone technology in an easy to use interface that translates natural language to drone control.", "AI": {"tldr": "This paper presents a novel interface connecting drones and large language models (LLMs) using the Model Context Protocol (MCP) standard. It allows universal and versatile drone control, demonstrating real and simulated drone operations with integrated real-time geographic and navigation data.", "motivation": "The paper aims to address the challenge of tedious and labor-intensive efforts required to connect large language models (LLMs) to drone command and control systems, emphasizing the need for an easy-to-use, universal interface.", "method": "The authors propose an interface strategy based on the Model Context Protocol (MCP) standard, deploying a cloud-based Linux machine hosting an MCP server. This server supports the Mavlink protocol to integrate drones and LLMs, enabling both real-world and simulation-based drone control.", "result": "The implementation successfully demonstrated flight control with an actual drone and extensive flight planning in a simulated drone. The integration with a Google Maps MCP server provided real-time navigation information.", "conclusion": "This universal interface strategy effectively bridges the gap between LLMs and drones, simplifying the integration process and enabling enhanced drone capabilities using natural language commands for control and navigation tasks."}}
{"id": "2601.15687", "pdf": "https://arxiv.org/pdf/2601.15687", "abs": "https://arxiv.org/abs/2601.15687", "authors": ["Khusrav Badalov", "Young Yoon"], "title": "FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Trigger-Action Programming (TAP) platforms such as IFTTT and Zapier enable Web of Things (WoT) automation by composing event-driven rules across heterogeneous services. A TAP applet links a trigger to an action and must bind trigger outputs (ingredients) to action inputs (fields) to be executable. Prior work largely treats TAP as service-level prediction from natural language, which often yields non-executable applets that still require manual configuration. We study the function-level configuration problem: generating complete applets with correct ingredient-to-field bindings. We propose FARM (Field-Aware Resolution Model), a two-stage architecture for automated applet generation with full configuration. Stage 1 trains contrastive dual encoders with selective layer freezing over schema-enriched representations, retrieving candidates from 1,724 trigger functions and 1,287 action functions (2.2M possible trigger-action pairs). Stage 2 performs selection and configuration using an LLM-based multi-agent pipeline. It includes intent analysis, trigger selection, action selection via cross-schema scoring, and configuration verification. Agents coordinate through shared state and agreement-based selection. FARM achieves 81% joint accuracy on Gold (62% Noisy, 70% One-shot) at the function level, where both trigger and action functions must match the ground truth. For comparison with service-level baselines, we map functions to their parent services and evaluate at the service level. FARM reaches 81% joint accuracy and improves over TARGE by 23 percentage points. FARM also generates ingredient-to-field bindings, producing executable automation configurations.", "AI": {"tldr": "The paper introduces FARM, a model focused on generating fully configured and executable applets for automated event-driven rules in Trigger-Action Programming platforms.", "motivation": "The paper addresses the challenge of generating executable applets for TAP platforms, aiming to improve the accuracy and automation of ingredient-to-field bindings for better Web of Things automation.", "method": "FARM employs a two-stage architecture: contrastive dual encoders in Stage 1 for candidate retrieval, and an LLM-based multi-agent pipeline in Stage 2 for configuration by coordinating agents and schema scoring.", "result": "FARM achieves 81% joint accuracy and surpasses existing baselines like TARGE by 23%. It produces executable applet configurations with accurate ingredient-to-field binding.", "conclusion": "FARM significantly advances TAP automation with high accuracy and full applet configuration, reducing manual intervention and improving system usability."}}
{"id": "2601.15390", "pdf": "https://arxiv.org/pdf/2601.15390", "abs": "https://arxiv.org/abs/2601.15390", "authors": ["Zhaolong Su", "Leheng Zhao", "Xiaoying Wu", "Ziyue Xu", "Jindong Wang"], "title": "FedUMM: A General Framework for Federated Learning with Unified Multimodal Models", "categories": ["cs.LG"], "comment": null, "summary": "Unified multimodal models (UMMs) are emerging as strong foundation models that can do both generation and understanding tasks in a single architecture. However, they are typically trained in centralized settings where all training and downstream datasets are gathered in a central server, limiting the deployment in privacy-sensitive and geographically distributed scenarios. In this paper, we present FedUMM, a general federated learning framework for UMMs under non-IID multimodal data with low communication cost. Built on NVIDIA FLARE, FedUMM instantiates federation for a BLIP3o backbone via parameter-efficient fine-tuning: clients train lightweight LoRA adapters while freezing the foundation models, and the server aggregates only adapter updates. We evaluate on VQA v2 and the GenEval compositional generation benchmarks under Dirichlet-controlled heterogeneity with up to 16 clients. Results show slight degradation as client count and heterogeneity increase, while remaining competitive with centralized training. We further analyze computation--communication trade-offs and demonstrate that adapter-only federation reduces per-round communication by over an order of magnitude compared to full fine-tuning, enabling practical federated UMM training. This work provides empirical experience for future research on privacy-preserving federated unified multimodal models.", "AI": {"tldr": "The paper introduces FedUMM, a federated learning framework for unified multimodal models (UMMs) focused on reducing communication costs and handling non-IID data.", "motivation": "To address privacy concerns and the limitations of centralized training by enabling federated learning for UMMs in geographically distributed and privacy-sensitive scenarios.", "method": "The authors propose FedUMM, which uses parameter-efficient fine-tuning (via lightweight LoRA adapters) for UMMs while freezing foundation model parameters, reducing communication by aggregating only adapter updates. Built on NVIDIA FLARE, it is tested on benchmarks like VQA v2 and GenEval.", "result": "FedUMM shows slight performance degradation with increased client count and heterogeneity but remains competitive with centralized training. It achieves significant communication cost reductions compared to full fine-tuning, making it practical for federated setups.", "conclusion": "FedUMM enables privacy-preserving and efficient federated learning for UMMs, offering a solid foundation for future research on federated multimodal models."}}
{"id": "2601.15322", "pdf": "https://arxiv.org/pdf/2601.15322", "abs": "https://arxiv.org/abs/2601.15322", "authors": ["Raffi Khatchadourian"], "title": "Replayable Financial Agents: A Determinism-Faithfulness Assurance Harness for Tool-Using LLM Agents", "categories": ["cs.AI", "cs.CL"], "comment": "23 pages, 5 figures, 9 tables. Code and data: https://github.com/ibm-client-engineering/output-drift-financial-llms", "summary": "LLM agents struggle with regulatory audit replay: when asked to reproduce a flagged transaction decision with identical inputs, most deployments fail to return consistent results. This paper introduces the Determinism-Faithfulness Assurance Harness (DFAH), a framework for measuring trajectory determinism and evidence-conditioned faithfulness in tool-using agents deployed in financial services.\n  Across 74 configurations (12 models, 4 providers, 8-24 runs each at T=0.0) in non-agentic baseline experiments, 7-20B parameter models achieved 100% determinism, while 120B+ models required 3.7x larger validation samples to achieve equivalent statistical reliability. Agentic tool-use introduces additional variance (see Tables 4-7). Contrary to the assumed reliability-capability trade-off, a positive Pearson correlation emerged (r = 0.45, p < 0.01, n = 51 at T=0.0) between determinism and faithfulness; models producing consistent outputs also tended to be more evidence-aligned.\n  Three financial benchmarks are provided (compliance triage, portfolio constraints, DataOps exceptions; 50 cases each) along with an open-source stress-test harness. In these benchmarks and under DFAH evaluation settings, Tier 1 models with schema-first architectures achieved determinism levels consistent with audit replay requirements.", "AI": {"tldr": "The paper introduces DFAH, a framework addressing LLM agents' struggles with consistent audit replay, showing trends between determinism and faithfulness, and benchmarks for financial services.", "motivation": "LLMs in financial services often fail to reproduce consistent decisions for flagged transactions during regulatory audits, creating compliance challenges.", "method": "The paper develops the DFAH framework to evaluate trajectory determinism and evidence-conditioned faithfulness, using experiments across various model configurations (12 models, multiple providers, runs at T=0.0) and financial benchmarks.", "result": "Findings indicate 7-20B parameter models achieve high determinism while 120B+ models require larger validation; a positive correlation exists between determinism and faithfulness, and schema-first Tier 1 models meet audit replay standards.", "conclusion": "The DFAH framework enhances consistency and evidence alignment in financial-service LLMs, providing tools and benchmarks to address audit replay requirements effectively."}}
{"id": "2601.15336", "pdf": "https://arxiv.org/pdf/2601.15336", "abs": "https://arxiv.org/abs/2601.15336", "authors": ["Daniel Brownell"], "title": "Learning Discrete Successor Transitions in Continuous Attractor Networks: Emergence, Limits, and Topological Constraints", "categories": ["q-bio.NC", "cs.AI"], "comment": "An open-source reference implementation is available at https://github.com/javadan/can-paper", "summary": "Continuous attractor networks (CANs) are a well-established class of models for representing low-dimensional continuous variables such as head direction, spatial position, and phase. In canonical spatial domains, transitions along the attractor manifold are driven by continuous displacement signals, such as angular velocity-provided by sensorimotor systems external to the CAN itself. When such signals are not explicitly provided as dedicated displacement inputs, it remains unclear whether attractor-based circuits can reliably acquire recurrent dynamics that support stable state transitions, or whether alternative predictive strategies dominate.\n  In this work, we present an experimental framework for training CANs to perform successor-like transitions between stable attractor states in the absence of externally provided displacement signals. We compare two recurrent topologies, a circular ring and a folded snake manifold, and systematically vary the temporal regime under which stability is evaluated. We find that, under short evaluation windows, networks consistently converge to impulse-driven associative solutions that achieve high apparent accuracy yet lack persistent attractor dynamics. Only when stability is explicitly enforced over extended free-run periods do genuine attractor-based transition dynamics emerge. This suggests that shortcut solutions are the default outcome of local learning in recurrent networks, while attractor dynamics represent a constrained regime rather than a generic result.\n  Furthermore, we demonstrate that topology strictly limits the capacity for learned transitions. While the continuous ring topology achieves perfect stability over long horizons, the folded snake topology hits a geometric limit characterized by failure at manifold discontinuities, which neither curriculum learning nor basal ganglia-inspired gating can fully overcome.", "AI": {"tldr": "This study investigates if continuous attractor networks (CANs) can reliably sustain transitions in the absence of explicit displacement inputs, analyzing different network topologies and stability conditions.", "motivation": "The paper aims to understand whether attractor-based circuits can exhibit stable recurrent dynamics without external displacement signals and explore how network topology and stability influence dynamics.", "method": "The study establishes an experimental framework to train CANs for stable transitions between attractor states. It compares a circular ring topology and a folded snake manifold while systematically varying temporal stability conditions.", "result": "Short evaluation windows lead to impulse-driven associative solutions dominating over attractor dynamics. Persistent attractor transitions only occur when stability is enforced over longer evaluation periods. Topology affects capacity; continuous ring topology achieves perfect stability, whereas folded snake topology fails at manifold discontinuities.", "conclusion": "Continuous attractor-based dynamics require constrained regimes rather than arising generically. Topology has a strict impact on stability, making continuous ring topology better suited for robust transitions compared to folded snake topology."}}
{"id": "2601.15301", "pdf": "https://arxiv.org/pdf/2601.15301", "abs": "https://arxiv.org/abs/2601.15301", "authors": ["Jivnesh Sandhan", "Harshit Jaiswal", "Fei Cheng", "Yugo Murawaki"], "title": "Can We Trust LLM Detectors?", "categories": ["cs.CL", "cs.AI"], "comment": "NLP2026, Utsunomiya, Japan", "summary": "The rapid adoption of LLMs has increased the need for reliable AI text detection, yet existing detectors often fail outside controlled benchmarks. We systematically evaluate 2 dominant paradigms (training-free and supervised) and show that both are brittle under distribution shift, unseen generators, and simple stylistic perturbations. To address these limitations, we propose a supervised contrastive learning (SCL) framework that learns discriminative style embeddings. Experiments show that while supervised detectors excel in-domain, they degrade sharply out-of-domain, and training-free methods remain highly sensitive to proxy choice. Overall, our results expose fundamental challenges in building domain-agnostic detectors. Our code is available at: https://github.com/HARSHITJAIS14/DetectAI", "AI": {"tldr": "Current AI text detectors struggle with reliability under varying conditions. This paper proposes a supervised contrastive learning framework to create more robust domain-agnostic detectors.", "motivation": "The growing use of large language models (LLMs) has increased the need for trustworthy AI text detection, as existing methods fail under real-world scenarios.", "method": "The authors systematically assess training-free and supervised detection methods, introduce supervised contrastive learning (SCL) to enhance discriminative style embeddings, and evaluate detector performance across domains.", "result": "Supervised detectors work well in specific domains but fail in others, while training-free methods are highly sensitive to their chosen proxies.", "conclusion": "Building truly domain-agnostic AI text detectors remains a complex challenge, and this study highlights the limitations and suggests areas for improvement in detection approaches."}}
{"id": "2601.15475", "pdf": "https://arxiv.org/pdf/2601.15475", "abs": "https://arxiv.org/abs/2601.15475", "authors": ["Yunshan Qi", "Lin Zhu", "Nan Bao", "Yifan Zhao", "Jia Li"], "title": "Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events", "categories": ["cs.CV"], "comment": null, "summary": "Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.", "AI": {"tldr": "This paper introduces a framework for producing sharp HDR novel view synthesis using single-exposure blurry LDR images and event data.", "motivation": "The motivation stems from the difficulty of achieving HDR and sharp 3D representations from blurry LDR images under extreme lighting, especially considering the sensor-physics mismatches overlooked by prior methods.", "method": "The method involves a physics-grounded NeRF framework that models the actual radiance of a 3D scene in the HDR domain. It employs pixel-wise RGB and event mapping fields to align rendered pixel values and sensor outputs, leveraging both spatial and temporal dynamics in events.", "result": "The proposed approach achieves superior deblurring and HDR novel view synthesis results through experiments on both public and curated datasets.", "conclusion": "This work highlights the effectiveness of sensor-physics alignment and event data integration in generating sharp HDR 3D scenes, setting a new benchmark for such tasks."}}
{"id": "2601.16174", "pdf": "https://arxiv.org/pdf/2601.16174", "abs": "https://arxiv.org/abs/2601.16174", "authors": ["Yiyao Yang"], "title": "Beyond Predictive Uncertainty: Reliable Representation Learning with Structural Constraints", "categories": ["stat.ML", "cs.LG"], "comment": "22 pages, 5 figures, 5 propositions", "summary": "Uncertainty estimation in machine learning has traditionally focused on the prediction stage, aiming to quantify confidence in model outputs while treating learned representations as deterministic and reliable by default. In this work, we challenge this implicit assumption and argue that reliability should be regarded as a first-class property of learned representations themselves. We propose a principled framework for reliable representation learning that explicitly models representation-level uncertainty and leverages structural constraints as inductive biases to regularize the space of feasible representations. Our approach introduces uncertainty-aware regularization directly in the representation space, encouraging representations that are not only predictive but also stable, well-calibrated, and robust to noise and structural perturbations. Structural constraints, such as sparsity, relational structure, or feature-group dependencies, are incorporated to define meaningful geometry and reduce spurious variability in learned representations, without assuming fully correct or noise-free structure. Importantly, the proposed framework is independent of specific model architectures and can be integrated with a wide range of representation learning methods.", "AI": {"tldr": "The paper proposes a framework to incorporate uncertainty estimation directly into learned representations using structural constraints, ensuring stable, calibrated, and robust learning.", "motivation": "Traditional machine learning often assumes learned representations are deterministic and reliable, neglecting their inherent uncertainty. This paper aims to redefine reliability at the representation level.", "method": "The framework introduces uncertainty-aware regularization in representation space with structural constraints, such as sparsity or feature relationships, to refine learned representations.", "result": "Learned representations become robust to noise and perturbations, are better calibrated, and adhere to meaningful structures, regardless of model architecture.", "conclusion": "Reliability in representations should become a core element, and the proposed approach integrates uncertainty into representation learning effectively without architectural dependence."}}
{"id": "2601.15541", "pdf": "https://arxiv.org/pdf/2601.15541", "abs": "https://arxiv.org/abs/2601.15541", "authors": ["Heng Zhang", "Wei-Hsing Huang", "Qiyi Tong", "Gokhan Solak", "Puze Liu", "Sheng Liu", "Jan Peters", "Arash Ajoudani"], "title": "CompliantVLA-adaptor: VLM-Guided Variable Impedance Action for Safe Contact-Rich Manipulation", "categories": ["cs.RO"], "comment": "under review", "summary": "We propose a CompliantVLA-adaptor that augments the state-of-the-art Vision-Language-Action (VLA) models with vision-language model (VLM)-informed context-aware variable impedance control (VIC) to improve the safety and effectiveness of contact-rich robotic manipulation tasks. Existing VLA systems (e.g., RDT, Pi0, OpenVLA-oft) typically output position, but lack force-aware adaptation, leading to unsafe or failed interactions in physical tasks involving contact, compliance, or uncertainty. In the proposed CompliantVLA-adaptor, a VLM interprets task context from images and natural language to adapt the stiffness and damping parameters of a VIC controller. These parameters are further regulated using real-time force/torque feedback to ensure interaction forces remain within safe thresholds. We demonstrate that our method outperforms the VLA baselines on a suite of complex contact-rich tasks, both in simulation and on real hardware, with improved success rates and reduced force violations. The overall success rate across all tasks increases from 9.86\\% to 17.29\\%, presenting a promising path towards safe contact-rich manipulation using VLAs. We release our code, prompts, and force-torque-impedance-scenario context datasets at https://sites.google.com/view/compliantvla.", "AI": {"tldr": "The paper introduces CompliantVLA-adaptor, a system enhancing Vision-Language-Action (VLA) models with context-aware variable impedance control for safe and effective robotic tasks.", "motivation": "Current VLA systems struggle with tasks involving physical contact due to their lack of force awareness, leading to unsafe or failed robotic interactions.", "method": "The proposed method combines a Vision-Language Model (VLM) to interpret context and a Variable Impedance Control (VIC) regulated by real-time force feedback for safety and effectiveness.", "result": "The system achieves improved success rates, increasing from 9.86% to 17.29%, with reduced force violations in contact-rich tasks.", "conclusion": "CompliantVLA-adaptor provides a promising solution for safe robotic manipulation in contact-rich contexts by combining vision-language understanding with force-adaptive control."}}
{"id": "2601.15879", "pdf": "https://arxiv.org/pdf/2601.15879", "abs": "https://arxiv.org/abs/2601.15879", "authors": ["Jiajun Zhang", "Zeyu Cui", "Lei Zhang", "Jian Yang", "Jiaxi Yang", "Qiang Liu", "Zilei Wang", "Binyuan Hui", "Liang Wang", "Junyang Lin"], "title": "Evaluating and Achieving Controllable Code Completion in Code LLM", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "Code completion has become a central task, gaining significant attention with the rise of large language model (LLM)-based tools in software engineering. Although recent advances have greatly improved LLMs' code completion abilities, evaluation methods have not advanced equally. Most current benchmarks focus solely on functional correctness of code completions based on given context, overlooking models' ability to follow user instructions during completion-a common scenario in LLM-assisted programming. To address this limitation, we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench), comprising 2,195 carefully designed completion tasks. Through comprehensive evaluation of over 40 mainstream LLMs across C3-Bench and conventional benchmarks, we reveal substantial gaps in instruction-following capabilities between open-source and advanced proprietary models during code completion tasks. Moreover, we develop a straightforward data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality instruction-completion pairs for supervised fine-tuning (SFT). The resulting model, Qwen2.5-Coder-C3, achieves state-of-the-art performance on C3-Bench. Our findings provide valuable insights for enhancing LLMs' code completion and instruction-following capabilities, establishing new directions for future research in code LLMs. To facilitate reproducibility and foster further research in code LLMs, we open-source all code, datasets, and models.", "AI": {"tldr": "The paper introduces C3-Bench, the first benchmark targeting LLMs' ability to follow instructions during code completion, and reveals performance gaps among LLMs. It also provides a novel model achieving state-of-the-art results and open-sources tools for further research.", "motivation": "Current evaluations of code completion models focus on functional correctness while neglecting instruction-following capabilities, which are critical in LLM-assisted programming.", "method": "The authors design a benchmark (C3-Bench) with 2,195 tasks, evaluate over 40 LLMs, and develop a data synthesis pipeline leveraging Qwen2.5-Coder for supervised fine-tuning.", "result": "C3-Bench reveals significant gaps in LLMs' instruction-following abilities, and the fine-tuned model Qwen2.5-Coder-C3 achieves state-of-the-art performance on this benchmark.", "conclusion": "The research highlights opportunities to improve LLMs' instruction-following and code completion skills, providing open-source resources to drive further advancements in the field."}}
{"id": "2601.15399", "pdf": "https://arxiv.org/pdf/2601.15399", "abs": "https://arxiv.org/abs/2601.15399", "authors": ["Ashna Nawar Ahmed", "Banooqa Banday", "Terry Jones", "Tanzima Z. Islam"], "title": "Attention-Informed Surrogates for Navigating Power-Performance Trade-offs in HPC", "categories": ["cs.LG"], "comment": "13 pages, 6 figures Published in MLForSys workshop in NeurIPS 2025 Link: https://openreview.net/forum?id=R0Vc9lnDd5", "summary": "High-Performance Computing (HPC) schedulers must balance user performance with facility-wide resource constraints. The task boils down to selecting the optimal number of nodes for a given job. We present a surrogate-assisted multi-objective Bayesian optimization (MOBO) framework to automate this complex decision. Our core hypothesis is that surrogate models informed by attention-based embeddings of job telemetry can capture performance dynamics more effectively than standard regression techniques. We pair this with an intelligent sample acquisition strategy to ensure the approach is data-efficient. On two production HPC datasets, our embedding-informed method consistently identified higher-quality Pareto fronts of runtime-power trade-offs compared to baselines. Furthermore, our intelligent data sampling strategy drastically reduced training costs while improving the stability of the results. To our knowledge, this is the first work to successfully apply embedding-informed surrogates in a MOBO framework to the HPC scheduling problem, jointly optimizing for performance and power on production workloads.", "AI": {"tldr": "The study presents a novel framework using embedding-informed surrogate models in a multi-objective Bayesian optimization (MOBO) setting to optimize HPC scheduling.", "motivation": "The paper addresses the challenge of balancing user performance with resource constraints in HPC scheduling by optimizing the selection of compute nodes for jobs.", "method": "The proposed method uses surrogate models informed by attention-based embeddings from job telemetry, combined with an intelligent sample acquisition strategy, to power a data-efficient MOBO framework.", "result": "The approach consistently identified better Pareto fronts for runtime-power trade-offs, reducing training costs and enhancing result stability, as demonstrated on two production HPC datasets.", "conclusion": "Embedding-informed surrogates in MOBO are a promising technique for improving HPC scheduling by optimizing performance and power in a resource-efficient manner."}}
{"id": "2601.15324", "pdf": "https://arxiv.org/pdf/2601.15324", "abs": "https://arxiv.org/abs/2601.15324", "authors": ["Mark Wind"], "title": "Prometheus Mind: Retrofitting Memory to Frozen Language Models", "categories": ["cs.AI"], "comment": "28 pages", "summary": "Adding memory to pretrained language models typically requires architectural changes or weight modification. We present Prometheus Mind, which retrofits memory to a frozen Qwen3-4B using 11 modular adapters (530MB, 7% overhead) -- fully reversible by removing the adapters. Building this system required solving four problems: (1) Extraction -- we develop Contrastive Direction Discovery (CDD), which finds semantic directions via minimal pairs without labeled data. (2) Training -- end-to-end optimization collapses; stage-wise training of each adapter on simple proxy tasks succeeds. (3) Injection -- learned encoders fail to generalize; we find that lm_head.weight rows already provide the mapping we need, requiring no training. (4) Hidden state collapse -- transformers make ``wife'' and ``brother'' 0.98+ similar; we train projections to recover distinction (0.98 $\\rightarrow$ 0.09). On PrometheusExtract-132 (132 cases), the system achieves 94.4% retrieval on clean inputs (n=54, 95% CI: [84.9%, 98.1%]), degrading to 19.4% on informal inputs with ellipsis, filler words, or implicit subjects (n=36). The primary bottleneck is relation classification (47.3% accuracy), responsible for most extraction errors.", "AI": {"tldr": "The paper introduces Prometheus Mind, a system enabling memory in a frozen language model (Qwen3-4B) using reversible adapters. It addresses challenges like semantic representation, training strategy, encoder generalization, and hidden state collapse.", "motivation": "Existing methods for adding memory to language models often focus on architectural alterations or weight modifications, which can be cumbersome. This paper aims to provide a modular and reversible solution for integrating memory into frozen LMs.", "method": "The authors propose Prometheus Mind, which retrofits memory using 11 modular adapters and employs novel techniques like Contrastive Direction Discovery (CDD) for semantic extraction, stage-wise adapter training, leveraging pre-trained mapping for encoding, and addressing hidden state collapse through projections.", "result": "Prometheus Mind achieves 94.4% retrieval accuracy on clean input cases but struggles with informal inputs, with degraded performance at 19.4%. The primary limitation lies in the relation classification step, which has a 47.3% accuracy rate.", "conclusion": "The proposed system successfully demonstrates module-based memory integration into a frozen language model, but there is room for improving generalization and robustness, particularly in handling informal input and relation classification."}}
{"id": "2601.15344", "pdf": "https://arxiv.org/pdf/2601.15344", "abs": "https://arxiv.org/abs/2601.15344", "authors": ["Cole Korponay"], "title": "A Dual-Head Transformer-State-Space Architecture for Neurocircuit Mechanism Decomposition from fMRI", "categories": ["q-bio.NC"], "comment": null, "summary": "Precision psychiatry aspires to elucidate brain-based biomarkers of psychopathology to bolster disease risk assessment and treatment development. To this end, functional magnetic resonance imaging (fMRI) has helped triangulate brain circuits whose functional features are correlated with or even predictive of forms of psychopathology. Yet, fMRI biomarkers to date remain largely descriptive identifiers of where, rather than how, neurobiology is aberrant, limiting their utility for guiding treatment. We present a method for decomposing fMRI-based functional connectivity (FC) into constituent biomechanisms - output drive, input responsivity, modulator gating - with clearer alignment to differentiable therapeutic interventions. Neurocircuit mechanism decomposition (NMD) integrates (i) a graph-constrained, lag-aware transformer to estimate directed, pathway-specific routing distributions and drive signals, with (ii) a measurement-aware state-space model (SSM) that models hemodynamic convolution and recovers intrinsic latent dynamics. This dual-head architecture yields interpretable circuit parameters that may provide a more direct bridge from fMRI to treatment strategy selection. We instantiate the model in an anatomically and electrophysiologically well-defined circuit: the cortico-basal ganglia-thalamo-cortical loop.", "AI": {"tldr": "The paper aims to improve precision psychiatry by introducing a method to derive therapeutic insights from fMRI data through neurocircuit mechanism decomposition.", "motivation": "To enhance precision psychiatry, there is a need to move beyond descriptive fMRI biomarkers toward actionable neurobiological understanding for better risk assessment and treatment guidance.", "method": "The authors propose Neurocircuit Mechanism Decomposition (NMD), which combines a lag-aware transformer for pathway-specific signals and a state-space model to disentangle functional connectivity into biomechanistic components.", "result": "The proposed dual-head architecture offers interpretable circuit parameters that align with potential therapeutic strategies, tested within a cortico-basal ganglia-thalamo-cortical loop circuit.", "conclusion": "This approach provides a pathway to develop fMRI biomarkers that go beyond descriptive correlations to actionable insights, bridging neuroscience and clinical treatment strategies."}}
{"id": "2601.15330", "pdf": "https://arxiv.org/pdf/2601.15330", "abs": "https://arxiv.org/abs/2601.15330", "authors": ["Zhebo Wang", "Xiaohu Mu", "Zijie Zhou", "Mohan Li", "Wenpeng Xing", "Dezhang Kong", "Meng Han"], "title": "ICPO: Illocution-Calibrated Policy Optimization for Multi-Turn Conversation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICASSP 2026", "summary": "Large Language Models (LLMs) in multi-turn conversations often suffer from a ``lost-in-conversation'' phenomenon, where they struggle to recover from early incorrect assumptions, particularly when users provide ambiguous initial instructions. We find that standard post-training techniques like Reinforcement Learning with Verifiable Rewards (RLVR) exacerbate this issue by rewarding confident, direct answers, thereby inducing overconfidence and discouraging the model from seeking clarification. To address this, we propose Illocution-Calibrated Policy Optimization (ICPO), a novel training framework that sensitizes the model to instruction ambiguity. ICPO augments the training corpus with underspecified prompts and conditions the reward signal on the user's illocutionary intent, rewarding the model for expressing uncertainty or asking for clarification when faced with ambiguity. Experiments demonstrate that ICPO fosters appropriate humility, yielding a substantial average improvement of 75\\% in multi-turn conversation, while preserving robust performance on single-turn benchmarks. Our work presents a practical path toward more robust and collaborative conversational AI that can better navigate the nuances of human interaction.", "AI": {"tldr": "The paper identifies issues with LLMs in multi-turn conversations, particularly struggling with ambiguity, and proposes a training framework (ICPO) that improves conversation handling while maintaining single-turn performance.", "motivation": "To address the problem of LLMs failing in multi-turn conversations due to overconfidence and poor handling of initial ambiguities, which leads to ineffective human interactions.", "method": "Proposed the 'Illocution-Calibrated Policy Optimization' (ICPO), a framework that uses an augmented training dataset with ambiguous prompts and rewards the model for expressing uncertainty or seeking clarification based on inferred user intent.", "result": "Experiments showed a 75% improvement in handling multi-turn conversations while maintaining strong performance in single-turn tasks.", "conclusion": "The ICPO framework can enable LLMs to conduct more robust and collaborative conversations by effectively handling ambiguous instructions and nuances in human interaction."}}
{"id": "2601.15490", "pdf": "https://arxiv.org/pdf/2601.15490", "abs": "https://arxiv.org/abs/2601.15490", "authors": ["Jobeal Solomon", "Ali Mohammed Mansoor Alsahag", "Seyed Sahand Mohammadi Ziabari"], "title": "Hybrid Vision Transformer_GAN Attribute Neutralizer for Mitigating Bias in Chest X_Ray Diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "Bias in chest X-ray classifiers frequently stems from sex- and age-related shortcuts, leading to systematic underdiagnosis of minority subgroups. Previous pixel-space attribute neutralizers, which rely on convolutional encoders, lessen but do not fully remove this attribute leakage at clinically usable edit strengths. This study evaluates whether substituting the U-Net convolutional encoder with a Vision Transformer backbone in the Attribute-Neutral Framework can reduce demographic attribute leakage while preserving diagnostic accuracy. A data-efficient Image Transformer Small (DeiT-S) neutralizer was trained on the ChestX-ray14 dataset. Its edited images, generated across eleven edit-intensity levels, were evaluated with an independent AI judge for attribute leakage and with a convolutional neural network (ConvNet) for disease prediction. At a moderate edit level (alpha = 0.5), the Vision Transformer (ViT) neutralizer reduces patient sex-recognition area under the curve (AUC) to approximately 0.80, about 10 percentage points below the original framework's convolutional U-Net encoder, despite being trained for only half as many epochs. Meanwhile, macro receiver operating characteristic area under the curve (ROC AUC) across 15 findings stays within five percentage points of the unedited baseline, and the worst-case subgroup AUC remains near 0.70. These results indicate that global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, suggesting a practical route toward fairer chest X-ray AI.", "AI": {"tldr": "This paper investigates using Vision Transformer backbones in chest X-ray classifiers to reduce demographic bias while maintaining diagnostic accuracy.", "motivation": "Chest X-ray classifiers exhibit bias due to demographic shortcuts, leading to underdiagnosis in minority groups. Current convolutional methods do not fully address this issue.", "method": "The study integrates a Vision Transformer backbone into the Attribute-Neutral Framework, using the DeiT-S model trained on ChestX-ray14 and evaluating bias reduction and prediction accuracy across different edit levels.", "result": "The Vision Transformer neutralizer decreases demographic attribute leakage significantly compared to U-Net, while maintaining diagnostic accuracy within acceptable limits.", "conclusion": "Global self-attention-based models enhance fairness in chest X-ray AI without compromising clinical utility, providing a pathway to reduce bias effectively."}}
{"id": "2601.15353", "pdf": "https://arxiv.org/pdf/2601.15353", "abs": "https://arxiv.org/abs/2601.15353", "authors": ["Asim H. Gazi", "Yongyi Guo", "Daiqi Gao", "Ziping Xu", "Kelly W. Zhang", "Susan A. Murphy"], "title": "Statistical Reinforcement Learning in the Real World: A Survey of Challenges and Future Directions", "categories": ["stat.AP", "cs.LG", "stat.ML"], "comment": null, "summary": "Reinforcement learning (RL) has achieved remarkable success in real-world decision-making across diverse domains, including gaming, robotics, online advertising, public health, and natural language processing. Despite these advances, a substantial gap remains between RL research and its deployment in many practical settings. Two recurring challenges often underlie this gap. First, many settings offer limited opportunity for the agent to interact extensively with the target environment due to practical constraints. Second, many target environments often undergo substantial changes, requiring redesign and redeployment of RL systems (e.g., advancements in science and technology that change the landscape of healthcare delivery). Addressing these challenges and bridging the gap between basic research and application requires theory and methodology that directly inform the design, implementation, and continual improvement of RL systems in real-world settings.\n  In this paper, we frame the application of RL in practice as a three-component process: (i) online learning and optimization during deployment, (ii) post- or between-deployment offline analyses, and (iii) repeated cycles of deployment and redeployment to continually improve the RL system. We provide a narrative review of recent advances in statistical RL that address these components, including methods for maximizing data utility for between-deployment inference, enhancing sample efficiency for online learning within-deployment, and designing sequences of deployments for continual improvement. We also outline future research directions in statistical RL that are use-inspired -- aiming for impactful application of RL in practice.", "AI": {"tldr": "This paper addresses challenges in real-world reinforcement learning (RL) deployment and proposes a framework to optimize RL systems through continual improvement.", "motivation": "There is a gap between RL research and real-world applications due to limited interaction with target environments and frequent changes in those environments.", "method": "The study proposes a three-component process: online learning and optimization during deployment, offline analyses between deployments, and repeated cycles of deployment and redevelopment, alongside reviewing statistical RL advances.", "result": "The paper reviews approaches for enhancing data utility, improving sample efficiency, and designing efficient deployment cycles to address practical RL challenges.", "conclusion": "The study suggests future research directions in statistical RL that prioritize impactful real-world applications and continual improvement of RL systems."}}
{"id": "2601.15545", "pdf": "https://arxiv.org/pdf/2601.15545", "abs": "https://arxiv.org/abs/2601.15545", "authors": ["Zhifan Yan", "Chang Liu", "Yiyang Jiang", "Wenxuan Zheng", "Xinhao Chen", "Axel Krieger"], "title": "A Mobile Magnetic Manipulation Platform for Gastrointestinal Navigation with Deep Reinforcement Learning Control", "categories": ["cs.RO"], "comment": null, "summary": "Targeted drug delivery in the gastrointestinal (GI) tract using magnetic robots offers a promising alternative to systemic treatments. However, controlling these robots is a major challenge. Stationary magnetic systems have a limited workspace, while mobile systems (e.g., coils on a robotic arm) suffer from a \"model-calibration bottleneck\", requiring complex, pre-calibrated physical models that are time-consuming to create and computationally expensive. This paper presents a compact, low-cost mobile magnetic manipulation platform that overcomes this limitation using Deep Reinforcement Learning (DRL). Our system features a compact four-electromagnet array mounted on a UR5 collaborative robot. A Soft Actor-Critic (SAC)-based control strategy is trained through a sim-to-real pipeline, enabling effective policy deployment within 15 minutes and significantly reducing setup time. We validated the platform by controlling a 7-mm magnetic capsule along 2D trajectories. Our DRL-based controller achieved a root-mean-square error (RMSE) of 1.18~mm for a square path and 1.50~mm for a circular path. We also demonstrated successful tracking over a clinically relevant, 30 cm * 20 cm workspace. This work demonstrates a rapidly deployable, model-free control framework capable of precise magnetic manipulation in a large workspace,validated using a 2D GI phantom.", "AI": {"tldr": "Magnetic robots are used for targeted drug delivery in the gastrointestinal tract, but control challenges persist. This paper introduces a cost-effective mobile magnetic manipulation platform using Deep Reinforcement Learning (DRL), achieving precise control and rapid deployment.", "motivation": "Current magnetic robot systems for targeted drug delivery face challenges like limited workspace and model calibration complexity, necessitating a more efficient solution.", "method": "The study utilizes a compact four-electromagnet array on a robotic arm with a Soft Actor-Critic-based Deep Reinforcement Learning (DRL) strategy. Training is conducted via a sim-to-real pipeline, enabling rapid controller deployment.", "result": "The system successfully controlled a 7-mm magnetic capsule along 2D trajectories with low error (RMSE: 1.18 mm for square paths, 1.50 mm for circular paths) and achieved tracking in a clinically relevant workspace (30 cm x 20 cm).", "conclusion": "This demonstrates a deployable, model-free control framework for magnetic robots, offering precise manipulation in a large gastrointestinal workspace within a short setup time."}}
{"id": "2601.16009", "pdf": "https://arxiv.org/pdf/2601.16009", "abs": "https://arxiv.org/abs/2601.16009", "authors": ["Giovanna Broccia", "Sira Vegas", "Alessio Ferrari"], "title": "The Role of Cognitive Abilities in Requirements Inspection: Comparing UML and Textual Representations", "categories": ["cs.SE"], "comment": null, "summary": "The representation of requirements plays a critical role in the accuracy of requirements inspection. While visual representations, such as UML diagrams, are widely used alongside text-based requirements, their effectiveness in supporting inspection is still debated. Cognitive abilities, such as working memory and mental rotation skills, may also influence inspection accuracy. This study aims to evaluate whether the use of UML sequence diagrams alongside text-based requirements improves the accuracy of requirements inspection compared to text-based requirements alone and to explore whether cognitive abilities are associated with differences in performance across the two treatments (text vs text with UML support). We conducted a crossover experiment with 38 participants to assess the accuracy of requirements inspection under the two treatments in terms of issues found and justifications provided. Linear mixed-effects and generalized linear models were used to analyse the effects of treatment, period, sequence, and cognitive abilities. The results indicate a significant three-way interaction between representation type, working memory capacity, and mental rotation ability. This finding suggests that the effectiveness of UML support is not uniform across individuals: participants with high scores in both cognitive abilities experienced reduced performance when using UML for violation detection. Conversely, the same cognitive profile was associated with improved justification accuracy under UML-aided inspection, indicating that higher cognitive abilities may support deeper reasoning processes when dealing with multi-modal information, i.e., diagrams and text.", "AI": {"tldr": "The study investigates whether using UML sequence diagrams with text-based requirements enhances the accuracy of requirements inspections, revealing a complex relationship with cognitive abilities.", "motivation": "To determine if UML sequence diagrams can enhance the accuracy of requirements inspection and examine how cognitive abilities influence inspection performance.", "method": "A crossover experiment with 38 participants assessing requirements inspection under two treatments: text-only and text with UML diagrams. Linear mixed-effects and generalized linear models were employed.", "result": "A significant interaction between representation type, working memory, and mental rotation skills was observed. Participants with high cognitive abilities had mixed performance effects.", "conclusion": "The study concludes that UML support for inspection benefits depend on cognitive profiles. High-cognitive individuals may struggle with violation detection but perform better in justification accuracy with UML diagrams."}}
{"id": "2601.15417", "pdf": "https://arxiv.org/pdf/2601.15417", "abs": "https://arxiv.org/abs/2601.15417", "authors": ["Adri\u00e1n Rodr\u00edguez-Mu\u00f1oz", "William Daspit", "Adam Klivans", "Antonio Torralba", "Constantinos Daskalakis", "Giannis Daras"], "title": "Ambient Dataloops: Generative Models for Dataset Refinement", "categories": ["cs.LG", "cs.AI"], "comment": "27 pages, 9 figures, 11 tables", "summary": "We propose Ambient Dataloops, an iterative framework for refining datasets that makes it easier for diffusion models to learn the underlying data distribution. Modern datasets contain samples of highly varying quality, and training directly on such heterogeneous data often yields suboptimal models. We propose a dataset-model co-evolution process; at each iteration of our method, the dataset becomes progressively higher quality, and the model improves accordingly. To avoid destructive self-consuming loops, at each generation, we treat the synthetically improved samples as noisy, but at a slightly lower noisy level than the previous iteration, and we use Ambient Diffusion techniques for learning under corruption. Empirically, Ambient Dataloops achieve state-of-the-art performance in unconditional and text-conditional image generation and de novo protein design. We further provide a theoretical justification for the proposed framework that captures the benefits of the data looping procedure.", "AI": {"tldr": "Proposing Ambient Dataloops, an iterative framework for improving training datasets and models for diffusion-based learning.", "motivation": "Training diffusion models on datasets with varying quality often hinders model performance.", "method": "Introduces a co-evolution process where datasets are iteratively refined using Ambient Diffusion techniques to handle noisy samples.", "result": "Achieves state-of-the-art performance in image generation and protein design applications.", "conclusion": "The framework allows datasets and models to mutually improve, with theoretical validation provided."}}
{"id": "2601.15347", "pdf": "https://arxiv.org/pdf/2601.15347", "abs": "https://arxiv.org/abs/2601.15347", "authors": ["Chuanqing Wang", "Zhenmin Zhao", "Shanshan Du", "Chaoqun Fei", "Songmao Zhang", "Ruqian Lu"], "title": "Logic Programming on Knowledge Graph Networks And its Application in Medical Domain", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "33 pages", "summary": "The rash development of knowledge graph research has brought big driving force to its application in many areas, including the medicine and healthcare domain. However, we have found that the application of some major information processing techniques on knowledge graph still lags behind. This defect includes the failure to make sufficient use of advanced logic reasoning, advanced artificial intelligence techniques, special-purpose programming languages, modern probabilistic and statistic theories et al. on knowledge graphs development and application. In particular, the multiple knowledge graphs cooperation and competition techniques have not got enough attention from researchers. This paper develops a systematic theory, technique and application of the concept 'knowledge graph network' and its application in medical and healthcare domain. Our research covers its definition, development, reasoning, computing and application under different conditions such as unsharp, uncertain, multi-modal, vectorized, distributed, federated. Almost in each case we provide (real data) examples and experiment results. Finally, a conclusion of innovation is provided.", "AI": {"tldr": "This paper proposes a systematic approach to 'knowledge graph networks' and their application in the medical and healthcare domain.", "motivation": "The motivation lies in addressing the current limitations in applying advanced techniques like logic reasoning, artificial intelligence, specialized programming, and probabilistic theories to knowledge graphs, especially when it pertains to cooperation and competition among multiple knowledge graphs.", "method": "The authors develop a framework for 'knowledge graph networks' covering definition, development, reasoning, computation, and application across various conditions such as uncertainty, multimodality, and distribution, supported by real data examples and experiment results.", "result": "The paper demonstrates the systematic development of knowledge graph networks with real data experiments proving its effectiveness in medical and healthcare scenarios.", "conclusion": "The authors conclude by emphasizing their innovative contribution in the application and theory of knowledge graph networks to tackle practical challenges in the field of medicine and healthcare."}}
{"id": "2601.15462", "pdf": "https://arxiv.org/pdf/2601.15462", "abs": "https://arxiv.org/abs/2601.15462", "authors": ["Shoshana Chipman", "Brent Doiron"], "title": "Dynamic Mean Field Theories for Nonlinear Noise in Recurrent Neuronal Networks", "categories": ["q-bio.NC", "math.DS"], "comment": "31 pages, 9 figures", "summary": "Strong, correlated noise in recurrent neural circuits often passes through nonlinear transfer functions, complicating dynamical mean-field analyses of complex phenomena such as transients and bifurcations. We introduce a method that replaces nonlinear functions of Ornstein-Uhlenbeck (OU) noise with a Gaussian-equivalent process matched in mean and covariance, and combine this with a lognormal moment closure for expansive nonlinearities to derive a closed dynamical mean-field theory for recurrent neuronal networks. The resulting theory captures order-one transients, fixed points, and noise-induced shifts of bifurcation structure, and outperforms standard linearization-based approximations in the strong-fluctuation regime. More broadly, the approach applies whenever dynamics depend smoothly on OU processes via nonlinear transformations, offering a tractable route to noise-dependent phase diagrams in computational neuroscience models.", "AI": {"tldr": "The paper proposes a novel approach to analyze systems with strong correlated noise using a nonlinear transformation and Gaussian-equivalent process methodology, improving upon standard approximations.", "motivation": "To address challenges posed by strong correlated noise and nonlinearity in neural circuits, which complicate standard dynamical mean-field analysis.", "method": "Develop a dynamical mean-field theory that combines Gaussian-equivalent processes for nonlinear transformations of Ornstein-Uhlenbeck noise and lognormal moment closure.", "result": "The theory accurately captures transients, fixed points, and bifurcation shifts under strong fluctuations, outperforming linearization-based methods.", "conclusion": "The proposed method provides a tractable framework for modeling noise-dependent dynamics in computational neuroscience and analyzing neural network behaviors."}}
