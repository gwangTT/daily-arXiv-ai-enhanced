<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 8]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.CL](#cs.CL) [Total: 44]
- [cs.CV](#cs.CV) [Total: 85]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.LG](#cs.LG) [Total: 72]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 21]
- [cs.SE](#cs.SE) [Total: 9]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [stat.ML](#stat.ML) [Total: 7]
- [cs.MA](#cs.MA) [Total: 3]
- [eess.SP](#eess.SP) [Total: 3]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [eess.IV](#eess.IV) [Total: 11]
- [cs.DS](#cs.DS) [Total: 2]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 7]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.SI](#cs.SI) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [stat.AP](#stat.AP) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 3]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [stat.CO](#stat.CO) [Total: 2]
- [cs.IR](#cs.IR) [Total: 3]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [eess.SY](#eess.SY) [Total: 3]
- [cs.SD](#cs.SD) [Total: 6]
- [quant-ph](#quant-ph) [Total: 2]
- [stat.ME](#stat.ME) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 14]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.NI](#cs.NI) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Digital Wargames to Enhance Military Medical Evacuation Decision-Making](https://arxiv.org/abs/2507.06373)
*Jeremy Fischer,Ram Krishnamoorthy,Vishal Kumar,Mahdi Al-Husseini*

Main category: cs.AI

TL;DR: The Medical Evacuation Wargaming Initiative (MEWI) is a 3D simulation designed to train U.S. Army personnel in medical evacuation planning and decision-making, significantly enhancing their skills and understanding.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of a classroom-compatible medium to simulate medical evacuation networks and optimize offline planning and online decision-making for U.S. Army medical evacuations.

Method: The researchers developed MEWI, a Unity-based 3D multiplayer simulation mimicking real-world battlefield medical evacuation scenarios, including patient transports and facility interactions, tested in operational contexts like amphibious assaults and road/river conflicts.

Result: MEWI trials conducted during the U.S. Army's Medical Evacuation Doctrine Course demonstrated improved decision-making and adherence to doctrinal lessons, as supported by survey and observer data.

Conclusion: MEWI represents a significant advancement in medical evacuation education, providing a high-fidelity training tool that enhances cooperative decision-making and operational efficacy.

Abstract: Medical evacuation is one of the United States Army's most storied and
critical mission sets, responsible for efficiently and expediently evacuating
the battlefield ill and injured. Medical evacuation planning involves designing
a robust network of medical platforms and facilities capable of moving and
treating large numbers of casualties. Until now, there has not been a medium to
simulate these networks in a classroom setting and evaluate both offline
planning and online decision-making performance. This work describes the
Medical Evacuation Wargaming Initiative (MEWI), a three-dimensional multiplayer
simulation developed in Unity that replicates battlefield constraints and
uncertainties. MEWI accurately models patient interactions at casualty
collection points, ambulance exchange points, medical treatment facilities, and
evacuation platforms. Two operational scenarios are introduced: an amphibious
island assault in the Pacific and a Eurasian conflict across a sprawling road
and river network. These scenarios pit students against the clock to save as
many casualties as possible while adhering to doctrinal lessons learned during
didactic training. We visualize performance data collected from two iterations
of the MEWI Pacific scenario executed in the United States Army's Medical
Evacuation Doctrine Course. We consider post-wargame Likert survey data from
student participants and external observer notes to identify key planning
decision points, document medical evacuation lessons learned, and quantify
general utility. Results indicate that MEWI participation substantially
improves uptake of medical evacuation lessons learned and co-operative
decision-making. MEWI is a substantial step forward in the field of
high-fidelity training tools for medical education, and our study findings
offer critical insights into improving medical evacuation education and
operations across the joint force.

</details>


### [2] [Representing Prompting Patterns with PDL: Compliance Agent Case Study](https://arxiv.org/abs/2507.06396)
*Mandana Vaziri,Louis Mandel,Yuji Watanabe,Hirokuni Kitahara,Martin Hirzel,Anca Sailer*

Main category: cs.AI

TL;DR: The paper introduces the Prompt Declaration Language (PDL), a new and flexible approach to address the complexity of prompt engineering for large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenges in prompt engineering, where existing tools either obscure complexity with restrictive APIs or offer rigid templates unsuitable for advanced use cases.

Method: The authors developed PDL, a declarative language to represent and compose prompts, enabling manual and automatic tuning along with integration of rule-based code and external tools.

Result: The case study of a compliance agent showed that using PDL allowed for up to 4x performance improvement compared to predefined patterns.

Conclusion: PDL provides a flexible and productive way to handle prompt engineering for LLMs, making it easier to optimize and customize prompts effectively.

Abstract: Prompt engineering for LLMs remains complex, with existing frameworks either
hiding complexity behind restrictive APIs or providing inflexible canned
patterns that resist customization -- making sophisticated agentic programming
challenging. We present the Prompt Declaration Language (PDL), a novel approach
to prompt representation that tackles this fundamental complexity by bringing
prompts to the forefront, enabling manual and automatic prompt tuning while
capturing the composition of LLM calls together with rule-based code and
external tools. By abstracting away the plumbing for such compositions, PDL
aims at improving programmer productivity while providing a declarative
representation that is amenable to optimization. This paper demonstrates PDL's
utility through a real-world case study of a compliance agent. Tuning the
prompting pattern of this agent yielded up to 4x performance improvement
compared to using a canned agent and prompt pattern.

</details>


### [3] [Jolting Technologies: Superexponential Acceleration in AI Capabilities and Implications for AGI](https://arxiv.org/abs/2507.06398)
*David Orban*

Main category: cs.AI

TL;DR: This paper investigates a hypothesis of superexponential growth in AI capabilities, develops theoretical frameworks, and validates methods through simulations.


<details>
  <summary>Details</summary>
Motivation: To understand whether AI capabilities are accelerating at a superexponential rate and explore its implications for AGI emergence.

Method: The authors formalized a theoretical framework, conducted Monte Carlo simulations, and proposed tools to detect superexponential growth.

Result: The study provides mathematical foundations and simulation-validated methods for analyzing accelerated AI development trends.

Conclusion: The work lays the groundwork for future empirical studies and offers insights into potential AI trajectories and the need for informed policy.

Abstract: This paper investigates the Jolting Technologies Hypothesis, which posits
superexponential growth (increasing acceleration, or a positive third
derivative) in the development of AI capabilities. We develop a theoretical
framework and validate detection methodologies through Monte Carlo simulations,
while acknowledging that empirical validation awaits suitable longitudinal
data. Our analysis focuses on creating robust tools for future empirical
studies and exploring the potential implications should the hypothesis prove
valid. The study examines how factors such as shrinking idea-to-action
intervals and compounding iterative AI improvements drive this jolting pattern.
By formalizing jolt dynamics and validating detection methods through
simulation, this work provides the mathematical foundation necessary for
understanding potential AI trajectories and their consequences for AGI
emergence, offering insights for research and policy.

</details>


### [4] [Comparing Dialectical Systems: Contradiction and Counterexample in Belief Change (Extended Version)](https://arxiv.org/abs/2507.06798)
*Uri Andrews,Luca San Mauro*

Main category: cs.AI

TL;DR: This paper analyzes dialectical systems used for modeling belief updates in agents and proves that q-dialectical systems are more powerful than p-dialectical systems.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance the understanding of how agents, mathematicians, or communities refine beliefs for truth and consistency, and evaluates the strengths of dialectical systems in automated belief revision.

Method: The authors compare three types of dialectical systems—d-, p-, and q-dialectical—focusing on belief revision mechanisms based on contradictions and counterexamples. They prove the relative strengths through formal mathematical reasoning.

Result: The research resolves an open problem in showing that q-dialectical systems are strictly more powerful than p-dialectical systems, which in turn are strictly stronger than d-dialectical systems.

Conclusion: Q-dialectical systems offer a superior approach to belief revision by utilizing both counterexamples and contradictions, advancing the application in automated reasoning and knowledge refinement processes.

Abstract: Dialectical systems are a mathematical formalism for modeling an agent
updating a knowledge base seeking consistency. Introduced in the 1970s by
Roberto Magari, they were originally conceived to capture how a working
mathematician or a research community refines beliefs in the pursuit of truth.
Dialectical systems also serve as natural models for the belief change of an
automated agent, offering a unifying, computable framework for dynamic belief
management.
  The literature distinguishes three main models of dialectical systems:
(d-)dialectical systems based on revising beliefs when they are seen to be
inconsistent, p-dialectical systems based on revising beliefs based on finding
a counterexample, and q-dialectical systems which can do both. We answer an
open problem in the literature by proving that q-dialectical systems are
strictly more powerful than p-dialectical systems, which are themselves known
to be strictly stronger than (d-)dialectical systems. This result highlights
the complementary roles of counterexample and contradiction in automated belief
revision, and thus also in the reasoning processes of mathematicians and
research communities.

</details>


### [5] [SCC-recursiveness in infinite argumentation (extended version)](https://arxiv.org/abs/2507.06852)
*Uri Andrews,Luca San Mauro*

Main category: cs.AI

TL;DR: The paper addresses the challenge of extending SCC-recursive semantics used in argumentation frameworks for finite settings to infinite argumentation frameworks and evaluates their behavior.


<details>
  <summary>Details</summary>
Motivation: Existing SCC-recursive semantics are effective for finite argumentation frameworks but fail to generalize to infinite ones due to well-foundedness issues.

Method: The paper proposes two approaches to extend SCC-recursiveness to infinite frameworks, systematically evaluating these using established criteria like directionality.

Result: The evaluations show that directionality fails in general within infinite frameworks, but finitary frameworks exhibit some compliance.

Conclusion: These findings contribute to theories of infinite argumentation and pave the way for reasoning systems capable of handling evolving or unbounded domains.

Abstract: Argumentation frameworks (AFs) are a foundational tool in artificial
intelligence for modeling structured reasoning and conflict. SCC-recursiveness
is a well-known design principle in which the evaluation of arguments is
decomposed according to the strongly connected components (SCCs) of the attack
graph, proceeding recursively from "higher" to "lower" components. While
SCC-recursive semantics such as \cft and \stgt have proven effective for finite
AFs, Baumann and Spanring showed the failure of SCC-recursive semantics to
generalize reliably to infinite AFs due to issues with well-foundedness.
  We propose two approaches to extending SCC-recursiveness to the infinite
setting. We systematically evaluate these semantics using Baroni and Giacomin's
established criteria, showing in particular that directionality fails in
general. We then examine these semantics' behavior in finitary frameworks,
where we find some of our semantics satisfy directionality. These results
advance the theory of infinite argumentation and lay the groundwork for
reasoning systems capable of handling unbounded or evolving domains.

</details>


### [6] [Scaling Towards the Information Boundary of Instruction Set: InfinityInstruct-Subject Technical Report](https://arxiv.org/abs/2507.06968)
*Li Du,Hanyu Zhao,Yiming Ju,Tengfei Pan*

Main category: cs.AI

TL;DR: The paper introduces a framework for systematically constructing high-quality instruction datasets to enhance the abilities of large-scale pretrained models in following complex instructions and covering diverse tasks. The framework also enables iterative quality improvement of these datasets.


<details>
  <summary>Details</summary>
Motivation: Existing instruction datasets often lack sufficient coverage of task types and knowledge areas, as well as instruction complexity, limiting the performance of models fine-tuned on these datasets.

Method: The paper proposes an iterative closed-loop framework with components like a hierarchical labeling system, a seed selection algorithm, evolutionary data synthesis, and targeted data generation informed by model deficiency diagnosis.

Result: The framework was used to create the InfinityInstruct-Subject dataset containing ~1.5 million instructions. Experiments showed its effectiveness in improving instruction-following capabilities, with expanded coverage and depth compared to other datasets.

Conclusion: The study establishes both theoretical and practical groundwork for evolving instruction datasets from quantity-driven expansion to qualitative enhancement, thereby improving their utility for large-scale pretrained models.

Abstract: Instruction tuning has become a foundation for unlocking the capabilities of
large-scale pretrained models and improving their performance on complex tasks.
Thus, the construction of high-quality instruction datasets is crucial for
enhancing model performance and generalizability. Although current instruction
datasets have reached tens of millions of samples, models finetuned on them may
still struggle with complex instruction following and tasks in rare domains.
This is primarily due to limited expansion in both ``coverage'' (coverage of
task types and knowledge areas) and ``depth'' (instruction complexity) of the
instruction set. To address this issue, we propose a systematic instruction
data construction framework, which integrates a hierarchical labeling system,
an informative seed selection algorithm, an evolutionary data synthesis
process, and a model deficiency diagnosis with targeted data generation. These
components form an iterative closed-loop to continuously enhance the coverage
and depth of instruction data. Based on this framework, we construct
InfinityInstruct-Subject, a high-quality dataset containing ~1.5 million
instructions. Experiments on multiple foundation models and benchmark tasks
demonstrate its effectiveness in improving instruction-following capabilities.
Further analyses suggest that InfinityInstruct-Subject shows enlarged coverage
and depth compared to comparable synthesized instruction datasets. Our work
lays a theoretical and practical foundation for the efficient, continuous
evolution of instruction datasets, moving from data quantity expansion to
qualitative improvement.

</details>


### [7] [The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced Planning, Navigation, and Dynamic Adaptation](https://arxiv.org/abs/2507.06993)
*Jieren Deng,Aleksandar Cvetkovic,Pak Kiu Chung,Dragomir Yankov,Chiqun Zhang*

Main category: cs.AI

TL;DR: The paper presents a novel travel system with three agents improving trip planning, fine navigation, and dynamic adaptation.


<details>
  <summary>Details</summary>
Motivation: Address gaps in traditional travel systems: trip planning, last-100-meter navigation, and adaptability to disruptions.

Method: Proposed three cooperative agents: Travel Planning Agent, Destination Assistant Agent, and Local Discovery Agent.

Result: Substantial improvements in query interpretation, navigation accuracy, and resilience to itinerary disruptions.

Conclusion: The system shows potential for enhancing urban exploration and emergency response scenarios.

Abstract: Traditional travel-planning systems are often static and fragmented, leaving
them ill-equipped to handle real-world complexities such as evolving
environmental conditions and unexpected itinerary disruptions. In this paper,
we identify three gaps between existing service providers causing frustrating
user experience: intelligent trip planning, precision "last-100-meter"
navigation, and dynamic itinerary adaptation. We propose three cooperative
agents: a Travel Planning Agent that employs grid-based spatial grounding and
map analysis to help resolve complex multi-modal user queries; a Destination
Assistant Agent that provides fine-grained guidance for the final navigation
leg of each journey; and a Local Discovery Agent that leverages image
embeddings and Retrieval-Augmented Generation (RAG) to detect and respond to
trip plan disruptions. With evaluations and experiments, our system
demonstrates substantial improvements in query interpretation, navigation
accuracy, and disruption resilience, underscoring its promise for applications
from urban exploration to emergency response.

</details>


### [8] [First Return, Entropy-Eliciting Explore](https://arxiv.org/abs/2507.07017)
*Tianyu Zheng,Tianshun Xing,Qingshui Gu,Taoran Liang,Xingwei Qu,Xin Zhou,Yizhi Li,Zhoufutu Wen,Chenghua Lin,Wenhao Huang,Qian Liu,Ge Zhang,Zejun Ma*

Main category: cs.AI

TL;DR: The paper proposes FR3E, a framework to tackle unstable exploration in reinforcement learning for LLMs, leading to better reasoning and performance.


<details>
  <summary>Details</summary>
Motivation: Reinforcement Learning from Verifiable Rewards (RLVR) aids LLM reasoning, but its exploration process is unstable during training.

Method: FR3E identifies uncertain decision points in LLM reasoning and performs targeted rollouts, creating intermediate feedback to guide learning without needing extensive supervision.

Result: Empirical tests on the AIME24 mathematical reasoning benchmark showed that FR3E enhances stable training, enables coherent and lengthy responses, and boosts the number of correct reasoning trajectories.

Conclusion: The FR3E framework offers a structured and effective exploration mechanism that strengthens reasoning capabilities in LLMs, overcoming exploration instability.

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning
abilities of Large Language Models (LLMs) but it struggles with unstable
exploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a
structured exploration framework that identifies high-uncertainty decision
points in reasoning trajectories and performs targeted rollouts to construct
semantically grounded intermediate feedback. Our method provides targeted
guidance without relying on dense supervision. Empirical results on
mathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable
training, produces longer and more coherent responses, and increases the
proportion of fully correct trajectories. These results highlight the
framework's effectiveness in improving LLM reasoning through more robust and
structured exploration.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [SLDB: An End-To-End Heterogeneous System-on-Chip Benchmark Suite for LLM-Aided Design](https://arxiv.org/abs/2507.06376)
*Elisavet Lydia Alvanaki,Kevin Lee,Luca P. Carloni*

Main category: cs.AR

TL;DR: This paper introduces SLDB, a benchmark for evaluating Large Language Models (LLMs) in system-level design tasks in Electronic Design Automation (EDA).


<details>
  <summary>Details</summary>
Motivation: Existing EDA benchmarks for LLMs focus on component-level designs and lack support for system-level design evaluation, limiting LLM advancements in hardware design.

Method: The authors created a System-Level Design Benchmark (SLDB) featuring 10 baseline SoC designs and adaptable system configurations using a synthetic library, compatible with the ESP platform.

Result: SLDB supports full SoC configurations, accelerator integration, communication setups, and performance evaluation at the system-level, enabling advanced testing in EDA.

Conclusion: SLDB addresses the gap in system-level design benchmarks, enhancing the evaluation and integration of LLMs into system-level hardware design tasks.

Abstract: Over the last few years, Large Language Models (LLMs) have emerged as a
valuable tool for Electronic Design Automation (EDA). State-of-the-art research
in LLM-aided design has demonstrated the ability of LLMs to generate
syntactically correct RTL code, showcasing encouraging prospects for
integrating AI into the hardware design process. A key enabler of these
advancements is the availability of high-quality benchmarks to evaluate new
approaches. However, existing datasets and benchmarks fall short of
system-level design, as they focus primarily on component-level information and
low-complexity designs. To address this gap, we introduce the System-Level
Design Benchmark (SLDB), a dataset tailored for evaluating LLMs in system-level
integration and configuration tasks. SLDB includes a curated benchmark suite of
10 baseline SoC designs, whose components can be combined into an exponential
number of distinct tile-based SoCs through a synthetic library. The dataset
provides full SoC configurations, accelerator integration code, communication
parameters, and accelerator-aware system configurations, along with
testing-application code, compatible with the ESP platform[1].

</details>


### [10] [Towards LLM-based Root Cause Analysis of Hardware Design Failures](https://arxiv.org/abs/2507.06512)
*Siyu Qiu,Muzhi Wang,Raheel Afsharmazayejani,Mohammad Moradi Shahmiri,Benjamin Tan,Hammond Pearce*

Main category: cs.AR

TL;DR: Researchers investigated the use of Large Language Models (LLMs) to assist in identifying and explaining causes of issues in hardware design processes, achieving highly promising results.


<details>
  <summary>Details</summary>
Motivation: The study aims to leverage rapid advances in LLMs to aid the hardware design and security processes by explaining root causes of bugs and design issues during synthesis/simulation.

Method: The researchers tested multiple LLMs, including OpenAI's o3-mini reasoning model, on 34 buggy hardware scenarios using pass@5 scoring and retrieval-augmented generation approaches.

Result: OpenAI's o3-mini model achieved 100% correctness under pass@5 scoring, while other models generally scored over 80% and above 90% with retrieval-augmented generation.

Conclusion: LLMs show strong potential in supporting hardware design debugging and security analysis tasks, paving the way for their adoption in the field.

Abstract: With advances in large language models (LLMs), new opportunities have emerged
to develop tools that support the digital hardware design process. In this
work, we explore how LLMs can assist with explaining the root cause of design
issues and bugs that are revealed during synthesis and simulation, a necessary
milestone on the pathway towards widespread use of LLMs in the hardware design
process and for hardware security analysis. We find promising results: for our
corpus of 34 different buggy scenarios, OpenAI's o3-mini reasoning model
reached a correct determination 100% of the time under pass@5 scoring, with
other state of the art models and configurations usually achieving more than
80% performance and more than 90% when assisted with retrieval-augmented
generation.

</details>


### [11] [Opto-ViT: Architecting a Near-Sensor Region of Interest-Aware Vision Transformer Accelerator with Silicon Photonics](https://arxiv.org/abs/2507.07044)
*Mehrdad Morsali,Chengwei Zhou,Deniz Najafi,Sreetama Sarkar,Pietro Mercati,Navid Khoshavi,Peter Beerel,Mahdi Nikdast,Gourav Datta,Shaahin Angizi*

Main category: cs.AR

TL;DR: The paper proposes OptoViT, a Vision Transformer (ViT) accelerator that leverages silicon photonics for energy-efficient and real-time vision tasks.


<details>
  <summary>Details</summary>
Motivation: ViTs are powerful for computer vision tasks but require substantial compute and memory resources, making them unsuitable for energy and bandwidth-constrained scenarios.

Method: OptoViT uses a hybrid electronic-photonic architecture with an optical core for compute-intensive tasks, complemented by a lightweight Mask Generation Network (MGNet) to prune irrelevant patches. It also employs quantization-aware training and matrix decomposition tailored for photonic constraints.

Result: OptoViT achieves 100.4 KFPS/W with up to 84% energy savings and less than 1.6% accuracy loss, showcasing its efficiency and scalability for various tasks.

Conclusion: The OptoViT accelerator demonstrates the feasibility of leveraging silicon photonics for real-time, energy-efficient ViT deployment, making it a promising solution for edge-based applications.

Abstract: Vision Transformers (ViTs) have emerged as a powerful architecture for
computer vision tasks due to their ability to model long-range dependencies and
global contextual relationships. However, their substantial compute and memory
demands hinder efficient deployment in scenarios with strict energy and
bandwidth limitations. In this work, we propose OptoViT, the first near-sensor,
region-aware ViT accelerator leveraging silicon photonics (SiPh) for real-time
and energy-efficient vision processing. Opto-ViT features a hybrid
electronic-photonic architecture, where the optical core handles
compute-intensive matrix multiplications using Vertical-Cavity Surface-Emitting
Lasers (VCSELs) and Microring Resonators (MRs), while nonlinear functions and
normalization are executed electronically. To reduce redundant computation and
patch processing, we introduce a lightweight Mask Generation Network (MGNet)
that identifies regions of interest in the current frame and prunes irrelevant
patches before ViT encoding. We further co-optimize the ViT backbone using
quantization-aware training and matrix decomposition tailored for photonic
constraints. Experiments across device fabrication, circuit and architecture
co-design, to classification, detection, and video tasks demonstrate that
OptoViT achieves 100.4 KFPS/W with up to 84% energy savings with less than 1.6%
accuracy loss, while enabling scalable and efficient ViT deployment at the
edge.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities](https://arxiv.org/abs/2507.06261)
*Gheorghe Comanici,Eric Bieber,Mike Schaekermann,Ice Pasupat,Noveen Sachdeva,Inderjit Dhillon,Marcel Blistein,Ori Ram,Dan Zhang,Evan Rosen,Luke Marris,Sam Petulla,Colin Gaffney,Asaf Aharoni,Nathan Lintz,Tiago Cardal Pais,Henrik Jacobsson,Idan Szpektor,Nan-Jiang Jiang,Krishna Haridasan,Ahmed Omran,Nikunj Saunshi,Dara Bahri,Gaurav Mishra,Eric Chu,Toby Boyd,Brad Hekman,Aaron Parisi,Chaoyi Zhang,Kornraphop Kawintiranon,Tania Bedrax-Weiss,Oliver Wang,Ya Xu,Ollie Purkiss,Uri Mendlovic,Ilaï Deutel,Nam Nguyen,Adam Langley,Flip Korn,Lucia Rossazza,Alexandre Ramé,Sagar Waghmare,Helen Miller,Vaishakh Keshava,Ying Jian,Xiaofan Zhang,Raluca Ada Popa,Kedar Dhamdhere,Blaž Bratanič,Kyuyeun Kim,Terry Koo,Ferran Alet,Yi-ting Chen,Arsha Nagrani,Hannah Muckenhirn,Zhiyuan Zhang,Corbin Quick,Filip Pavetić,Duc Dung Nguyen,Joao Carreira,Michael Elabd,Haroon Qureshi,Fabian Mentzer,Yao-Yuan Yang,Danielle Eisenbud,Anmol Gulati,Ellie Talius,Eric Ni,Sahra Ghalebikesabi,Edouard Yvinec,Alaa Saade,Thatcher Ulrich,Lorenzo Blanco,Dan A. Calian,Muhuan Huang,Aäron van den Oord,Naman Goyal,Terry Chen,Praynaa Rawlani,Christian Schallhart,Swachhand Lokhande,Xianghong Luo,Jyn Shan,Ceslee Montgomery,Victoria Krakovna,Federico Piccinini,Omer Barak,Jingyu Cui,Yiling Jia,Mikhail Dektiarev,Alexey Kolganov,Shiyu Huang,Zhe Chen,Xingyu Wang,Jessica Austin,Peter de Boursac,Evgeny Sluzhaev,Frank Ding,Huijian Li,Surya Bhupatiraju,Mohit Agarwal,Sławek Kwasiborski,Paramjit Sandhu,Patrick Siegler,Ahmet Iscen,Eyal Ben-David,Shiraz Butt,Miltos Allamanis,Seth Benjamin,Robert Busa-Fekete,Felix Hernandez-Campos,Sasha Goldshtein,Matt Dibb,Weiyang Zhang,Annie Marsden,Carey Radebaugh,Stephen Roller,Abhishek Nayyar,Jacob Austin,Tayfun Terzi,Bhargav Kanagal Shamanna,Pete Shaw,Aayush Singh,Florian Luisier,Artur Mendonça,Vaibhav Aggarwal,Larisa Markeeva,Claudio Fantacci,Sergey Brin,HyunJeong Choe,Guanyu Wang,Hartwig Adam,Avigail Dabush,Tatsuya Kiyono,Eyal Marcus,Jeremy Cole,Theophane Weber,Hongrae Lee,Ronny Huang,Alex Muzio,Leandro Kieliger,Maigo Le,Courtney Biles,Long Le,Archit Sharma,Chengrun Yang,Avery Lamp,Dave Dopson,Nate Hurley,Katrina,Xu,Zhihao Shan,Shuang Song,Jiewen Tan,Alexandre Senges,George Zhang,Chong You,Yennie Jun,David Raposo,Susanna Ricco,Xuan Yang,Weijie Chen,Prakhar Gupta,Arthur Szlam,Kevin Villela,Chun-Sung Ferng,Daniel Kasenberg,Chen Liang,Rui Zhu,Arunachalam Narayanaswamy,Florence Perot,Paul Pucciarelli,Anna Shekhawat,Alexey Stern,Rishikesh Ingale,Stefani Karp,Sanaz Bahargam,Adrian Goedeckemeyer,Jie Han,Sicheng Li,Andrea Tacchetti,Dian Yu,Abhishek Chakladar,Zhiying Zhang,Mona El Mahdy,Xu Gao,Dale Johnson,Samrat Phatale,AJ Piergiovanni,Hyeontaek Lim,Clement Farabet,Carl Lebsack,Theo Guidroz,John Blitzer,Nico Duduta,David Madras,Steve Li,Daniel von Dincklage,Xin Li,Mahdis Mahdieh,George Tucker,Ganesh Jawahar,Owen Xiao,Danny Tarlow,Robert Geirhos,Noam Velan,Daniel Vlasic,Kalesha Bullard,SK Park,Nishesh Gupta,Kellie Webster,Ayal Hitron,Jieming Mao,Julian Eisenschlos,Laurel Prince,Nina D'Souza,Kelvin Zheng,Sara Nasso,Gabriela Botea,Carl Doersch,Caglar Unlu,Chris Alberti,Alexey Svyatkovskiy,Ankita Goel,Krzysztof Choromanski,Pan-Pan Jiang,Richard Nguyen,Four Flynn,Daria Ćurko,Peter Chen,Nicholas Roth,Kieran Milan,Caleb Habtegebriel,Shashi Narayan,Michael Moffitt,Jake Marcus,Thomas Anthony,Brendan McMahan,Gowoon Cheon,Ruibo Liu,Megan Barnes,Lukasz Lew,Rebeca Santamaria-Fernandez,Mayank Upadhyay,Arjun Akula,Arnar Mar Hrafnkelsson,Alvaro Caceres,Andrew Bunner,Michal Sokolik,Subha Puttagunta,Lawrence Moore,Berivan Isik,Weilun Chen,Jay Hartford,Lawrence Chan,Pradeep Shenoy,Dan Holtmann-Rice,Jane Park,Fabio Viola,Alex Salcianu,Sujeevan Rajayogam,Ian Stewart-Binks,Zelin Wu,Richard Everett,Xi Xiong,Pierre-Antoine Manzagol,Gary Leung,Carl Saroufim,Bo Pang,Dawid Wegner,George Papamakarios,Jennimaria Palomaki,Helena Pankov,Guangda Lai,Guilherme Tubone,Shubin Zhao,Theofilos Strinopoulos,Seth Neel,Mingqiu Wang,Joe Kelley,Li Li,Pingmei Xu,Anitha Vijayakumar,Andrea D'olimpio,Omer Levy,Massimo Nicosia,Grigory Rozhdestvenskiy,Ni Lao,Sirui Xie,Yash Katariya,Jon Simon,Sanjiv Kumar,Florian Hartmann,Michael Kilgore,Jinhyuk Lee,Aroma Mahendru,Roman Ring,Tom Hennigan,Fiona Lang,Colin Cherry,David Steiner,Dawsen Hwang,Ray Smith,Pidong Wang,Jeremy Chen,Ming-Hsuan Yang,Sam Kwei,Philippe Schlattner,Donnie Kim,Ganesh Poomal Girirajan,Nikola Momchev,Ayushi Agarwal,Xingyi Zhou,Ilkin Safarli,Zachary Garrett,AJ Pierigiovanni,Sarthak Jauhari,Alif Raditya Rochman,Shikhar Vashishth,Quan Yuan,Christof Angermueller,Jon Blanton,Xinying Song,Nitesh Bharadwaj Gundavarapu,Thi Avrahami,Maxine Deines,Subhrajit Roy,Manish Gupta,Christopher Semturs,Shobha Vasudevan,Aditya Srikanth Veerubhotla,Shriya Sharma,Josh Jacob,Zhen Yang,Andreas Terzis,Dan Karliner,Auriel Wright,Tania Rojas-Esponda,Ashley Brown,Abhijit Guha Roy,Pawan Dogra,Andrei Kapishnikov,Peter Young,Wendy Kan,Vinodh Kumar Rajendran,Maria Ivanova,Salil Deshmukh,Chia-Hua Ho,Mike Kwong,Stav Ginzburg,Annie Louis,KP Sawhney,Slav Petrov,Jing Xie,Yunfei Bai,Georgi Stoyanov,Alex Fabrikant,Rajesh Jayaram,Yuqi Li,Joe Heyward,Justin Gilmer,Yaqing Wang,Radu Soricut,Luyang Liu,Qingnan Duan,Jamie Hayes,Maura O'Brien,Gaurav Singh Tomar,Sivan Eiger,Bahar Fatemi,Jeffrey Hui,Catarina Barros,Adaeze Chukwuka,Alena Butryna,Saksham Thakur,Austin Huang,Zhufeng Pan,Haotian Tang,Serkan Cabi,Tulsee Doshi,Michiel Bakker,Sumit Bagri,Ruy Ley-Wild,Adam Lelkes,Jennie Lees,Patrick Kane,David Greene,Shimu Wu,Jörg Bornschein,Gabriela Surita,Sarah Hodkinson,Fangtao Li,Chris Hidey,Sébastien Pereira,Sean Ammirati,Phillip Lippe,Adam Kraft,Pu Han,Sebastian Gerlach,Zifeng Wang,Liviu Panait,Feng Han,Brian Farris,Yingying Bi,Hannah DeBalsi,Miaosen Wang,Gladys Tyen,James Cohan,Susan Zhang,Jarred Barber,Da-Woon Chung,Jaeyoun Kim,Markus Kunesch,Steven Pecht,Nami Akazawa,Abe Friesen,James Lyon,Ali Eslami,Junru Wu,Jie Tan,Yue Song,Ravi Kumar,Chris Welty,Ilia Akolzin,Gena Gibson,Sean Augenstein,Arjun Pillai,Nancy Yuen,Du Phan,Xin Wang,Iain Barr,Heiga Zen,Nan Hua,Casper Liu,Jilei,Wang,Tanuj Bhatia,Hao Xu,Oded Elyada,Pushmeet Kohli,Mirek Olšák,Ke Chen,Azalia Mirhoseini,Noam Shazeer,Shoshana Jakobovits,Maggie Tran,Nolan Ramsden,Tarun Bharti,Fred Alcober,Yunjie Li,Shilpa Shetty,Jing Chen,Dmitry Kalashnikov,Megha Nawhal,Sercan Arik,Hanwen Chen,Michiel Blokzijl,Shubham Gupta,James Rubin,Rigel Swavely,Sophie Bridgers,Ian Gemp,Chen Su,Arun Suggala,Juliette Pluto,Mary Cassin,Alain Vaucher,Kaiyang Ji,Jiahao Cai,Andrew Audibert,Animesh Sinha,David Tian,Efrat Farkash,Amy Hua,Jilin Chen,Duc-Hieu Tran,Edward Loper,Nicole Brichtova,Lara McConnaughey,Ballie Sandhu,Robert Leland,Doug DeCarlo,Andrew Over,James Huang,Xing Wu,Connie Fan,Eric Li,Yun Lei,Deepak Sharma,Cosmin Paduraru,Luo Yu,Matko Bošnjak,Phuong Dao,Min Choi,Sneha Kudugunta,Jakub Adamek,Carlos Guía,Ali Khodaei,Jie Feng,Wenjun Zeng,David Welling,Sandeep Tata,Christina Butterfield,Andrey Vlasov,Seliem El-Sayed,Swaroop Mishra,Tara Sainath,Shentao Yang,RJ Skerry-Ryan,Jeremy Shar,Robert Berry,Arunkumar Rajendran,Arun Kandoor,Andrea Burns,Deepali Jain,Tom Stone,Wonpyo Park,Shibo Wang,Albin Cassirer,Guohui Wang,Hayato Kobayashi,Sergey Rogulenko,Vineetha Govindaraj,Mikołaj Rybiński,Nadav Olmert,Colin Evans,Po-Sen Huang,Kelvin Xu,Premal Shah,Terry Thurk,Caitlin Sikora,Mu Cai,Jin Xie,Elahe Dabir,Saloni Shah,Norbert Kalb,Carrie Zhang,Shruthi Prabhakara,Amit Sabne,Artiom Myaskovsky,Vikas Raunak,Blanca Huergo,Behnam Neyshabur,Jon Clark,Ye Zhang,Shankar Krishnan,Eden Cohen,Dinesh Tewari,James Lottes,Yumeya Yamamori,Hui,Li,Mohamed Elhawaty,Ada Maksutaj Oflazer,Adrià Recasens,Sheryl Luo,Duy Nguyen,Taylor Bos,Kalyan Andra,Ana Salazar,Ed Chi,Jeongwoo Ko,Matt Ginsberg,Anders Andreassen,Anian Ruoss,Todor Davchev,Elnaz Davoodi,Chenxi Liu,Min Kim,Santiago Ontanon,Chi Ming To,Dawei Jia,Rosemary Ke,Jing Wang,Anna Korsun,Moran Ambar,Ilya Kornakov,Irene Giannoumis,Toni Creswell,Denny Zhou,Yi Su,Ishaan Watts,Aleksandr Zaks,Evgenii Eltyshev,Ziqiang Feng,Sidharth Mudgal,Alex Kaskasoli,Juliette Love,Kingshuk Dasgupta,Sam Shleifer,Richard Green,Sungyong Seo,Chansoo Lee,Dale Webster,Prakash Shroff,Ganna Raboshchuk,Isabel Leal,James Manyika,Sofia Erell,Daniel Murphy,Zhisheng Xiao,Anton Bulyenov,Julian Walker,Mark Collier,Matej Kastelic,Nelson George,Sushant Prakash,Sailesh Sidhwani,Alexey Frolov,Steven Hansen,Petko Georgiev,Tiberiu Sosea,Chris Apps,Aishwarya Kamath,David Reid,Emma Cooney,Charlotte Magister,Oriana Riva,Alec Go,Pu-Chin Chen,Sebastian Krause,Nir Levine,Marco Fornoni,Ilya Figotin,Nick Roy,Parsa Mahmoudieh,Vladimir Magay,Mukundan Madhavan,Jin Miao,Jianmo Ni,Yasuhisa Fujii,Ian Chou,George Scrivener,Zak Tsai,Siobhan Mcloughlin,Jeremy Selier,Sandra Lefdal,Jeffrey Zhao,Abhijit Karmarkar,Kushal Chauhan,Shivanker Goel,Zhaoyi Zhang,Vihan Jain,Parisa Haghani,Mostafa Dehghani,Jacob Scott,Erin Farnese,Anastasija Ilić,Steven Baker,Julia Pawar,Li Zhong,Josh Camp,Yoel Zeldes,Shravya Shetty,Anand Iyer,Vít Listík,Jiaxian Guo,Luming Tang,Mark Geller,Simon Bucher,Yifan Ding,Hongzhi Shi,Carrie Muir,Dominik Grewe,Ramy Eskander,Octavio Ponce,Boqing Gong,Derek Gasaway,Samira Khan,Umang Gupta,Angelos Filos,Weicheng Kuo,Klemen Kloboves,Jennifer Beattie,Christian Wright,Leon Li,Alicia Jin,Sandeep Mariserla,Miteyan Patel,Jens Heitkaemper,Dilip Krishnan,Vivek Sharma,David Bieber,Christian Frank,John Lambert,Paul Caron,Martin Polacek,Mai Giménez,Himadri Choudhury,Xing Yu,Sasan Tavakkol,Arun Ahuja,Franz Och,Rodolphe Jenatton,Wojtek Skut,Bryan Richter,David Gaddy,Andy Ly,Misha Bilenko,Megh Umekar,Ethan Liang,Martin Sevenich,Mandar Joshi,Hassan Mansoor,Rebecca Lin,Sumit Sanghai,Abhimanyu Singh,Xiaowei Li,Sudheendra Vijayanarasimhan,Zaheer Abbas,Yonatan Bitton,Hansa Srinivasan,Manish Reddy Vuyyuru,Alexander Frömmgen,Yanhua Sun,Ralph Leith,Alfonso Castaño,DJ Strouse,Le Yan,Austin Kyker,Satish Kambala,Mary Jasarevic,Thibault Sellam,Chao Jia,Alexander Pritzel,Raghavender R,Huizhong Chen,Natalie Clay,Sudeep Gandhe,Sean Kirmani,Sayna Ebrahimi,Hannah Kirkwood,Jonathan Mallinson,Chao Wang,Adnan Ozturel,Kuo Lin,Shyam Upadhyay,Vincent Cohen-Addad,Sean Purser-haskell,Yichong Xu,Ebrahim Songhori,Babi Seal,Alberto Magni,Almog Gueta,Tingting Zou,Guru Guruganesh,Thais Kagohara,Hung Nguyen,Khalid Salama,Alejandro Cruzado Ruiz,Justin Frye,Zhenkai Zhu,Matthias Lochbrunner,Simon Osindero,Wentao Yuan,Lisa Lee,Aman Prasad,Lam Nguyen Thiet,Daniele Calandriello,Victor Stone,Qixuan Feng,Han Ke,Maria Voitovich,Geta Sampemane,Lewis Chiang,Ling Wu,Alexander Bykovsky,Matt Young,Luke Vilnis,Ishita Dasgupta,Aditya Chawla,Qin Cao,Bowen Liang,Daniel Toyama,Szabolcs Payrits,Anca Stefanoiu,Dimitrios Vytiniotis,Ankesh Anand,Tianxiao Shen,Blagoj Mitrevski,Michael Tschannen,Sreenivas Gollapudi,Aishwarya P S,José Leal,Zhe Shen,Han Fu,Wei Wang,Arvind Kannan,Doron Kukliansky,Sergey Yaroshenko,Svetlana Grant,Umesh Telang,David Wood,Alexandra Chronopoulou,Alexandru Ţifrea,Tao Zhou,Tony,Nguy\~ên,Muge Ersoy,Anima Singh,Meiyan Xie,Emanuel Taropa,Woohyun Han,Eirikur Agustsson,Andrei Sozanschi,Hui Peng,Alex Chen,Yoel Drori,Efren Robles,Yang Gao,Xerxes Dotiwalla,Ying Chen,Anudhyan Boral,Alexei Bendebury,John Nham,Chris Tar,Luis Castro,Jiepu Jiang,Canoee Liu,Felix Halim,Jinoo Baek,Andy Wan,Jeremiah Liu,Yuan Cao,Shengyang Dai,Trilok Acharya,Ruoxi Sun,Fuzhao Xue,Saket Joshi,Morgane Lustman,Yongqin Xian,Rishabh Joshi,Deep Karkhanis,Nora Kassner,Jamie Hall,Xiangzhuo Ding,Gan Song,Gang Li,Chen Zhu,Yana Kulizhskaya,Bin Ni,Alexey Vlaskin,Solomon Demmessie,Lucio Dery,Salah Zaiem,Yanping Huang,Cindy Fan,Felix Gimeno,Ananth Balashankar,Koji Kojima,Hagai Taitelbaum,Maya Meng,Dero Gharibian,Sahil Singla,Wei Chen,Ambrose Slone,Guanjie Chen,Sujee Rajayogam,Max Schumacher,Suyog Kotecha,Rory Blevins,Qifei Wang,Mor Hazan Taege,Alex Morris,Xin Liu,Fayaz Jamil,Richard Zhang,Pratik Joshi,Ben Ingram,Tyler Liechty,Ahmed Eleryan,Scott Baird,Alex Grills,Gagan Bansal,Shan Han,Kiran Yalasangi,Shawn Xu,Majd Al Merey,Isabel Gao,Felix Weissenberger,Igor Karpov,Robert Riachi,Ankit Anand,Gautam Prasad,Kay Lamerigts,Reid Hayes,Jamie Rogers,Mandy Guo,Ashish Shenoy,Qiong,Hu,Kyle He,Yuchen Liu,Polina Zablotskaia,Sagar Gubbi,Yifan Chang,Jay Pavagadhi,Kristian Kjems,Archita Vadali,Diego Machado,Yeqing Li,Renshen Wang,Dipankar Ghosh,Aahil Mehta,Dana Alon,George Polovets,Alessio Tonioni,Nate Kushman,Joel D'sa,Lin Zhuo,Allen Wu,Rohin Shah,John Youssef,Jiayu Ye,Justin Snyder,Karel Lenc,Senaka Buthpitiya,Matthew Tung,Jichuan Chang,Tao Chen,David Saxton,Jenny Lee,Lydia Lihui Zhang,James Qin,Prabakar Radhakrishnan,Maxwell Chen,Piotr Ambroszczyk,Metin Toksoz-Exley,Yan Zhong,Nitzan Katz,Brendan O'Donoghue,Tamara von Glehn,Adi Gerzi Rosenthal,Aga Świetlik,Xiaokai Zhao,Nick Fernando,Jinliang Wei,Jieru Mei,Sergei Vassilvitskii,Diego Cedillo,Pranjal Awasthi,Hui Zheng,Koray Kavukcuoglu,Itay Laish,Joseph Pagadora,Marc Brockschmidt,Christopher A. Choquette-Choo,Arunkumar Byravan,Yifeng Lu,Xu Chen,Mia Chen,Kenton Lee,Rama Pasumarthi,Sijal Bhatnagar,Aditya Shah,Qiyin Wu,Zhuoyuan Chen,Zack Nado,Bartek Perz,Zixuan Jiang,David Kao,Ganesh Mallya,Nino Vieillard,Lantao Mei,Sertan Girgin,Mandy Jordan,Yeongil Ko,Alekh Agarwal,Yaxin Liu,Yasemin Altun,Raoul de Liedekerke,Anastasios Kementsietsidis,Daiyi Peng,Dangyi Liu,Utku Evci,Peter Humphreys,Austin Tarango,Xiang Deng,Yoad Lewenberg,Kevin Aydin,Chengda Wu,Bhavishya Mittal,Tsendsuren Munkhdalai,Kleopatra Chatziprimou,Rodrigo Benenson,Uri First,Xiao Ma,Jinning Li,Armand Joulin,Hamish Tomlinson,Tingnan Zhang,Milad Nasr,Zhi Hong,Michaël Sander,Lisa Anne Hendricks,Anuj Sharma,Andrew Bolt,Eszter Vértes,Jiri Simsa,Tomer Levinboim,Olcan Sercinoglu,Divyansh Shukla,Austin Wu,Craig Swanson,Danny Vainstein,Fan Bu,Bo Wang,Ryan Julian,Charles Yoon,Sergei Lebedev,Antonious Girgis,Bernd Bandemer,David Du,Todd Wang,Xi Chen,Ying Xiao,Peggy Lu,Natalie Ha,Vlad Ionescu,Simon Rowe,Josip Matak,Federico Lebron,Andreas Steiner,Lalit Jain,Manaal Faruqui,Nicolas Lacasse,Georgie Evans,Neesha Subramaniam,Dean Reich,Giulia Vezzani,Aditya Pandey,Joe Stanton,Tianhao Zhou,Liam McCafferty,Henry Griffiths,Verena Rieser,Soheil Hassas Yeganeh,Eleftheria Briakou,Lu Huang,Zichuan Wei,Liangchen Luo,Erik Jue,Gabby Wang,Victor Cotruta,Myriam Khan,Jongbin Park,Qiuchen Guo,Peiran Li,Rong Rong,Diego Antognini,Anastasia Petrushkina,Chetan Tekur,Eli Collins,Parul Bhatia,Chester Kwak,Wenhu Chen,Arvind Neelakantan,Immanuel Odisho,Sheng Peng,Vincent Nallatamby,Vaibhav Tulsyan,Fabian Pedregosa,Peng Xu,Raymond Lin,Yulong Wang,Emma Wang,Sholto Douglas,Reut Tsarfaty,Elena Gribovskaya,Renga Aravamudhan,Manu Agarwal,Mara Finkelstein,Qiao Zhang,Elizabeth Cole,Phil Crone,Sarmishta Velury,Anil Das,Chris Sauer,Luyao Xu,Danfeng Qin,Chenjie Gu,Dror Marcus,CJ Zheng,Wouter Van Gansbeke,Sobhan Miryoosefi,Haitian Sun,YaGuang Li,Charlie Chen,Jae Yoo,Pavel Dubov,Alex Tomala,Adams Yu,Paweł Wesołowski,Alok Gunjan,Eddie Cao,Jiaming Luo,Nikhil Sethi,Arkadiusz Socala,Laura Graesser,Tomas Kocisky,Arturo BC,Minmin Chen,Edward Lee,Sophie Wang,Weize Kong,Qiantong Xu,Nilesh Tripuraneni,Yiming Li,Xinxin Yu,Allen Porter,Paul Voigtlaender,Biao Zhang,Arpi Vezer,Sarah York,Qing Wei,Geoffrey Cideron,Mark Kurzeja,Seungyeon Kim,Benny Li,Angéline Pouget,Hyo Lee,Kaspar Daugaard,Yang Li,Dave Uthus,Aditya Siddhant,Paul Cavallaro,Sriram Ganapathy,Maulik Shah,Rolf Jagerman,Jeff Stanway,Piermaria Mendolicchio,Li Xiao,Kayi Lee,Tara Thompson,Shubham Milind Phal,Jason Chase,Sun Jae Lee,Adrian N Reyes,Disha Shrivastava,Zhen Qin,Roykrong Sukkerd,Seth Odoom,Lior Madmoni,John Aslanides,Jonathan Herzig,Elena Pochernina,Sheng Zhang,Parker Barnes,Daisuke Ikeda,Qiujia Li,Shuo-yiin Chang,Shakir Mohamed,Jim Sproch,Richard Powell,Bidisha Samanta,Domagoj Ćevid,Anton Kovsharov,Shrestha Basu Mallick,Srinivas Tadepalli,Anne Zheng,Kareem Ayoub,Andreas Noever,Christian Reisswig,Zhuo Xu,Junhyuk Oh,Martin Matysiak,Tim Blyth,Shereen Ashraf,Julien Amelot,Boone Severson,Michele Bevilacqua,Motoki Sano,Ethan Dyer,Ofir Roval,Anu Sinha,Yin Zhong,Sagi Perel,Tea Sabolić,Johannes Mauerer,Willi Gierke,Mauro Verzetti,Rodrigo Cabrera,Alvin Abdagic,Steven Hemingray,Austin Stone,Jong Lee,Farooq Ahmad,Karthik Raman,Lior Shani,Jonathan Lai,Orhan Firat,Nathan Waters,Eric Ge,Mo Shomrat,Himanshu Gupta,Rajeev Aggarwal,Tom Hudson,Bill Jia,Simon Baumgartner,Palak Jain,Joe Kovac,Junehyuk Jung,Ante Žužul,Will Truong,Morteza Zadimoghaddam,Songyou Peng,Marco Liang,Rachel Sterneck,Balaji Lakshminarayanan,Machel Reid,Oliver Woodman,Tong Zhou,Jianling Wang,Vincent Coriou,Arjun Narayanan,Jay Hoover,Yenai Ma,Apoorv Jindal,Clayton Sanford,Doug Reid,Swaroop Ramaswamy,Alex Kurakin,Roland Zimmermann,Yana Lunts,Dragos Dena,Zalán Borsos,Vered Cohen,Shujian Zhang,Will Grathwohl,Robert Dadashi,Morgan Redshaw,Joshua Kessinger,Julian Odell,Silvano Bonacina,Zihang Dai,Grace Chen,Ayush Dubey,Pablo Sprechmann,Mantas Pajarskas,Wenxuan Zhou,Niharika Ahuja,Tara Thomas,Martin Nikoltchev,Matija Kecman,Bharath Mankalale,Andrey Ryabtsev,Jennifer She,Christian Walder,Jiaming Shen,Lu Li,Carolina Parada,Sheena Panthaplackel,Okwan Kwon,Matt Lawlor,Utsav Prabhu,Yannick Schroecker,Marc'aurelio Ranzato,Pete Blois,Iurii Kemaev,Ting Yu,Dmitry,Lepikhin,Hao Xiong,Sahand Sharifzadeh,Oleaser Johnson,Jeremiah Willcock,Rui Yao,Greg Farquhar,Sujoy Basu,Hidetoshi Shimokawa,Nina Anderson,Haiguang Li,Khiem Pham,Yizhong Liang,Sebastian Borgeaud,Alexandre Moufarek,Hideto Kazawa,Blair Kutzman,Marcin Sieniek,Sara Smoot,Ruth Wang,Natalie Axelsson,Nova Fallen,Prasha Sundaram,Yuexiang Zhai,Varun Godbole,Petros Maniatis,Alek Wang,Ilia Shumailov,Santhosh Thangaraj,Remi Crocker,Nikita Gupta,Gang Wu,Phil Chen,Gellért Weisz,Celine Smith,Mojtaba Seyedhosseini,Boya Fang,Xiyang Luo,Roey Yogev,Zeynep Cankara,Andrew Hard,Helen Ran,Rahul Sukthankar,George Necula,Gaël Liu,Honglong Cai,Praseem Banzal,Daniel Keysers,Sanjay Ghemawat,Connie Tao,Emma Dunleavy,Aditi Chaudhary,Wei Li,Maciej Mikuła,Chen-Yu Lee,Tiziana Refice,Krishna Somandepalli,Alexandre Fréchette,Dan Bahir,John Karro,Keith Rush,Sarah Perrin,Bill Rosgen,Xiaomeng Yang,Clara Huiyi Hu,Mahmoud Alnahlawi,Justin Mao-Jones,Roopal Garg,Hoang Nguyen,Bat-Orgil Batsaikhan,Iñaki Iturrate,Anselm Levskaya,Avi Singh,Ashyana Kachra,Tony Lu,Denis Petek,Zheng Xu,Mark Graham,Lukas Zilka,Yael Karov,Marija Kostelac,Fangyu Liu,Yaohui Guo,Weiyue Wang,Bernd Bohnet,Emily Pitler,Tony Bruguier,Keisuke Kinoshita,Chrysovalantis Anastasiou,Nilpa Jha,Ting Liu,Jerome Connor,Phil Wallis,Philip Pham,Eric Bailey,Shixin Li,Heng-Tze Cheng,Sally Ma,Haiqiong Li,Akanksha Maurya,Kate Olszewska,Manfred Warmuth,Christy Koh,Dominik Paulus,Siddhartha Reddy Jonnalagadda,Enrique Piqueras,Ali Elqursh,Geoff Brown,Hadar Shemtov,Loren Maggiore,Fei Xia,Ryan Foley,Beka Westberg,George van den Driessche,Livio Baldini Soares,Arjun Kar,Michael Quinn,Siqi Zuo,Jialin Wu,Kyle Kastner,Anna Bortsova,Aijun Bai,Ales Mikhalap,Luowei Zhou,Jennifer Brennan,Vinay Ramasesh,Honglei Zhuang,John Maggs,Johan Schalkwyk,Yuntao Xu,Hui Huang,Andrew Howard,Sasha Brown,Linting Xue,Gloria Shen,Brian Albert,Neha Jha,Daniel Zheng,Varvara Krayvanova,Spurthi Amba Hombaiah,Olivier Lacombe,Gautam Vasudevan,Dan Graur,Tian Xie,Meet Gandhi,Bangju Wang,Dustin Zelle,Harman Singh,Dahun Kim,Sébastien Cevey,Victor Ungureanu,Natasha Noy,Fei Liu,Annie Xie,Fangxiaoyu Feng,Katerina Tsihlas,Daniel Formoso,Neera Vats,Quentin Wellens,Yinan Wang,Niket Kumar Bhumihar,Samrat Ghosh,Matt Hoffman,Tom Lieber,Oran Lang,Kush Bhatia,Tom Paine,Aroonalok Pyne,Ronny Votel,Madeleine Clare Elish,Benoit Schillings,Alex Panagopoulos,Haichuan Yang,Adam Raveret,Zohar Yahav,Shuang Liu,Warren Chen,Dalia El Badawy,Nishant Agrawal,Mohammed Badawi,Mahdi Mirzazadeh,Carla Bromberg,Fan Ye,Chang Liu,Tatiana Sholokhova,George-Cristian Muraru,Gargi Balasubramaniam,Jonathan Malmaud,Alen Carin,Danilo Martins,Irina Jurenka,Pankil Botadra,Dave Lacey,Richa Singh,Mariano Schain,Dan Zheng,Isabelle Guyon,Victor Lavrenko,Seungji Lee,Xiang Zhou,Demis Hassabis,Jeshwanth Challagundla,Derek Cheng,Nikhil Mehta,Matthew Mauger,Michela Paganini,Pushkar Mishra,Kate Lee,Zhang Li,Lexi Baugher,Ondrej Skopek,Max Chang,Amir Zait,Gaurav Menghani,Lizzetth Bellot,Guangxing Han,Jean-Michel Sarr,Sharat Chikkerur,Himanshu Sahni,Rohan Anil,Arun Narayanan,Chandu Thekkath,Daniele Pighin,Hana Strejček,Marko Velic,Fred Bertsch,Manuel Tragut,Keran Rong,Alicia Parrish,Kai Bailey,Jiho Park,Isabela Albuquerque,Abhishek Bapna,Rajesh Venkataraman,Alec Kosik,Johannes Griesser,Zhiwei Deng,Alek Andreev,Qingyun Dou,Kevin Hui,Fanny Wei,Xiaobin Yu,Lei Shu,Avia Aharon,David Barker,Badih Ghazi,Sebastian Flennerhag,Chris Breaux,Yuchuan Liu,Matthew Bilotti,Josh Woodward,Uri Alon,Stephanie Winkler,Tzu-Kuo Huang,Kostas Andriopoulos,João Gabriel Oliveira,Penporn Koanantakool,Berkin Akin,Michael Wunder,Cicero Nogueira dos Santos,Mohammad Hossein Bateni,Lin Yang,Dan Horgan,Beer Changpinyo,Keyvan Amiri,Min Ma,Dayeong Lee,Lihao Liang,Anirudh Baddepudi,Tejasi Latkar,Raia Hadsell,Jun Xu,Hairong Mu,Michael Han,Aedan Pope,Snchit Grover,Frank Kim,Ankit Bhagatwala,Guan Sun,Yamini Bansal,Amir Globerson,Alireza Nazari,Samira Daruki,Hagen Soltau,Jane Labanowski,Laurent El Shafey,Matt Harvey,Yanif Ahmad,Elan Rosenfeld,William Kong,Etienne Pot,Yi-Xuan Tan,Aurora Wei,Victoria Langston,Marcel Prasetya,Petar Veličković,Richard Killam,Robin Strudel,Darren Ni,Zhenhai Zhu,Aaron Archer,Kavya Kopparapu,Lynn Nguyen,Emilio Parisotto,Hussain Masoom,Sravanti Addepalli,Jordan Grimstad,Hexiang Hu,Joss Moore,Avinatan Hassidim,Le Hou,Mukund Raghavachari,Jared Lichtarge,Adam R. Brown,Hilal Dib,Natalia Ponomareva,Justin Fu,Yujing Zhang,Altaf Rahman,Joana Iljazi,Edouard Leurent,Gabriel Dulac-Arnold,Cosmo Du,Chulayuth Asawaroengchai,Larry Jin,Ela Gruzewska,Ziwei Ji,Benigno Uria,Daniel De Freitas,Paul Barham,Lauren Beltrone,Víctor Campos,Jun Yan,Neel Kovelamudi,Arthur Nguyen,Elinor Davies,Zhichun Wu,Zoltan Egyed,Kristina Toutanova,Nithya Attaluri,Hongliang Fei,Peter Stys,Siddhartha Brahma,Martin Izzard,Siva Velusamy,Scott Lundberg,Vincent Zhuang,Kevin Sequeira,Adam Santoro,Ehsan Amid,Ophir Aharoni,Shuai Ye,Mukund Sundararajan,Lijun Yu,Yu-Cheng Ling,Stephen Spencer,Hugo Song,Josip Djolonga,Christo Kirov,Sonal Gupta,Alessandro Bissacco,Clemens Meyer,Mukul Bhutani,Andrew Dai,Weiyi Wang,Siqi Liu,Ashwin Sreevatsa,Qijun Tan,Maria Wang,Lucy Kim,Yicheng Wang,Alex Irpan,Yang Xiao,Stanislav Fort,Yifan He,Alex Gurney,Bryan Gale,Yue Ma,Monica Roy,Viorica Patraucean,Taylan Bilal,Golnaz Ghiasi,Anahita Hosseini,Melvin Johnson,Zhuowan Li,Yi Tay,Benjamin Beyret,Katie Millican,Josef Broder,Mayank Lunayach,Danny Swisher,Eugen Vušak,David Parkinson,MH Tessler,Adi Mayrav Gilady,Richard Song,Allan Dafoe,Yves Raimond,Masa Yamaguchi,Itay Karo,Elizabeth Nielsen,Kevin Kilgour,Mike Dusenberry,Rajiv Mathews,Jiho Choi,Siyuan Qiao,Harsh Mehta,Sahitya Potluri,Chris Knutsen,Jialu Liu,Tat Tan,Kuntal Sengupta,Keerthana Gopalakrishnan,Abodunrinwa Toki,Mencher Chiang,Mike Burrows,Grace Vesom,Zafarali Ahmed,Ilia Labzovsky,Siddharth Vashishtha,Preeti Singh,Ankur Sharma,Ada Ma,Jinyu Xie,Pranav Talluri,Hannah Forbes-Pollard,Aarush Selvan,Joel Wee,Loic Matthey,Tom Funkhouser,Parthasarathy Gopavarapu,Lev Proleev,Cheng Li,Matt Thomas,Kashyap Kolipaka,Zhipeng Jia,Ashwin Kakarla,Srinivas Sunkara,Joan Puigcerver,Suraj Satishkumar Sheth,Emily Graves,Chen Wang,Sadh MNM Khan,Kai Kang,Shyamal Buch,Fred Zhang,Omkar Savant,David Soergel,Kevin Lee,Linda Friso,Xuanyi Dong,Rahul Arya,Shreyas Chandrakaladharan,Connor Schenck,Greg Billock,Tejas Iyer,Anton Bakalov,Leslie Baker,Alex Ruiz,Angad Chandorkar,Trieu Trinh,Matt Miecnikowski,Yanqi Zhou,Yangsibo Huang,Jiazhong Nie,Ali Shah,Ashish Thapliyal,Sam Haves,Lun Wang,Uri Shaham,Patrick Morris-Suzuki,Soroush Radpour,Leonard Berrada,Thomas Strohmann,Chaochao Yan,Jingwei Shen,Sonam Goenka,Tris Warkentin,Petar Dević,Dan Belov,Albert Webson,Madhavi Yenugula,Puranjay Datta,Jerry Chang,Nimesh Ghelani,Aviral Kumar,Vincent Perot,Jessica Lo,Yang Song,Herman Schmit,Jianmin Chen,Vasilisa Bashlovkina,Xiaoyue Pan,Diana Mincu,Paul Roit,Isabel Edkins,Andy Davis,Yujia Li,Ben Horn,Xinjian Li,Pradeep Kumar S,Eric Doi,Wanzheng Zhu,Sri Gayatri Sundara Padmanabhan,Siddharth Verma,Jasmine Liu,Heng Chen,Mihajlo Velimirović,Malcolm Reynolds,Priyanka Agrawal,Nick Sukhanov,Abhinit Modi,Siddharth Goyal,John Palowitch,Nima Khajehnouri,Wing Lowe,David Klinghoffer,Sharon Silver,Vinh Tran,Candice Schumann,Francesco Piccinno,Xi Liu,Mario Lučić,Xiaochen Yang,Sandeep Kumar,Ajay Kannan,Ragha Kotikalapudi,Mudit Bansal,Fabian Fuchs,Javad Hosseini,Abdelrahman Abdelhamed,Dawn Bloxwich,Tianhe Yu,Ruoxin Sang,Gregory Thornton,Karan Gill,Yuchi Liu,Virat Shejwalkar,Jason Lin,Zhipeng Yan,Kehang Han,Thomas Buschmann,Michael Pliskin,Zhi Xing,Susheel Tatineni,Junlin Zhang,Sissie Hsiao,Gavin Buttimore,Marcus Wu,Zefei Li,Geza Kovacs,Legg Yeung,Tao Huang,Aaron Cohen,Bethanie Brownfield,Averi Nowak,Mikel Rodriguez,Tianze Shi,Hado van Hasselt,Kevin Cen,Deepanway Ghoshal,Kushal Majmundar,Weiren Yu,Warren,Chen,Danila Sinopalnikov,Hao Zhang,Vlado Galić,Di Lu,Zeyu Zheng,Maggie Song,Gary Wang,Gui Citovsky,Swapnil Gawde,Isaac Galatzer-Levy,David Silver,Ivana Balazevic,Dipanjan Das,Kingshuk Majumder,Yale Cong,Praneet Dutta,Dustin Tran,Hui Wan,Junwei Yuan,Daniel Eppens,Alanna Walton,Been Kim,Harry Ragan,James Cobon-Kerr,Lu Liu,Weijun Wang,Bryce Petrini,Jack Rae,Rakesh Shivanna,Yan Xiong,Chace Lee,Pauline Coquinot,Yiming Gu,Lisa Patel,Blake Hechtman,Aviel Boag,Orion Jankowski,Alex Wertheim,Alex Lee,Paul Covington,Hila Noga,Sam Sobell,Shanthal Vasanth,William Bono,Chirag Nagpal,Wei Fan,Xavier Garcia,Kedar Soparkar,Aybuke Turker,Nathan Howard,Sachit Menon,Yuankai Chen,Vikas Verma,Vladimir Pchelin,Harish Rajamani,Valentin Dalibard,Ana Ramalho,Yang Guo,Kartikeya Badola,Seojin Bang,Nathalie Rauschmayr,Julia Proskurnia,Sudeep Dasari,Xinyun Chen,Mikhail Sushkov,Anja Hauth,Pauline Sho,Abhinav Singh,Bilva Chandra,Allie Culp,Max Dylla,Olivier Bachem,James Besley,Heri Zhao,Timothy Lillicrap,Wei Wei,Wael Al Jishi,Ning Niu,Alban Rrustemi,Raphaël Lopez Kaufman,Ryan Poplin,Jewel Zhao,Minh Truong,Shikhar Bharadwaj,Ester Hlavnova,Eli Stickgold,Cordelia Schmid,Georgi Stephanov,Zhaoqi Leng,Frederick Liu,Léonard Hussenot,Shenil Dodhia,Juliana Vicente Franco,Lesley Katzen,Abhanshu Sharma,Sarah Cogan,Zuguang Yang,Aniket Ray,Sergi Caelles,Shen Yan,Ravin Kumar,Daniel Gillick,Renee Wong,Joshua Ainslie,Jonathan Hoech,Séb Arnold,Dan Abolafia,Anca Dragan,Ben Hora,Grace Hu,Alexey Guseynov,Yang Lu,Chas Leichner,Jinmeng Rao,Abhimanyu Goyal,Nagabhushan Baddi,Daniel Hernandez Diaz,Tim McConnell,Max Bain,Jake Abernethy,Qiqi Yan,Rylan Schaeffer,Paul Vicol,Will Thompson,Montse Gonzalez Arenas,Mathias Bellaiche,Pablo Barrio,Stefan Zinke,Riccardo Patana,Pulkit Mehta,JK Kearns,Avraham Ruderman,Scott Pollom,David D'Ambrosio,Cath Hope,Yang Yu,Andrea Gesmundo,Kuang-Huei Lee,Aviv Rosenberg,Yiqian Zhou,Yaoyiran Li,Drew Garmon,Yonghui Wu,Safeen Huda,Gil Fidel,Martin Baeuml,Jian Li,Phoebe Kirk,Rhys May,Tao Tu,Sara Mc Carthy,Toshiyuki Fukuzawa,Miranda Aperghis,Chih-Kuan Yeh,Toshihiro Yoshino,Bo Li,Austin Myers,Kaisheng Yao,Ben Limonchik,Changwan Ryu,Rohun Saxena,Alex Goldin,Ruizhe Zhao,Rocky Rhodes,Tao Zhu,Divya Tyam,Heidi Howard,Nathan Byrd,Hongxu Ma,Yan Wu,Ryan Mullins,Qingze Wang,Aida Amini,Sebastien Baur,Yiran Mao,Subhashini Venugopalan,Will Song,Wen Ding,Paul Collins,Sashank Reddi,Megan Shum,Andrei Rusu,Luisa Zintgraf,Kelvin Chan,Sheela Goenka,Mathieu Blondel,Michael Collins,Renke Pan,Marissa Giustina,Nikolai Chinaev,Christian Schuler,Ce Zheng,Jonas Valfridsson,Alyssa Loo,Alex Yakubovich,Jamie Smith,Tao Jiang,Rich Munoz,Gabriel Barcik,Rishabh Bansal,Mingyao Yang,Yilun Du,Pablo Duque,Mary Phuong,Alexandra Belias,Kunal Lad,Zeyu Liu,Tal Schuster,Karthik Duddu,Jieru Hu,Paige Kunkle,Matthew Watson,Jackson Tolins,Josh Smith,Denis Teplyashin,Garrett Bingham,Marvin Ritter,Marco Andreetto,Divya Pitta,Mohak Patel,Shashank Viswanadha,Trevor Strohman,Catalin Ionescu,Jincheng Luo,Yogesh Kalley,Jeremy Wiesner,Dan Deutsch,Derek Lockhart,Peter Choy,Rumen Dangovski,Chawin Sitawarin,Cat Graves,Tanya Lando,Joost van Amersfoort,Ndidi Elue,Zhouyuan Huo,Pooya Moradi,Jean Tarbouriech,Henryk Michalewski,Wenting Ye,Eunyoung Kim,Alex Druinsky,Florent Altché,Xinyi Chen,Artur Dwornik,Da-Cheng Juan,Rivka Moroshko,Horia Toma,Jarrod Kahn,Hai Qian,Maximilian Sieb,Irene Cai,Roman Goldenberg,Praneeth Netrapalli,Sindhu Raghuram,Yuan Gong,Lijie Fan,Evan Palmer,Yossi Matias,Valentin Gabeur,Shreya Pathak,Tom Ouyang,Don Metzler,Geoff Bacon,Srinivasan Venkatachary,Sridhar Thiagarajan,Alex Cullum,Eran Ofek,Vytenis Sakenas,Mohamed Hammad,Cesar Magalhaes,Mayank Daswani,Oscar Chang,Ashok Popat,Ruichao Li,Komal Jalan,Yanhan Hou,Josh Lipschultz,Antoine He,Wenhao Jia,Pier Giuseppe Sessa,Prateek Kolhar,William Wong,Sumeet Singh,Lukas Haas,Jay Whang,Hanna Klimczak-Plucińska,Georges Rotival,Grace Chung,Yiqing Hua,Anfal Siddiqui,Nicolas Serrano,Dongkai Chen,Billy Porter,Libin Bai,Keshav Shivam,Sho Arora,Partha Talukdar,Tom Cobley,Sangnie Bhardwaj,Evgeny Gladchenko,Simon Green,Kelvin Guu,Felix Fischer,Xiao Wu,Eric Wang,Achintya Singhal,Tatiana Matejovicova,James Martens,Hongji Li,Roma Patel,Elizabeth Kemp,Jiaqi Pan,Lily Wang,Blake JianHang Chen,Jean-Baptiste Alayrac,Navneet Potti,Erika Gemzer,Eugene Ie,Kay McKinney,Takaaki Saeki,Edward Chou,Pascal Lamblin,SQ Mah,Zach Fisher,Martin Chadwick,Jon Stritar,Obaid Sarvana,Andrew Hogue,Artem Shtefan,Hadi Hashemi,Yang Xu,Jindong Gu,Sharad Vikram,Chung-Ching Chang,Sabela Ramos,Logan Kilpatrick,Weijuan Xi,Jenny Brennan,Yinghao Sun,Abhishek Jindal,Ionel Gog,Dawn Chen,Felix Wu,Jason Lee,Sudhindra Kopalle,Srinadh Bhojanapalli,Oriol Vinyals,Natan Potikha,Burcu Karagol Ayan,Yuan Yuan,Michael Riley,Piotr Stanczyk,Sergey Kishchenko,Bing Wang,Dan Garrette,Antoine Yang,Vlad Feinberg,CJ Carey,Javad Azizi,Viral Shah,Erica Moreira,Chongyang Shi,Josh Feldman,Elizabeth Salesky,Thomas Lampe,Aneesh Pappu,Duhyeon Kim,Jonas Adler,Avi Caciularu,Brian Walker,Yunhan Xu,Yochai Blau,Dylan Scandinaro,Terry Huang,Sam El-Husseini,Abhishek Sinha,Lijie Ren,Taylor Tobin,Patrik Sundberg,Tim Sohn,Vikas Yadav,Mimi Ly,Emily Xue,Jing Xiong,Afzal Shama Soudagar,Sneha Mondal,Nikhil Khadke,Qingchun Ren,Ben Vargas,Stan Bileschi,Sarah Chakera,Cindy Wang,Boyu Wang,Yoni Halpern,Joe Jiang,Vikas Sindhwani,Petre Petrov,Pranavaraj Ponnuramu,Sanket Vaibhav Mehta,Yu Watanabe,Betty Chan,Matheus Wisniewski,Trang Pham,Jingwei Zhang,Conglong Li,Dario de Cesare,Art Khurshudov,Alex Vasiloff,Melissa Tan,Zoe Ashwood,Bobak Shahriari,Maryam Majzoubi,Garrett Tanzer,Olga Kozlova,Robin Alazard,James Lee-Thorp,Nguyet Minh Phu,Isaac Tian,Junwhan Ahn,Andy Crawford,Lauren Lax,Yuan,Shangguan,Iftekhar Naim,David Ross,Oleksandr Ferludin,Tongfei Guo,Andrea Banino,Hubert Soyer,Xiaoen Ju,Dominika Rogozińska,Ishaan Malhi,Marcella Valentine,Daniel Balle,Apoorv Kulshreshtha,Maciej Kula,Yiwen Song,Sophia Austin,John Schultz,Roy Hirsch,Arthur Douillard,Apoorv Reddy,Michael Fink,Summer Yue,Khyatti Gupta,Adam Zhang,Norman Rink,Daniel McDuff,Lei Meng,András György,Yasaman Razeghi,Ricky Liang,Kazuki Osawa,Aviel Atias,Matan Eyal,Tyrone Hill,Nikolai Grigorev,Zhengdong Wang,Nitish Kulkarni,Rachel Soh,Ivan Lobov,Zachary Charles,Sid Lall,Kazuma Hashimoto,Ido Kessler,Victor Gomes,Zelda Mariet,Danny Driess,Alessandro Agostini,Canfer Akbulut,Jingcao Hu,Marissa Ikonomidis,Emily Caveness,Kartik Audhkhasi,Saurabh Agrawal,Ioana Bica,Evan Senter,Jayaram Mudigonda,Kelly Chen,Jingchen Ye,Xuanhui Wang,James Svensson,Philipp Fränken,Josh Newlan,Li Lao,Eva Schnider,Sami Alabed,Joseph Kready,Jesse Emond,Afief Halumi,Tim Zaman,Chengxi Ye,Naina Raisinghani,Vilobh Meshram,Bo Chang,Ankit Singh Rawat,Axel Stjerngren,Sergey Levi,Rui Wang,Xiangzhu Long,Mitchelle Rasquinha,Steven Hand,Aditi Mavalankar,Lauren Agubuzu,Sudeshna Roy,Junquan Chen,Jarek Wilkiewicz,Hao Zhou,Michal Jastrzebski,Qiong Hu,Agustin Dal Lago,Ramya Sree Boppana,Wei-Jen Ko,Jennifer Prendki,Yao Su,Zhi Li,Eliza Rutherford,Girish Ramchandra Rao,Ramona Comanescu,Adrià Puigdomènech,Qihang Chen,Dessie Petrova,Christine Chan,Vedrana Milutinovic,Felipe Tiengo Ferreira,Chin-Yi Cheng,Ming Zhang,Tapomay Dey,Sherry Yang,Ramesh Sampath,Quoc Le,Howard Zhou,Chu-Cheng Lin,Hoi Lam,Christine Kaeser-Chen,Kai Hui,Dean Hirsch,Tom Eccles,Basil Mustafa,Shruti Rijhwani,Morgane Rivière,Yuanzhong Xu,Junjie Wang,Xinyang Geng,Xiance Si,Arjun Khare,Cheolmin Kim,Vahab Mirrokni,Kamyu Lee,Khuslen Baatarsukh,Nathaniel Braun,Lisa Wang,Pallavi LV,Richard Tanburn,Yuvein,Zhu,Fangda Li,Setareh Ariafar,Dan Goldberg,Ken Burke,Daniil Mirylenka,Meiqi Guo,Olaf Ronneberger,Hadas Natalie Vogel,Liqun Cheng,Nishita Shetty,Johnson Jia,Thomas Jimma,Corey Fry,Ted Xiao,Martin Sundermeyer,Ryan Burnell,Yannis Assael,Mario Pinto,JD Chen,Rohit Sathyanarayana,Donghyun Cho,Jing Lu,Rishabh Agarwal,Sugato Basu,Lucas Gonzalez,Dhruv Shah,Meng Wei,Dre Mahaarachchi,Rohan Agrawal,Tero Rissa,Yani Donchev,Ramiro Leal-Cavazos,Adrian Hutter,Markus Mircea,Alon Jacovi,Faruk Ahmed,Jiageng Zhang,Shuguang Hu,Bo-Juen Chen,Jonni Kanerva,Guillaume Desjardins,Andrew Lee,Nikos Parotsidis,Asier Mujika,Tobias Weyand,Jasper Snoek,Jo Chick,Kai Chen,Paul Chang,Ethan Mahintorabi,Zi Wang,Tolly Powell,Orgad Keller,Abhirut Gupta,Claire Sha,Kanav Garg,Nicolas Heess,Ágoston Weisz,Cassidy Hardin,Bartek Wydrowski,Ben Coleman,Karina Zainullina,Pankaj Joshi,Alessandro Epasto,Terry Spitz,Binbin Xiong,Kai Zhao,Arseniy Klimovskiy,Ivy Zheng,Johan Ferret,Itay Yona,Waleed Khawaja,Jean-Baptiste Lespiau,Maxim Krikun,Siamak Shakeri,Timothee Cour,Bonnie Li,Igor Krivokon,Dan Suh,Alex Hofer,Jad Al Abdallah,Nikita Putikhin,Oscar Akerlund,Silvio Lattanzi,Anurag Kumar,Shane Settle,Himanshu Srivastava,Folawiyo Campbell-Ajala,Edouard Rosseel,Mihai Dorin Istin,Nishanth Dikkala,Anand Rao,Nick Young,Kate Lin,Dhruva Bhaswar,Yiming Wang,Jaume Sanchez Elias,Kritika Muralidharan,James Keeling,Dayou Du,Siddharth Gopal,Gregory Dibb,Charles Blundell,Manolis Delakis,Jacky Liang,Marco Tulio Ribeiro,Georgi Karadzhov,Guillermo Garrido,Ankur Bapna,Jiawei Cao,Adam Sadovsky,Pouya Tafti,Arthur Guez,Coline Devin,Yixian Di,Jinwei Xing,Chuqiao,Xu,Hanzhao Lin,Chun-Te Chu,Sameera Ponda,Wesley Helmholz,Fan Yang,Yue Gao,Sara Javanmardi,Wael Farhan,Alex Ramirez,Ricardo Figueira,Khe Chai Sim,Yuval Bahat,Ashwin Vaswani,Liangzhe Yuan,Gufeng Zhang,Leland Rechis,Hanjun Dai,Tayo Oguntebi,Alexandra Cordell,Eugénie Rives,Kaan Tekelioglu,Naveen Kumar,Bing Zhang,Aurick Zhou,Nikolay Savinov,Andrew Leach,Alex Tudor,Sanjay Ganapathy,Yanyan Zheng,Mirko Rossini,Vera Axelrod,Arnaud Autef,Yukun Zhu,Zheng Zheng,Mingda Zhang,Baochen Sun,Jie Ren,Nenad Tomasev,Nithish Kannan,Amer Sinha,Charles Chen,Louis O'Bryan,Alex Pak,Aditya Kusupati,Weel Yang,Deepak Ramachandran,Patrick Griffin,Seokhwan Kim,Philipp Neubeck,Craig Schiff,Tammo Spalink,Mingyang Ling,Arun Nair,Ga-Young Joung,Linda Deng,Avishkar Bhoopchand,Lora Aroyo,Tom Duerig,Jordan Griffith,Gabe Barth-Maron,Jake Ades,Alex Haig,Ankur Taly,Yunting Song,Paul Michel,Dave Orr,Dean Weesner,Corentin Tallec,Carrie Grimes Bostock,Paul Niemczyk,Andy Twigg,Mudit Verma,Rohith Vallu,Henry Wang,Marco Gelmi,Kiranbir Sodhia,Aleksandr Chuklin,Omer Goldman,Jasmine George,Liang Bai,Kelvin Zhang,Petar Sirkovic,Efrat Nehoran,Golan Pundak,Jiaqi Mu,Alice Chen,Alex Greve,Paulo Zacchello,David Amos,Heming Ge,Eric Noland,Colton Bishop,Jeffrey Dudek,Youhei Namiki,Elena Buchatskaya,Jing Li,Dorsa Sadigh,Masha Samsikova,Dan Malkin,Damien Vincent,Robert David,Rob Willoughby,Phoenix Meadowlark,Shawn Gao,Yan Li,Raj Apte,Amit Jhindal,Stein Xudong Lin,Alex Polozov,Zhicheng Wang,Tomas Mery,Anirudh GP,Varun Yerram,Sage Stevens,Tianqi Liu,Noah Fiedel,Charles Sutton,Matthew Johnson,Xiaodan Song,Kate Baumli,Nir Shabat,Muqthar Mohammad,Hao Liu,Marco Selvi,Yichao Zhou,Mehdi Hafezi Manshadi,Chu-ling Ko,Anthony Chen,Michael Bendersky,Jorge Gonzalez Mendez,Nisarg Kothari,Amir Zandieh,Yiling Huang,Daniel Andor,Ellie Pavlick,Idan Brusilovsky,Jitendra Harlalka,Sally Goldman,Andrew Lampinen,Guowang Li,Asahi Ushio,Somit Gupta,Lei Zhang,Chuyuan Kelly Fu,Madhavi Sewak,Timo Denk,Jed Borovik,Brendan Jou,Avital Zipori,Prateek Jain,Junwen Bai,Thang Luong,Jonathan Tompson,Alice Li,Li Liu,George Powell,Jiajun Shen,Alex Feng,Grishma Chole,Da Yu,Yinlam Chow,Tongxin Yin,Eric Malmi,Kefan Xiao,Yash Pande,Shachi Paul,Niccolò Dal Santo,Adil Dostmohamed,Sergio Guadarrama,Aaron Phillips,Thanumalayan Sankaranarayana Pillai,Gal Yona,Amin Ghafouri,Preethi Lahoti,Benjamin Lee,Dhruv Madeka,Eren Sezener,Simon Tokumine,Adrian Collister,Nicola De Cao,Richard Shin,Uday Kalra,Parker Beak,Emily Nottage,Ryo Nakashima,Ivan Jurin,Vikash Sehwag,Meenu Gaba,Junhao Zeng,Kevin R. McKee,Fernando Pereira,Tamar Yakar,Amayika Panda,Arka Dhar,Peilin Zhong,Daniel Sohn,Mark Brand,Lars Lowe Sjoesund,Viral Carpenter,Sharon Lin,Shantanu Thakoor,Marcus Wainwright,Ashwin Chaugule,Pranesh Srinivasan,Muye Zhu,Bernett Orlando,Jack Weber,Ayzaan Wahid,Gilles Baechler,Apurv Suman,Jovana Mitrović,Gabe Taubman,Honglin Yu,Helen King,Josh Dillon,Cathy Yip,Dhriti Varma,Tomas Izo,Levent Bolelli,Borja De Balle Pigem,Julia Di Trapani,Fotis Iliopoulos,Adam Paszke,Nishant Ranka,Joe Zou,Francesco Pongetti,Jed McGiffin,Alex Siegman,Rich Galt,Ross Hemsley,Goran Žužić,Victor Carbune,Tao Li,Myle Ott,Félix de Chaumont Quitry,David Vilar Torres,Yuri Chervonyi,Tomy Tsai,Prem Eruvbetine,Samuel Yang,Matthew Denton,Jake Walker,Slavica Andačić,Idan Heimlich Shtacher,Vittal Premachandran,Harshal Tushar Lehri,Cip Baetu,Damion Yates,Lampros Lamprou,Mariko Iinuma,Ioana Mihailescu,Ben Albrecht,Shachi Dave,Susie Sargsyan,Bryan Perozzi,Lucas Manning,Chiyuan Zhang,Denis Vnukov,Igor Mordatch,Raia Hadsell Wolfgang Macherey,Ryan Kappedal,Jim Stephan,Aditya Tripathi,Klaus Macherey,Jun Qian,Abhishek Bhowmick,Shekoofeh Azizi,Rémi Leblond,Shiva Mohan Reddy Garlapati,Timothy Knight,Matthew Wiethoff,Wei-Chih Hung,Anelia Angelova,Georgios Evangelopoulos,Pawel Janus,Dimitris Paparas,Matthew Rahtz,Ken Caluwaerts,Vivek Sampathkumar,Daniel Jarrett,Shadi Noghabi,Antoine Miech,Chak Yeung,Geoff Clark,Henry Prior,Fei Zheng,Jean Pouget-Abadie,Indro Bhattacharya,Kalpesh Krishna,Will Bishop,Zhe Yuan,Yunxiao Deng,Ashutosh Sathe,Kacper Krasowiak,Ciprian Chelba,Cho-Jui Hsieh,Kiran Vodrahalli,Buhuang Liu,Thomas Köppe,Amr Khalifa,Lubo Litchev,Pichi Charoenpanit,Reed Roberts,Sachin Yadav,Yasumasa Onoe,Desi Ivanov,Megha Mohabey,Vighnesh Birodkar,Nemanja Rakićević,Pierre Sermanet,Vaibhav Mehta,Krishan Subudhi,Travis Choma,Will Ng,Luheng He,Kathie Wang,Tasos Kementsietsidis,Shane Gu,Mansi Gupta,Andrew Nystrom,Mehran Kazemi,Timothy Chung,Nacho Cano,Nikhil Dhawan,Yufei Wang,Jiawei Xia,Trevor Yacovone,Eric Jia,Mingqing Chen,Simeon Ivanov,Ashrith Sheshan,Sid Dalmia,Paweł Stradomski,Pengcheng Yin,Salem Haykal,Congchao Wang,Dennis Duan,Neslihan Bulut,Greg Kochanski,Liam MacDermed,Namrata Godbole,Shitao Weng,Jingjing Chen,Rachana Fellinger,Ramin Mehran,Daniel Suo,Hisham Husain,Tong He,Kaushal Patel,Joshua Howland,Randall Parker,Kelvin Nguyen,Sharath Maddineni,Chris Rawles,Mina Khan,Shlomi Cohen-Ganor,Amol Mandhane,Xinyi Wu,Chenkai Kuang,Iulia Comşa,Ramya Ganeshan,Hanie Sedghi,Adam Bloniarz,Nuo Wang Pierse,Anton Briukhov,Petr Mitrichev,Anita Gergely,Serena Zhan,Allan Zhou,Nikita Saxena,Eva Lu,Josef Dean,Ashish Gupta,Nicolas Perez-Nieves,Renjie Wu,Cory McLean,Wei Liang,Disha Jindal,Anton Tsitsulin,Wenhao Yu,Kaiz Alarakyia,Tom Schaul,Piyush Patil,Peter Sung,Elijah Peake,Hongkun Yu,Feryal Behbahani,JD Co-Reyes,Alan Ansell,Sean Sun,Clara Barbu,Jonathan Lee,Seb Noury,James Allingham,Bilal Piot,Mohit Sharma,Christopher Yew,Ivan Korotkov,Bibo Xu,Demetra Brady,Goran Petrovic,Shibl Mourad,Claire Cui,Aditya Gupta,Parker Schuh,Saarthak Khanna,Anna Goldie,Abhinav Arora,Vadim Zubov,Amy Stuart,Mark Epstein,Yun Zhu,Jianqiao Liu,Yury Stuken,Ziyue Wang,Karolis Misiunas,Dee Guo,Ashleah Gill,Ale Hartman,Zaid Nabulsi,Aurko Roy,Aleksandra Faust,Jason Riesa,Ben Withbroe,Mengchao Wang,Marco Tagliasacchi,Andreea Marzoca,James Noraky,Serge Toropov,Malika Mehrotra,Bahram Raad,Sanja Deur,Steve Xu,Marianne Monteiro,Zhongru Wu,Yi Luan,Sam Ritter,Nick Li,Håvard Garnes,Yanzhang He,Martin Zlocha,Jifan Zhu,Matteo Hessel,Will Wu,Spandana Raj Babbula,Chizu Kawamoto,Yuanzhen Li,Mehadi Hassen,Yan Wang,Brian Wieder,James Freedman,Yin Zhang,Xinyi Bai,Tianli Yu,David Reitter,XiangHai Sheng,Mateo Wirth,Aditya Kini,Dima Damen,Mingcen Gao,Rachel Hornung,Michael Voznesensky,Brian Roark,Adhi Kuncoro,Yuxiang Zhou,Rushin Shah,Anthony Brohan,Kuangyuan Chen,James Wendt,David Rim,Paul Kishan Rubenstein,Jonathan Halcrow,Michelle Liu,Ty Geri,Yunhsuan Sung,Jane Shapiro,Shaan Bijwadia,Chris Duvarney,Christina Sorokin,Paul Natsev,Reeve Ingle,Pramod Gupta,Young Maeng,Ndaba Ndebele,Kexin Zhu,Valentin Anklin,Katherine Lee,Yuan Liu,Yaroslav Akulov,Shaleen Gupta,Guolong Su,Flavien Prost,Tianlin Liu,Vitaly Kovalev,Pol Moreno,Martin Scholz,Sam Redmond,Zongwei Zhou,Alex Castro-Ros,André Susano Pinto,Dia Kharrat,Michal Yarom,Rachel Saputro,Jannis Bulian,Ben Caine,Ji Liu,Abbas Abdolmaleki,Shariq Iqbal,Tautvydas Misiunas,Mikhail Sirotenko,Shefali Garg,Guy Bensky,Huan Gui,Xuezhi Wang,Raphael Koster,Mike Bernico,Da Huang,Romal Thoppilan,Trevor Cohn,Ben Golan,Wenlei Zhou,Andrew Rosenberg,Markus Freitag,Tynan Gangwani,Vincent Tsang,Anand Shukla,Xiaoqi Ren,Minh Giang,Chi Zou,Andre Elisseeff,Charline Le Lan,Dheeru Dua,Shuba Lall,Pranav Shyam,Frankie Garcia,Sarah Nguyen,Michael Guzman,AJ Maschinot,Marcello Maggioni,Ming-Wei Chang,Karol Gregor,Lotte Weerts,Kumaran Venkatesan,Bogdan Damoc,Leon Liu,Jan Wassenberg,Lewis Ho,Becca Roelofs,Majid Hadian,François-Xavier Aubet,Yu Liang,Sami Lachgar,Danny Karmon,Yong Cheng,Amelio Vázquez-Reina,Angie Chen,Zhuyun Dai,Andy Brock,Shubham Agrawal,Chenxi Pang,Peter Garst,Mariella Sanchez-Vargas,Ivor Rendulic,Aditya Ayyar,Andrija Ražnatović,Olivia Ma,Roopali Vij,Neha Sharma,Ashwin Balakrishna,Bingyuan Liu,Ian Mackinnon,Sorin Baltateanu,Petra Poklukar,Gabriel Ibagon,Colin Ji,Hongyang Jiao,Isaac Noble,Wojciech Stokowiec,Zhihao Li,Jeff Dean,David Lindner,Mark Omernick,Kristen Chiafullo,Mason Dimarco,Vitor Rodrigues,Vittorio Selo,Garrett Honke,Xintian,Wu,Wei He,Adam Hillier,Anhad Mohananey,Vihari Piratla,Chang Ye,Chase Malik,Sebastian Riedel,Samuel Albanie,Zi Yang,Kenny Vassigh,Maria Bauza,Sheng Li,Yiqing Tao,Nevan Wichers,Andrii Maksai,Abe Ittycheriah,Ross Mcilroy,Bryan Seybold,Noah Goodman,Romina Datta,Steven M. Hernandez,Tian Shi,Yony Kochinski,Anna Bulanova,Ken Franko,Mikita Sazanovich,Nicholas FitzGerald,Praneeth Kacham,Shubha Srinivas Raghvendra,Vincent Hellendoorn,Alexander Grushetsky,Julian Salazar,Angeliki Lazaridou,Jason Chang,Jan-Thorsten Peter,Sushant Kafle,Yann Dauphin,Abhishek Rao,Filippo Graziano,Izhak Shafran,Yuguo Liao,Tianli Ding,Geng Yan,Grace Chu,Zhao Fu,Vincent Roulet,Gabriel Rasskin,Duncan Williams,Shahar Drath,Alex Mossin,Raphael Hoffmann,Jordi Orbay,Francesco Bertolini,Hila Sheftel,Justin Chiu,Siyang Xue,Yuheng Kuang,Ferjad Naeem,Swaroop Nath,Nana Nti,Phil Culliton,Kashyap Krishnakumar,Michael Isard,Pei Sun,Ayan Chakrabarti,Nathan Clement,Regev Cohen,Arissa Wongpanich,GS Oh,Ashwin Murthy,Hao Zheng,Jessica Hamrick,Oskar Bunyan,Suhas Ganesh,Nitish Gupta,Roy Frostig,John Wieting,Yury Malkov,Pierre Marcenac,Zhixin,Lai,Xiaodan Tang,Mohammad Saleh,Fedir Zubach,Chinmay Kulkarni,Huanjie Zhou,Vicky Zayats,Nan Ding,Anshuman Tripathi,Arijit Pramanik,Patrik Zochbauer,Harish Ganapathy,Vedant Misra,Zach Behrman,Hugo Vallet,Mingyang Zhang,Mukund Sridhar,Ye Jin,Mohammad Babaeizadeh,Siim Põder,Megha Goel,Divya Jain,Tajwar Nasir,Shubham Mittal,Tim Dozat,Diego Ardila,Aliaksei Severyn,Fabio Pardo,Sammy Jerome,Siyang Qin,Louis Rouillard,Amir Yazdanbakhsh,Zizhao Zhang,Shivani Agrawal,Kaushik Shivakumar,Caden Lu,Praveen Kallakuri,Rachita Chhaparia,Kanishka Rao,Charles Kwong,Asya Fadeeva,Shitij Nigam,Yan Virin,Yuan Zhang,Balaji Venkatraman,Beliz Gunel,Marc Wilson,Huiyu Wang,Abhinav Gupta,Xiaowei Xu,Adrien Ali Taïga,Kareem Mohamed,Doug Fritz,Daniel Rodriguez,Zoubin Ghahramani,Harry Askham,Lior Belenki,James Zhao,Rahul Gupta,Krzysztof Jastrzębski,Takahiro Kosakai,Kaan Katircioglu,Jon Schneider,Rina Panigrahy,Konstantinos Bousmalis,Peter Grabowski,Prajit Ramachandran,Chaitra Hegde,Mihaela Rosca,Angelo Scorza Scarpati,Kyriakos Axiotis,Ying Xu,Zach Gleicher,Assaf Hurwitz Michaely,Mandar Sharma,Sanil Jain,Christoph Hirnschall,Tal Marian,Xuhui Jia,Kevin Mather,Kilol Gupta,Linhai Qiu,Nigamaa Nayakanti,Lucian Ionita,Steven Zheng,Lucia Loher,Kurt Shuster,Igor Petrovski,Roshan Sharma,Rahma Chaabouni,Angel Yeh,James An,Arushi Gupta,Steven Schwarcz,Seher Ellis,Sam Conway-Rahman,Javier Snaider,Alex Zhai,James Atwood,Daniel Golovin,Liqian Peng,Te I,Vivian Xia,Salvatore Scellato,Mahan Malihi,Arthur Bražinskas,Vlad-Doru Ion,Younghoon Jun,James Swirhun,Soroosh Mariooryad,Jiao Sun,Steve Chien,Rey Coaguila,Ariel Brand,Yi Gao,Tom Kwiatkowski,Roee Aharoni,Cheng-Chun Lee,Mislav Žanić,Yichi Zhang,Dan Ethier,Vitaly Nikolaev,Pranav Nair,Yoav Ben Shalom,Hen Fitoussi,Jai Gupta,Hongbin Liu,Dee Cattle,Tolga Bolukbasi,Ben Murdoch,Fantine Huot,Yin Li,Chris Hahn*

Main category: cs.CL

TL;DR: Introduces the Gemini 2.X model family, including variants optimized for performance, cost, and reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: To address diverse needs in complex coding, reasoning, and multimodal tasks while balancing performance and cost.

Method: Developing models with specialized attributes like multimodal understanding, extended video processing, and optimized compute capabilities.

Result: Gemini 2.5 Pro achieves state-of-the-art (SoTA) benchmarks while other models in the family cater to varied performance-cost tradeoffs.

Conclusion: The Gemini 2.X models enable a range of agentic workflows, offering choices across capability and latency considerations for users.

Abstract: In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and
Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite
models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA
performance on frontier coding and reasoning benchmarks. In addition to its
incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that
excels at multimodal understanding and it is now able to process up to 3 hours
of video content. Its unique combination of long context, multimodal and
reasoning capabilities can be combined to unlock new agentic workflows. Gemini
2.5 Flash provides excellent reasoning abilities at a fraction of the compute
and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high
performance at low latency and cost. Taken together, the Gemini 2.X model
generation spans the full Pareto frontier of model capability vs cost, allowing
users to explore the boundaries of what is possible with complex agentic
problem solving.

</details>


### [13] [Humans overrely on overconfident language models, across languages](https://arxiv.org/abs/2507.06306)
*Neil Rathi,Dan Jurafsky,Kaitlyn Zhou*

Main category: cs.CL

TL;DR: This paper studies linguistic overconfidence in large language models across five languages, finding high risks of user overreliance on overly confident responses.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the global deployment of LLMs and highlights the need for responses calibrated to the linguistic and cultural context to prevent overreliance due to overconfidence in generated outputs.

Method: The researchers analyzed the usage of epistemic markers in five languages and conducted experiments to measure human reliance rates on LLM outputs while examining cross-linguistic variations.

Result: The study found that LLMs exhibit overconfidence in all analyzed languages, with variations in the frequency of uncertainty markers by language (e.g., Japanese shows more uncertainty markers). Users were prone to relying on confident outputs irrespective of linguistic variations.

Conclusion: The findings underscore the need for culturally and linguistically sensitive evaluations while developing model safety mechanisms to mitigate reliance risks on overconfident model outputs globally.

Abstract: As large language models (LLMs) are deployed globally, it is crucial that
their responses are calibrated across languages to accurately convey
uncertainty and limitations. Previous work has shown that LLMs are
linguistically overconfident in English, leading users to overrely on confident
generations. However, the usage and interpretation of epistemic markers (e.g.,
'It's definitely,' 'I think') can differ sharply across languages. Here, we
study the risks of multilingual linguistic (mis)calibration, overconfidence,
and overreliance across five languages to evaluate the safety of LLMs in a
global context.
  We find that overreliance risks are high across all languages. We first
analyze the distribution of LLM-generated epistemic markers, and observe that
while LLMs are cross-linguistically overconfident, they are also sensitive to
documented linguistic variation. For example, models generate the most markers
of uncertainty in Japanese and the most markers of certainty in German and
Mandarin. We then measure human reliance rates across languages, finding that
while users strongly rely on confident LLM generations in all languages,
reliance behaviors differ cross-linguistically: for example, users rely
significantly more on expressions of uncertainty in Japanese than in English.
Taken together, these results indicate high risk of reliance on overconfident
model generations across languages. Our findings highlight the challenges of
multilingual linguistic calibration and stress the importance of culturally and
linguistically contextualized model safety evaluations.

</details>


### [14] [ETT: Expanding the Long Context Understanding Capability of LLMs at Test-Time](https://arxiv.org/abs/2507.06313)
*Kiarash Zahirnia,Zahra Golpayegani,Walid Ahmad,Yang Liu*

Main category: cs.CL

TL;DR: The paper tackles the challenge of scaling Transformer-based LLMs to longer contexts using ETT, a method that offers linear computation costs with constant memory. ETT extends context length during testing and improves accuracy by fine-tuning selective parameters efficiently.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of scaling Transformer-based LLMs for processing long sequences, particularly due to their quadratic computation and memory costs.

Method: ETT (Extend at Test-Time) method extends context length of Transformers at test-time by fine-tuning the model's parameters on overlapping smaller input chunks, ensuring constant memory and linear computation overhead.

Result: ETT was evaluated up to 32k token contexts on models like GPT-Large and Phi-2, yielding up to a 30% improvement in accuracy. The study also found that selectively fine-tuning the second FFN layer is more effective than full model fine-tuning.

Conclusion: ETT enables efficient and effective extension of context length in LLMs, significantly improving accuracy with manageable resource requirements, advancing their utility for long-sequence tasks.

Abstract: Transformer-based Language Models' computation and memory overhead increase
quadratically as a function of sequence length. The quadratic cost poses
challenges when employing LLMs for processing long sequences. In this work, we
introduce \ourmodelacronym~(Extend at Test-Time), method for extending the
context length of short context Transformer-based LLMs, with constant memory
requirement and linear computation overhead. ETT enable the extension of the
context length at test-time by efficient fine-tuning the model's parameters on
the input context, chunked into overlapping small subsequences. We evaluate ETT
on LongBench by extending the context length of GPT-Large and Phi-2 up to 32
times, increasing from 1k to 32k tokens. This results in up to a 30 percent
improvement in the model's accuracy. We also study how context can be stored in
LLM's weights effectively and efficiently. Through a detailed ablation study,
we examine which Transformer modules are most beneficial to fine-tune at
test-time. Interestingly, we find that fine-tuning the second layer of the FFNs
is more effective than full fine-tuning, leading to a further improvement in
the models' accuracy.

</details>


### [15] [Could the Road to Grounded, Neuro-symbolic AI be Paved with Words-as-Classifiers?](https://arxiv.org/abs/2507.06335)
*Casey Kennington,David Schlangen*

Main category: cs.CL

TL;DR: This paper explores unifying formal, distributional, and grounded semantic theories using words-as-classifier models, supported by literature review, cognitive science insights, and experimental evidence.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations of formal, distributional, and grounded semantic theories by proposing a unified semantic framework utilizing words-as-classifiers.

Method: The authors review existing literature, incorporate cognitive science findings, conduct a small experiment, and propose a unified semantic model using words-as-classifiers.

Result: The results include a conceptual proposal and experimental evidence supporting the applicability of the words-as-classifier model across the three semantic theories.

Conclusion: The authors conclude that the words-as-classifier model offers promise in unifying formal, distributional, and grounded semantic approaches into a single cohesive framework.

Abstract: Formal, Distributional, and Grounded theories of computational semantics each
have their uses and their drawbacks. There has been a shift to ground models of
language by adding visual knowledge, and there has been a call to enrich models
of language with symbolic methods to gain the benefits from formal,
distributional, and grounded theories. In this paper, we attempt to make the
case that one potential path forward in unifying all three semantic fields is
paved with the words-as-classifier model, a model of word-level grounded
semantics that has been incorporated into formalisms and distributional
language models in the literature, and it has been well-tested within
interactive dialogue settings. We review that literature, motivate the
words-as-classifiers model with an appeal to recent work in cognitive science,
and describe a small experiment. Finally, we sketch a model of semantics
unified through words-as-classifiers.

</details>


### [16] [Evaluating Morphological Alignment of Tokenizers in 70 Languages](https://arxiv.org/abs/2507.06378)
*Catherine Arnett,Marisa Hudspeth,Brendan O'Connor*

Main category: cs.CL

TL;DR: This paper expands the MorphScore metric to evaluate tokenizer quality across 70 languages, focusing on the alignment of token boundaries with morphological boundaries.


<details>
  <summary>Details</summary>
Motivation: The motivation is to assess tokenizer quality more effectively by evaluating how well tokenizations align with linguistic subword structures and morphological boundaries.

Method: The authors enhance MorphScore to cover 70 languages, address its previous limitations, and conduct correlation analysis between morphological alignment scores and downstream performance in language models.

Result: The study reveals that morphological alignment shows minimal variance in predicting model performance, indicating its limited effectiveness as a measure of tokenizer quality.

Conclusion: Morphological alignment alone is insufficient as a complete metric for evaluating tokenizer quality in relation to downstream model performance.

Abstract: While tokenization is a key step in language modeling, with effects on model
training and performance, it remains unclear how to effectively evaluate
tokenizer quality. One proposed dimension of tokenizer quality is the extent to
which tokenizers preserve linguistically meaningful subwords, aligning token
boundaries with morphological boundaries within a word. We expand MorphScore
(Arnett & Bergen, 2025), which previously covered 22 languages, to support a
total of 70 languages. The updated MorphScore offers more flexibility in
evaluation and addresses some of the limitations of the original version. We
then correlate our alignment scores with downstream task performance for five
pre-trained languages models on seven tasks, with at least one task in each of
the languages in our sample. We find that morphological alignment does not
explain very much variance in model performance, suggesting that morphological
alignment alone does not measure dimensions of tokenization quality relevant to
model performance.

</details>


### [17] [Hypermagmas and Colored Operads: Heads, Phases, and Theta Roles](https://arxiv.org/abs/2507.06393)
*Matilde Marcolli,Riny Huijbregts,Richard K. Larson*

Main category: cs.CL

TL;DR: The paper introduces the concept of hypermagma to analyze syntactic structures, connecting notions like c-command, m-command, phases, theta roles, and filtering via colored operads.


<details>
  <summary>Details</summary>
Motivation: The authors aimed to understand the formal representation and interaction between syntactic structures, phases, and movement rules in language with mathematical foundations, specifically colored operads and hypermagma.

Method: The study utilizes hypermagma and colored operad systems to model syntactic elements, filter syntactic objects, and address movement rules within linguistic theory.

Result: C-command and m-command relations fit into the hypermagma model; colored operads demonstrate equivalence in parsing syntactic objects and are linked to movement rules and compatibility within phase structures.

Conclusion: The colored operad framework successfully integrates various linguistic rules and phenomena into a unified mathematical structure, advancing the theoretical understanding of syntax.

Abstract: We show that head functions on syntactic objects extend the magma structure
to a hypermagma, with the c-command relation compatible with the magma
operation and the m-command relation with the hypermagma. We then show that the
structure of head and complement and specifier, additional modifier positions,
and the structure of phases in the Extended Projection can be formulated as a
bud generating system of a colored operad, in a form similar to the structure
of theta roles. We also show that, due to the special form of the colored
operad generators, the filtering of freely generated syntactic objects by these
coloring rules can be equivalently formulated as a filtering in the course of
structure formation via a colored Merge, which can in turn be related to the
hypermagma structure. The rules on movement by Internal Merge with respect to
phases, the Extended Projection Principle, Empty Category Principle, and Phase
Impenetrability Condition are all subsumed into the form of the colored operad
generators. Movement compatibilities between the phase structure and the theta
roles assignments can then be formulated in terms of the respective colored
operads and a transduction of colored operads.

</details>


### [18] [PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning](https://arxiv.org/abs/2507.06415)
*Zeming Chen,Angelika Romanou,Gail Weiss,Antoine Bosselut*

Main category: cs.CL

TL;DR: PERK is a scalable approach for encoding long, noisy contexts using lightweight model adapters during test-time learning. It demonstrates significant performance improvement over baseline methods, particularly in long-context reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Current methods to encode long, noisy contexts for reasoning are limited due to high memory requirements, making them unsuitable for extensive contexts.

Method: PERK employs a meta-learning framework with nested optimization loops. The inner loop updates a low-rank adapter to serve as an efficient memory module, while the outer loop learns to use the adapter for recalling and reasoning over encoded contexts.

Result: PERK achieves considerable performance gains on long-context reasoning tasks, with improvements of up to 90% for smaller models like GPT-2 and 27% for larger models like Qwen-2.5-0.5B.

Conclusion: PERK offers a scalable and efficient solution for long-context reasoning, outperforming prompt-based baselines while being computationally efficient during inference.

Abstract: Long-context reasoning requires accurately identifying relevant information
in extensive, noisy input contexts. Previous research shows that using
test-time learning to encode context directly into model parameters can
effectively enable reasoning over noisy information. However, meta-learning
methods for enabling test-time learning are prohibitively memory-intensive,
preventing their application to long context settings. In this work, we propose
PERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for
learning to encode long input contexts using gradient updates to a lightweight
model adapter at test time. Specifically, PERK employs two nested optimization
loops in a meta-training phase. The inner loop rapidly encodes contexts into a
low-rank adapter (LoRA) that serves as a parameter-efficient memory module for
the base model. Concurrently, the outer loop learns to use the updated adapter
to accurately recall and reason over relevant information from the encoded long
context. Our evaluations on several long-context reasoning tasks show that PERK
significantly outperforms the standard prompt-based long-context baseline,
achieving average absolute performance gains of up to 90% for smaller models
(GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In
general, PERK is more robust to reasoning complexity, length extrapolation, and
the locations of relevant information in contexts. Finally, we show that while
PERK is memory-intensive during training, it scales more efficiently at
inference time than prompt-based long-context inference.

</details>


### [19] [Reward Models Can Improve Themselves: Reward-Guided Adversarial Failure Mode Discovery for Robust Reward Modeling](https://arxiv.org/abs/2507.06419)
*Pankayaraj Pathmanathan,Furong Huang*

Main category: cs.CL

TL;DR: This paper introduces REFORM, a framework to improve reward model robustness in large language models by self-generating adversarial examples for training data augmentation.


<details>
  <summary>Details</summary>
Motivation: To address the challenges RL models face in capturing human preferences due to dataset limitations, adversarial perturbations, and distributional shifts.

Method: The paper proposes reward-guided controlled decoding to identify failure modes and utilizes these adversarial examples for training data augmentation in a framework called REFORM.

Result: Using preference datasets (Anthropic Helpful Harmless and PKU Beavertails), REFORM demonstrated enhanced robustness and alignment without degrading performance quality.

Conclusion: REFORM improves reward model robustness under various conditions while maintaining high-quality rewards and eliminating spurious correlations.

Abstract: Reward modeling (RM), which captures human preferences to align large
language models (LLMs), is increasingly employed in tasks such as model
finetuning, response filtering, and ranking. However, due to the inherent
complexity of human preferences and the limited coverage of available datasets,
reward models often fail under distributional shifts or adversarial
perturbations. Existing approaches for identifying such failure modes typically
rely on prior knowledge about preference distributions or failure attributes,
limiting their practicality in real-world settings where such information is
unavailable. In this work, we propose a tractable, preference-distribution
agnostic method for discovering reward model failure modes via reward guided
controlled decoding. Building on this, we introduce REFORM, a self-improving
reward modeling framework that enhances robustness by using the reward model
itself to guide the generation of falsely scored responses. These adversarial
examples are then used to augment the training data and patch the reward
model's misaligned behavior. We evaluate REFORM on two widely used preference
datasets Anthropic Helpful Harmless (HH) and PKU Beavertails and demonstrate
that it significantly improves robustness without sacrificing reward quality.
Notably, REFORM preserves performance both in direct evaluation and in
downstream policy training, and further improves alignment quality by removing
spurious correlations.

</details>


### [20] [Exploring Task Performance with Interpretable Models via Sparse Auto-Encoders](https://arxiv.org/abs/2507.06427)
*Shun Wang,Tyler Loakman,Youbo Lei,Yi Liu,Bohao Yang,Yuting Zhao,Dong Yang,Chenghua Lin*

Main category: cs.CL

TL;DR: The paper introduces a method to break down LLMs using sparse autoencoders to improve understanding and performance on tasks like reasoning and metaphor detection.


<details>
  <summary>Details</summary>
Motivation: To address trustworthiness issues and improve LLM performance by better interpreting internal mechanisms.

Method: Utilizes sparse autoencoders for dictionary-learning to extract monosemantic features from polysemantic neurons in LLMs.

Result: Automated prompt reformulation enhances LLM interpretation, leading to substantial gains in tasks like mathematical reasoning and metaphor detection.

Conclusion: The decomposition method improves both the interpretability and downstream task performance of LLMs through better prompt handling and model insights.

Abstract: Large Language Models (LLMs) are traditionally viewed as black-box
algorithms, therefore reducing trustworthiness and obscuring potential
approaches to increasing performance on downstream tasks. In this work, we
apply an effective LLM decomposition method using a dictionary-learning
approach with sparse autoencoders. This helps extract monosemantic features
from polysemantic LLM neurons. Remarkably, our work identifies model-internal
misunderstanding, allowing the automatic reformulation of the prompts with
additional annotations to improve the interpretation by LLMs. Moreover, this
approach demonstrates a significant performance improvement in downstream
tasks, such as mathematical reasoning and metaphor detection.

</details>


### [21] [Temporal Analysis of Climate Policy Discourse: Insights from Dynamic Embedded Topic Modeling](https://arxiv.org/abs/2507.06435)
*Rafiu Adekoya Badekale,Adewale Akinfaderin*

Main category: cs.CL

TL;DR: The paper applies a dynamic embedded topic model (DETM) to analyze how global climate policy discourse evolves over time based on UNFCCC policy decisions from 1995 to 2023, revealing shifts in thematic focus.


<details>
  <summary>Details</summary>
Motivation: Understanding temporal shifts in policy language is critical for dissecting global responses to challenges like climate change. Traditional manual analysis techniques are limited, necessitating more scalable, ML-based approaches.

Method: The authors use the DETM, a probabilistic machine-learning tool designed to track the temporal dynamics of topics, applied to preprocess, model, and visualize UNFCCC data spanning 1995-2023.

Result: DETM successfully captures shifts in topic focus, highlighting movement from greenhouse gases and conventions to implementation, technical collaboration, finance, and capacity building in recent years.

Conclusion: Dynamic machine learning models, such as DETM, are scalable and effective tools for analyzing policy evolution, with potential applications to other domains.

Abstract: Understanding how policy language evolves over time is critical for assessing
global responses to complex challenges such as climate change. Temporal
analysis helps stakeholders, including policymakers and researchers, to
evaluate past priorities, identify emerging themes, design governance
strategies, and develop mitigation measures. Traditional approaches, such as
manual thematic coding, are time-consuming and limited in capturing the
complex, interconnected nature of global policy discourse. With the increasing
relevance of unsupervised machine learning, these limitations can be addressed,
particularly under high-volume, complex, and high-dimensional data conditions.
In this work, we explore a novel approach that applies the dynamic embedded
topic model (DETM) to analyze the evolution of global climate policy discourse.
A probabilistic model designed to capture the temporal dynamics of topics over
time. We collected a corpus of United Nations Framework Convention on Climate
Change (UNFCCC) policy decisions from 1995 to 2023, excluding 2020 due to the
postponement of COP26 as a result of the COVID-19 pandemic. The model reveals
shifts from early emphases on greenhouse gases and international conventions to
recent focuses on implementation, technical collaboration, capacity building,
finance, and global agreements. Section 3 presents the modeling pipeline,
including preprocessing, model training, and visualization of temporal word
distributions. Our results show that DETM is a scalable and effective tool for
analyzing the evolution of global policy discourse. Section 4 discusses the
implications of these findings and we concluded with future directions and
refinements to extend this approach to other policy domains.

</details>


### [22] [Perception-Aware Policy Optimization for Multimodal Reasoning](https://arxiv.org/abs/2507.06448)
*Zhenhailong Wang,Xuehang Guo,Sofia Stoica,Haiyang Xu,Hongru Wang,Hyeonjeong Ha,Xiusi Chen,Yangyi Chen,Ming Yan,Fei Huang,Heng Ji*

Main category: cs.CL

TL;DR: This paper presents PAPO, an extension of GRPO designed to improve multimodal reasoning tasks for LLMs by addressing perception errors, with demonstrated performance improvements and reduced perception failures.


<details>
  <summary>Details</summary>
Motivation: The need to address suboptimal performance of RLVR methods in multimodal reasoning tasks, especially issues arising from visual perception.

Method: PAPO introduces an Implicit Perception Loss via KL divergence into RLVR, enhancing perceptual learning without relying on external data or proprietary models.

Result: PAPO yields a 4.4% improvement in multimodal benchmarks and an 8.0% improvement in tasks with high vision dependency, along with a significant 30.5% reduction in perception errors.

Conclusion: The integration of perception-aware supervision into RLVR learning enhances visually grounded reasoning capabilities and provides a new methodology for multimodal reasoning tasks.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a
highly effective strategy for endowing Large Language Models (LLMs) with robust
multi-step reasoning abilities. However, its design and optimizations remain
tailored to purely textual domains, resulting in suboptimal performance when
applied to multimodal reasoning tasks. In particular, we observe that a major
source of error in current multimodal reasoning lies in the perception of
visual inputs. To address this bottleneck, we propose Perception-Aware Policy
Optimization (PAPO), a simple yet effective extension of GRPO that encourages
the model to learn to perceive while learning to reason, entirely from internal
supervision signals. Notably, PAPO does not rely on additional data curation,
external reward models, or proprietary models. Specifically, we introduce the
Implicit Perception Loss in the form of a KL divergence term to the GRPO
objective, which, despite its simplicity, yields significant overall
improvements (4.4%) on diverse multimodal benchmarks. The improvements are more
pronounced, approaching 8.0%, on tasks with high vision dependency. We also
observe a substantial reduction (30.5%) in perception errors, indicating
improved perceptual capabilities with PAPO. We conduct comprehensive analysis
of PAPO and identify a unique loss hacking issue, which we rigorously analyze
and mitigate through a Double Entropy Loss. Overall, our work introduces a
deeper integration of perception-aware supervision into RLVR learning
objectives and lays the groundwork for a new RL framework that encourages
visually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.

</details>


### [23] [A Semantic Parsing Framework for End-to-End Time Normalization](https://arxiv.org/abs/2507.06450)
*Xin Su,Sungduk Yu,Phillip Howard,Steven Bethard*

Main category: cs.CL

TL;DR: Time normalization converts language temporal expressions into machine-readable forms. This paper proposes a new approach using the SCATE framework and large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Existing methods for time normalization struggle with complex temporal expressions and are limited by traditional schemas like ISO-TimeML.

Method: The authors propose a code generation approach based on the SCATE framework, implement a Python library, and use LLMs to create annotated data for model training.

Result: Experiments show that small models trained using the augmented data outperform even the LLMs used for data generation.

Conclusion: The new approach enables accurate, interpretable, and practical time normalization, advancing capabilities for downstream applications.

Abstract: Time normalization is the task of converting natural language temporal
expressions into machine-readable representations. It underpins many downstream
applications in information retrieval, question answering, and clinical
decision-making. Traditional systems based on the ISO-TimeML schema limit
expressivity and struggle with complex constructs such as compositional,
event-relative, and multi-span time expressions. In this work, we introduce a
novel formulation of time normalization as a code generation task grounded in
the SCATE framework, which defines temporal semantics through symbolic and
compositional operators. We implement a fully executable SCATE Python library
and demonstrate that large language models (LLMs) can generate executable SCATE
code. Leveraging this capability, we develop an automatic data augmentation
pipeline using LLMs to synthesize large-scale annotated data with code-level
validation. Our experiments show that small, locally deployable models trained
on this augmented data can achieve strong performance, outperforming even their
LLM parents and enabling practical, accurate, and interpretable time
normalization.

</details>


### [24] [A Systematic Analysis of Hybrid Linear Attention](https://arxiv.org/abs/2507.06457)
*Dustin Wang,Rui-Jie Zhu,Steven Abreu,Yong Shan,Taylor Kergan,Yuqi Pan,Yuhong Chou,Zheng Li,Ge Zhang,Wenhao Huang,Jason Eshraghian*

Main category: cs.CL

TL;DR: This paper evaluates various linear attention models in standalone and hybrid architectures to address Transformers' inefficiencies with long sequences. It provides insights into linear-to-full attention ratios and optimal architectures for balanced recall and language modeling.


<details>
  <summary>Details</summary>
Motivation: Transformers face computational challenges with long sequences due to quadratic complexity and memory demands, prompting research into linear attention mechanisms for efficiency.

Method: The authors systematically evaluate six linear attention variants and five hybrid ratios by training 72 models (340M and 1.3B parameters) on large-scale token datasets and benchmarking them on language modeling and recall tasks.

Result: Results show that while language modeling performance remains stable across hybrid configurations, recall improves significantly with lower linear-to-full attention ratios, particularly below a 3:1 ratio. Superior standalone linear models don't always excel in hybrids.

Conclusion: Selective gating, hierarchical recurrence, and controlled forgetting are identified as key features for effective hybrid models. Recommended architectures include HGRN-2 and GatedDeltaNet with linear-to-full ratios between 3:1 and 6:1 for optimal performance.

Abstract: Transformers face quadratic complexity and memory issues with long sequences,
prompting the adoption of linear attention mechanisms using fixed-size hidden
states. However, linear models often suffer from limited recall performance,
leading to hybrid architectures that combine linear and full attention layers.
Despite extensive hybrid architecture research, the choice of linear attention
component has not been deeply explored. We systematically evaluate various
linear attention models across generations - vector recurrences to advanced
gating mechanisms - both standalone and hybridized. To enable this
comprehensive analysis, we trained and open-sourced 72 models: 36 at 340M
parameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six
linear attention variants across five hybridization ratios. Benchmarking on
standard language modeling and recall tasks reveals that superior standalone
linear models do not necessarily excel in hybrids. While language modeling
remains stable across linear-to-full attention ratios, recall significantly
improves with increased full attention layers, particularly below a 3:1 ratio.
Our study highlights selective gating, hierarchical recurrence, and controlled
forgetting as critical for effective hybrid models. We recommend architectures
such as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1
to achieve Transformer-level recall efficiently. Our models are open-sourced at
https://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e.

</details>


### [25] [On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks](https://arxiv.org/abs/2507.06489)
*Stephen Obadinma,Xiaodan Zhu*

Main category: cs.CL

TL;DR: This study evaluates the robustness of verbal confidence in large language models (LLMs) and identifies vulnerabilities under various adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the necessity of robust verbal confidence in LLMs to ensure trust, transparency, and safety in human-AI interactions, particularly in high-stakes settings.

Method: The researchers employ a novel adversarial attack framework, including perturbation and jailbreak-based methods, to test confidence reliability. They analyze different prompting strategies, model sizes, and application domains.

Result: The study finds that verbal confidence scores in LLMs are vulnerable to adversarial attacks, causing frequent inaccuracies in confidence expressions and response changes. Existing defense techniques are inadequate.

Conclusion: Current methods for verbal confidence elicitation are fragile, necessitating the development of more robust mechanisms to ensure reliability and trust in LLM-generated confidence statements.

Abstract: Robust verbal confidence generated by large language models (LLMs) is crucial
for the deployment of LLMs to ensure transparency, trust, and safety in
human-AI interactions across many high-stakes applications. In this paper, we
present the first comprehensive study on the robustness of verbal confidence
under adversarial attacks. We introduce a novel framework for attacking verbal
confidence scores through both perturbation and jailbreak-based methods, and
show that these attacks can significantly jeopardize verbal confidence
estimates and lead to frequent answer changes. We examine a variety of
prompting strategies, model sizes, and application domains, revealing that
current confidence elicitation methods are vulnerable and that commonly used
defence techniques are largely ineffective or counterproductive. Our findings
underscore the urgent need to design more robust mechanisms for confidence
expression in LLMs, as even subtle semantic-preserving modifications can lead
to misleading confidence in responses.

</details>


### [26] [Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning and Phonetic-Semantic Embeddings](https://arxiv.org/abs/2507.06506)
*Russell Taylor,Benjamin Herbert,Michael Sana*

Main category: cs.CL

TL;DR: This research presents a novel three-stage methodology for translating English puns into French, leveraging advanced language models for creative wordplay.


<details>
  <summary>Details</summary>
Motivation: Challenges in translating wordplay across languages, particularly in capturing linguistic creativity and humor.

Method: The paper uses a three-stage approach: baseline establishment with contrastive learning, guided chain-of-thought pipeline, and a multi-agent generator-discriminator framework.

Result: The methodology earned top positions in the CLEF JOKER 2025 Task 2 competition, evaluated by native French experts.

Conclusion: The study bridges translation studies and computational linguistics, offering linguistically-informed techniques for handling semantic ambiguity and phonetic similarity in humorous puns.

Abstract: Translating wordplay across languages presents unique challenges that have
long confounded both professional human translators and machine translation
systems. This research proposes a novel approach for translating puns from
English to French by combining state-of-the-art large language models with
specialized techniques for wordplay generation.
  Our methodology employs a three-stage approach. First, we establish a
baseline using multiple frontier large language models with feedback based on a
new contrastive learning dataset. Second, we implement a guided
chain-of-thought pipeline with combined phonetic-semantic embeddings. Third, we
implement a multi-agent generator-discriminator framework for evaluating and
regenerating puns with feedback.
  Moving beyond the limitations of literal translation, our methodology's
primary objective is to capture the linguistic creativity and humor of the
source text wordplay, rather than simply duplicating its vocabulary. Our best
runs earned first and second place in the CLEF JOKER 2025 Task 2 competition
where they were evaluated manually by expert native French speakers.
  This research addresses a gap between translation studies and computational
linguistics by implementing linguistically-informed techniques for wordplay
translation, advancing our understanding of how language models can be
leveraged to handle the complex interplay between semantic ambiguity, phonetic
similarity, and the implicit cultural and linguistic awareness needed for
successful humor.

</details>


### [27] [SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers](https://arxiv.org/abs/2507.06517)
*Zicong Tang,Shi Luohe,Zuchao Li,Baoyuan Qi,Guoming Liu,Lefei Zhang,Ping Wang*

Main category: cs.CL

TL;DR: The paper introduces SpindleKV, a novel KV cache reduction method improving shallow and deep layer efficiency for memory reduction in LLMs, while maintaining or enhancing performance.


<details>
  <summary>Details</summary>
Motivation: To address the increasing memory consumption challenges posed by KV cache during LLM inference and improve reduction efficiency, particularly in shallower layers.

Method: The authors developed SpindleKV using attention-weight eviction for deep layers and a codebook-based similarity-merging policy for shallow layers. It also tackles the Grouped-Query Attention (GQA) issue.

Result: Experiments on two benchmarks with three LLMs showed SpindleKV achieved superior KV cache reduction compared to baseline methods, preserving or enhancing model performance.

Conclusion: SpindleKV balances memory reduction across shallow and deep layers, overcoming GQA problems, and demonstrating practical effectiveness while maintaining model quality.

Abstract: Large Language Models (LLMs) have achieved impressive accomplishments in
recent years. However, the increasing memory consumption of KV cache has
possessed a significant challenge to the inference system. Eviction methods
have revealed the inherent redundancy within the KV cache, demonstrating its
potential for reduction, particularly in deeper layers. However, KV cache
reduction for shallower layers has been found to be insufficient. Based on our
observation that, the KV cache exhibits a high degree of similarity. Based on
this observation, we proposed a novel KV cache reduction method, SpindleKV,
which balances both shallow and deep layers. For deep layers, we employ an
attention weight based eviction method, while for shallow layers, we apply a
codebook based replacement approach which is learnt by similarity and merging
policy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma
faced by other attention based eviction methods. Experiments on two common
benchmarks with three different LLMs shown that SpindleKV obtained better KV
cache reduction effect compared to baseline methods, while preserving similar
or even better model performance.

</details>


### [28] [InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior](https://arxiv.org/abs/2507.06528)
*Huisheng Wang,Zhuoshi Pan,Hangjing Zhang,Mingxiao Liu,Hanqing Gao,H. Vicky Zhao*

Main category: cs.CL

TL;DR: The paper introduces InvestAlign, a framework for training LLMs to mimic human investor behavior in herd scenarios using theoretical datasets instead of costly real-user data. The resulting agent, InvestAgent, aligns better with real behaviors.


<details>
  <summary>Details</summary>
Motivation: The scarcity and privacy risks of real-user data hamper the use of supervised fine-tuning (SFT) to align LLMs with human decision-making in behavioral finance, especially under herd behavior.

Method: The authors construct high-quality SFT datasets based on theoretical solutions to simple investment problems, train LLMs with these datasets, and build InvestAgent to test alignment with real-user data.

Result: LLMs trained with InvestAlign datasets achieve faster parameter convergence and better alignment to human behavior in both simple and complex investment scenarios compared to pre-SFT models.

Conclusion: InvestAlign is a promising framework for training LLMs to tackle complex investment problems and align with human decision-making processes efficiently, without relying on expensive and private real-user data.

Abstract: Aligning Large Language Models (LLMs) with investor decision-making processes
under herd behavior is a critical challenge in behavioral finance, which
grapples with a fundamental limitation: the scarcity of real-user data needed
for Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM
outputs and human behavioral patterns, its reliance on massive authentic data
imposes substantial collection costs and privacy risks. We propose InvestAlign,
a novel framework that constructs high-quality SFT datasets by leveraging
theoretical solutions to similar and simple optimal investment problems rather
than complex scenarios. Our theoretical analysis demonstrates that training
LLMs with InvestAlign-generated data achieves faster parameter convergence than
using real-user data, suggesting superior learning efficiency. Furthermore, we
develop InvestAgent, an LLM agent fine-tuned with InvestAlign, which
demonstrates significantly closer alignment to real-user data than pre-SFT
models in both simple and complex investment problems. This highlights our
proposed InvestAlign as a promising approach with the potential to address
complex optimal investment problems and align LLMs with investor
decision-making processes under herd behavior. Our code is publicly available
at https://github.com/thu-social-network-research-group/InvestAlign.

</details>


### [29] [Large Language Model for Extracting Complex Contract Information in Industrial Scenes](https://arxiv.org/abs/2507.06539)
*Yunyang Cao,Yanjun Li,Silong Dai*

Main category: cs.CL

TL;DR: The paper presents a method to improve contract information extraction using a specialized dataset and GPT models, leading to enhanced performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in extracting complex contract information in industrial settings and improve model robustness and accuracy.

Method: The approach includes clustering contract texts, leveraging GPT-4 and GPT-3.5 for data annotation, generating unstructured text for data augmentation, and fine-tuning a large language model using the improved dataset.

Result: The method achieved high recall and precision while maintaining parsing efficiency, with techniques like LoRA and data augmentation contributing to enhanced model accuracy and robustness.

Conclusion: The proposed method offers an effective solution for industrial contract information extraction, demonstrating practical applicability and improved model performance.

Abstract: This paper proposes a high-quality dataset construction method for complex
contract information extraction tasks in industrial scenarios and fine-tunes a
large language model based on this dataset. Firstly, cluster analysis is
performed on industrial contract texts, and GPT-4 and GPT-3.5 are used to
extract key information from the original contract data, obtaining high-quality
data annotations. Secondly, data augmentation is achieved by constructing new
texts, and GPT-3.5 generates unstructured contract texts from randomly combined
keywords, improving model robustness. Finally, the large language model is
fine-tuned based on the high-quality dataset. Experimental results show that
the model achieves excellent overall performance while ensuring high field
recall and precision and considering parsing efficiency. LoRA, data balancing,
and data augmentation effectively enhance model accuracy and robustness. The
proposed method provides a novel and efficient solution for industrial contract
information extraction tasks.

</details>


### [30] [The Flaws of Others: An LLM-driven Framework for Scientific Knowledge Production](https://arxiv.org/abs/2507.06565)
*Juan B. Gutiérrez*

Main category: cs.CL

TL;DR: The paper presents a model focusing on discursive exchanges between humans and large language models (LLMs). It aims to address invalidations caused by error types like drift, self-repair, fabrication, and detection. It proposes the FOO algorithm for peer review as a way to stabilize networks into a truth-dominant state.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by the interactions between humans and large language models (LLMs), such as factual inaccuracies and logical breaches, while leveraging this new medium for reliable discourse.

Method: The authors developed a discursive-network model to study invalidations and proposed the mathematical framework along with the 'Flaws-of-Others (FOO) algorithm' to operationalize peer review between agents and merge critiques.

Result: The model demonstrates stabilization with modest errors when drift and self-repair are considered. However, fabrication leads to high error rates comparable to current observations. Peer review processes improve reliability by shifting the system to truth-dominant states.

Conclusion: Reliability in human-LLM interactions depends not on perfecting individual models but on creating interconnected systems that critique and stabilize each other's output, fostering cultural shifts in understanding of truth in this new medium.

Abstract: Large-language models turn writing into a live exchange between humans and
software. We capture this new medium with a discursive-network model that
treats people and LLMs as equal nodes and tracks how their statements
circulate. Broadening the focus from isolated hallucinations, we define
invalidation (any factual, logical, or structural breach) and show it follows
four hazards: drift from truth, self-repair, fresh fabrication, and external
detection. A general mathematical model of discursive networks is developed to
provide valuable insights: A network governed only by drift and self-repair
stabilizes at a modest error rate; adding fabrication reproduces the high rates
seen in current LLMs. Giving each false claim even a small chance of peer
review shifts the system to a truth-dominant state. We operationalize peer
review with the open-source \emph{Flaws-of-Others (FOO) algorithm}: a
configurable loop in which any set of agents critique one another while a
harmoniser merges their verdicts. The takeaway is practical and cultural:
reliability in this new medium comes not from perfecting single models but from
wiring imperfect ones into networks that keep each other honest.

</details>


### [31] [Enhancing Food-Domain Question Answering with a Multimodal Knowledge Graph: Hybrid QA Generation and Diversity Analysis](https://arxiv.org/abs/2507.06571)
*Srihari K B,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: This paper proposes a novel QA framework combining a multimodal knowledge graph (MMKG) and generative AI for the food domain, showcasing substantial improvements in reliability and diversity.


<details>
  <summary>Details</summary>
Motivation: Enhancing food-domain QA by integrating structured knowledge and generative AI to address factual and visual accuracy challenges.

Method: A unified MMKG linking 13,000 recipes, 3,000 ingredients, 140,000 relationships, and 14,000 images, coupled with AI-generated QA pairs and joint fine-tuning of LLaMA 3.1-8B and Stable Diffusion 3.5.

Result: Improvements in BERTScore (+16.2%), reduced FID (-37.8%), better CLIP alignment (+31.1%), and high accuracy in image reuse (94.1%) and synthesis adequacy (85%).

Conclusion: Combining structured knowledge with multimodal generative AI enhances both factual accuracy and diversity in food-domain QA applications.

Abstract: We propose a unified food-domain QA framework that combines a large-scale
multimodal knowledge graph (MMKG) with generative AI. Our MMKG links 13,000
recipes, 3,000 ingredients, 140,000 relations, and 14,000 images. We generate
40,000 QA pairs using 40 templates and LLaVA/DeepSeek augmentation. Joint
fine-tuning of Meta LLaMA 3.1-8B and Stable Diffusion 3.5-Large improves
BERTScore by 16.2\%, reduces FID by 37.8\%, and boosts CLIP alignment by
31.1\%. Diagnostic analyses-CLIP-based mismatch detection (35.2\% to 7.3\%) and
LLaVA-driven hallucination checks-ensure factual and visual fidelity. A hybrid
retrieval-generation strategy achieves 94.1\% accurate image reuse and 85\%
adequacy in synthesis. Our results demonstrate that structured knowledge and
multimodal generation together enhance reliability and diversity in food QA.

</details>


### [32] [Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation](https://arxiv.org/abs/2507.06607)
*Liliang Ren,Congcong Chen,Haoran Xu,Young Jin Kim,Adam Atkinson,Zheng Zhan,Jiankai Sun,Baolin Peng,Liyuan Liu,Shuohang Wang,Hao Cheng,Jianfeng Gao,Weizhu Chen,Yelong Shen*

Main category: cs.CL

TL;DR: This paper introduces the Gated Memory Unit (GMU) for efficient memory sharing in State Space Models (SSMs), leading to performance gains in decoder architectures while improving efficiency and scalability.


<details>
  <summary>Details</summary>
Motivation: The authors aim to explore the efficiency potential of memory representation sharing in SSM layers, which had not been investigated in prior works.

Method: The paper presents the Gated Memory Unit (GMU) for cross-layer memory sharing and applies it in a novel architecture, SambaY, to improve decoding efficiency and performance without explicit positional encoding.

Result: SambaY achieves superior scalability, better performance on reasoning tasks, and significantly enhanced decoding throughput compared to strong baselines.

Conclusion: Through the introduction of GMU and rigorous testing, the paper demonstrates significant improvements in efficiency, scalability, and task performance, contributing to advancements in efficient sequence modeling.

Abstract: Recent advances in language modeling have demonstrated the effectiveness of
State Space Models (SSMs) for efficient sequence modeling. While hybrid
architectures such as Samba and the decoder-decoder architecture, YOCO, have
shown promising performance gains over Transformers, prior works have not
investigated the efficiency potential of representation sharing between SSM
layers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet
effective mechanism for efficient memory sharing across layers. We apply it to
create SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in
the cross-decoder to share memory readout states from a Samba-based
self-decoder. SambaY significantly enhances decoding efficiency, preserves
linear pre-filling time complexity, and boosts long-context performance, all
while eliminating the need for explicit positional encoding. Through extensive
scaling experiments, we demonstrate that our model exhibits a significantly
lower irreducible loss compared to a strong YOCO baseline, indicating superior
performance scalability under large-scale compute regimes. Our largest model
enhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves
significantly better performance than Phi4-mini-Reasoning on reasoning tasks
such as Math500, AIME24/25, and GPQA Diamond without any reinforcement
learning, while delivering up to 10x higher decoding throughput on 2K-length
prompts with 32K generation length under the vLLM inference framework. We
release our training codebase on open-source data at
https://github.com/microsoft/ArchScale.

</details>


### [33] [FuDoBa: Fusing Document and Knowledge Graph-based Representations with Bayesian Optimisation](https://arxiv.org/abs/2507.06622)
*Boshko Koloski,Senja Pollak,Roberto Navigli,Blaž Škrlj*

Main category: cs.CL

TL;DR: FuDoBa improves LLM-based embeddings by integrating domain-specific structured knowledge, yielding low-dimensional, efficient representations.


<details>
  <summary>Details</summary>
Motivation: LLM embeddings are computationally expensive and often too generic for domain-specific applications.

Method: A Bayesian optimization-based approach, combining LLM embeddings with domain-specific structured knowledge to create low-dimensional and interpretable representations.

Result: Across six datasets, FuDoBa matches or surpasses proprietary LLM-based embeddings in performance, especially when paired with AutoML-based classifiers.

Conclusion: FuDoBa offers efficient and interpretable embeddings suitable for domain-specific tasks, addressing critical limitations of generic LLM embeddings.

Abstract: Building on the success of Large Language Models (LLMs), LLM-based
representations have dominated the document representation landscape, achieving
great performance on the document embedding benchmarks. However, the
high-dimensional, computationally expensive embeddings from LLMs tend to be
either too generic or inefficient for domain-specific applications. To address
these limitations, we introduce FuDoBa a Bayesian optimisation-based method
that integrates LLM-based embeddings with domain-specific structured knowledge,
sourced both locally and from external repositories like WikiData. This fusion
produces low-dimensional, task-relevant representations while reducing training
complexity and yielding interpretable early-fusion weights for enhanced
classification performance. We demonstrate the effectiveness of our approach on
six datasets in two domains, showing that when paired with robust AutoML-based
classifiers, our proposed representation learning approach performs on par
with, or surpasses, those produced solely by the proprietary LLM-based
embedding baselines.

</details>


### [34] [Discrete Diffusion Models for Language Generation](https://arxiv.org/abs/2507.07050)
*Ashen Weligalle*

Main category: cs.CL

TL;DR: This paper explores discrete diffusion models for natural language generation, comparing them to traditional autoregressive models and analyzing trade-offs in quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to investigate whether diffusion models, which excel in continuous data domains like images, can effectively generate discrete sequences such as natural language, overcoming challenges like token dependency and undefined generation order.

Method: The study evaluates the Discrete Denoising Diffusion Probabilistic Model (D3PM) against autoregressive models using metrics like Bits Per Token, Negative Log-Likelihood, Perplexity, and Batch Processing Speed, with 100,000 token generations for each model under consistent conditions.

Result: The D3PM model achieved a Bits Per Token score of 5.72, with better parallel processing speed of 3.97 batches per second. Meanwhile, autoregressive models showed superior compression with a lower mean BPT of 4.59.

Conclusion: While diffusion models demonstrate promise in parallel generation and efficiency, they currently have limitations in generative quality compared to autoregressive models, warranting further exploration in this area.

Abstract: Diffusion models have emerged as a powerful class of generative models,
achieving state-of-the-art results in continuous data domains such as image and
video generation. Their core mechanism involves a forward diffusion process
that gradually transforms structured data into a Gaussian-like distribution,
followed by a learned reverse process to reconstruct the data. While successful
in continuous modalities, applying this framework to discrete data-particularly
natural language-remains challenging due to token dependency complexities and
the lack of a defined generation order.This thesis investigates the feasibility
and performance of discrete diffusion models for natural language generation.
Specifically, we evaluate the Discrete Denoising Diffusion Probabilistic Model
(D3PM) and compare it with traditional autoregressive (AR) language models. To
assess generative performance, we use Bits Per Token (BPT), Negative
Log-Likelihood (NLL), Perplexity (PPL), and Batch Processing Speed.
  Results show the best-performing D3PM model achieves a BPT of 5.72, with a
mean of 8.05. The AR model outperforms in compression with a lower mean BPT of
4.59, but D3PM achieves higher processing speed, reaching up to 3.97 batches
per sec., indicating potential for parallel generation.All evaluations were
conducted under consistent conditions-generating 100,000 tokens per model with
a fixed batch size of four-for fair comparison. This research presents a
detailed analysis of diffusion-based vs. autoregressive models, highlighting
trade-offs in generative quality and efficiency. Findings emphasize both the
promise and limitations of diffusion models for discrete data, supporting
future work in non-autoregressive language generation.

</details>


### [35] [Expediting data extraction using a large language model (LLM) and scoping review protocol: a methodological study within a complex scoping review](https://arxiv.org/abs/2507.06623)
*James Stewart-Evans,Emma Wilson,Tessa Langley,Andrew Prayle,Angela Hands,Karen Exley,Jo Leonardi-Bee*

Main category: cs.CL

TL;DR: This study evaluates the use of the Claude 3.5 Sonnet LLM for data extraction in scoping reviews. While LLMs demonstrate high precision for straightforward data, they struggle with complex or subjective data. The authors recommend further evaluation of LLM-based methods.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of large language models (LLMs) for expediting the data extraction stages of reviews, which are traditionally resource-intensive processes.

Method: Two protocol-driven approaches were trialed using Claude 3.5 Sonnet on 10 evidence sources from a case study scoping review. Limited performance evaluation was carried out to assess accuracy, precision, recall, and F1 scores.

Result: High accuracy (83.3% and 100%) was observed for simple data extraction, but performance dropped for complex or subjective items (accuracy: 9.6% and 15.8%; recall: <25%; F1: <40%). Limited error detection was noted with a dataset featuring deliberate errors.

Conclusion: LLM-based methods show promise for simple data extraction but require enhanced evaluation across diverse contexts. Researchers are encouraged to assess and report LLM performance and adapt protocols based on LLM feedback for future reviews.

Abstract: The data extraction stages of reviews are resource-intensive, and researchers
may seek to expediate data extraction using online (large language models) LLMs
and review protocols. Claude 3.5 Sonnet was used to trial two approaches that
used a review protocol to prompt data extraction from 10 evidence sources
included in a case study scoping review. A protocol-based approach was also
used to review extracted data. Limited performance evaluation was undertaken
which found high accuracy for the two extraction approaches (83.3% and 100%)
when extracting simple, well-defined citation details; accuracy was lower (9.6%
and 15.8%) when extracting more complex, subjective data items. Considering all
data items, both approaches had precision >90% but low recall (<25%) and F1
scores (<40%). The context of a complex scoping review, open response types and
methodological approach likely impacted performance due to missed and
misattributed data. LLM feedback considered the baseline extraction accurate
and suggested minor amendments: four of 15 (26.7%) to citation details and 8 of
38 (21.1%) to key findings data items were considered to potentially add value.
However, when repeating the process with a dataset featuring deliberate errors,
only 2 of 39 (5%) errors were detected. Review-protocol-based methods used for
expediency require more robust performance evaluation across a range of LLMs
and review contexts with comparison to conventional prompt engineering
approaches. We recommend researchers evaluate and report LLM performance if
using them similarly to conduct data extraction or review extracted data. LLM
feedback contributed to protocol adaptation and may assist future review
protocol drafting.

</details>


### [36] [Elite Polarization in European Parliamentary Speeches: a Novel Measurement Approach Using Large Language Models](https://arxiv.org/abs/2507.06658)
*Gennadii Iakovlev*

Main category: cs.CL

TL;DR: This paper develops a novel AI-based metric to measure political elite polarization by analyzing interactions and emotional tone in parliamentary speeches.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for an accurate and dynamic measure of political elite polarization, which is crucial for understanding political dynamics and crises.

Method: The author employed AI tools for actor and subject detection in parliamentary speeches, identifying interactions, emotional tone, and aggregating polarization metrics into an index over decades.

Result: The polarization index exhibits strong validity by correlating with political events such as elections, crises, and power shifts in the UK, Hungary, and Italy.

Conclusion: The approach not only maps elite polarization effectively for different regions and periods but also paves the way for broader EU-wide studies with long-term datasets.

Abstract: This project introduces a new measure of elite polarization via actor and
subject detection using artificial intelligence. I identify when politicians
mention one another in parliamentary speeches, note who is speaking and who is
being addressed, and assess the emotional temperature behind these evaluations.
This maps how elites evaluate their various out-parties, allowing us to create
an index of mutual out-party hostility, that is, elite polarization. While I
analyzed polarization data over the past four decades for the UK, and two
decades for Hungary and Italy, my approach lays the groundwork for a
twenty-year, EU-wide time-series dataset on elite polarization. I obtain the
results that can be aggregated by party and quarter. The resulting index
demonstrates a good face validity: it reacts to events such as electoral
campaigns, country- and party-level crises, and to parties losing and assuming
power.

</details>


### [37] [CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and Context Aware Text Generation with LLMs](https://arxiv.org/abs/2507.06715)
*Garapati Keerthana,Manik Gupta*

Main category: cs.CL

TL;DR: This paper addresses challenges in using large language models (LLMs) for clinical text generation by developing a method called CLI-RAG for generating structured, clinically-grounded text using retrieval-augmented generation.


<details>
  <summary>Details</summary>
Motivation: Clinical text generation poses challenges due to unstructured, heterogeneous patient data and the semantic density of long clinical notes. LLMs need novel strategies to overcome limitations like context length and maintaining clinically relevant information.

Method: The authors propose CLI-RAG, a domain-specific framework employing hierarchical chunking and a dual-stage retrieval mechanism. This method involves identifying relevant document types and extracting high-value sections within clinical notes from the MIMIC-III dataset.

Result: CLI-RAG generates structured progress notes with improved temporal and semantic alignment, achieving an alignment score of 87.7%, exceeding the baseline of clinician-authored notes (80.7%). Outputs show consistency across LLMs for enhanced reproducibility and reliability.

Conclusion: CLI-RAG provides an effective framework for structured clinical text generation, addressing real-world challenges like scattered patient data and long, dense notes. This advancement supports reliable applications in clinical settings.

Abstract: Large language models (LLMs), including zero-shot and few-shot paradigms,
have shown promising capabilities in clinical text generation. However,
real-world applications face two key challenges: (1) patient data is highly
unstructured, heterogeneous, and scattered across multiple note types and (2)
clinical notes are often long and semantically dense, making naive prompting
infeasible due to context length constraints and the risk of omitting
clinically relevant information.
  We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a
domain-specific framework for structured and clinically grounded text
generation using LLMs. It incorporates a novel hierarchical chunking strategy
that respects clinical document structure and introduces a task-specific
dual-stage retrieval mechanism. The global stage identifies relevant note types
using evidence-based queries, while the local stage extracts high-value content
within those notes creating relevance at both document and section levels.
  We apply the system to generate structured progress notes for individual
hospital visits using 15 clinical note types from the MIMIC-III dataset.
Experiments show that it preserves temporal and semantic alignment across
visits, achieving an average alignment score of 87.7%, surpassing the 80.7%
baseline from real clinician-authored notes. The generated outputs also
demonstrate high consistency across LLMs, reinforcing deterministic behavior
essential for reproducibility, reliability, and clinical trust.

</details>


### [38] [On the Effect of Uncertainty on Layer-wise Inference Dynamics](https://arxiv.org/abs/2507.06722)
*Sunwoo Kim,Haneul Yoo,Alice Oh*

Main category: cs.CL

TL;DR: This study finds that large language models’ (LLMs) output token probability dynamics largely remain aligned for both certain and uncertain predictions, questioning simplistic uncertainty detection during inference.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand how LLMs represent and process uncertainty internally, as this influences their predictions and the potential for hallucinations.

Method: The researchers used a variant of the Logit Lens, called the Tuned Lens, to analyze the layer-wise probability trajectories of final prediction tokens across 11 datasets and 5 models, focusing on incorrect predictions reflecting epistemic uncertainty.

Result: Their analysis revealed aligned probability dynamics for both certain and uncertain outputs, with confidence increases occurring abruptly at similar layers. Competent models were observed to process uncertainty differently.

Conclusion: The study concludes that simplistic methods may not suffice for detecting uncertainty at inference time, and interpretability techniques can be a valuable tool for understanding how uncertainty impacts inference processes.

Abstract: Understanding how large language models (LLMs) internally represent and
process their predictions is central to detecting uncertainty and preventing
hallucinations. While several studies have shown that models encode uncertainty
in their hidden states, it is underexplored how this affects the way they
process such hidden states. In this work, we demonstrate that the dynamics of
output token probabilities across layers for certain and uncertain outputs are
largely aligned, revealing that uncertainty does not seem to affect inference
dynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to
analyze the layer-wise probability trajectories of final prediction tokens
across 11 datasets and 5 models. Using incorrect predictions as those with
higher epistemic uncertainty, our results show aligned trajectories for certain
and uncertain predictions that both observe abrupt increases in confidence at
similar layers. We balance this finding by showing evidence that more competent
models may learn to process uncertainty differently. Our findings challenge the
feasibility of leveraging simplistic methods for detecting uncertainty at
inference. More broadly, our work demonstrates how interpretability methods may
be used to investigate the way uncertainty affects inference.

</details>


### [39] [KAConvText: Novel Approach to Burmese Sentence Classification using Kolmogorov-Arnold Convolution](https://arxiv.org/abs/2507.06753)
*Ye Kyaw Thu,Thura Aung,Thazin Myint Oo,Thepchai Supnithi*

Main category: cs.CL

TL;DR: The paper introduces KAConvText for sentence classification, achieving high accuracies in tasks like hate speech detection, news classification, and language identification.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of Kolmogorov-Arnold Convolution for Text (KAConvText) in sentence classification tasks and improve accuracy with interpretability.

Method: Compare various embedding configurations (random, static, fine-tuned fastText), and combine KAConvText with classification heads (MLP and KAN) against standard CNN baselines.

Result: KAConvText-MLP with fine-tuned fastText embeddings achieved exceptional accuracy ranging from 91.23% to 99.82% in the three targeted classification tasks.

Conclusion: KAConvText demonstrates superior performance and interpretability, proving its effectiveness for diverse sentence classification tasks.

Abstract: This paper presents the first application of Kolmogorov-Arnold Convolution
for Text (KAConvText) in sentence classification, addressing three tasks:
imbalanced binary hate speech detection, balanced multiclass news
classification, and imbalanced multiclass ethnic language identification. We
investigate various embedding configurations, comparing random to fastText
embeddings in both static and fine-tuned settings, with embedding dimensions of
100 and 300 using CBOW and Skip-gram models. Baselines include standard CNNs
and CNNs augmented with a Kolmogorov-Arnold Network (CNN-KAN). In addition, we
investigated KAConvText with different classification heads - MLP and KAN,
where using KAN head supports enhanced interpretability. Results show that
KAConvText-MLP with fine-tuned fastText embeddings achieves the best
performance of 91.23% accuracy (F1-score = 0.9109) for hate speech detection,
92.66% accuracy (F1-score = 0.9267) for news classification, and 99.82%
accuracy (F1-score = 0.9982) for language identification.

</details>


### [40] [Checklist Engineering Empowers Multilingual LLM Judges](https://arxiv.org/abs/2507.06774)
*Mohammad Ghiasvand Mohammadkhani,Hamid Beigy*

Main category: cs.CL

TL;DR: The paper introduces CE-Judge, a training-free multilingual text evaluation framework using checklist-based reasoning with open-source LLMs, showing promising performance against strong baselines.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need for cost-effective, efficient, and adaptable multilingual text evaluation methods, as existing approaches often rely on proprietary models or demand extensive fine-tuning.

Method: The authors propose the CE-Judge framework, leveraging checklist-based reasoning and an open-source LLM for multilingual evaluation without requiring additional training.

Result: Experiments on multiple languages and three benchmark datasets demonstrate that CE-Judge outperforms existing baselines and achieves comparable results to GPT-4o in both pointwise and pairwise evaluation tasks.

Conclusion: CE-Judge proves to be an efficient, effective, and adaptable solution for multilingual text evaluation, offering a strong alternative to proprietary or resource-intensive models.

Abstract: Automated text evaluation has long been a central issue in Natural Language
Processing (NLP). Recently, the field has shifted toward using Large Language
Models (LLMs) as evaluators-a trend known as the LLM-as-a-Judge paradigm. While
promising and easily adaptable across tasks, this approach has seen limited
exploration in multilingual contexts. Existing multilingual studies often rely
on proprietary models or require extensive training data for fine-tuning,
raising concerns about cost, time, and efficiency. In this paper, we propose
Checklist Engineering based LLM-as-a-Judge (CE-Judge), a training-free
framework that uses checklist intuition for multilingual evaluation with an
open-source model. Experiments across multiple languages and three benchmark
datasets, under both pointwise and pairwise settings, show that our method
generally surpasses the baselines and performs on par with the GPT-4o model.

</details>


### [41] [Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining: Method, Evaluation and Applications](https://arxiv.org/abs/2507.06795)
*Seonwu Kim,Yohan Na,Kihun Kim,Hanhee Cho,Geun Lim,Mintae Kim,Seongik Park,Ki Hyun Kim,Youngsub Han,Byoung-Ki Jeon*

Main category: cs.CL

TL;DR: The paper explores Domain Adaptive Continual Pretraining (DACP) and validates its effectiveness for improving small LLMs (sLLMs) in commercial applications.


<details>
  <summary>Details</summary>
Motivation: Organizations lack infrastructure to deploy large LLMs, leading to the need for cost-efficient approaches like sLLMs. DACP methods have been under-examined for enterprise applications.

Method: The authors employ DACP recipes on various foundation models and service domains, complemented by extensive experiments and real-world evaluations.

Result: DACP-applied sLLMs show significant gains in target domain performance while retaining general capabilities.

Conclusion: DACP offers a scalable, cost-efficient solution for deploying adaptable sLLMs in enterprise environments.

Abstract: The emergence of open-source large language models (LLMs) has expanded
opportunities for enterprise applications; however, many organizations still
lack the infrastructure to deploy and maintain large-scale models. As a result,
small LLMs (sLLMs) have become a practical alternative, despite their inherent
performance limitations. While Domain Adaptive Continual Pretraining (DACP) has
been previously explored as a method for domain adaptation, its utility in
commercial applications remains under-examined. In this study, we validate the
effectiveness of applying a DACP-based recipe across diverse foundation models
and service domains. Through extensive experiments and real-world evaluations,
we demonstrate that DACP-applied sLLMs achieve substantial gains in target
domain performance while preserving general capabilities, offering a
cost-efficient and scalable solution for enterprise-level deployment.

</details>


### [42] [Text to model via SysML: Automated generation of dynamical system computational models from unstructured natural language text via enhanced System Modeling Language diagrams](https://arxiv.org/abs/2507.06803)
*Matthew Anderson Hendricks,Alice Cicirello*

Main category: cs.CL

TL;DR: The paper proposes an automated strategy using SysML diagrams enhanced by NLP and LLMs to generate computational models for dynamical systems from domain-specific documents.


<details>
  <summary>Details</summary>
Motivation: The challenge in efficiently designing and deploying dynamical systems stems from the need to leverage domain-specific knowledge systematically in computational model generation.

Method: The approach integrates automated SysML diagram creation using NLP and LLMs, followed by code generation and computational modeling steps. It uses extracted keywords, relationships, attributes, and structures to automate processes.

Result: The methodology demonstrated improved performance and accuracy in generating models, with notable applicability illustrated through case studies, including a simple pendulum model.

Conclusion: This approach enables faster, more accurate generation of dynamical system computational models, showcasing versatility across domains and improved outcomes compared to using LLMs alone.

Abstract: This paper contributes to speeding up the design and deployment of
engineering dynamical systems by proposing a strategy for exploiting domain and
expert knowledge for the automated generation of dynamical system computational
model starting from a corpus of document relevant to the dynamical system of
interest and an input document describing the specific system. This strategy is
implemented in five steps and, crucially, it uses system modeling language
diagrams (SysML) to extract accurate information about the dependencies,
attributes, and operations of components. Natural Language Processing (NLP)
strategies and Large Language Models (LLMs) are employed in specific tasks to
improve intermediate outputs of the SySML diagrams automated generation, such
as: list of key nouns; list of extracted relationships; list of key phrases and
key relationships; block attribute values; block relationships; and BDD diagram
generation. The applicability of automated SysML diagram generation is
illustrated with different case studies. The computational models of complex
dynamical systems from SysML diagrams are then obtained via code generation and
computational model generation steps. In the code generation step, NLP
strategies are used for summarization, while LLMs are used for validation only.
The proposed approach is not limited to a specific system, domain, or
computational software. The applicability of the proposed approach is shown via
an end-to-end example from text to model of a simple pendulum, showing improved
performance compared to results yielded by LLMs only.

</details>


### [43] [Adaptive Termination for Multi-round Parallel Reasoning: An Universal Semantic Entropy-Guided Framework](https://arxiv.org/abs/2507.06829)
*Zenan Xu,Zexuan Qiu,Guanhua Huang,Kun Li,Siheng Li,Chenchen Zhang,Kejiao Li,Qi Yi,Yuhao Jiang,Bo Zhou,Fengzong Lian,Zhanhui Kang*

Main category: cs.CL

TL;DR: The paper proposes a hybrid inference framework combining sequential and parallel reasoning approaches, addressing their limitations through the introduction of semantic entropy as a metric.


<details>
  <summary>Details</summary>
Motivation: The paper addresses inefficiencies and limitations of current approaches in scaling inference with large language models, aiming to combine the strengths of sequential and parallel reasoning.

Method: A test-time collaborative inference framework is developed, where 'semantic entropy' is introduced as a metric to measure reasoning quality and enable dynamic control during inference.

Result: Semantic entropy is demonstrated to have a strong negative correlation with reasoning accuracy, facilitating efficient evaluation of inference quality without intrusive model adjustments.

Conclusion: The paper establishes semantic entropy as a promising tool to enhance reasoning processes, bridging sequential and parallel methods for more effective large language model inference.

Abstract: Recent advances in large language models (LLMs) have accelerated progress
toward artificial general intelligence, with inference-time scaling emerging as
a key technique. Contemporary approaches leverage either sequential reasoning
(iteratively extending chains of thought) or parallel reasoning (generating
multiple solutions simultaneously) to scale inference. However, both paradigms
face fundamental limitations: sequential scaling typically relies on arbitrary
token budgets for termination, leading to inefficiency or premature cutoff;
while parallel scaling often lacks coordination among parallel branches and
requires intrusive fine-tuning to perform effectively. In light of these
challenges, we aim to design a flexible test-time collaborative inference
framework that exploits the complementary strengths of both sequential and
parallel reasoning paradigms. Towards this goal, the core challenge lies in
developing an efficient and accurate intrinsic quality metric to assess model
responses during collaborative inference, enabling dynamic control and early
termination of the reasoning trace. To address this challenge, we introduce
semantic entropy (SE), which quantifies the semantic diversity of parallel
model responses and serves as a robust indicator of reasoning quality due to
its strong negative correlation with accuracy...

</details>


### [44] [Shifting from Ranking to Set Selection for Retrieval Augmented Generation](https://arxiv.org/abs/2507.06838)
*Dahyun Lee,Yongrae Jo,Haeju Park,Moontae Lee*

Main category: cs.CL

TL;DR: This paper introduces SETR, a method that improves retrieval in Retrieval-Augmented Generation (RAG) by selecting a set of passages that collectively satisfies complex query requirements.


<details>
  <summary>Details</summary>
Motivation: Current retrieval methods often focus on individual passage relevance rather than how passages collectively satisfy a query, which limits performance for complex multi-hop queries.

Method: The authors propose SETR, which uses Chain-of-Thought reasoning to identify query requirements and selects passages that together fulfill these needs.

Result: SETR outperforms proprietary and open-source retrieval methods on multi-hop RAG benchmarks by improving answer correctness and retrieval quality.

Conclusion: SETR is a more effective and efficient alternative to traditional rerankers in RAG systems, particularly for complex queries, and its code is publicly available.

Abstract: Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved
passages are not only individually relevant but also collectively form a
comprehensive set. Existing approaches primarily rerank top-k passages based on
their individual relevance, often failing to meet the information needs of
complex queries in multi-hop question answering. In this work, we propose a
set-wise passage selection approach and introduce SETR, which explicitly
identifies the information requirements of a query through Chain-of-Thought
reasoning and selects an optimal set of passages that collectively satisfy
those requirements. Experiments on multi-hop RAG benchmarks show that SETR
outperforms both proprietary LLM-based rerankers and open-source baselines in
terms of answer correctness and retrieval quality, providing an effective and
efficient alternative to traditional rerankers in RAG systems. The code is
available at https://github.com/LGAI-Research/SetR

</details>


### [45] [Developing and Maintaining an Open-Source Repository of AI Evaluations: Challenges and Insights](https://arxiv.org/abs/2507.06893)
*Alexandra Abbas,Celia Waggoner,Justin Olive*

Main category: cs.CL

TL;DR: The paper details insights from managing an open-source AI evaluation repository, addressing challenges in scaling, comparison, and quality control.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve the quality and scalability of AI evaluations to better assess large language models' capabilities and safety.

Method: Insights are derived from eight months of maintaining the $inspect_evals repository, leveraging structured contributions, statistical methodologies, and systematic quality control.

Result: Three solutions were developed: cohort management for contributions, statistical methods for comparisons, and quality control for reproducibility in AI evaluations.

Conclusion: Effective AI evaluations require dedicated infrastructure, statistical precision, and coordinated community efforts beyond traditional practices.

Abstract: AI evaluations have become critical tools for assessing large language model
capabilities and safety. This paper presents practical insights from eight
months of maintaining $inspect\_evals$, an open-source repository of 70+
community-contributed AI evaluations. We identify key challenges in
implementing and maintaining AI evaluations and develop solutions including:
(1) a structured cohort management framework for scaling community
contributions, (2) statistical methodologies for optimal resampling and
cross-model comparison with uncertainty quantification, and (3) systematic
quality control processes for reproducibility. Our analysis reveals that AI
evaluation requires specialized infrastructure, statistical rigor, and
community coordination beyond traditional software development practices.

</details>


### [46] [SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label Contrastive Learning and Bayesian kNN](https://arxiv.org/abs/2507.06895)
*Luca Mariotti,Veronica Guidetti,Federica Mandreoli*

Main category: cs.CL

TL;DR: The paper introduces SCoRE, a cost-effective relation extraction system that operates without fine-tuning and is adaptable to diverse settings. It outperforms state-of-the-art methods in efficiency and energy consumption while highlighting the drawbacks of overly complex models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of relation extraction (RE), particularly in low-supervision settings, and enhance the integration of RE systems with pre-trained large language models (PLMs) in noisy and diverse corpora.

Method: The authors propose SCoRE, which combines supervised contrastive learning and a Bayesian k-Nearest Neighbors classifier for sentence-level multi-label classification. The system is modular, requires no fine-tuning, and supports flexibility in using different PLMs. It also introduces two new evaluation metrics: Correlation Structure Distance (CSD) and Precision at R (P@R).

Result: SCoRE matches or outperforms state-of-the-art RE methods across five benchmarks while significantly reducing energy consumption. The analysis indicates that increasing model complexity degrades performance in noisy settings.

Conclusion: SCoRE is an efficient, scalable, and modular RE solution well-suited for real-world applications, providing strong performance without the need for finely tuned or overly complex models.

Abstract: The growing demand for efficient knowledge graph (KG) enrichment leveraging
external corpora has intensified interest in relation extraction (RE),
particularly under low-supervision settings. To address the need for adaptable
and noise-resilient RE solutions that integrate seamlessly with pre-trained
large language models (PLMs), we introduce SCoRE, a modular and cost-effective
sentence-level RE system. SCoRE enables easy PLM switching, requires no
finetuning, and adapts smoothly to diverse corpora and KGs. By combining
supervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN)
classifier for multi-label classification, it delivers robust performance
despite the noisy annotations of distantly supervised corpora. To improve RE
evaluation, we propose two novel metrics: Correlation Structure Distance (CSD),
measuring the alignment between learned relational patterns and KG structures,
and Precision at R (P@R), assessing utility as a recommender system. We also
release Wiki20d, a benchmark dataset replicating real-world RE conditions where
only KG-derived annotations are available. Experiments on five benchmarks show
that SCoRE matches or surpasses state-of-the-art methods while significantly
reducing energy consumption. Further analyses reveal that increasing model
complexity, as seen in prior work, degrades performance, highlighting the
advantages of SCoRE's minimal design. Combining efficiency, modularity, and
scalability, SCoRE stands as an optimal choice for real-world RE applications.

</details>


### [47] [VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation](https://arxiv.org/abs/2507.06899)
*Ziang Ye,Yang Zhang,Wentao Shi,Xiaoyu You,Fuli Feng,Tat-Seng Chua*

Main category: cs.CL

TL;DR: The paper uncovers backdoor attack vulnerabilities in Large Vision-Language Model (LVLM)-based GUI agents, proposing a method called VisualTrap to exploit these weaknesses.


<details>
  <summary>Details</summary>
Motivation: To explore potential security risks, particularly backdoor attack vulnerabilities, associated with GUI agents powered by LVLMs, which are increasingly used to automate human-device interactions.

Method: The researchers developed a method called VisualTrap, which uses poisoned data during pre-training to hijack the visual grounding of GUI agents, thereby misleading them to connect textual plans to incorrect targets.

Result: Empirical tests demonstrated that VisualTrap could hijack visual grounding with as little as 5% poisoned data and undetectable visual triggers, and that these attacks generalize across tasks and devices, persisting even after clean fine-tuning.

Conclusion: The study emphasizes the need for further research to address backdoor attack vulnerabilities in GUI agents, as these pose significant security risks across various device and application environments.

Abstract: Graphical User Interface (GUI) agents powered by Large Vision-Language Models
(LVLMs) have emerged as a revolutionary approach to automating human-machine
interactions, capable of autonomously operating personal devices (e.g., mobile
phones) or applications within the device to perform complex real-world tasks
in a human-like manner. However, their close integration with personal devices
raises significant security concerns, with many threats, including backdoor
attacks, remaining largely unexplored. This work reveals that the visual
grounding of GUI agent-mapping textual plans to GUI elements-can introduce
vulnerabilities, enabling new types of backdoor attacks. With backdoor attack
targeting visual grounding, the agent's behavior can be compromised even when
given correct task-solving plans. To validate this vulnerability, we propose
VisualTrap, a method that can hijack the grounding by misleading the agent to
locate textual plans to trigger locations instead of the intended targets.
VisualTrap uses the common method of injecting poisoned data for attacks, and
does so during the pre-training of visual grounding to ensure practical
feasibility of attacking. Empirical results show that VisualTrap can
effectively hijack visual grounding with as little as 5% poisoned data and
highly stealthy visual triggers (invisible to the human eye); and the attack
can be generalized to downstream tasks, even after clean fine-tuning. Moreover,
the injected trigger can remain effective across different GUI environments,
e.g., being trained on mobile/web and generalizing to desktop environments.
These findings underscore the urgent need for further research on backdoor
attack risks in GUI agents.

</details>


### [48] [MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection](https://arxiv.org/abs/2507.06908)
*Ziyan Liu,Chunxiao Fan,Haoran Lou,Yuexin Wu,Kaiwei Deng*

Main category: cs.CL

TL;DR: MIND is a framework for zero-shot harmful meme detection, overcoming challenges with evolving memes and lack of annotated data.


<details>
  <summary>Details</summary>
Motivation: The lack of effective tools to detect harmful memes, made difficult by their evolving nature and scarcity of up-to-date annotated data, motivates this research.

Method: MIND uses three strategies: similar meme retrieval for context, a bi-directional insight mechanism for deeper understanding, and a multi-agent debate mechanism for robust decisions.

Result: Experiments on three datasets show that MIND outperforms existing zero-shot methods, demonstrating strong generalization across various architectures and scales.

Conclusion: MIND offers a scalable and effective solution for zero-shot harmful meme detection without relying on annotated data, marking a significant advancement in this area.

Abstract: The rapid expansion of memes on social media has highlighted the urgent need
for effective approaches to detect harmful content. However, traditional
data-driven approaches struggle to detect new memes due to their evolving
nature and the lack of up-to-date annotated data. To address this issue, we
propose MIND, a multi-agent framework for zero-shot harmful meme detection that
does not rely on annotated data. MIND implements three key strategies: 1) We
retrieve similar memes from an unannotated reference set to provide contextual
information. 2) We propose a bi-directional insight derivation mechanism to
extract a comprehensive understanding of similar memes. 3) We then employ a
multi-agent debate mechanism to ensure robust decision-making through reasoned
arbitration. Extensive experiments on three meme datasets demonstrate that our
proposed framework not only outperforms existing zero-shot approaches but also
shows strong generalization across different model architectures and parameter
scales, providing a scalable solution for harmful meme detection. The code is
available at https://github.com/destroy-lonely/MIND.

</details>


### [49] [MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal Prediction](https://arxiv.org/abs/2507.06909)
*Xiao Wang,Jiahuan Pei,Diancheng Shui,Zhiguang Han,Xin Sun,Dawei Zhu,Xiaoyu Shen*

Main category: cs.CL

TL;DR: The paper introduces a dataset called MultiJustice-MPMCP to evaluate legal large language models (LLMs) on legal judgment prediction tasks involving single and multiple defendants and charges.


<details>
  <summary>Details</summary>
Motivation: The research explores whether multiple defendants and charges should be handled separately within legal judgment prediction tasks to improve accuracy and understanding.

Method: The study compares several legal LLMs using the MultiJustice-MPMCP dataset in four scenarios (single vs multiple defendants/charges) for charge and penalty term prediction.

Result: The scenario with multiple defendants and multiple charges (S4) proved most challenging for models, showcasing significant performance differences between simpler scenarios and complex ones.

Conclusion: Handling multiple defendants and charges requires improved model capabilities as current LLMs struggle with complex legal judgment scenarios, highlighting distinct variability in model performance.

Abstract: Legal judgment prediction offers a compelling method to aid legal
practitioners and researchers. However, the research question remains
relatively under-explored: Should multiple defendants and charges be treated
separately in LJP? To address this, we introduce a new dataset namely
multi-person multi-charge prediction (MPMCP), and seek the answer by evaluating
the performance of several prevailing legal large language models (LLMs) on
four practical legal judgment scenarios: (S1) single defendant with a single
charge, (S2) single defendant with multiple charges, (S3) multiple defendants
with a single charge, and (S4) multiple defendants with multiple charges. We
evaluate the dataset across two LJP tasks, i.e., charge prediction and penalty
term prediction. We have conducted extensive experiments and found that the
scenario involving multiple defendants and multiple charges (S4) poses the
greatest challenges, followed by S2, S3, and S1. The impact varies
significantly depending on the model. For example, in S4 compared to S1,
InternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD,
while Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD.
Our dataset and code are available at
https://github.com/lololo-xiao/MultiJustice-MPMCP.

</details>


### [50] [Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in Dialogues](https://arxiv.org/abs/2507.06910)
*Fareya Ikram,Alexander Scarlatos,Andrew Lan*

Main category: cs.CL

TL;DR: The paper explores the ability of LLMs like Llama 3 and GPT-4o to predict tutor strategies and student outcomes in math tutoring dialogues, finding that current LLMs struggle with predicting tutor strategies.


<details>
  <summary>Details</summary>
Motivation: To investigate whether current large language models can predict tutor behaviors and their influence on student outcomes in educational dialogues.

Method: Used datasets from math tutoring dialogues, employing Llama 3 and GPT-4o to predict future tutor strategies and student outcomes.

Result: LLMs were shown to struggle with predicting tutor strategies, despite tutor strategies being closely linked to student outcomes.

Conclusion: Current LLMs require further improvements to better predict tutor behaviors, which are crucial for student success in educational tutoring dialogues.

Abstract: Tutoring dialogues have gained significant attention in recent years, given
the prominence of online learning and the emerging tutoring abilities of
artificial intelligence (AI) agents powered by large language models (LLMs).
Recent studies have shown that the strategies used by tutors can have
significant effects on student outcomes, necessitating methods to predict how
tutors will behave and how their actions impact students. However, few works
have studied predicting tutor strategy in dialogues. Therefore, in this work we
investigate the ability of modern LLMs, particularly Llama 3 and GPT-4o, to
predict both future tutor moves and student outcomes in dialogues, using two
math tutoring dialogue datasets. We find that even state-of-the-art LLMs
struggle to predict future tutor strategy while tutor strategy is highly
indicative of student outcomes, outlining a need for more powerful methods to
approach this task.

</details>


### [51] [Rethinking Verification for LLM Code Generation: From Generation to Testing](https://arxiv.org/abs/2507.06920)
*Zihan Ma,Taolin Zhang,Maosong Cao,Wenwei Zhang,Minnan Luo,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: LLMs excel in code-generation benchmarks but face issues in detecting subtle faults due to limited test cases. This study proposes improved metrics, human-LLM collaboration method (SAGA), and a TCGBench framework to address these shortcomings.


<details>
  <summary>Details</summary>
Motivation: Current code-generation benchmarks fail to detect subtle faults due to limited, homogeneous test cases, artificially inflating LLM performance and compromising RLVR frameworks.

Method: The authors propose multi-dimensional metrics, human-LLM collaborative method (SAGA), and introduce TCGBench for systematic evaluation of test-suite quality.

Result: SAGA achieved a 90.62% detection rate and 32.58% verifier accuracy on TCGBench. SAGA's benchmark evaluation outperformed LiveCodeBench-v6 by 10.78% in verifier accuracy.

Conclusion: The research enhances LLM code evaluation reliability and supports RLVR frameworks by improving test-suite coverage, quality, and performance metrics.

Abstract: Large language models (LLMs) have recently achieved notable success in
code-generation benchmarks such as HumanEval and LiveCodeBench. However, a
detailed examination reveals that these evaluation suites often comprise only a
limited number of homogeneous test cases, resulting in subtle faults going
undetected. This not only artificially inflates measured performance but also
compromises accurate reward estimation in reinforcement learning frameworks
utilizing verifiable rewards (RLVR). To address these critical shortcomings, we
systematically investigate the test-case generation (TCG) task by proposing
multi-dimensional metrics designed to rigorously quantify test-suite
thoroughness. Furthermore, we introduce a human-LLM collaborative method
(SAGA), leveraging human programming expertise with LLM reasoning capability,
aimed at significantly enhancing both the coverage and the quality of generated
test cases. In addition, we develop a TCGBench to facilitate the study of the
TCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a
verifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)
of the code generation evaluation benchmark synthesized by SAGA is 10.78%
higher than that of LiveCodeBench-v6. These results demonstrate the
effectiveness of our proposed method. We hope this work contributes to building
a scalable foundation for reliable LLM code evaluation, further advancing RLVR
in code generation, and paving the way for automated adversarial test synthesis
and adaptive benchmark integration.

</details>


### [52] [Investigating the Robustness of Retrieval-Augmented Generation at the Query Level](https://arxiv.org/abs/2507.06956)
*Sezen Perçin,Xin Su,Qutub Sha Syed,Phillip Howard,Aleksei Kuvshinov,Leo Schwinn,Kay-Ulrich Scholl*

Main category: cs.CL

TL;DR: The paper examines the sensitivity of Retrieval-Augmented Generation (RAG) systems to query changes and proposes an evaluation framework for robustness.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are limited in their ability to update information dynamically and efficiently. Retrieval-Augmented Generation (RAG) tries to address this by integrating external knowledge during inference, but it faces challenges in its dependency on effective query formulation.

Method: The paper investigates how query perturbations affect RAG systems by isolating individual pipeline components and testing their combined question-answering functionality across general and domain-specific datasets. It also introduces a new framework to evaluate query robustness systematically.

Result: The study found that minor variations in query formulation can significantly degrade the performance of common retrievers. Insights are drawn from over 1092 experiments conducted on RAG pipelines.

Conclusion: The research highlights the importance of query robustness in RAG systems and provides recommendations for optimizing performance and minimizing query sensitivity.

Abstract: Large language models (LLMs) are very costly and inefficient to update with
new information. To address this limitation, retrieval-augmented generation
(RAG) has been proposed as a solution that dynamically incorporates external
knowledge during inference, improving factual consistency and reducing
hallucinations. Despite its promise, RAG systems face practical challenges-most
notably, a strong dependence on the quality of the input query for accurate
retrieval. In this paper, we investigate the sensitivity of different
components in the RAG pipeline to various types of query perturbations. Our
analysis reveals that the performance of commonly used retrievers can degrade
significantly even under minor query variations. We study each module in
isolation as well as their combined effect in an end-to-end question answering
setting, using both general-domain and domain-specific datasets. Additionally,
we propose an evaluation framework to systematically assess the query-level
robustness of RAG pipelines and offer actionable recommendations for
practitioners based on the results of more than 1092 experiments we performed.

</details>


### [53] [FRaN-X: FRaming and Narratives-eXplorer](https://arxiv.org/abs/2507.06974)
*Artur Muratov,Hana Fatima Shaikh,Vanshikaa Jani,Tarek Mahmoud,Zhuohan Xie,Daniil Orel,Aaryamonvikram Singh,Yuxia Wang,Aadi Joshi,Hasan Iqbal,Ming Shan Hee,Dhruv Sahnan,Nikolaos Nikolaidis,Purificação Silvano,Dimitar Dimitrov,Roman Yangarber,Ricardo Campos,Alípio Jorge,Nuno Guimarães,Elisa Sartori,Nicolas Stefanovitch,Giovanni Da San Martino,Jakub Piskorski,Preslav Nakov*

Main category: cs.CL

TL;DR: FRaN-X is an automated system for detecting and classifying entities' narrative roles in text, available in five languages and two domains, with interactive tools for media analysis.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting and classifying narrative roles of entities automatically in multi-language and domain-specific contexts for media analysis.

Method: A two-stage system combining sequence labeling with fine-grained role classification, featuring a taxonomy of 22 narrative roles nested under protagonists, antagonists, and innocents.

Result: Provides tools for analyzing and comparing entity framing across articles, supports up to four articles at once, offers visualization, search, and role transition tracking capabilities.

Conclusion: FRaN-X is a versatile, publicly accessible tool for media analysts to explore and understand the narrative framing in text through an intuitive and interactive interface.

Abstract: We present FRaN-X, a Framing and Narratives Explorer that automatically
detects entity mentions and classifies their narrative roles directly from raw
text. FRaN-X comprises a two-stage system that combines sequence labeling with
fine-grained role classification to reveal how entities are portrayed as
protagonists, antagonists, or innocents, using a unique taxonomy of 22
fine-grained roles nested under these three main categories. The system
supports five languages (Bulgarian, English, Hindi, Russian, and Portuguese)
and two domains (the Russia-Ukraine Conflict and Climate Change). It provides
an interactive web interface for media analysts to explore and compare framing
across different sources, tackling the challenge of automatically detecting and
labeling how entities are framed. Our system allows end users to focus on a
single article as well as analyze up to four articles simultaneously. We
provide aggregate level analysis including an intuitive graph visualization
that highlights the narrative a group of articles are pushing. Our system
includes a search feature for users to look up entities of interest, along with
a timeline view that allows analysts to track an entity's role transitions
across different contexts within the article. The FRaN-X system and the trained
models are licensed under an MIT License. FRaN-X is publicly accessible at
https://fran-x.streamlit.app/ and a video demonstration is available at
https://youtu.be/VZVi-1B6yYk.

</details>


### [54] [FlexOlmo: Open Language Models for Flexible Data Use](https://arxiv.org/abs/2507.07024)
*Weijia Shi,Akshita Bhagia,Kevin Farhat,Niklas Muennighoff,Pete Walsh,Jacob Morrison,Dustin Schwenk,Shayne Longpre,Jake Poznanski,Allyson Ettinger,Daogao Liu,Margaret Li,Dirk Groeneveld,Mike Lewis,Wen-tau Yih,Luca Soldaini,Kyle Lo,Noah A. Smith,Luke Zettlemoyer,Pang Wei Koh,Hannaneh Hajishirzi,Ali Farhadi,Sewon Min*

Main category: cs.CL

TL;DR: FlexOlmo introduces a novel language model that incorporates distributed training on closed datasets and data-flexible inference through a mixture-of-experts (MoE) architecture, achieving superior results in combining general and domain-specific experts.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of enabling collaboration on language model training and inference despite data sharing restrictions while ensuring performance improvement and respecting data ownership.

Method: FlexOlmo uses a mixture-of-experts architecture with independently trained experts on closed datasets, integrated via domain-informed routing. No joint training is required, and it is trained on a curated corpus, FlexMix, combining both public and domain-specific datasets.

Result: FlexOlmo demonstrated a 41% relative improvement when combining general and domain-specific experts and outperformed prior model merging methods by 10.1% on average while respecting data restrictions.

Conclusion: FlexOlmo provides a scalable and efficient solution for working with sensitive or protected data, enabling flexible and high-performance language model training and inference without compromising data ownership or privacy.

Abstract: We introduce FlexOlmo, a new class of language models (LMs) that supports (1)
distributed training without data sharing, where different model parameters are
independently trained on closed datasets, and (2) data-flexible inference,
where these parameters along with their associated data can be flexibly
included or excluded from model inferences with no further training. FlexOlmo
employs a mixture-of-experts (MoE) architecture where each expert is trained
independently on closed datasets and later integrated through a new
domain-informed routing without any joint training. FlexOlmo is trained on
FlexMix, a corpus we curate comprising publicly available datasets alongside
seven domain-specific sets, representing realistic approximations of closed
sets. We evaluate models with up to 37 billion parameters (20 billion active)
on 31 diverse downstream tasks. We show that a general expert trained on public
data can be effectively combined with independently trained experts from other
data owners, leading to an average 41% relative improvement while allowing
users to opt out of certain data based on data licensing or permission
requirements. Our approach also outperforms prior model merging methods by
10.1% on average and surpasses the standard MoE trained without data
restrictions using the same training FLOPs. Altogether, this research presents
a solution for both data owners and researchers in regulated industries with
sensitive or protected data. FlexOlmo enables benefiting from closed data while
respecting data owners' preferences by keeping their data local and supporting
fine-grained control of data access during inference.

</details>


### [55] [UniConv: Unifying Retrieval and Response Generation for Large Language Models in Conversations](https://arxiv.org/abs/2507.07030)
*Fengran Mo,Yifan Gao,Chuan Meng,Xin Liu,Zhuofeng Wu,Kelong Mao,Zhengyang Wang,Pei Chen,Zheng Li,Xian Li,Bing Yin,Meng Jiang*

Main category: cs.CL

TL;DR: This paper proposes a unified model combining dense retrieval and response generation in conversational search systems to enhance effectiveness and address existing limitations.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is to overcome the inefficiencies in conversational search systems, where separated models restrict leveraging their intrinsic knowledge and limit the mutual benefits between retrieval and response generation.

Method: The authors conducted joint fine-tuning with different objectives and incorporated two mechanisms designed to reduce inconsistency risks and mitigate data discrepancy between dense retrieval and response generation.

Result: Evaluations on five conversational search datasets reveal that the unified model mutually improves retrieval and response generation tasks and outperforms existing baseline methods.

Conclusion: The study concludes that unifying dense retrieval and response generation tasks can lead to better performance in conversational search systems, addressing existing gaps in understanding context, retrieval management, and response generation.

Abstract: The rapid advancement of conversational search systems revolutionizes how
information is accessed by enabling the multi-turn interaction between the user
and the system. Existing conversational search systems are usually built with
two different models. This separation restricts the system from leveraging the
intrinsic knowledge of the models simultaneously, which cannot ensure the
effectiveness of retrieval benefiting the generation. The existing studies for
developing unified models cannot fully address the aspects of understanding
conversational context, managing retrieval independently, and generating
responses. In this paper, we explore how to unify dense retrieval and response
generation for large language models in conversation. We conduct joint
fine-tuning with different objectives and design two mechanisms to reduce the
inconsistency risks while mitigating data discrepancy. The evaluations on five
conversational search datasets demonstrate that our unified model can mutually
improve both tasks and outperform the existing baselines.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [56] [Unveiling the Underwater World: CLIP Perception Model-Guided Underwater Image Enhancement](https://arxiv.org/abs/2507.06234)
*Jiangzhong Cao,Zekai Zeng,Xu Zhang,Huan Zhang,Chunling Fan,Gangyi Jiang,Weisi Lin*

Main category: cs.CV

TL;DR: This paper proposes an underwater image enhancement method using CLIP-based perception loss and curricular contrastive regularization to improve both human-perceived quality and content restoration.


<details>
  <summary>Details</summary>
Motivation: Current underwater image enhancement methods lack focus on human perception and sufficient solution space constraints, leading to lower perceptual quality and poor restoration.

Method: The proposed method uses the CLIP model for visual-semantic feature extraction to develop a perception model, integrate it as a loss module, and employ curriculum contrastive regularization to enhance constraint effectiveness.

Result: Extensive experiments show that this approach surpasses existing methods in visual quality and generalization ability.

Conclusion: The integration of human perception-based constraints and curriculum regularization significantly improves underwater image enhancement outcomes.

Abstract: High-quality underwater images are essential for both machine vision tasks
and viewers with their aesthetic appeal.However, the quality of underwater
images is severely affected by light absorption and scattering. Deep
learning-based methods for Underwater Image Enhancement (UIE) have achieved
good performance. However, these methods often overlook considering human
perception and lack sufficient constraints within the solution space.
Consequently, the enhanced images often suffer from diminished perceptual
quality or poor content restoration.To address these issues, we propose a UIE
method with a Contrastive Language-Image Pre-Training (CLIP) perception loss
module and curriculum contrastive regularization. Above all, to develop a
perception model for underwater images that more aligns with human visual
perception, the visual semantic feature extraction capability of the CLIP model
is leveraged to learn an appropriate prompt pair to map and evaluate the
quality of underwater images. This CLIP perception model is then incorporated
as a perception loss module into the enhancement network to improve the
perceptual quality of enhanced images. Furthermore, the CLIP perception model
is integrated with the curriculum contrastive regularization to enhance the
constraints imposed on the enhanced images within the CLIP perceptual space,
mitigating the risk of both under-enhancement and over-enhancement.
Specifically, the CLIP perception model is employed to assess and categorize
the learning difficulty level of negatives in the regularization process,
ensuring comprehensive and nuanced utilization of distorted images and
negatives with varied quality levels. Extensive experiments demonstrate that
our method outperforms state-of-the-art methods in terms of visual quality and
generalization ability.

</details>


### [57] [SPARC: Concept-Aligned Sparse Autoencoders for Cross-Model and Cross-Modal Interpretability](https://arxiv.org/abs/2507.06265)
*Ali Nasiri-Sarvi,Hassan Rivaz,Mahdi S. Hosseini*

Main category: cs.CV

TL;DR: SPARC enables unified and interpretable concept representation across AI models by introducing sparsity and semantic alignment mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing AI models generate isolated representations, hindering cross-model interpretability and a unified understanding of concepts.

Method: SPARC uses a Global TopK sparsity mechanism and a Cross-Reconstruction Loss to align different AI models into a shared latent space.

Result: SPARC improves Jaccard similarity to 0.80 for concept alignment, tripling the performance of prior approaches, and allows for practical applications like cross-modal retrieval.

Conclusion: SPARC successfully creates a unified sparse latent space for concept representation, advancing cross-model and cross-modal interpretability.

Abstract: Understanding how different AI models encode the same high-level concepts,
such as objects or attributes, remains challenging because each model typically
produces its own isolated representation. Existing interpretability methods
like Sparse Autoencoders (SAEs) produce latent concepts individually for each
model, resulting in incompatible concept spaces and limiting cross-model
interpretability. To address this, we introduce SPARC (Sparse Autoencoders for
Aligned Representation of Concepts), a new framework that learns a single,
unified latent space shared across diverse architectures and modalities (e.g.,
vision models like DINO, and multimodal models like CLIP). SPARC's alignment is
enforced through two key innovations: (1) a Global TopK sparsity mechanism,
ensuring all input streams activate identical latent dimensions for a given
concept; and (2) a Cross-Reconstruction Loss, which explicitly encourages
semantic consistency between models. On Open Images, SPARC dramatically
improves concept alignment, achieving a Jaccard similarity of 0.80, more than
tripling the alignment compared to previous methods. SPARC creates a shared
sparse latent space where individual dimensions often correspond to similar
high-level concepts across models and modalities, enabling direct comparison of
how different architectures represent identical concepts without requiring
manual alignment or model-specific analysis. As a consequence of this aligned
representation, SPARC also enables practical applications such as text-guided
spatial localization in vision-only models and cross-model/cross-modal
retrieval. Code and models are available at
https://github.com/AtlasAnalyticsLab/SPARC.

</details>


### [58] [A Probabilistic Approach to Uncertainty Quantification Leveraging 3D Geometry](https://arxiv.org/abs/2507.06269)
*Rushil Desai,Frederik Warburg,Trevor Darrell,Marissa Ramirez de Chanlatte*

Main category: cs.CV

TL;DR: This paper introduces BayesSDF, a probabilistic framework for uncertainty quantification in neural implicit Signed Distance Functions (SDFs), addressing computational and geometric challenges.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need for better uncertainty quantification in 3D neural implicit SDF representations, especially for applications like simulation in complex environments (e.g., forests), where accurate geometry and uncertainty awareness are critical.

Method: The method leverages a Laplace approximation and Hessian-based metrics to efficiently quantify local surface instability and provide surface-aware uncertainty estimation.

Result: The results show that BayesSDF produces uncertainty predictions closely aligned with poorly reconstructed geometry, offering confidence measures that outperform existing methods in calibration and geometric consistency.

Conclusion: BayesSDF establishes itself as a reliable framework for uncertainty-aware 3D reconstruction and has potential applications in simulations and robotic decision-making tasks.

Abstract: Quantifying uncertainty in neural implicit 3D representations, particularly
those utilizing Signed Distance Functions (SDFs), remains a substantial
challenge due to computational inefficiencies, scalability issues, and
geometric inconsistencies. Existing methods typically neglect direct geometric
integration, leading to poorly calibrated uncertainty maps. We introduce
BayesSDF, a novel probabilistic framework for uncertainty quantification in
neural implicit SDF models, motivated by scientific simulation applications
with 3D environments (e.g., forests) such as modeling fluid flow through
forests, where precise surface geometry and awareness of fidelity surface
geometric uncertainty are essential. Unlike radiance-based models such as NeRF
or 3D Gaussian splatting, which lack explicit surface formulations, SDFs define
continuous and differentiable geometry, making them better suited for physical
modeling and analysis. BayesSDF leverages a Laplace approximation to quantify
local surface instability via Hessian-based metrics, enabling computationally
efficient, surface-aware uncertainty estimation. Our method shows that
uncertainty predictions correspond closely with poorly reconstructed geometry,
providing actionable confidence measures for downstream use. Extensive
evaluations on synthetic and real-world datasets demonstrate that BayesSDF
outperforms existing methods in both calibration and geometric consistency,
establishing a strong foundation for uncertainty-aware 3D scene reconstruction,
simulation, and robotic decision-making.

</details>


### [59] [LIRA: Inferring Segmentation in Large Multi-modal Models with Local Interleaved Region Assistance](https://arxiv.org/abs/2507.06272)
*Zhang Li,Biao Yang,Qiang Liu,Shuo Zhang,Zhiyin Ma,Shuo Zhang,Liang Yin,Linger Deng,Yabo Sun,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: The paper introduces LIRA, a framework aimed to address issues of inaccurate segmentation and hallucinated comprehension in large multi-modal models, leveraging semantic integration and localized visual description generation.


<details>
  <summary>Details</summary>
Motivation: Large multi-modal models (LMMs) face challenges of weak visual comprehension and a lack of fine-grained perception, impacting segmentation accuracy and comprehension reliability.

Method: LIRA employs a Semantic-Enhanced Feature Extractor to integrate semantic and pixel-level features, and an Interleaved Local Visual Coupling mechanism generates localized descriptions based on segmentation masks. An Attributes Evaluation dataset quantifies semantic inference ability.

Result: LIRA shows state-of-the-art performance in improving segmentation accuracy and mitigating mistaken comprehension in experimental studies.

Conclusion: The proposed framework successfully addresses LMM limitations, enhancing visual comprehension and fine-grained perception integration, with code provided for further exploration.

Abstract: While large multi-modal models (LMMs) demonstrate promising capabilities in
segmentation and comprehension, they still struggle with two limitations:
inaccurate segmentation and hallucinated comprehension. These challenges stem
primarily from constraints in weak visual comprehension and a lack of
fine-grained perception. To alleviate these limitations, we propose LIRA, a
framework that capitalizes on the complementary relationship between visual
comprehension and segmentation via two key components: (1) Semantic-Enhanced
Feature Extractor (SEFE) improves object attribute inference by fusing semantic
and pixel-level features, leading to more accurate segmentation; (2)
Interleaved Local Visual Coupling (ILVC) autoregressively generates local
descriptions after extracting local features based on segmentation masks,
offering fine-grained supervision to mitigate hallucinations. Furthermore, we
find that the precision of object segmentation is positively correlated with
the latent related semantics of the <seg> token. To quantify this relationship
and the model's potential semantic inferring ability, we introduce the
Attributes Evaluation (AttrEval) dataset. Our experiments show that LIRA
achieves state-of-the-art performance in both segmentation and comprehension
tasks. Code will be available at https://github.com/echo840/LIRA.

</details>


### [60] [Advancing Offline Handwritten Text Recognition: A Systematic Review of Data Augmentation and Generation Techniques](https://arxiv.org/abs/2507.06275)
*Yassin Hussein Rassul,Aram M. Ahmed,Polla Fattah,Bryar A. Hassan,Arwaa W. Abdulkareem,Tarik A. Rashid,Joan Lu*

Main category: cs.CV

TL;DR: This paper surveys methods to enhance Offline Handwritten Text Recognition (HTR) by reviewing data augmentation and generation techniques.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of limited annotated training data in HTR systems, especially for low-resource languages and complex scripts.

Method: A systematic review using the PRISMA methodology was performed, analyzing traditional and advanced deep learning techniques, including GANs and diffusion models, on data augmentation and generation.

Result: From 1,302 initial studies, 848 were selected for analysis, focusing on datasets, metrics, and methodologies to identify research gaps and future directions.

Conclusion: The survey outlines existing challenges and offers insights for advancing handwritten text generation across diverse linguistic and stylistic contexts.

Abstract: Offline Handwritten Text Recognition (HTR) systems play a crucial role in
applications such as historical document digitization, automatic form
processing, and biometric authentication. However, their performance is often
hindered by the limited availability of annotated training data, particularly
for low-resource languages and complex scripts. This paper presents a
comprehensive survey of offline handwritten data augmentation and generation
techniques designed to improve the accuracy and robustness of HTR systems. We
systematically examine traditional augmentation methods alongside recent
advances in deep learning, including Generative Adversarial Networks (GANs),
diffusion models, and transformer-based approaches. Furthermore, we explore the
challenges associated with generating diverse and realistic handwriting
samples, particularly in preserving script authenticity and addressing data
scarcity. This survey follows the PRISMA methodology, ensuring a structured and
rigorous selection process. Our analysis began with 1,302 primary studies,
which were filtered down to 848 after removing duplicates, drawing from key
academic sources such as IEEE Digital Library, Springer Link, Science Direct,
and ACM Digital Library. By evaluating existing datasets, assessment metrics,
and state-of-the-art methodologies, this survey identifies key research gaps
and proposes future directions to advance the field of handwritten text
generation across diverse linguistic and stylistic landscapes.

</details>


### [61] [Centralized Copy-Paste: Enhanced Data Augmentation Strategy for Wildland Fire Semantic Segmentation](https://arxiv.org/abs/2507.06321)
*Joon Tai Kim,Tianle Chen,Ziyu Dong,Nishanth Kunchala,Alexander Guller,Daniel Ospina Acero,Roger Williams,Mrinal Kumar*

Main category: cs.CV

TL;DR: The paper introduces the Centralized Copy-Paste Data Augmentation method to improve fire-class segmentation by diversifying training datasets with focused augmentation strategies.


<details>
  <summary>Details</summary>
Motivation: There is a scarcity of annotated datasets for training segmentation models in wildland fire science, making model development cost-prohibitive.

Method: The CCPDA method includes identifying fire clusters, centralizing these clusters to enhance focus on fire areas, and pasting them onto target images to augment dataset diversity.

Result: CCPDA was numerically validated and shown to improve fire-class segmentation performance compared to other augmentation methods.

Conclusion: The CCPDA method effectively addresses the challenges of small, manually labeled datasets, aiding in the advancement of fire-class segmentation in wildland fire science.

Abstract: Collecting and annotating images for the purpose of training segmentation
models is often cost prohibitive. In the domain of wildland fire science, this
challenge is further compounded by the scarcity of reliable public datasets
with labeled ground truth. This paper presents the Centralized Copy-Paste Data
Augmentation (CCPDA) method, for the purpose of assisting with the training of
deep-learning multiclass segmentation models, with special focus on improving
segmentation outcomes for the fire-class. CCPDA has three main steps: (i)
identify fire clusters in the source image, (ii) apply a centralization
technique to focus on the core of the fire area, and (iii) paste the refined
fire clusters onto a target image. This method increases dataset diversity
while preserving the essential characteristics of the fire class. The
effectiveness of this augmentation technique is demonstrated via numerical
analysis and comparison against various other augmentation methods using a
weighted sum-based multi-objective optimization approach. This approach helps
elevate segmentation performance metrics specific to the fire class, which
carries significantly more operational significance than other classes (fuel,
ash, or background). Numerical performance assessment validates the efficacy of
the presented CCPDA method in alleviating the difficulties associated with
small, manually labeled training datasets. It also illustrates that CCPDA
outperforms other augmentation strategies in the application scenario
considered, particularly in improving fire-class segmentation performance.

</details>


### [62] [AR2: Attention-Guided Repair for the Robustness of CNNs Against Common Corruptions](https://arxiv.org/abs/2507.06332)
*Fuyuan Zhang,Qichen Wang,Jianjun Zhao*

Main category: cs.CV

TL;DR: The paper introduces AR2, a method to improve deep neural network robustness against common corruptions by aligning attention maps between clean and corrupted inputs.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks lose performance under common corruptions like noise or blur, hindering their application in real-world scenarios.

Method: AR2 aligns class activation maps of clean and corrupted visuals through iterative refinement and fine-tuning, without altering the architecture.

Result: AR2 greatly improves robustness on corruption benchmarks such as CIFAR-10-C, CIFAR-100-C, and ImageNet-C, surpassing state-of-the-art methods.

Conclusion: AR2 offers a scalable and effective solution to enhance model reliability across diverse input perturbations.

Abstract: Deep neural networks suffer from significant performance degradation when
exposed to common corruptions such as noise, blur, weather, and digital
distortions, limiting their reliability in real-world applications. In this
paper, we propose AR2 (Attention-Guided Repair for Robustness), a simple yet
effective method to enhance the corruption robustness of pretrained CNNs. AR2
operates by explicitly aligning the class activation maps (CAMs) between clean
and corrupted images, encouraging the model to maintain consistent attention
even under input perturbations. Our approach follows an iterative repair
strategy that alternates between CAM-guided refinement and standard
fine-tuning, without requiring architectural changes. Extensive experiments
show that AR2 consistently outperforms existing state-of-the-art methods in
restoring robustness on standard corruption benchmarks (CIFAR-10-C, CIFAR-100-C
and ImageNet-C), achieving a favorable balance between accuracy on clean data
and corruption robustness. These results demonstrate that AR2 provides a robust
and scalable solution for enhancing model reliability in real-world
environments with diverse corruptions.

</details>


### [63] [When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking](https://arxiv.org/abs/2507.06400)
*Weiran Li,Yeqiang Liu,Qiannan Guo,Yijie Wei,Hwa Liang Leo,Zhenbo Li*

Main category: cs.CV

TL;DR: A new underwater fish tracking dataset (MFT25) and specialized tracking framework (SU-T) are introduced, enabling advancements in underwater tracking research.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of datasets and methods specific to underwater fish tracking, which is crucial for marine ecology and aquaculture.

Method: The authors developed MFT25, a comprehensive annotated dataset for underwater scenarios, and SU-T, a novel tracking framework incorporating Unscented Kalman Filter and FishIoU matching.

Result: SU-T achieved state-of-the-art tracking performance on the MFT25 dataset, highlighting differences between underwater and terrestrial tracking.

Conclusion: MFT25 and SU-T provide critical tools for research in underwater tracking, with applications in marine biology and environmental conservation.

Abstract: Multiple object tracking (MOT) technology has made significant progress in
terrestrial applications, but underwater tracking scenarios remain
underexplored despite their importance to marine ecology and aquaculture. We
present Multiple Fish Tracking Dataset 2025 (MFT25), the first comprehensive
dataset specifically designed for underwater multiple fish tracking, featuring
15 diverse video sequences with 408,578 meticulously annotated bounding boxes
across 48,066 frames. Our dataset captures various underwater environments,
fish species, and challenging conditions including occlusions, similar
appearances, and erratic motion patterns. Additionally, we introduce
Scale-aware and Unscented Tracker (SU-T), a specialized tracking framework
featuring an Unscented Kalman Filter (UKF) optimized for non-linear fish
swimming patterns and a novel Fish-Intersection-over-Union (FishIoU) matching
that accounts for the unique morphological characteristics of aquatic species.
Extensive experiments demonstrate that our SU-T baseline achieves
state-of-the-art performance on MFT25, with 34.1 HOTA and 44.6 IDF1, while
revealing fundamental differences between fish tracking and terrestrial object
tracking scenarios. MFT25 establishes a robust foundation for advancing
research in underwater tracking systems with important applications in marine
biology, aquaculture monitoring, and ecological conservation. The dataset and
codes are released at https://vranlee.github.io/SU-T/.

</details>


### [64] [VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting](https://arxiv.org/abs/2507.05116)
*Juyi Lin,Amir Taherin,Arash Akbari,Arman Akbari,Lei Lu,Guangyu Chen,Taskin Padir,Xiaomeng Yang,Weiwei Chen,Yiqian Li,Xue Lin,David Kaeli,Pu Zhao,Yanzhi Wang*

Main category: cs.CV

TL;DR: The paper introduces VOTE, a framework enhancing efficiency and generalization of Vision Language Action (VLA) models by eliminating unnecessary computational overhead while improving performance.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency and limited generalization of existing Vision Language Action (VLA) models when dealing with novel objects or unfamiliar environments.

Method: Proposes VOTE, using a tokenizer-free fine-tuning approach for parallel, efficient action prediction and an ensemble voting strategy to improve performance and generalization.

Result: Achieves state-of-the-art performance with significantly faster inference (35x improvement) and throughput of 145 Hz.

Conclusion: VOTE provides an efficient, generalizable framework for VLA models, offering improved performance without costly additional computational techniques. Codes will be open-sourced.

Abstract: Recent large-scale Vision Language Action (VLA) models have shown superior
performance in robotic manipulation tasks guided by natural language. However,
their generalization remains limited when applied to novel objects or
unfamiliar environments that lie outside the training distribution. To address
this, many existing approaches integrate additional components such as depth
estimation, segmentation, or even diffusion to improve generalization, at the
cost of adding significant computation overhead, resulting in low efficiency.
This motivates the exploration of efficient action prediction methods, which
are independent of additional high-level visual representations or diffusion
techniques. In this work, we propose VOTE, an efficient and general framework
for the optimization and acceleration of VLA models. In details, we propose a
novel tokenizer-free fine-tuning approach for parallel accurate action
prediction, which reduces computational overhead and accelerates inference
speed. Additionally, we adopt an ensemble voting strategy for the action
sampling, which significantly improves model performance and enhances
generalization. Experimental results show that our method achieves
state-of-the-art performance with 35$\times$ faster inference and 145 Hz
throughput. All the details and codes will be open-sourced.

</details>


### [65] [SImpHAR: Advancing impedance-based human activity recognition using 3D simulation and text-to-motion models](https://arxiv.org/abs/2507.06405)
*Lala Shakti Swarup Ray,Mengxi Liu,Deepika Gurung,Bo Zhou,Sungho Suh,Paul Lukowicz*

Main category: cs.CV

TL;DR: The paper introduces SImpHAR, a Human Activity Recognition framework using bio-impedance sensors, combining simulation-driven augmentation and modular training to achieve better performance on HAR tasks.


<details>
  <summary>Details</summary>
Motivation: To improve Human Activity Recognition using bio-impedance signals, which are ideal for fine-grained motion tracking but hindered by insufficient labeled data.

Method: The authors developed a simulation pipeline for generating bio-impedance signals using advanced modeling techniques and a novel two-stage modular training approach to enhance activity recognition.

Result: SImpHAR demonstrated consistent performance improvements, achieving accuracy gains of up to 22.3% and macro F1 score gains of up to 21.8% compared to leading methods, validated on a new dataset and public benchmarks.

Conclusion: Simulation-driven data augmentation and modular training strategies offer great potential in advancing impedance-based HAR systems.

Abstract: Human Activity Recognition (HAR) with wearable sensors is essential for
applications in healthcare, fitness, and human-computer interaction.
Bio-impedance sensing offers unique advantages for fine-grained motion capture
but remains underutilized due to the scarcity of labeled data. We introduce
SImpHAR, a novel framework addressing this limitation through two core
contributions. First, we propose a simulation pipeline that generates realistic
bio-impedance signals from 3D human meshes using shortest-path estimation,
soft-body physics, and text-to-motion generation serving as a digital twin for
data augmentation. Second, we design a two-stage training strategy with
decoupled approach that enables broader activity coverage without requiring
label-aligned synthetic data. We evaluate SImpHAR on our collected ImpAct
dataset and two public benchmarks, showing consistent improvements over
state-of-the-art methods, with gains of up to 22.3% and 21.8%, in terms of
accuracy and macro F1 score, respectively. Our results highlight the promise of
simulation-driven augmentation and modular training for impedance-based HAR.

</details>


### [66] [Hierarchical Multi-Stage Transformer Architecture for Context-Aware Temporal Action Localization](https://arxiv.org/abs/2507.06411)
*Hayat Ullah,Arslan Munir,Oliver Nina*

Main category: cs.CV

TL;DR: The paper introduces PCL-Former, a hierarchical multi-stage transformer architecture tailored for temporal action localization (TAL), achieving state-of-the-art results on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: The study aims to harness the strengths of transformers and multi-stage architectures to effectively address the temporal action localization problem by leveraging spatio-temporal properties.

Method: PCL-Former integrates three transformer modules (Proposal-Former, Classification-Former, and Localization-Former) in a hierarchical multi-stage design, each targeting a specific TAL task with specialized loss functions.

Result: PCL-Former outperformed existing TAL approaches by 2.8%, 1.2%, and 4.8% on THUMOS-14, ActivityNet-1.3, and HACS Segments datasets, respectively.

Conclusion: The proposed multi-stage transformer-based PCL-Former framework is highly effective for TAL tasks, achieving superior accuracy compared to state-of-the-art methods.

Abstract: Inspired by the recent success of transformers and multi-stage architectures
in video recognition and object detection domains. We thoroughly explore the
rich spatio-temporal properties of transformers within a multi-stage
architecture paradigm for the temporal action localization (TAL) task. This
exploration led to the development of a hierarchical multi-stage transformer
architecture called PCL-Former, where each subtask is handled by a dedicated
transformer module with a specialized loss function. Specifically, the
Proposal-Former identifies candidate segments in an untrimmed video that may
contain actions, the Classification-Former classifies the action categories
within those segments, and the Localization-Former precisely predicts the
temporal boundaries (i.e., start and end) of the action instances. To evaluate
the performance of our method, we have conducted extensive experiments on three
challenging benchmark datasets: THUMOS-14, ActivityNet-1.3, and HACS Segments.
We also conducted detailed ablation experiments to assess the impact of each
individual module of our PCL-Former. The obtained quantitative results validate
the effectiveness of the proposed PCL-Former, outperforming state-of-the-art
TAL approaches by 2.8%, 1.2%, and 4.8% on THUMOS14, ActivityNet-1.3, and HACS
datasets, respectively.

</details>


### [67] [THOR: Thermal-guided Hand-Object Reasoning via Adaptive Vision Sampling](https://arxiv.org/abs/2507.06442)
*Soroush Shahi,Farzad Shahabi,Rama Nabulsi,Glenn Fernandes,Aggelos Katsaggelos,Nabil Alshurafa*

Main category: cs.CV

TL;DR: The study introduces THOR, a method utilizing thermal sensing for efficient processing of wearable camera data to monitor hand-related activities, reducing energy usage and addressing privacy concerns while maintaining high activity recognition performance.


<details>
  <summary>Details</summary>
Motivation: Wearable cameras are useful for observing and intervening in human behaviors but suffer from privacy, power consumption, and computational inefficiencies when handling continuous RGB image processing.

Method: The paper presents THOR, which employs thermal sensors to detect activity switches, adjust RGB frame sampling rates, and crop relevant hand-object areas within RGB frames. This optimizes processing efficiency without sacrificing performance.

Result: THOR captures all activity segments using only 3% of RGB video data and achieves a high F1-score (95%) for activity recognition, on par with using entire RGB videos.

Conclusion: The approach enables practical long-term use of wearable cameras for real-time activity monitoring, addressing privacy, battery life, and data efficiency challenges.

Abstract: Wearable cameras are increasingly used as an observational and interventional
tool for human behaviors by providing detailed visual data of hand-related
activities. This data can be leveraged to facilitate memory recall for logging
of behavior or timely interventions aimed at improving health. However,
continuous processing of RGB images from these cameras consumes significant
power impacting battery lifetime, generates a large volume of unnecessary video
data for post-processing, raises privacy concerns, and requires substantial
computational resources for real-time analysis. We introduce THOR, a real-time
adaptive spatio-temporal RGB frame sampling method that leverages thermal
sensing to capture hand-object patches and classify them in real-time. We use
low-resolution thermal camera data to identify moments when a person switches
from one hand-related activity to another, and adjust the RGB frame sampling
rate by increasing it during activity transitions and reducing it during
periods of sustained activity. Additionally, we use the thermal cues from the
hand to localize the region of interest (i.e., the hand-object interaction) in
each RGB frame, allowing the system to crop and process only the necessary part
of the image for activity recognition. We develop a wearable device to validate
our method through an in-the-wild study with 14 participants and over 30
activities, and further evaluate it on Ego4D (923 participants across 9
countries, totaling 3,670 hours of video). Our results show that using only 3%
of the original RGB video data, our method captures all the activity segments,
and achieves hand-related activity recognition F1-score (95%) comparable to
using the entire RGB video (94%). Our work provides a more practical path for
the longitudinal use of wearable cameras to monitor hand-related activities and
health-risk behaviors in real time.

</details>


### [68] [EA: An Event Autoencoder for High-Speed Vision Sensing](https://arxiv.org/abs/2507.06459)
*Riadul Islam,Joey Mulé,Dhandeep Challagundla,Shahmir Rizvi,Sean Carson*

Main category: cs.CV

TL;DR: A novel event autoencoder designed to compress and reconstruct event camera data with high efficiency achieved competitive accuracy while significantly reducing computational requirements.


<details>
  <summary>Details</summary>
Motivation: Traditional frame-based vision systems struggle with challenges like motion blur and high latency, especially in dynamic environments, necessitating better alternatives like event cameras for real-time scenarios.

Method: The study proposes an event autoencoder that utilizes convolutional encoding, adaptive threshold selection, and a lightweight classifier to improve accuracy and computational efficiency.

Result: The model performed comparably to YOLO-v4 in accuracy while using up to 35.5x fewer parameters and achieved superior frame rates (up to 44.8 FPS) on embedded platforms.

Conclusion: This approach drastically enhances event-based vision performance, making it highly suitable for low-power, high-speed edge computing applications in real-time settings.

Abstract: High-speed vision sensing is essential for real-time perception in
applications such as robotics, autonomous vehicles, and industrial automation.
Traditional frame-based vision systems suffer from motion blur, high latency,
and redundant data processing, limiting their performance in dynamic
environments. Event cameras, which capture asynchronous brightness changes at
the pixel level, offer a promising alternative but pose challenges in object
detection due to sparse and noisy event streams. To address this, we propose an
event autoencoder architecture that efficiently compresses and reconstructs
event data while preserving critical spatial and temporal features. The
proposed model employs convolutional encoding and incorporates adaptive
threshold selection and a lightweight classifier to enhance recognition
accuracy while reducing computational complexity. Experimental results on the
existing Smart Event Face Dataset (SEFD) demonstrate that our approach achieves
comparable accuracy to the YOLO-v4 model while utilizing up to $35.5\times$
fewer parameters. Implementations on embedded platforms, including Raspberry Pi
4B and NVIDIA Jetson Nano, show high frame rates ranging from 8 FPS up to 44.8
FPS. The proposed classifier exhibits up to 87.84x better FPS than the
state-of-the-art and significantly improves event-based vision performance,
making it ideal for low-power, high-speed applications in real-time edge
computing.

</details>


### [69] [Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning](https://arxiv.org/abs/2507.06485)
*Ziyang Wang,Jaehong Yoon,Shoubin Yu,Md Mohaiminul Islam,Gedas Bertasius,Mohit Bansal*

Main category: cs.CV

TL;DR: Video-RTS improves video reasoning with significantly reduced data, surpassing existing models in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The expensive and hard-to-scale supervised fine-tuning for RL-based video reasoning necessitates a more data-efficient and scalable approach.

Method: The paper introduces Video-RTS, utilizing pure RL training with output-based rewards while skipping supervised fine-tuning, and employing a sparse-to-dense video TTS strategy.

Result: Video-RTS achieves improved video reasoning accuracy by an average of 2.4%, using only 3.6% of the training data compared to existing models.

Conclusion: The proposed Video-RTS combines efficient RL training with adaptive TTS, demonstrating strong performance and reduced resource requirements in video reasoning benchmarks.

Abstract: Despite advances in reinforcement learning (RL)-based video reasoning with
large language models (LLMs), data collection and finetuning remain significant
challenges. These methods often rely on large-scale supervised fine-tuning
(SFT) with extensive video data and long Chain-of-Thought (CoT) annotations,
making them costly and hard to scale. To address this, we present Video-RTS, a
new approach to improve video reasoning capability with drastically improved
data efficiency by combining data-efficient RL with a video-adaptive test-time
scaling (TTS) strategy. Based on observations about the data scaling of RL
samples, we skip the resource-intensive SFT step and employ efficient pure-RL
training with output-based rewards, requiring no additional annotations or
extensive fine-tuning. Furthermore, to utilize computational resources more
efficiently, we introduce a sparse-to-dense video TTS strategy that improves
inference by iteratively adding frames based on output consistency. We validate
our approach on multiple video reasoning benchmarks, showing that Video-RTS
surpasses existing video reasoning models by an average of 2.4% in accuracy
using only 3.6% training samples. For example, Video-RTS achieves a 4.2%
improvement on Video-Holmes, a recent and challenging video reasoning
benchmark, and a 2.6% improvement on MMVU. Notably, our pure RL training and
adaptive video TTS offer complementary strengths, enabling Video-RTS's strong
reasoning performance.

</details>


### [70] [Mask6D: Masked Pose Priors For 6D Object Pose Estimation](https://arxiv.org/abs/2507.06486)
*Yuechen Xie,Haobo Jiang,Jin Xie*

Main category: cs.CV

TL;DR: This paper proposes a pre-training strategy called Mask6D to enhance the robustness of 6D object pose estimation using monocular RGB images, especially in cluttered and occluded scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing pose estimation networks face challenges in extracting discriminative and pose-aware features from RGB images alone, particularly in cluttered or occluded settings.

Method: The authors introduced Mask6D, which incorporates 2D-3D correspondence and visible mask maps along with RGB inputs for reconstruction-based pre-training. A specialized loss function was also designed to emphasize object-focused features.

Result: The proposed Mask6D method demonstrated superior performance by outperforming previous end-to-end 6D pose estimation approaches in extensive experiments.

Conclusion: Mask6D effectively mitigates background interference and leverages pose-aware pre-training, making it more reliable for 6D object pose estimation in challenging environments.

Abstract: Robust 6D object pose estimation in cluttered or occluded conditions using
monocular RGB images remains a challenging task. One reason is that current
pose estimation networks struggle to extract discriminative, pose-aware
features using 2D feature backbones, especially when the available RGB
information is limited due to target occlusion in cluttered scenes. To mitigate
this, we propose a novel pose estimation-specific pre-training strategy named
Mask6D. Our approach incorporates pose-aware 2D-3D correspondence maps and
visible mask maps as additional modal information, which is combined with RGB
images for the reconstruction-based model pre-training. Essentially, this 2D-3D
correspondence maps a transformed 3D object model to 2D pixels, reflecting the
pose information of the target in camera coordinate system. Meanwhile, the
integrated visible mask map can effectively guide our model to disregard
cluttered background information. In addition, an object-focused pre-training
loss function is designed to further facilitate our network to remove the
background interference. Finally, we fine-tune our pre-trained pose prior-aware
network via conventional pose training strategy to realize the reliable pose
prediction. Extensive experiments verify that our method outperforms previous
end-to-end pose estimation methods.

</details>


### [71] [Bilateral Collaboration with Large Vision-Language Models for Open Vocabulary Human-Object Interaction Detection](https://arxiv.org/abs/2507.06510)
*Yupeng Hu,Changxing Ding,Chang Sun,Shaoli Huang,Xiangmin Xu*

Main category: cs.CV

TL;DR: The paper introduces BC-HOI, a Bilateral Collaboration framework aimed at improving open vocabulary Human-Object Interaction (HOI) detection by enhancing the interaction features' fine-grained quality.


<details>
  <summary>Details</summary>
Motivation: Address the need for improved HOI detection in open vocabulary tasks where traditional visual features from Vision-Language Models are too coarse-grained for detection.

Method: Introduces BC-HOI framework with two key components: (1) Attention Bias Guidance (ABG) to guide VLMs towards fine-grained instance-level features, and (2) Large Language Model-based Supervision Guidance (LSG) to provide token-level supervision enhancing ABG's quality.

Result: Experiments on HICO-DET and V-COCO benchmarks show consistently superior performance for open vocabulary and closed settings compared to existing methods.

Conclusion: BC-HOI framework effectively addresses shortcomings in open vocabulary HOI detection by refining interaction features, providing an innovative solution that outperforms existing approaches.

Abstract: Open vocabulary Human-Object Interaction (HOI) detection is a challenging
task that detects all <human, verb, object> triplets of interest in an image,
even those that are not pre-defined in the training set. Existing approaches
typically rely on output features generated by large Vision-Language Models
(VLMs) to enhance the generalization ability of interaction representations.
However, the visual features produced by VLMs are holistic and coarse-grained,
which contradicts the nature of detection tasks. To address this issue, we
propose a novel Bilateral Collaboration framework for open vocabulary HOI
detection (BC-HOI). This framework includes an Attention Bias Guidance (ABG)
component, which guides the VLM to produce fine-grained instance-level
interaction features according to the attention bias provided by the HOI
detector. It also includes a Large Language Model (LLM)-based Supervision
Guidance (LSG) component, which provides fine-grained token-level supervision
for the HOI detector by the LLM component of the VLM. LSG enhances the ability
of ABG to generate high-quality attention bias. We conduct extensive
experiments on two popular benchmarks: HICO-DET and V-COCO, consistently
achieving superior performance in the open vocabulary and closed settings. The
code will be released in Github.

</details>


### [72] [What Demands Attention in Urban Street Scenes? From Scene Understanding towards Road Safety: A Survey of Vision-driven Datasets and Studies](https://arxiv.org/abs/2507.06513)
*Yaoqi Huang,Julie Stephany Berrio,Mao Shan,Stewart Worrall*

Main category: cs.CV

TL;DR: A systematic survey categorizes traffic-related entities into anomalies and critical entities using a taxonomy, analyzes 35 vision tasks, reviews 73 datasets, and discusses weaknesses in road safety research.


<details>
  <summary>Details</summary>
Motivation: Improve road safety by leveraging advancements in vision-based sensors and computer vision algorithms.

Method: Develop a taxonomy categorizing traffic entities and analyze related vision tasks and datasets comprehensively.

Result: Highlights 35 vision-driven tasks and examines 73 datasets, assessing their pros and cons for standards unification and optimization.

Conclusion: Provides researchers with a structured overview, aids resource selection, and identifies research gaps for advancing in the field.

Abstract: Advances in vision-based sensors and computer vision algorithms have
significantly improved the analysis and understanding of traffic scenarios. To
facilitate the use of these improvements for road safety, this survey
systematically categorizes the critical elements that demand attention in
traffic scenarios and comprehensively analyzes available vision-driven tasks
and datasets. Compared to existing surveys that focus on isolated domains, our
taxonomy categorizes attention-worthy traffic entities into two main groups
that are anomalies and normal but critical entities, integrating ten categories
and twenty subclasses. It establishes connections between inherently related
fields and provides a unified analytical framework. Our survey highlights the
analysis of 35 vision-driven tasks and comprehensive examinations and
visualizations of 73 available datasets based on the proposed taxonomy. The
cross-domain investigation covers the pros and cons of each benchmark with the
aim of providing information on standards unification and resource
optimization. Our article concludes with a systematic discussion of the
existing weaknesses, underlining the potential effects and promising solutions
from various perspectives. The integrated taxonomy, comprehensive analysis, and
recapitulatory tables serve as valuable contributions to this rapidly evolving
field by providing researchers with a holistic overview, guiding strategic
resource selection, and highlighting critical research gaps.

</details>


### [73] [FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation](https://arxiv.org/abs/2507.06523)
*Liqiang Jing,Viet Lai,Seunghyun Yoon,Trung Bui,Xinya Du*

Main category: cs.CV

TL;DR: The paper introduces FIFA, a fidelity evaluation framework for VideoMLLMs, addressing hallucinations in free-form responses. Additionally, it proposes a Post-Correction method to improve responses.


<details>
  <summary>Details</summary>
Motivation: VideoMLLMs excel in video-related tasks but often produce hallucinated content. Evaluating faithfulness in their outputs, especially open-ended responses, remains unsatisfactory.

Method: The paper proposes FIFA, which utilizes Spatio-Temporal Semantic Dependency Graphs modeled via extracted descriptive facts, verified by VideoQA models, and complemented by a Post-Correction tool.

Result: FIFA better aligns with human evaluations compared to existing methods, and Post-Correction enhances factual consistency in generated outputs.

Conclusion: This framework remedies evaluation gaps in VideoMLLMs, setting a benchmark for assessing faithfulness and improving video-text generation.

Abstract: Video Multimodal Large Language Models (VideoMLLMs) have achieved remarkable
progress in both Video-to-Text and Text-to-Video tasks. However, they often
suffer fro hallucinations, generating content that contradicts the visual
input. Existing evaluation methods are limited to one task (e.g., V2T) and also
fail to assess hallucinations in open-ended, free-form responses. To address
this gap, we propose FIFA, a unified FaIthFulness evAluation framework that
extracts comprehensive descriptive facts, models their semantic dependencies
via a Spatio-Temporal Semantic Dependency Graph, and verifies them using
VideoQA models. We further introduce Post-Correction, a tool-based correction
framework that revises hallucinated content. Extensive experiments demonstrate
that FIFA aligns more closely with human judgment than existing evaluation
methods, and that Post-Correction effectively improves factual consistency in
both text and video generation.

</details>


### [74] [Concept Unlearning by Modeling Key Steps of Diffusion Process](https://arxiv.org/abs/2507.06526)
*Chaoshuo Zhang,Chenhao Lin,Zhengyu Zhao,Le Yang,Qian Wang,Chao Shen*

Main category: cs.CV

TL;DR: This paper introduces the Key Step Concept Unlearning (KSCU) method to balance unlearning unwanted concepts and retaining generative abilities in text-to-image diffusion models.


<details>
  <summary>Details</summary>
Motivation: To address the security risks posed by the misuse of text-to-image diffusion models, while preserving their generative capabilities.

Method: The KSCU method selectively focuses on and fine-tunes pivotal denoising steps in the generative process to unlearn specific concepts without affecting the rest of the model.

Result: KSCU effectively blocks unwanted image generation and retains the models' ability to generate high-quality content.

Conclusion: By strategically targeting key generative steps, KSCU achieves a superior balance of security and generative capacity, with experimental benchmarks confirming its effectiveness.

Abstract: Text-to-image diffusion models (T2I DMs), represented by Stable Diffusion,
which generate highly realistic images based on textual input, have been widely
used. However, their misuse poses serious security risks. While existing
concept unlearning methods aim to mitigate these risks, they struggle to
balance unlearning effectiveness with generative retainability.To overcome this
limitation, we innovatively propose the Key Step Concept Unlearning (KSCU)
method, which ingeniously capitalizes on the unique stepwise sampling
characteristic inherent in diffusion models during the image generation
process. Unlike conventional approaches that treat all denoising steps equally,
KSCU strategically focuses on pivotal steps with the most influence over the
final outcome by dividing key steps for different concept unlearning tasks and
fine-tuning the model only at those steps. This targeted approach reduces the
number of parameter updates needed for effective unlearning, while maximizing
the retention of the model's generative capabilities.Through extensive
benchmark experiments, we demonstrate that KSCU effectively prevents T2I DMs
from generating undesirable images while better retaining the model's
generative capabilities.Our code will be released.

</details>


### [75] [Speak2Sign3D: A Multi-modal Pipeline for English Speech to American Sign Language Animation](https://arxiv.org/abs/2507.06530)
*Kazi Mahathir Rahman,Naveed Imtiaz Nafis,Md. Farhan Sadik,Mohammad Al Rafi,Mehedi Hasan Shahed*

Main category: cs.CV

TL;DR: This paper introduces a pipeline to translate spoken English into lifelike 3D sign language animations, using speech-to-text translation, American Sign Language (ASL) gloss generation, and motion generation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to assist deaf and hard-of-hearing individuals by focusing on creating natural sign language animations from spoken English, an area often overlooked compared to text translation.

Method: The method integrates Whisper for speech-to-text, MarianMT for text to ASL gloss translation, and a 3D motion system trained on data from Sign3D-WLASL to animate the gloss. Word embeddings and an additional dataset, BookGlossCorpus-CG, improve translation accuracy and animation quality.

Result: The system demonstrates effective ASL gloss translation with high BLEU scores (0.7714, 0.8923) and produces smooth, realistic 3D animations by optimizing the transition between signs.

Conclusion: The research successfully delivers a comprehensive framework that bridges speech, text, and motion, extending past works and offering a full translation pipeline from English speech to 3D sign language.

Abstract: Helping deaf and hard-of-hearing people communicate more easily is the main
goal of Automatic Sign Language Translation. Although most past research has
focused on turning sign language into text, doing the reverse, turning spoken
English into sign language animations, has been largely overlooked. That's
because it involves multiple steps, such as understanding speech, translating
it into sign-friendly grammar, and generating natural human motion. In this
work, we introduce a complete pipeline that converts English speech into
smooth, realistic 3D sign language animations. Our system starts with Whisper
to translate spoken English into text. Then, we use a MarianMT machine
translation model to translate that text into American Sign Language (ASL)
gloss, a simplified version of sign language that captures meaning without
grammar. This model performs well, reaching BLEU scores of 0.7714 and 0.8923.
To make the gloss translation more accurate, we also use word embeddings such
as Word2Vec and FastText to understand word meanings. Finally, we animate the
translated gloss using a 3D keypoint-based motion system trained on
Sign3D-WLASL, a dataset we created by extracting body, hand, and face key
points from real ASL videos in the WLASL dataset. To support the gloss
translation stage, we also built a new dataset called BookGlossCorpus-CG, which
turns everyday English sentences from the BookCorpus dataset into ASL gloss
using grammar rules. Our system stitches everything together by smoothly
interpolating between signs to create natural, continuous animations. Unlike
previous works like How2Sign and Phoenix-2014T that focus on recognition or use
only one type of data, our pipeline brings together audio, text, and motion in
a single framework that goes all the way from spoken English to lifelike 3D
sign language animation.

</details>


### [76] [ILNet: Trajectory Prediction with Inverse Learning Attention for Enhancing Intention Capture](https://arxiv.org/abs/2507.06531)
*Mingjin Zeng,Nan Ouyang,Wenkang Wan,Lei Ao,Qing Cai,Kai Sheng*

Main category: cs.CV

TL;DR: ILNet enhances multi-agent trajectory prediction with two main innovations: Inverse Learning (IL) attention for spatio-temporal interactions and Dynamic Anchor Selection (DAS) for adaptive trajectory modeling, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in trajectory prediction approaches that struggle with static modeling and fixed-anchor strategies, which fail to capture complex, dynamic interactions in multi-agent driving environments.

Method: The paper introduces ILNet, which incorporates Inverse Learning (IL) attention to model interaction intentions over time and a Dynamic Anchor Selection (DAS) module that adaptively identifies key trajectory points for future scenarios.

Result: ILNet achieves state-of-the-art performance on INTERACTION and Argoverse datasets, with improved accuracy and multimodal trajectory distributions in complex interaction scenarios while maintaining low parameter growth.

Conclusion: ILNet effectively models spatio-temporal dynamics and adapts to varying interaction scenarios, outperforming prior methods and providing a more robust foundation for trajectory prediction in multi-agent environments.

Abstract: Trajectory prediction for multi-agent interaction scenarios is a crucial
challenge. Most advanced methods model agent interactions by efficiently
factorized attention based on the temporal and agent axes. However, this static
and foward modeling lacks explicit interactive spatio-temporal coordination,
capturing only obvious and immediate behavioral intentions. Alternatively, the
modern trajectory prediction framework refines the successive predictions by a
fixed-anchor selection strategy, which is difficult to adapt in different
future environments. It is acknowledged that human drivers dynamically adjust
initial driving decisions based on further assumptions about the intentions of
surrounding vehicles. Motivated by human driving behaviors, this paper proposes
ILNet, a multi-agent trajectory prediction method with Inverse Learning (IL)
attention and Dynamic Anchor Selection (DAS) module. IL Attention employs an
inverse learning paradigm to model interactions at neighboring moments,
introducing proposed intentions to dynamically encode the spatio-temporal
coordination of interactions, thereby enhancing the model's ability to capture
complex interaction patterns. Then, the learnable DAS module is proposed to
extract multiple trajectory change keypoints as anchors in parallel with almost
no increase in parameters. Experimental results show that the ILNet achieves
state-of-the-art performance on the INTERACTION and Argoverse motion
forecasting datasets. Particularly, in challenged interaction scenarios, ILNet
achieves higher accuracy and more multimodal distributions of trajectories over
fewer parameters. Our codes are available at https://github.com/mjZeng11/ILNet.

</details>


### [77] [A model-agnostic active learning approach for animal detection from camera traps](https://arxiv.org/abs/2507.06537)
*Thi Thu Thuy Nguyen,Duc Thanh Nguyen*

Main category: cs.CV

TL;DR: The paper introduces a model-agnostic active learning method for camera trap wildlife monitoring that achieves strong results while requiring significantly fewer labeled samples.


<details>
  <summary>Details</summary>
Motivation: Data labeling for camera trap wildlife monitoring is resource-intensive. Existing active learning methods depend on full accessibility to machine learning models, limiting their use in such applications.

Method: A model-agnostic active learning technique that selects informative samples by integrating uncertainty and diversity metrics at both object and image levels.

Result: The approach enables a state-of-the-art animal detector to perform as well as using the full dataset by leveraging only 30% of the training data.

Conclusion: The model-agnostic active learning method significantly reduces data labeling efforts while maintaining model performance, aiding automated wildlife monitoring and conservation efforts.

Abstract: Smart data selection is becoming increasingly important in data-driven
machine learning. Active learning offers a promising solution by allowing
machine learning models to be effectively trained with optimal data including
the most informative samples from large datasets. Wildlife data captured by
camera traps are excessive in volume, requiring tremendous effort in data
labelling and animal detection models training. Therefore, applying active
learning to optimise the amount of labelled data would be a great aid in
enabling automated wildlife monitoring and conservation. However, existing
active learning techniques require that a machine learning model (i.e., an
object detector) be fully accessible, limiting the applicability of the
techniques. In this paper, we propose a model-agnostic active learning approach
for detection of animals captured by camera traps. Our approach integrates
uncertainty and diversity quantities of samples at both the object-based and
image-based levels into the active learning sample selection process. We
validate our approach in a benchmark animal dataset. Experimental results
demonstrate that, using only 30% of the training data selected by our approach,
a state-of-the-art animal detector can achieve a performance of equal or
greater than that with the use of the complete training dataset.

</details>


### [78] [Token Bottleneck: One Token to Remember Dynamics](https://arxiv.org/abs/2507.06543)
*Taekyung Kim,Dongyoon Han,Byeongho Heo,Jeongeun Park,Sangdoo Yun*

Main category: cs.CV

TL;DR: The paper proposes Token Bottleneck (ToBo), a self-supervised learning method for compact and temporally aware scene representations, showing its effectiveness in tasks like video label propagation and robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to derive compact and temporally aware visual representations from dynamic scenes for better performance in sequential scene understanding tasks.

Method: The method involves the ToBo pipeline, using a bottleneck token to encode scenes and predicting temporal dynamics with minimal target patches, enhancing temporal awareness and representation learning.

Result: Experiments reveal that ToBo outperforms other methods in sequential tasks such as video label propagation and robotic manipulation in both simulated and real-world conditions.

Conclusion: ToBo is effective, robust, and scalable for deriving temporal and compact visual representations, confirming its utility across various model scales and environments.

Abstract: Deriving compact and temporally aware visual representations from dynamic
scenes is essential for successful execution of sequential scene understanding
tasks such as visual tracking and robotic manipulation. In this paper, we
introduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised
learning pipeline that squeezes a scene into a bottleneck token and predicts
the subsequent scene using minimal patches as hints. The ToBo pipeline
facilitates the learning of sequential scene representations by conservatively
encoding the reference scene into a compact bottleneck token during the squeeze
step. In the expansion step, we guide the model to capture temporal dynamics by
predicting the target scene using the bottleneck token along with few target
patches as hints. This design encourages the vision backbone to embed temporal
dependencies, thereby enabling understanding of dynamic transitions across
scenes. Extensive experiments in diverse sequential tasks, including video
label propagation and robot manipulation in simulated environments demonstrate
the superiority of ToBo over baselines. Moreover, deploying our pre-trained
model on physical robots confirms its robustness and effectiveness in
real-world environments. We further validate the scalability of ToBo across
different model scales.

</details>


### [79] [Concept-TRAK: Understanding how diffusion models learn concepts through concept-level attribution](https://arxiv.org/abs/2507.06547)
*Yonghyun Park,Chieh-Hsin Lai,Satoshi Hayakawa,Yuhta Takida,Naoki Murata,Wei-Hsiang Liao,Woosung Choi,Kin Wai Cheuk,Junghyun Koo,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: The paper introduces Concept-TRAK, a method for concept-level attribution in diffusion models, addressing their transparency and copyright concerns.


<details>
  <summary>Details</summary>
Motivation: To tackle the lack of fine-grained attribution methods for determining specific element contributions (styles, objects) in diffusion models.

Method: Proposes Concept-TRAK with two innovations: reformulated diffusion training loss and a concept-aware reward function, allowing precise attribution.

Result: Concept-TRAK outperforms prior methods on the AbC benchmark and provides actionable insights through diverse case studies.

Conclusion: Concept-level attribution is essential for responsible AI, aiding in copyright enforcement, safety, and model transparency.

Abstract: While diffusion models excel at image generation, their growing adoption
raises critical concerns around copyright issues and model transparency.
Existing attribution methods identify training examples influencing an entire
image, but fall short in isolating contributions to specific elements, such as
styles or objects, that matter most to stakeholders. To bridge this gap, we
introduce \emph{concept-level attribution} via a novel method called
\emph{Concept-TRAK}. Concept-TRAK extends influence functions with two key
innovations: (1) a reformulated diffusion training loss based on diffusion
posterior sampling, enabling robust, sample-specific attribution; and (2) a
concept-aware reward function that emphasizes semantic relevance. We evaluate
Concept-TRAK on the AbC benchmark, showing substantial improvements over prior
methods. Through diverse case studies--ranging from identifying IP-protected
and unsafe content to analyzing prompt engineering and compositional
learning--we demonstrate how concept-level attribution yields actionable
insights for responsible generative AI development and governance.

</details>


### [80] [MK-Pose: Category-Level Object Pose Estimation via Multimodal-Based Keypoint Learning](https://arxiv.org/abs/2507.06662)
*Yifan Yang,Peili Song,Enfan Lan,Dong Liu,Jingtai Liu*

Main category: cs.CV

TL;DR: This paper introduces MK-Pose, a multimodal framework for category-level object pose estimation by combining RGB images, point clouds, and textual descriptions to overcome occlusion and generalization issues.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in pose estimation for objects within categories, specifically combating occlusion and improving generalization across various instances and categories, vital for sectors like warehouse automation.

Method: MK-Pose employs a multimodal approach integrating RGB images, point clouds, and textual descriptions, with self-supervised keypoint detection features such as attention-based queries, soft heatmap matching, and graph-based modeling. A graph-enhanced feature fusion module merges global and local information.

Result: Evaluations on CAMERA25, REAL275, and HouseCat6D datasets demonstrate that MK-Pose surpasses state-of-the-art pose estimation methods in IoU and average precision metrics without relying on shape priors.

Conclusion: MK-Pose effectively integrates multimodal data and advanced modeling techniques to enhance category-level object pose estimation, offering improved accuracy and generalization, with promising results across multiple datasets.

Abstract: Category-level object pose estimation, which predicts the pose of objects
within a known category without prior knowledge of individual instances, is
essential in applications like warehouse automation and manufacturing. Existing
methods relying on RGB images or point cloud data often struggle with object
occlusion and generalization across different instances and categories. This
paper proposes a multimodal-based keypoint learning framework (MK-Pose) that
integrates RGB images, point clouds, and category-level textual descriptions.
The model uses a self-supervised keypoint detection module enhanced with
attention-based query generation, soft heatmap matching and graph-based
relational modeling. Additionally, a graph-enhanced feature fusion module is
designed to integrate local geometric information and global context. MK-Pose
is evaluated on CAMERA25 and REAL275 dataset, and is further tested for
cross-dataset capability on HouseCat6D dataset. The results demonstrate that
MK-Pose outperforms existing state-of-the-art methods in both IoU and average
precision without shape priors. Codes will be released at
\href{https://github.com/yangyifanYYF/MK-Pose}{https://github.com/yangyifanYYF/MK-Pose}.

</details>


### [81] [Divergence-Based Similarity Function for Multi-View Contrastive Learning](https://arxiv.org/abs/2507.06560)
*Jae Hyoung Jeon,Cheolsu Lim,Myungjoo Kang*

Main category: cs.CV

TL;DR: This paper introduces a divergence-based similarity function (DSF) that captures joint structure across augmented views for contrastive learning, improving performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current contrastive learning methods primarily use pairwise relationships between augmented views, missing the opportunity to model joint structures across all views.

Method: The proposed method introduces DSF, representing augmented view sets as distributions and using divergence between these distributions as a similarity measure, with theoretical links to cosine similarity.

Result: Experiments demonstrate DSF’s consistent improvements in tasks like kNN classification and linear evaluation, along with efficiency gains over other methods.

Conclusion: DSF effectively captures joint view structures, improves task performance, and eliminates the need for temperature hyperparameters seen in cosine similarity.

Abstract: Recent success in contrastive learning has sparked growing interest in more
effectively leveraging multiple augmented views of an instance. While prior
methods incorporate multiple views at the loss or feature level, they primarily
capture pairwise relationships and fail to model the joint structure across all
views. In this work, we propose a divergence-based similarity function (DSF)
that explicitly captures the joint structure by representing each set of
augmented views as a distribution and measuring similarity as the divergence
between distributions. Extensive experiments demonstrate that DSF consistently
improves performance across various tasks, including kNN classification and
linear evaluation, while also offering greater efficiency compared to other
multi-view methods. Furthermore, we establish a theoretical connection between
DSF and cosine similarity, and show that, unlike cosine similarity, DSF
operates effectively without requiring a temperature hyperparameter.

</details>


### [82] [StixelNExT++: Lightweight Monocular Scene Segmentation and Representation for Collective Perception](https://arxiv.org/abs/2507.06687)
*Marcel Vosshans,Omar Ait-Aider,Youcef Mezouar,Markus Enzweiler*

Main category: cs.CV

TL;DR: This paper introduces StixelNExT++, a lightweight monocular scene representation method achieving real-time object segmentation and 3D scene comprehension.


<details>
  <summary>Details</summary>
Motivation: To enhance scene representation for monocular perception systems and enable efficient object segmentation and 3D inference.

Method: A novel approach using 3D Stixels clustering with a lightweight neural network trained on LiDAR-based ground truth, ensuring adaptability and real-time performance.

Result: StixelNExT++ demonstrated high compression, real-time computation at 10 ms/frame, and competitive performance within a 30-meter range on the Waymo dataset.

Conclusion: The method proves promising for collective perception in autonomous systems, offering efficiency and adaptability for real-world applications.

Abstract: This paper presents StixelNExT++, a novel approach to scene representation
for monocular perception systems. Building on the established Stixel
representation, our method infers 3D Stixels and enhances object segmentation
by clustering smaller 3D Stixel units. The approach achieves high compression
of scene information while remaining adaptable to point cloud and
bird's-eye-view representations. Our lightweight neural network, trained on
automatically generated LiDAR-based ground truth, achieves real-time
performance with computation times as low as 10 ms per frame. Experimental
results on the Waymo dataset demonstrate competitive performance within a
30-meter range, highlighting the potential of StixelNExT++ for collective
perception in autonomous systems.

</details>


### [83] [Edge-Boundary-Texture Loss: A Tri-Class Generalization of Weighted Binary Cross-Entropy for Enhanced Edge Detection](https://arxiv.org/abs/2507.06569)
*Hao Shu*

Main category: cs.CV

TL;DR: The paper introduces the Edge-Boundary-Texture (EBT) loss for edge detection, improving precision and context localization by categorizing pixels into edge, boundary, and texture.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings of the Weighted Binary Cross-Entropy (WBCE) loss, which treats non-edge pixels uniformly, leading to blurred edge predictions.

Method: The authors propose the EBT loss, which divides pixels into three categories (edge, boundary, texture) and assigns unique supervisory weights. This enables more focused learning.

Result: The EBT loss outperforms WBCE in edge detection benchmarks, delivering superior quantitative and perceptual results. It also generalizes WBCE as a limit case.

Conclusion: The EBT loss enhances edge detection performance with minimal fine-tuning required, making it practical and robust across models and datasets.

Abstract: Edge detection (ED) remains a fundamental task in computer vision, yet its
performance is often hindered by the ambiguous nature of non-edge pixels near
object boundaries. The widely adopted Weighted Binary Cross-Entropy (WBCE) loss
treats all non-edge pixels uniformly, overlooking the structural nuances around
edges and often resulting in blurred predictions. In this paper, we propose the
Edge-Boundary-Texture (EBT) loss, a novel objective that explicitly divides
pixels into three categories, edge, boundary, and texture, and assigns each a
distinct supervisory weight. This tri-class formulation enables more structured
learning by guiding the model to focus on both edge precision and contextual
boundary localization. We theoretically show that the EBT loss generalizes the
WBCE loss, with the latter becoming a limit case. Extensive experiments across
multiple benchmarks demonstrate the superiority of the EBT loss both
quantitatively and perceptually. Furthermore, the consistent use of unified
hyperparameters across all models and datasets, along with robustness to their
moderate variations, indicates that the EBT loss requires minimal fine-tuning
and is easily deployable in practice.

</details>


### [84] [A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding](https://arxiv.org/abs/2507.06719)
*Zhenyang Liu,Sixiao Zheng,Siyu Chen,Cairong Zhao,Longfei Liang,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TL;DR: The paper proposes SpatialReasoner, a framework combining large language models and enhanced visual representations to improve open-vocabulary 3D visual grounding by addressing spatial reasoning in language queries and 3D scenes.


<details>
  <summary>Details</summary>
Motivation: Open-vocabulary 3D visual grounding is essential for embodied AI tasks, but current methods struggle with correctly interpreting spatial relationships in language queries, such as those describing object positions, limiting their effectiveness.

Method: SpatialReasoner enhances 3D visual grounding using a fine-tuned large language model for spatial reasoning in queries, combined with visual property-enhanced hierarchical feature fields. It incorporates CLIP features and Segment Anything Model (SAM) masks for improved instance localization.

Result: Experiments demonstrate that SpatialReasoner integrates with neural representations and outperforms baseline models in 3D visual grounding tasks while significantly improving spatial reasoning capabilities.

Conclusion: SpatialReasoner bridges the gap in spatial reasoning for 3D visual grounding and shows potential for more accurate localization of objects in complex embodied AI applications.

Abstract: Open-vocabulary 3D visual grounding aims to localize target objects based on
free-form language queries, which is crucial for embodied AI applications such
as autonomous navigation, robotics, and augmented reality. Learning 3D language
fields through neural representations enables accurate understanding of 3D
scenes from limited viewpoints and facilitates the localization of target
objects in complex environments. However, existing language field methods
struggle to accurately localize instances using spatial relations in language
queries, such as ``the book on the chair.'' This limitation mainly arises from
inadequate reasoning about spatial relations in both language queries and 3D
scenes. In this work, we propose SpatialReasoner, a novel neural
representation-based framework with large language model (LLM)-driven spatial
reasoning that constructs a visual properties-enhanced hierarchical feature
field for open-vocabulary 3D visual grounding. To enable spatial reasoning in
language queries, SpatialReasoner fine-tunes an LLM to capture spatial
relations and explicitly infer instructions for the target, anchor, and spatial
relation. To enable spatial reasoning in 3D scenes, SpatialReasoner
incorporates visual properties (opacity and color) to construct a hierarchical
feature field. This field represents language and instance features using
distilled CLIP features and masks extracted via the Segment Anything Model
(SAM). The field is then queried using the inferred instructions in a
hierarchical manner to localize the target 3D instance based on the spatial
relation in the language query. Extensive experiments show that our framework
can be seamlessly integrated into different neural representations,
outperforming baseline models in 3D visual grounding while empowering their
spatial reasoning capability.

</details>


### [85] [MOST: Motion Diffusion Model for Rare Text via Temporal Clip Banzhaf Interaction](https://arxiv.org/abs/2507.06590)
*Yin Wang,Mu li,Zhiying Leng,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.CV

TL;DR: MOST introduces a motion diffusion model leveraging temporal clip Banzhaf interaction to boost text-to-motion generation with rare language prompts, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating human motion aligned with rare language prompts by overcoming coarse-grained matching and redundancy issues.

Method: MOST employs temporal clip Banzhaf interaction for fine-grained text-to-motion coherence and uses a motion prompt module for generating consistent movements.

Result: MOST delivers state-of-the-art performance in text-to-motion retrieval and generation, demonstrating its efficiency, particularly for rare prompts.

Conclusion: This paper comprehensively addresses challenges in text-to-motion generation, confirming the effectiveness of MOST through both quantitative and qualitative evaluations.

Abstract: We introduce MOST, a novel motion diffusion model via temporal clip Banzhaf
interaction, aimed at addressing the persistent challenge of generating human
motion from rare language prompts. While previous approaches struggle with
coarse-grained matching and overlook important semantic cues due to motion
redundancy, our key insight lies in leveraging fine-grained clip relationships
to mitigate these issues. MOST's retrieval stage presents the first formulation
of its kind - temporal clip Banzhaf interaction - which precisely quantifies
textual-motion coherence at the clip level. This facilitates direct,
fine-grained text-to-motion clip matching and eliminates prevalent redundancy.
In the generation stage, a motion prompt module effectively utilizes retrieved
motion clips to produce semantically consistent movements. Extensive
evaluations confirm that MOST achieves state-of-the-art text-to-motion
retrieval and generation performance by comprehensively addressing previous
challenges, as demonstrated through quantitative and qualitative results
highlighting its effectiveness, especially for rare prompts.

</details>


### [86] [Hallucinating 360°: Panoramic Street-View Generation via Local Scenes Diffusion and Probabilistic Prompting](https://arxiv.org/abs/2507.06971)
*Fei Teng,Kai Luo,Sheng Wu,Siyu Li,Pujun Guo,Jiale Wei,Kunyu Peng,Jiaming Zhang,Kailun Yang*

Main category: cs.CV

TL;DR: The paper proposes Percep360, a novel method for generating high-quality, controllable panoramic data for autonomous driving, overcoming data distribution and coherence challenges.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving requires comprehensive and high-quality panoramic data, yet traditional methods are labor-intensive and lack controllability in generation.

Method: The paper introduces two methods: Local Scenes Diffusion Method (LSDM) for coherent panoramic generation and Probabilistic Prompting Method (PPM) for controllability in data generation.

Result: Percep360 consistently produces higher-quality panoramic data and enhances the performance of downstream perception models compared to traditional methods.

Conclusion: Percep360 effectively addresses coherence and controllability challenges in panoramic data generation, offering advantages in autonomous driving tasks, with the source code available for further exploration.

Abstract: Panoramic perception holds significant potential for autonomous driving,
enabling vehicles to acquire a comprehensive 360{\deg} surround view in a
single shot. However, autonomous driving is a data-driven task. Complete
panoramic data acquisition requires complex sampling systems and annotation
pipelines, which are time-consuming and labor-intensive. Although existing
street view generation models have demonstrated strong data regeneration
capabilities, they can only learn from the fixed data distribution of existing
datasets and cannot achieve high-quality, controllable panoramic generation. In
this paper, we propose the first panoramic generation method Percep360 for
autonomous driving. Percep360 enables coherent generation of panoramic data
with control signals based on the stitched panoramic data. Percep360 focuses on
two key aspects: coherence and controllability. Specifically, to overcome the
inherent information loss caused by the pinhole sampling process, we propose
the Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama
generation as a spatially continuous diffusion process, bridging the gaps
between different data distributions. Additionally, to achieve the controllable
generation of panoramic images, we propose a Probabilistic Prompting Method
(PPM). PPM dynamically selects the most relevant control cues, enabling
controllable panoramic image generation. We evaluate the effectiveness of the
generated images from three perspectives: image quality assessment (i.e.,
no-reference and with reference), controllability, and their utility in
real-world Bird's Eye View (BEV) segmentation. Notably, the generated data
consistently outperforms the original stitched images in no-reference quality
metrics and enhances downstream perception models. The source code will be
publicly available at https://github.com/Bryant-Teng/Percep360.

</details>


### [87] [Ambiguity-aware Point Cloud Segmentation by Adaptive Margin Contrastive Learning](https://arxiv.org/abs/2507.06592)
*Yang Chen,Yueqi Duan,Haowen Sun,Jiwen Lu,Yap-Peng Tan*

Main category: cs.CV

TL;DR: The paper proposes AMContrast3D, an adaptive contrastive learning method for improving 3D semantic segmentation on point clouds by addressing point-specific ambiguities, and extends it to AMContrast3D++ with parallel branches and ambiguity prediction for enhanced performance.


<details>
  <summary>Details</summary>
Motivation: Current 3D semantic segmentation methods inadequately handle ambiguous and less discriminative points, particularly in transition regions where annotated labels are often unreliable.

Method: The paper introduces AMContrast3D, which incorporates contrastive learning with an ambiguity estimation framework. It adapts training objectives based on the ambiguity of individual points and is extended to AMContrast3D++, integrating a new ambiguity prediction module trained in parallel.

Result: Experimental results on S3DIS and ScanNet datasets demonstrate that the proposed methods achieve improved segmentation performance and robustness.

Conclusion: AMContrast3D and its extended version AMContrast3D++ effectively address point ambiguities in 3D semantic segmentation, boosting performance through adaptive objectives and ambiguity-aware training.

Abstract: This paper proposes an adaptive margin contrastive learning method for 3D
semantic segmentation on point clouds. Most existing methods use equally
penalized objectives, which ignore the per-point ambiguities and less
discriminated features stemming from transition regions. However, as highly
ambiguous points may be indistinguishable even for humans, their manually
annotated labels are less reliable, and hard constraints over these points
would lead to sub-optimal models. To address this, we first design
AMContrast3D, a method comprising contrastive learning into an ambiguity
estimation framework, tailored to adaptive objectives for individual points
based on ambiguity levels. As a result, our method promotes model training,
which ensures the correctness of low-ambiguity points while allowing mistakes
for high-ambiguity points. As ambiguities are formulated based on position
discrepancies across labels, optimization during inference is constrained by
the assumption that all unlabeled points are uniformly unambiguous, lacking
ambiguity awareness. Inspired by the insight of joint training, we further
propose AMContrast3D++ integrating with two branches trained in parallel, where
a novel ambiguity prediction module concurrently learns point ambiguities from
generated embeddings. To this end, we design a masked refinement mechanism that
leverages predicted ambiguities to enable the ambiguous embeddings to be more
reliable, thereby boosting segmentation performance and enhancing robustness.
Experimental results on 3D indoor scene datasets, S3DIS and ScanNet,
demonstrate the effectiveness of the proposed method. Code is available at
https://github.com/YangChenApril/AMContrast3D.

</details>


### [88] [Capturing Stable HDR Videos Using a Dual-Camera System](https://arxiv.org/abs/2507.06593)
*Qianyu Zhang,Bolun Zheng,Hangjia Pan,Lingyu Zhu,Zunjie Zhu,Zongpeng Li,Shiqi Wang*

Main category: cs.CV

TL;DR: This paper proposes a novel dual-camera system (DCS) with an exposure-adaptive fusion network (EAFNet) to address flickering issues and improve HDR video reconstruction, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the flickering problem in HDR video reconstruction caused by exposure fluctuations in reference images captured using alternating exposure methods.

Method: The authors introduce a dual-camera system where one camera captures consistent reference sequences and the other captures supplementary sequences. They design an exposure-adaptive fusion network (EAFNet), incorporating a pre-alignment subnetwork for emphasizing valuable features across exposure levels, an asymmetric cross-feature fusion subnetwork for aligning and fusing features, and a reconstruction subnetwork using multiscale architecture to minimize ghosting artifacts.

Result: Experimental evaluations demonstrate that the DCS combined with EAFNet delivers superior performance across various datasets for HDR video reconstruction.

Conclusion: The proposed approach shows great promise for HDR video reconstruction by resolving exposure fluctuation issues and achieving robust, high-quality results, paving the way for future research and applications in the field.

Abstract: In HDR video reconstruction, exposure fluctuations in reference images from
alternating exposure methods often result in flickering. To address this issue,
we propose a dual-camera system (DCS) for HDR video acquisition, where one
camera is assigned to capture consistent reference sequences, while the other
is assigned to capture non-reference sequences for information supplementation.
To tackle the challenges posed by video data, we introduce an exposure-adaptive
fusion network (EAFNet) to achieve more robust results. EAFNet introduced a
pre-alignment subnetwork to explore the influence of exposure, selectively
emphasizing the valuable features across different exposure levels. Then, the
enhanced features are fused by the asymmetric cross-feature fusion subnetwork,
which explores reference-dominated attention maps to improve image fusion by
aligning cross-scale features and performing cross-feature fusion. Finally, the
reconstruction subnetwork adopts a DWT-based multiscale architecture to reduce
ghosting artifacts and refine features at different resolutions. Extensive
experimental evaluations demonstrate that the proposed method achieves
state-of-the-art performance on different datasets, validating the great
potential of the DCS in HDR video reconstruction. The codes and data captured
by DCS will be available at https://github.com/zqqqyu/DCS.

</details>


### [89] [Cross-Modal Dual-Causal Learning for Long-Term Action Recognition](https://arxiv.org/abs/2507.06603)
*Xu Shaowu,Jia Xibin,Gao Junyu,Sun Qianmei,Chang Jing,Fan Chao*

Main category: cs.CV

TL;DR: The paper tackles challenges in long-term action recognition (LTAR) by proposing a causal model for cross-modal bias correction in vision-language models (VLMs), achieving improved performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Long-term action recognition (LTAR) is hindered by complex temporal and cross-modal biases in visual and language data that existing models, including VLMs, fail to effectively address causally.

Method: The authors introduce Cross-Modal Dual-Causal Learning (CMDCL), which employs structural causal models for cross-modal interventions. This method removes text biases via textual causal intervention and handles visual confounders using visual causal interventions guided by text.

Result: CMDCL achieves superior performance on three LTAR benchmark datasets, including Charades, Breakfast, and COIN, validating the robustness of the dual-causal approach.

Conclusion: The proposed CMDCL method improves action recognition by effectively addressing cross-modal biases through dual-causal interventions, demonstrating its utility in LTAR tasks.

Abstract: Long-term action recognition (LTAR) is challenging due to extended temporal
spans with complex atomic action correlations and visual confounders. Although
vision-language models (VLMs) have shown promise, they often rely on
statistical correlations instead of causal mechanisms. Moreover, existing
causality-based methods address modal-specific biases but lack cross-modal
causal modeling, limiting their utility in VLM-based LTAR. This paper proposes
\textbf{C}ross-\textbf{M}odal \textbf{D}ual-\textbf{C}ausal \textbf{L}earning
(CMDCL), which introduces a structural causal model to uncover causal
relationships between videos and label texts.
  CMDCL addresses cross-modal biases in text embeddings via textual causal
intervention and removes confounders inherent in the visual modality through
visual causal intervention guided by the debiased text.
  These dual-causal interventions enable robust action representations to
address LTAR challenges. Experimental results on three benchmarks including
Charades, Breakfast and COIN, demonstrate the effectiveness of the proposed
model. Our code is available at https://github.com/xushaowu/CMDCL.

</details>


### [90] [Omni-Fusion of Spatial and Spectral for Hyperspectral Image Segmentation](https://arxiv.org/abs/2507.06606)
*Qing Zhang,Guoquan Pei,Yan Wang*

Main category: cs.CV

TL;DR: The paper introduces "Omni-Fuse," a novel spatial-spectral omni-fusion network designed to improve hyperspectral image segmentation by addressing the high dimensionality and redundancy in medical hyperspectral imaging (MHSI). Results show significant performance improvement compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome challenges in utilizing the high-dimensional and spectrally redundant data from medical hyperspectral imaging for disease diagnosis and hyperspectral image segmentation.

Method: A new network called Omni-Fuse is proposed, incorporating cross-dimensional fusion features. It includes a bidirectional attention mechanism, a spectral-guided spatial query selection, and a two-stage decoder that dynamically aligns focus on the selected spatial query.

Result: The Omni-Fuse model demonstrated over 5.73% improvement in DSC compared to existing methods, indicating its superior segmentation performance on two microscopic hyperspectral datasets.

Conclusion: Omni-Fuse enhances segmentation efficiency and accuracy in MHSI while managing computational efficiency, making it a promising tool for computational pathology.

Abstract: Medical Hyperspectral Imaging (MHSI) has emerged as a promising tool for
enhanced disease diagnosis, particularly in computational pathology, offering
rich spectral information that aids in identifying subtle biochemical
properties of tissues. Despite these advantages, effectively fusing both
spatial-dimensional and spectral-dimensional information from MHSIs remains
challenging due to its high dimensionality and spectral redundancy inherent
characteristics. To solve the above challenges, we propose a novel
spatial-spectral omni-fusion network for hyperspectral image segmentation,
named as Omni-Fuse. Here, we introduce abundant cross-dimensional feature
fusion operations, including a cross-dimensional enhancement module that
refines both spatial and spectral features through bidirectional attention
mechanisms, a spectral-guided spatial query selection to select the most
spectral-related spatial feature as the query, and a two-stage
cross-dimensional decoder which dynamically guide the model to focus on the
selected spatial query. Despite of numerous attention blocks, Omni-Fuse remains
efficient in execution. Experiments on two microscopic hyperspectral image
datasets show that our approach can significantly improve the segmentation
performance compared with the state-of-the-art methods, with over 5.73 percent
improvement in DSC. Code available at:
https://github.com/DeepMed-Lab-ECNU/Omni-Fuse.

</details>


### [91] [PointVDP: Learning View-Dependent Projection by Fireworks Rays for 3D Point Cloud Segmentation](https://arxiv.org/abs/2507.06618)
*Yang Chen,Yueqi Duan,Haowen Sun,Ziwei Wang,Jiwen Lu,Yap-Peng Tan*

Main category: cs.CV

TL;DR: This paper introduces a view-dependent projection (VDP) method for point cloud segmentation that adapitates dynamically to 3D spatial geometry, addressing inefficiencies in view-independent methods and improving performance with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Current projection-based methods for point cloud segmentation are limited by static and human-defined parameters, leading to inefficiencies in data processing and inability to capture diverse spatial features across different views.

Method: The authors proposed a data-driven framework called View-Dependent Projection (VDP), inspired by adaptive fireworks patterns, for generating efficient 3D-to-2D projections. Additionally, a color regularization approach was constructed to optimize feature emphasis and space utilization in the projections.

Result: The proposed method, PointVDP, delivered competitive results on S3DIS and ScanNet benchmarks while maintaining lightweight computational costs.

Conclusion: VDP offers a resource-efficient and effective approach to improve semantic understanding in point cloud segmentation, achieving dynamic and informative projections with marginal computational overhead.

Abstract: In this paper, we propose view-dependent projection (VDP) to facilitate point
cloud segmentation, designing efficient 3D-to-2D mapping that dynamically
adapts to the spatial geometry from view variations. Existing projection-based
methods leverage view-independent projection in complex scenes, relying on
straight lines to generate direct rays or upward curves to reduce occlusions.
However, their view independence provides projection rays that are limited to
pre-defined parameters by human settings, restricting point awareness and
failing to capture sufficient projection diversity across different view
planes. Although multiple projections per view plane are commonly used to
enhance spatial variety, the projected redundancy leads to excessive
computational overhead and inefficiency in image processing. To address these
limitations, we design a framework of VDP to generate data-driven projections
from 3D point distributions, producing highly informative single-image inputs
by predicting rays inspired by the adaptive behavior of fireworks. In addition,
we construct color regularization to optimize the framework, which emphasizes
essential features within semantic pixels and suppresses the non-semantic
features within black pixels, thereby maximizing 2D space utilization in a
projected image. As a result, our approach, PointVDP, develops lightweight
projections in marginal computation costs. Experiments on S3DIS and ScanNet
benchmarks show that our approach achieves competitive results, offering a
resource-efficient solution for semantic understanding.

</details>


### [92] [EXAONE Path 2.0: Pathology Foundation Model with End-to-End Supervision](https://arxiv.org/abs/2507.06639)
*Myungjang Pyeon,Janghyeon Lee,Minsoo Lee,Juseung Yun,Hwanil Choi,Jonghyun Kim,Jiwon Kim,Yi Hu,Jongseong Jang,Soonyoung Lee*

Main category: cs.CV

TL;DR: The paper introduces EXAONE Path 2.0, a foundation model for digital pathology that learns patch-level representations from slide-level supervision, achieving superior data efficiency and state-of-the-art biomarker prediction performance.


<details>
  <summary>Details</summary>
Motivation: Handling gigapixel-scale whole-slide images in digital pathology poses challenges, as existing approaches relying on self-supervised learning overlook essential domain-specific features for biomarker prediction and demand extensive datasets and computational resources.

Method: EXAONE Path 2.0 uses slide-level supervision to train patch-level representations, overcoming the limitations of self-supervised learning methods.

Result: The model was trained using only 37k whole-slide images and outperformed across 10 biomarker prediction tasks, showcasing its efficiency and effectiveness.

Conclusion: EXAONE Path 2.0 demonstrates its utility as a pathology foundation model, offering improved biomarker prediction using fewer resources than traditional methods.

Abstract: In digital pathology, whole-slide images (WSIs) are often difficult to handle
due to their gigapixel scale, so most approaches train patch encoders via
self-supervised learning (SSL) and then aggregate the patch-level embeddings
via multiple instance learning (MIL) or slide encoders for downstream tasks.
However, patch-level SSL may overlook complex domain-specific features that are
essential for biomarker prediction, such as mutation status and molecular
characteristics, as SSL methods rely only on basic augmentations selected for
natural image domains on small patch-level area. Moreover, SSL methods remain
less data efficient than fully supervised approaches, requiring extensive
computational resources and datasets to achieve competitive performance. To
address these limitations, we present EXAONE Path 2.0, a pathology foundation
model that learns patch-level representations under direct slide-level
supervision. Using only 37k WSIs for training, EXAONE Path 2.0 achieves
state-of-the-art average performance across 10 biomarker prediction tasks,
demonstrating remarkable data efficiency.

</details>


### [93] [Learning from Sparse Point Labels for Dense Carcinosis Localization in Advanced Ovarian Cancer Assessment](https://arxiv.org/abs/2507.06643)
*Farahdiba Zarin,Riccardo Oliva,Vinkle Srivastav,Armine Vardazaryan,Andrea Rosati,Alice Zampolini Faustini,Giovanni Scambia,Anna Fagotti,Pietro Mascagni,Nicolas Padoy*

Main category: cs.CV

TL;DR: This paper studies sparse heatmap regression for keypoint localization using a novel loss, Crag and Tail Loss, applied to ovarian cancer diagnostics.


<details>
  <summary>Details</summary>
Motivation: The medical domain commonly faces challenges due to sparse annotations, especially for dense pixel-level tasks, impacting diagnostic advancements.

Method: The authors present sparse heatmap regression paired with a new loss, Crag and Tail Loss, for better utilization of sparse pixel annotations.

Result: Their loss function improves dense localization accuracy under sparse labeling conditions, verified through ablation studies.

Conclusion: The approach enables progression in tasks requiring dense predictions but where annotations are scarce, particularly in ovarian cancer diagnostics.

Abstract: Learning from sparse labels is a challenge commonplace in the medical domain.
This is due to numerous factors, such as annotation cost, and is especially
true for newly introduced tasks. When dense pixel-level annotations are needed,
this becomes even more unfeasible. However, being able to learn from just a few
annotations at the pixel-level, while extremely difficult and underutilized,
can drive progress in studies where perfect annotations are not immediately
available. This work tackles the challenge of learning the dense prediction
task of keypoint localization from a few point annotations in the context of 2d
carcinosis keypoint localization from laparoscopic video frames for diagnostic
planning of advanced ovarian cancer patients. To enable this, we formulate the
problem as a sparse heatmap regression from a few point annotations per image
and propose a new loss function, called Crag and Tail loss, for efficient
learning. Our proposed loss function effectively leverages positive sparse
labels while minimizing the impact of false negatives or missed annotations.
Through an extensive ablation study, we demonstrate the effectiveness of our
approach in achieving accurate dense localization of carcinosis keypoints,
highlighting its potential to advance research in scenarios where dense
annotations are challenging to obtain.

</details>


### [94] [ClipGS: Clippable Gaussian Splatting for Interactive Cinematic Visualization of Volumetric Medical Data](https://arxiv.org/abs/2507.06647)
*Chengkun Li,Yuqi Tong,Kai Chen,Zhenya Yang,Ruiyang Li,Shi Qiu,Jason Ying-Kuen Chan,Pheng-Ann Heng,Qi Dou*

Main category: cs.CV

TL;DR: This paper presents ClipGS, a new Gaussian splatting framework for high-quality, interactive cinematic visualization of volumetric medical data, achieving high efficiency and rendering accuracy.


<details>
  <summary>Details</summary>
Motivation: Enhance diagnostic accuracy and surgical planning by overcoming challenges of high computing cost and low rendering speed in volumetric medical data visualization.

Method: Introduces ClipGS framework with a learnable truncation scheme and adaptive adjustment model to dynamically manage Gaussian primitives and refine rendering in response to clipping planes.

Result: Achieved 36.635 PSNR rendering quality, 156 FPS, and 16.1 MB model size, outperforming existing methods in both quality and efficiency.

Conclusion: ClipGS offers a robust solution for interactive, high-resolution visualization of medical data, making it a valuable tool for medical applications.

Abstract: The visualization of volumetric medical data is crucial for enhancing
diagnostic accuracy and improving surgical planning and education. Cinematic
rendering techniques significantly enrich this process by providing
high-quality visualizations that convey intricate anatomical details, thereby
facilitating better understanding and decision-making in medical contexts.
However, the high computing cost and low rendering speed limit the requirement
of interactive visualization in practical applications. In this paper, we
introduce ClipGS, an innovative Gaussian splatting framework with the clipping
plane supported, for interactive cinematic visualization of volumetric medical
data. To address the challenges posed by dynamic interactions, we propose a
learnable truncation scheme that automatically adjusts the visibility of
Gaussian primitives in response to the clipping plane. Besides, we also design
an adaptive adjustment model to dynamically adjust the deformation of Gaussians
and refine the rendering performance. We validate our method on five volumetric
medical data (including CT and anatomical slice data), and reach an average
36.635 PSNR rendering quality with 156 FPS and 16.1 MB model size,
outperforming state-of-the-art methods in rendering quality and efficiency.

</details>


### [95] [Diff$^2$I2P: Differentiable Image-to-Point Cloud Registration with Diffusion Prior](https://arxiv.org/abs/2507.06651)
*Juncheng Mu,Chengwei Ren,Weixiang Zhang,Liang Pan,Xiao-Ping Zhang,Yue Gao*

Main category: cs.CV

TL;DR: The paper introduces Diff$^2$I2P, a fully differentiable framework for Image-to-Point Cloud (I2P) registration using a diffusion model as a prior to bridge the modality gap, yielding significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Existing I2P registration methods struggle with accurate cross-modal correspondences due to the fundamental gap between image and point cloud modalities.

Method: The method leverages a depth-conditioned diffusion model with two key techniques: Control-Side Score Distillation (CSD) for direct transformation optimization and Deformable Correspondence Tuning (DCT) for differentiable correspondence estimation and transformation.

Result: The proposed Diff$^2$I2P approach achieves over 7% improvement in registration recall compared to state-of-the-art methods on the 7-Scenes benchmark.

Conclusion: Employing a diffusion model as a prior for robust cross-modal correspondence learning significantly enhances the performance of I2P registration frameworks.

Abstract: Learning cross-modal correspondences is essential for image-to-point cloud
(I2P) registration. Existing methods achieve this mostly by utilizing metric
learning to enforce feature alignment across modalities, disregarding the
inherent modality gap between image and point data. Consequently, this paradigm
struggles to ensure accurate cross-modal correspondences. To this end, inspired
by the cross-modal generation success of recent large diffusion models, we
propose Diff$^2$I2P, a fully Differentiable I2P registration framework,
leveraging a novel and effective Diffusion prior for bridging the modality gap.
Specifically, we propose a Control-Side Score Distillation (CSD) technique to
distill knowledge from a depth-conditioned diffusion model to directly optimize
the predicted transformation. However, the gradients on the transformation fail
to backpropagate onto the cross-modal features due to the non-differentiability
of correspondence retrieval and PnP solver. To this end, we further propose a
Deformable Correspondence Tuning (DCT) module to estimate the correspondences
in a differentiable way, followed by the transformation estimation using a
differentiable PnP solver. With these two designs, the Diffusion model serves
as a strong prior to guide the cross-modal feature learning of image and point
cloud for forming robust correspondences, which significantly improves the
registration. Extensive experimental results demonstrate that Diff$^2$I2P
consistently outperforms SoTA I2P registration methods, achieving over 7%
improvement in registration recall on the 7-Scenes benchmark.

</details>


### [96] [MS-DPPs: Multi-Source Determinantal Point Processes for Contextual Diversity Refinement of Composite Attributes in Text to Image Retrieval](https://arxiv.org/abs/2507.06654)
*Naoya Sogi,Takashi Shibata,Makoto Terao,Masanori Suganuma,Takayuki Okatani*

Main category: cs.CV

TL;DR: The paper introduces a novel task (CDR-CA) and a corresponding method (MS-DPP) to refine attribute diversities in Text-to-Image Retrieval based on contextual needs.


<details>
  <summary>Details</summary>
Motivation: Conventional Result Diversification (RD) methods overly focus on image appearance diversity without considering application-specific contextual needs, limiting their usefulness.

Method: The authors propose Multi-Source Determinantal Point Processes (MS-DPP), utilizing a unified similarity matrix and Tangent Normalization to refine diversity in contextual attributes.

Result: The proposed MS-DPP method demonstrates effectiveness through extensive experimentation.

Conclusion: The new approach addresses the challenge of context-specific diversity, offering an adaptable solution for diverse Text-to-Image Retrieval scenarios.

Abstract: Result diversification (RD) is a crucial technique in Text-to-Image Retrieval
for enhancing the efficiency of a practical application. Conventional methods
focus solely on increasing the diversity metric of image appearances. However,
the diversity metric and its desired value vary depending on the application,
which limits the applications of RD. This paper proposes a novel task called
CDR-CA (Contextual Diversity Refinement of Composite Attributes). CDR-CA aims
to refine the diversities of multiple attributes, according to the
application's context. To address this task, we propose Multi-Source DPPs, a
simple yet strong baseline that extends the Determinantal Point Process (DPP)
to multi-sources. We model MS-DPP as a single DPP model with a unified
similarity matrix based on a manifold representation. We also introduce Tangent
Normalization to reflect contexts. Extensive experiments demonstrate the
effectiveness of the proposed method. Our code is publicly available at
https://github.com/NEC-N-SOGI/msdpp.

</details>


### [97] [Enhancing Diffusion Model Stability for Image Restoration via Gradient Management](https://arxiv.org/abs/2507.06656)
*Hongjie Wu,Mingqin Zhang,Linchao He,Ji-Zhe Zhou,Jiancheng Lv*

Main category: cs.CV

TL;DR: The paper identifies gradient instabilities in diffusion models used for image restoration and proposes Stabilized Progressive Gradient Diffusion (SPGD) to address these issues, achieving improved restoration results.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance the stability of diffusion-based image restoration processes by addressing conflicts and fluctuations in gradient dynamics.

Method: Introducing SPGD with progressive likelihood warm-up and adaptive directional momentum smoothing to manage gradient interactions and disruptions.

Result: SPGD improves generation stability and achieves state-of-the-art performance across various image restoration tasks.

Conclusion: The findings demonstrate substantial benefits of SPGD in stabilizing the generative process, leading to better quantitative and visual outcomes.

Abstract: Diffusion models have shown remarkable promise for image restoration by
leveraging powerful priors. Prominent methods typically frame the restoration
problem within a Bayesian inference framework, which iteratively combines a
denoising step with a likelihood guidance step. However, the interactions
between these two components in the generation process remain underexplored. In
this paper, we analyze the underlying gradient dynamics of these components and
identify significant instabilities. Specifically, we demonstrate conflicts
between the prior and likelihood gradient directions, alongside temporal
fluctuations in the likelihood gradient itself. We show that these
instabilities disrupt the generative process and compromise restoration
performance. To address these issues, we propose Stabilized Progressive
Gradient Diffusion (SPGD), a novel gradient management technique. SPGD
integrates two synergistic components: (1) a progressive likelihood warm-up
strategy to mitigate gradient conflicts; and (2) adaptive directional momentum
(ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive
experiments across diverse restoration tasks demonstrate that SPGD
significantly enhances generation stability, leading to state-of-the-art
performance in quantitative metrics and visually superior results. Code is
available at \href{https://github.com/74587887/SPGD}{here}.

</details>


### [98] [FlexGaussian: Flexible and Cost-Effective Training-Free Compression for 3D Gaussian Splatting](https://arxiv.org/abs/2507.06671)
*Boyuan Tian,Qizhe Gao,Siran Xianyu,Xiaotong Cui,Minjia Zhang*

Main category: cs.CV

TL;DR: FlexGaussian is a training-free method for compressing 3D Gaussian splatting, achieving up to 96.4% compression with minimal rendering quality loss, optimized for mobile devices.


<details>
  <summary>Details</summary>
Motivation: The necessity to compress large-scale 3D Gaussian models for mobile and edge devices with limited computational resources and memory.

Method: FlexGaussian employs mixed-precision quantization and attribute-discriminative pruning for training-free 3D Gaussian compression.

Result: Evaluation shows FlexGaussian achieves up to 96.4% compression, maintains high rendering quality (<1 dB PSNR loss), deploys swiftly on mobile devices, and is faster than competing methods.

Conclusion: FlexGaussian provides an efficient, adaptable solution for compressing 3D Gaussian splatting, eliminating retraining requirements and optimizing performance for constrained devices.

Abstract: 3D Gaussian splatting has become a prominent technique for representing and
rendering complex 3D scenes, due to its high fidelity and speed advantages.
However, the growing demand for large-scale models calls for effective
compression to reduce memory and computation costs, especially on mobile and
edge devices with limited resources. Existing compression methods effectively
reduce 3D Gaussian parameters but often require extensive retraining or
fine-tuning, lacking flexibility under varying compression constraints.
  In this paper, we introduce FlexGaussian, a flexible and cost-effective
method that combines mixed-precision quantization with attribute-discriminative
pruning for training-free 3D Gaussian compression. FlexGaussian eliminates the
need for retraining and adapts easily to diverse compression targets.
Evaluation results show that FlexGaussian achieves up to 96.4% compression
while maintaining high rendering quality (<1 dB drop in PSNR), and is
deployable on mobile devices. FlexGaussian delivers high compression ratios
within seconds, being 1.7-2.1x faster than state-of-the-art training-free
methods and 10-100x faster than training-involved approaches. The code is being
prepared and will be released soon at:
https://github.com/Supercomputing-System-AI-Lab/FlexGaussian

</details>


### [99] [Text-promptable Object Counting via Quantity Awareness Enhancement](https://arxiv.org/abs/2507.06679)
*Miaojing Shi,Xiaowen Zhang,Zijie Yue,Yong Luo,Cairong Zhao,Li Li*

Main category: cs.CV

TL;DR: This paper introduces QUANet, improving object counting by using novel prompts, a vision-text alignment loss, and a dual-stream decoder architecture.


<details>
  <summary>Details</summary>
Motivation: Accurate object counting in images using vision-language models is currently limited by insufficient text-prompt designs.

Method: The paper proposes QUANet, which incorporates quantity-oriented text prompts, vision-text quantity alignment loss, a dual-stream decoder with Transformer and CNN streams, T2C-adapters for knowledge fusion, and a cross-stream quantity ranking loss.

Result: QUANet achieves strong generalization and state-of-the-art performance on benchmarks like FSC-147, CARPK, PUCPR+, and ShanghaiTech.

Conclusion: QUANet successfully addresses the limitations of previous methods in object counting and demonstrates robustness in zero-shot class-agnostic counting tasks.

Abstract: Recent advances in large vision-language models (VLMs) have shown remarkable
progress in solving the text-promptable object counting problem. Representative
methods typically specify text prompts with object category information in
images. This however is insufficient for training the model to accurately
distinguish the number of objects in the counting task. To this end, we propose
QUANet, which introduces novel quantity-oriented text prompts with a
vision-text quantity alignment loss to enhance the model's quantity awareness.
Moreover, we propose a dual-stream adaptive counting decoder consisting of a
Transformer stream, a CNN stream, and a number of Transformer-to-CNN
enhancement adapters (T2C-adapters) for density map prediction. The
T2C-adapters facilitate the effective knowledge communication and aggregation
between the Transformer and CNN streams. A cross-stream quantity ranking loss
is proposed in the end to optimize the ranking orders of predictions from the
two streams. Extensive experiments on standard benchmarks such as FSC-147,
CARPK, PUCPR+, and ShanghaiTech demonstrate our model's strong generalizability
for zero-shot class-agnostic counting. Code is available at
https://github.com/viscom-tongji/QUANet

</details>


### [100] [Spatial-Temporal Graph Mamba for Music-Guided Dance Video Synthesis](https://arxiv.org/abs/2507.06689)
*Hao Tang,Ling Shao,Zhenyu Zhang,Luc Van Gool,Nicu Sebe*

Main category: cs.CV

TL;DR: This paper introduces STG-Mamba, a method for creating dance videos from music using spatial-temporal graph and self-supervised regularization techniques.


<details>
  <summary>Details</summary>
Motivation: Building a system to automatically synthesize dance videos guided by music, aiming to capture spatial and temporal dependencies of human skeletons and improve translation quality.

Method: STG-Mamba uses a two-step translation method: music-to-skeleton translation with a spatial-temporal graph block, and skeleton-to-video translation with a self-supervised regularization network.

Result: STG-Mamba outperforms existing methods significantly, leveraging a novel dataset of 54,944 skeleton-video clips for evaluation.

Conclusion: STG-Mamba effectively bridges music, skeleton, and video translation through innovative graph and regularization techniques, advancing music-guided dance synthesis.

Abstract: We propose a novel spatial-temporal graph Mamba (STG-Mamba) for the
music-guided dance video synthesis task, i.e., to translate the input music to
a dance video. STG-Mamba consists of two translation mappings:
music-to-skeleton translation and skeleton-to-video translation. In the
music-to-skeleton translation, we introduce a novel spatial-temporal graph
Mamba (STGM) block to effectively construct skeleton sequences from the input
music, capturing dependencies between joints in both the spatial and temporal
dimensions. For the skeleton-to-video translation, we propose a novel
self-supervised regularization network to translate the generated skeletons,
along with a conditional image, into a dance video. Lastly, we collect a new
skeleton-to-video translation dataset from the Internet, containing 54,944
video clips. Extensive experiments demonstrate that STG-Mamba achieves
significantly better results than existing methods.

</details>


### [101] [Hierarchical Feature Alignment for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2507.06732)
*Sobhan Asasi,Mohamed Ilyes Lakhal,Richard Bowden*

Main category: cs.CV

TL;DR: This study introduces a hierarchical pre-training strategy for Sign Language Translation (SLT), leveraging pseudo-glosses and contrastive video-language alignment to enhance the translation of sign language videos into spoken sentences.


<details>
  <summary>Details</summary>
Motivation: SLT faces challenges in bridging the gap between visual sign videos and textual spoken sentences, especially in gloss-free methods, which require efficient alignment strategies.

Method: A novel hierarchical pre-training strategy was developed, extracting features at frame, segment, and video levels, incorporating pseudo-glosses and employing contrastive video-language alignment techniques.

Result: The proposed approach demonstrated improved translation performance as measured by BLEU-4 and ROUGE scores, while being computationally efficient.

Conclusion: The hierarchical strategy effectively enhances gloss-free SLT by combining linguistic structures and alignment strategies, improving translation quality without sacrificing efficiency.

Abstract: Sign Language Translation (SLT) attempts to convert sign language videos into
spoken sentences. However, many existing methods struggle with the disparity
between visual and textual representations during end-to-end learning.
Gloss-based approaches help to bridge this gap by leveraging structured
linguistic information. While, gloss-free methods offer greater flexibility and
remove the burden of annotation, they require effective alignment strategies.
Recent advances in Large Language Models (LLMs) have enabled gloss-free SLT by
generating text-like representations from sign videos. In this work, we
introduce a novel hierarchical pre-training strategy inspired by the structure
of sign language, incorporating pseudo-glosses and contrastive video-language
alignment. Our method hierarchically extracts features at frame, segment, and
video levels, aligning them with pseudo-glosses and the spoken sentence to
enhance translation quality. Experiments demonstrate that our approach improves
BLEU-4 and ROUGE scores while maintaining efficiency.

</details>


### [102] [MADPOT: Medical Anomaly Detection with CLIP Adaptation and Partial Optimal Transport](https://arxiv.org/abs/2507.06733)
*Mahshid Shiri,Cigdem Beyan,Vittorio Murino*

Main category: cs.CV

TL;DR: The paper proposes a novel approach to improve medical anomaly detection by combining advanced techniques like Partial Optimal Transport (POT) and contrastive learning with CLIP.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of medical anomaly detection, such as diverse imaging modalities, anatomical variations, and limited labeled data.

Method: A model integrates visual adapters and prompt learning using Partial Optimal Transport (POT) and contrastive learning (CL) to better adapt CLIP for medical anomaly detection, aligning local features with multiple prompts.

Result: The approach achieves state-of-the-art performance in few-shot, zero-shot, and cross-dataset scenarios without relying on synthetic data or memory banks.

Conclusion: The proposed method significantly enhances adaptability and effectiveness in medical anomaly detection, marking advancement in the field.

Abstract: Medical anomaly detection (AD) is challenging due to diverse imaging
modalities, anatomical variations, and limited labeled data. We propose a novel
approach combining visual adapters and prompt learning with Partial Optimal
Transport (POT) and contrastive learning (CL) to improve CLIP's adaptability to
medical images, particularly for AD. Unlike standard prompt learning, which
often yields a single representation, our method employs multiple prompts
aligned with local features via POT to capture subtle abnormalities. CL further
enforces intra-class cohesion and inter-class separation. Our method achieves
state-of-the-art results in few-shot, zero-shot, and cross-dataset scenarios
without synthetic data or memory banks. The code is available at
https://github.com/mahshid1998/MADPOT.

</details>


### [103] [Residual Prior-driven Frequency-aware Network for Image Fusion](https://arxiv.org/abs/2507.06735)
*Guan Zheng,Xue Wang,Wenhua Qian,Peng Liu,Runzhuo Ma*

Main category: cs.CV

TL;DR: This study introduces RPFNet, a model for image fusion that enhances fused image quality and high-level vision task performance through frequency-domain operations and a dual-branch feature extraction approach.


<details>
  <summary>Details</summary>
Motivation: Conventional methods face challenges in balancing computational costs associated with capturing global spatial features and the lack of ground-truth labels, which complicates effective complementary feature extraction.

Method: The paper proposes a dual-branch RPFNet framework: one branch (RPM) extracts complementary priors from residual maps and the other (FDFM) utilizes frequency-domain convolution for efficient global feature modeling. Additionally, a Cross Promotion Module facilitates local and global feature interaction. Training incorporates auxiliary decoder, saliency structure loss, and hybrid loss functions.

Result: RPFNet achieves high fusion performance, combining discriminative features while enhancing texture details and salient objects. It is validated as effective for high-level vision tasks.

Conclusion: The proposed RPFNet demonstrates significant efficacy in integrating complementary information, supporting high-level vision tasks, and addressing computational and data dependency challenges in image fusion.

Abstract: Image fusion aims to integrate complementary information across modalities to
generate high-quality fused images, thereby enhancing the performance of
high-level vision tasks. While global spatial modeling mechanisms show
promising results, constructing long-range feature dependencies in the spatial
domain incurs substantial computational costs. Additionally, the absence of
ground-truth exacerbates the difficulty of capturing complementary features
effectively. To tackle these challenges, we propose a Residual Prior-driven
Frequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a
dual-branch feature extraction framework: the Residual Prior Module (RPM)
extracts modality-specific difference information from residual maps, thereby
providing complementary priors for fusion; the Frequency Domain Fusion Module
(FDFM) achieves efficient global feature modeling and integration through
frequency-domain convolution. Additionally, the Cross Promotion Module (CPM)
enhances the synergistic perception of local details and global structures
through bidirectional feature interaction. During training, we incorporate an
auxiliary decoder and saliency structure loss to strengthen the model's
sensitivity to modality-specific differences. Furthermore, a combination of
adaptive weight-based frequency contrastive loss and SSIM loss effectively
constrains the solution space, facilitating the joint capture of local details
and global features while ensuring the retention of complementary information.
Extensive experiments validate the fusion performance of RPFNet, which
effectively integrates discriminative features, enhances texture details and
salient objects, and can effectively facilitate the deployment of the
high-level vision task.

</details>


### [104] [DIFFUMA: High-Fidelity Spatio-Temporal Video Prediction via Dual-Path Mamba and Diffusion Enhancement](https://arxiv.org/abs/2507.06738)
*Xinyu Xie,Weifeng Cao,Jun Shi,Yangyang Hu,Hui Liang,Wanyong Liang,Xiaoliang Qian*

Main category: cs.CV

TL;DR: The paper introduces a new CHDL dataset for semiconductor manufacturing and proposes DIFFUMA, a novel architecture improving spatio-temporal video prediction.


<details>
  <summary>Details</summary>
Motivation: Spatio-temporal video prediction in high-precision industrial settings lacks benchmark datasets, hindering advancements, particularly in scenarios like semiconductor wafer dicing.

Method: The authors created the Chip Dicing Lane Dataset (CHDL) for semiconductor processes and introduced DIFFUMA, which employs a dual-path architecture combining a Mamba module for temporal context and a diffusion module for spatial detail enhancement.

Result: Experiments showed DIFFUMA outperformed others on CHDL with a 39% MSE reduction and increased SSIM from 0.926 to 0.988. Its performance also generalizes well to other datasets.

Conclusion: The paper contributes a high-quality dataset and a state-of-the-art model that empowers future industrial AI research while achieving significant benchmark improvements.

Abstract: Spatio-temporal video prediction plays a pivotal role in critical domains,
ranging from weather forecasting to industrial automation. However, in
high-precision industrial scenarios such as semiconductor manufacturing, the
absence of specialized benchmark datasets severely hampers research on modeling
and predicting complex processes. To address this challenge, we make a twofold
contribution.First, we construct and release the Chip Dicing Lane Dataset
(CHDL), the first public temporal image dataset dedicated to the semiconductor
wafer dicing process. Captured via an industrial-grade vision system, CHDL
provides a much-needed and challenging benchmark for high-fidelity process
modeling, defect detection, and digital twin development.Second, we propose
DIFFUMA, an innovative dual-path prediction architecture specifically designed
for such fine-grained dynamics. The model captures global long-range temporal
context through a parallel Mamba module, while simultaneously leveraging a
diffusion module, guided by temporal features, to restore and enhance
fine-grained spatial details, effectively combating feature degradation.
Experiments demonstrate that on our CHDL benchmark, DIFFUMA significantly
outperforms existing methods, reducing the Mean Squared Error (MSE) by 39% and
improving the Structural Similarity (SSIM) from 0.926 to a near-perfect 0.988.
This superior performance also generalizes to natural phenomena datasets. Our
work not only delivers a new state-of-the-art (SOTA) model but, more
importantly, provides the community with an invaluable data resource to drive
future research in industrial AI.

</details>


### [105] [PromptTea: Let Prompts Tell TeaCache the Optimal Threshold](https://arxiv.org/abs/2507.06739)
*Zishen Huang,Chunyu Yang,Mengyuan Ren*

Main category: cs.CV

TL;DR: The paper addresses the inefficiency in video generation model acceleration by proposing adaptive reuse mechanisms based on scene complexity.


<details>
  <summary>Details</summary>
Motivation: To overcome the inefficiency of fixed-frequency caching in video generation models, which degrades quality in complex scenes and lacks robustness.

Method: The study introduces Prompt-Complexity-Aware (PCA) caching and DynCFGCache, leveraging prompt-derived scene complexity and dynamic reuse of classifier-free guidance outputs.

Result: Experiments show a 2.79x speedup in video generation while maintaining high visual quality across diverse scenes.

Conclusion: Adaptive reuse mechanisms based on scene complexity offer significant speed improvements in video generation without compromising quality.

Abstract: Despite recent progress in video generation, inference speed remains a major
bottleneck. A common acceleration strategy involves reusing model outputs via
caching mechanisms at fixed intervals. However, we find that such
fixed-frequency reuse significantly degrades quality in complex scenes, while
manually tuning reuse thresholds is inefficient and lacks robustness. To
address this, we propose Prompt-Complexity-Aware (PCA) caching, a method that
automatically adjusts reuse thresholds based on scene complexity estimated
directly from the input prompt. By incorporating prompt-derived semantic cues,
PCA enables more adaptive and informed reuse decisions than conventional
caching methods. We also revisit the assumptions behind TeaCache and identify a
key limitation: it suffers from poor input-output relationship modeling due to
an oversimplified prior. To overcome this, we decouple the noisy input, enhance
the contribution of meaningful textual information, and improve the model's
predictive accuracy through multivariate polynomial feature expansion. To
further reduce computational cost, we replace the static CFGCache with
DynCFGCache, a dynamic mechanism that selectively reuses classifier-free
guidance (CFG) outputs based on estimated output variations. This allows for
more flexible reuse without compromising output quality. Extensive experiments
demonstrate that our approach achieves significant acceleration-for example,
2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across
a range of scenes.

</details>


### [106] [Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised Text-to-Person Image Matching](https://arxiv.org/abs/2507.06744)
*Yafei Zhang,Yongle Shang,Huafeng Li*

Main category: cs.CV

TL;DR: This paper proposes a method to improve weakly supervised text-to-person image matching by addressing challenges in predicting complex identity relationships.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of current methods in weakly supervised text-to-person image matching, particularly their inability to handle complex one-to-many identity relationships, which hinders performance improvements.

Method: The authors introduce a dual-granularity identity association mechanism with local and global levels. Locally, they establish cross-modal identity relationships within batches. Globally, they create a dynamic cross-modal identity association network with a confidence-based adjustment. Additionally, they use an information-asymmetric sample pair construction method combined with consistency learning.

Result: Experimental results show that the method significantly improves cross-modal matching accuracy in text-to-person image matching tasks.

Conclusion: The proposed approach provides an efficient and practical solution to enhance accuracy and robustness in weakly supervised text-to-person image matching.

Abstract: Weakly supervised text-to-person image matching, as a crucial approach to
reducing models' reliance on large-scale manually labeled samples, holds
significant research value. However, existing methods struggle to predict
complex one-to-many identity relationships, severely limiting performance
improvements. To address this challenge, we propose a local-and-global
dual-granularity identity association mechanism. Specifically, at the local
level, we explicitly establish cross-modal identity relationships within a
batch, reinforcing identity constraints across different modalities and
enabling the model to better capture subtle differences and correlations. At
the global level, we construct a dynamic cross-modal identity association
network with the visual modality as the anchor and introduce a confidence-based
dynamic adjustment mechanism, effectively enhancing the model's ability to
identify weakly associated samples while improving overall sensitivity.
Additionally, we propose an information-asymmetric sample pair construction
method combined with consistency learning to tackle hard sample mining and
enhance model robustness. Experimental results demonstrate that the proposed
method substantially boosts cross-modal matching accuracy, providing an
efficient and practical solution for text-to-person image matching.

</details>


### [107] [Finetuning Vision-Language Models as OCR Systems for Low-Resource Languages: A Case Study of Manchu](https://arxiv.org/abs/2507.06761)
*Yan Hon Michael Chung,Donghyeok Choi*

Main category: cs.CV

TL;DR: This paper develops an effective OCR system for Manchu, a critically endangered language, using fine-tuned vision-language models, achieving high accuracy on both synthetic and real-world text.


<details>
  <summary>Details</summary>
Motivation: Manchu language, crucial for Eastern Eurasian historical study, lacks effective OCR systems capable of handling real-world historical documents.

Method: Fine-tuning of three open-source vision-language models on 60,000 synthetic Manchu word images using parameter-efficient training.

Result: The LLaMA-3.2-11B model achieved 98.3% word accuracy on synthetic data and 93.1% on real-world documents, outperforming traditional CRNN baselines significantly.

Conclusion: The study highlights a transferable OCR framework for endangered languages, enabling historians and linguists to process archives on accessible infrastructure without high costs or technical expertise.

Abstract: Manchu, a critically endangered language essential for understanding early
modern Eastern Eurasian history, lacks effective OCR systems that can handle
real-world historical documents. This study develops high-performing OCR
systems by fine-tuning three open-source vision-language models (LLaMA-3.2-11B,
Qwen2.5-VL-7B, Qwen2.5-VL-3B) on 60,000 synthetic Manchu word images using
parameter-efficient training. LLaMA-3.2-11B achieved exceptional performance
with 98.3\% word accuracy and 0.0024 character error rate on synthetic data,
while crucially maintaining 93.1\% accuracy on real-world handwritten
documents. Comparative evaluation reveals substantial advantages over
traditional approaches: while a CRNN baseline achieved 99.8\% synthetic
accuracy, it suffered severe degradation to 72.5\% on real documents. Our
approach demonstrates effective synthetic-to-real domain transfer, providing a
cost-effective solution deployable on accessible infrastructure. This work
establishes a transferable framework for endangered language OCR that removes
technical and financial barriers in digital humanities, enabling historians and
linguists to process historical archives without specialized computing
resources. Code and model weights are available at
https://github.com/mic7ch1/ManchuAI-OCR.

</details>


### [108] [FOLC-Net: A Federated-Optimized Lightweight Architecture for Enhanced MRI Disease Diagnosis across Axial, Coronal, and Sagittal Views](https://arxiv.org/abs/2507.06763)
*Saif Ur Rehman Khan,Muhammad Nabeel Asim,Sebastian Vollmer,Andreas Dengel*

Main category: cs.CV

TL;DR: The study introduces FOLC-Net, a lightweight MRI analysis framework that outperforms state-of-the-art models, specifically demonstrating improved accuracy on challenging sagittal views.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art MRI analysis models exhibit performance issues in processing different anatomical planes, necessitating a new framework to improve robustness and diagnosis accuracy.

Method: The FOLC-Net incorporates a federated-optimized lightweight architecture with MRFO for model structuring, global model cloning for scalability, and ConvNeXt for adaptability. It was tested on multi-view and single-view MRI data.

Result: FOLC-Net achieved a 92.44% accuracy on sagittal views, outperforming competing methods and improving accuracy across axial, coronal, and multi-view MRI scans.

Conclusion: FOLC-Net effectively addresses limitations of existing models with enhanced adaptability and robustness, proving its efficacy for real-world decentralized medical imaging applications.

Abstract: The framework is designed to improve performance in the analysis of combined
as well as single anatomical perspectives for MRI disease diagnosis. It
specifically addresses the performance degradation observed in state-of-the-art
(SOTA) models, particularly when processing axial, coronal, and sagittal
anatomical planes. The paper introduces the FOLC-Net framework, which
incorporates a novel federated-optimized lightweight architecture with
approximately 1.217 million parameters and a storage requirement of only 0.9
MB. FOLC-Net integrates Manta-ray foraging optimization (MRFO) mechanisms for
efficient model structure generation, global model cloning for scalable
training, and ConvNeXt for enhanced client adaptability. The model was
evaluated on combined multi-view data as well as individual views, such as
axial, coronal, and sagittal, to assess its robustness in various medical
imaging scenarios. Moreover, FOLC-Net tests a ShallowFed model on different
data to evaluate its ability to generalize beyond the training dataset. The
results show that FOLC-Net outperforms existing models, particularly in the
challenging sagittal view. For instance, FOLC-Net achieved an accuracy of
92.44% on the sagittal view, significantly higher than the 88.37% accuracy of
study method (DL + Residual Learning) and 88.95% of DL models. Additionally,
FOLC-Net demonstrated improved accuracy across all individual views, providing
a more reliable and robust solution for medical image analysis in decentralized
environments. FOLC-Net addresses the limitations of existing SOTA models by
providing a framework that ensures better adaptability to individual views
while maintaining strong performance in multi-view settings. The incorporation
of MRFO, global model cloning, and ConvNeXt ensures that FOLC-Net performs
better in real-world medical applications.

</details>


### [109] [Unlocking Thermal Aerial Imaging: Synthetic Enhancement of UAV Datasets](https://arxiv.org/abs/2507.06797)
*Antonella Barisic Kulas,Andreja Jurasovic,Stjepan Bogdan*

Main category: cs.CV

TL;DR: The paper introduces a procedural pipeline to generate synthetic thermal images for UAVs, enhancing datasets and demonstrating superior detection performance for thermal data over visible-light models.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the scarcity of large-scale and diverse thermal aerial datasets, which hinder the progress of deep learning models in applications like search and rescue, wildlife monitoring, and emergency response.

Method: They developed a novel method to generate synthetic thermal images by integrating arbitrary object classes into existing thermal images with controlled positioning, scaling, and alignment. They augmented popular datasets like HIT-UAV and MONET by introducing new objects like drones and animals.

Result: The augmented datasets achieved strong object detection performance for both new and existing classes, showcasing the effectiveness of thermal detectors compared to visible-light-trained counterparts. Aerial viewing angle replication was identified as important.

Conclusion: The proposed pipeline successfully expands thermal image datasets, enabling advancements in applications such as emergency and wildlife monitoring while validating the benefits of synthetic data and thermal imaging for UAVs.

Abstract: Thermal imaging from unmanned aerial vehicles (UAVs) holds significant
potential for applications in search and rescue, wildlife monitoring, and
emergency response, especially under low-light or obscured conditions. However,
the scarcity of large-scale, diverse thermal aerial datasets limits the
advancement of deep learning models in this domain, primarily due to the high
cost and logistical challenges of collecting thermal data. In this work, we
introduce a novel procedural pipeline for generating synthetic thermal images
from an aerial perspective. Our method integrates arbitrary object classes into
existing thermal backgrounds by providing control over the position, scale, and
orientation of the new objects, while aligning them with the viewpoints of the
background. We enhance existing thermal datasets by introducing new object
categories, specifically adding a drone class in urban environments to the
HIT-UAV dataset and an animal category to the MONET dataset. In evaluating
these datasets for object detection task, we showcase strong performance across
both new and existing classes, validating the successful expansion into new
applications. Through comparative analysis, we show that thermal detectors
outperform their visible-light-trained counterparts and highlight the
importance of replicating aerial viewing angles. Project page:
https://github.com/larics/thermal_aerial_synthetic.

</details>


### [110] [GreenHyperSpectra: A multi-source hyperspectral dataset for global vegetation trait prediction](https://arxiv.org/abs/2507.06806)
*Eya Cherif,Arthur Ouaknine,Luke A. Brown,Phuong D. Dao,Kyle R. Kovach,Bing Lu,Daniel Mederer,Hannes Feilhauer,Teja Kattenborn,David Rolnick*

Main category: cs.CV

TL;DR: GreenHyperSpectra is a novel pretraining dataset that improves plant trait prediction using hyperspectral data through semi- and self-supervised machine learning methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by label scarcity and domain shifts in using hyperspectral data for plant trait prediction, enabling better biodiversity and climate change studies.

Method: The study introduces GreenHyperSpectra, a pretraining dataset with real-world cross-sensor and cross-ecosystem samples for benchmarking. It utilizes semi- and self-supervised machine learning methods within both in-distribution and out-of-distribution scenarios.

Result: Pretrained label-efficient multi-output regression models were developed, showing improvements over the state-of-the-art supervised baselines for plant trait prediction.

Conclusion: This work demonstrates the value of GreenHyperSpectra in enhancing spectral representation learning for plant trait prediction, offering a robust framework that bridges representation learning with ecological research.

Abstract: Plant traits such as leaf carbon content and leaf mass are essential
variables in the study of biodiversity and climate change. However,
conventional field sampling cannot feasibly cover trait variation at
ecologically meaningful spatial scales. Machine learning represents a valuable
solution for plant trait prediction across ecosystems, leveraging hyperspectral
data from remote sensing. Nevertheless, trait prediction from hyperspectral
data is challenged by label scarcity and substantial domain shifts (\eg across
sensors, ecological distributions), requiring robust cross-domain methods.
Here, we present GreenHyperSpectra, a pretraining dataset encompassing
real-world cross-sensor and cross-ecosystem samples designed to benchmark trait
prediction with semi- and self-supervised methods. We adopt an evaluation
framework encompassing in-distribution and out-of-distribution scenarios. We
successfully leverage GreenHyperSpectra to pretrain label-efficient
multi-output regression models that outperform the state-of-the-art supervised
baseline. Our empirical analyses demonstrate substantial improvements in
learning spectral representations for trait prediction, establishing a
comprehensive methodological framework to catalyze research at the intersection
of representation learning and plant functional traits assessment. All code and
data are available at: https://github.com/echerif18/HyspectraSSL.

</details>


### [111] [Democratizing High-Fidelity Co-Speech Gesture Video Generation](https://arxiv.org/abs/2507.06812)
*Xu Yang,Shaoli Huang,Shenbo Xie,Xuelin Chen,Yifei Liu,Changxing Ding*

Main category: cs.CV

TL;DR: The paper introduces a lightweight framework for generating realistic co-speech gesture videos using 2D skeletons and a diffusion model for efficient audio-visual alignment, along with a new dataset for research.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in overcoming challenges of one-to-many mapping between audio and visual content, scarce large-scale datasets, and high computational requirements in co-speech gesture video generation.

Method: The proposed method uses a diffusion model conditioned on fine-grained audio segments and 2D skeletons from a reference image to predict skeletal motions, ensuring audio-video coordination. These skeletons are then passed to a human video generation model.

Result: The experiments demonstrate that the proposed method outperforms state-of-the-art techniques in visual quality and synchronization, with strong generalization across speakers and contexts.

Conclusion: The lightweight framework and accompanying CSG-405 dataset pave the way for efficient, high-quality, and reproducible research in co-speech gesture video generation.

Abstract: Co-speech gesture video generation aims to synthesize realistic,
audio-aligned videos of speakers, complete with synchronized facial expressions
and body gestures. This task presents challenges due to the significant
one-to-many mapping between audio and visual content, further complicated by
the scarcity of large-scale public datasets and high computational demands. We
propose a lightweight framework that utilizes 2D full-body skeletons as an
efficient auxiliary condition to bridge audio signals with visual outputs. Our
approach introduces a diffusion model conditioned on fine-grained audio
segments and a skeleton extracted from the speaker's reference image,
predicting skeletal motions through skeleton-audio feature fusion to ensure
strict audio coordination and body shape consistency. The generated skeletons
are then fed into an off-the-shelf human video generation model with the
speaker's reference image to synthesize high-fidelity videos. To democratize
research, we present CSG-405-the first public dataset with 405 hours of
high-resolution videos across 71 speech types, annotated with 2D skeletons and
diverse speaker demographics. Experiments show that our method exceeds
state-of-the-art approaches in visual quality and synchronization while
generalizing across speakers and contexts.

</details>


### [112] [HVI-CIDNet+: Beyond Extreme Darkness for Low-Light Image Enhancement](https://arxiv.org/abs/2507.06814)
*Qingsen Yan,Kangbiao Shi,Yixu Feng,Tao Hu,Peng Wu,Guansong Pang,Yanning Zhang*

Main category: cs.CV

TL;DR: This paper proposes a novel color space called Horizontal/Vertical-Intensity (HVI) and a network called HVI-CIDNet+ for improving low-light image enhancement (LLIE), addressing existing issues like color bias and brightness artifacts.


<details>
  <summary>Details</summary>
Motivation: Current LLIE approaches based on standard RGB or HSV color spaces face challenges such as color bias, brightness artifacts, and noise in dark conditions.

Method: The study introduces the HVI color space, which uses a HV color map and learnable intensity to handle red and black noise artifacts. It also proposes HVI-CIDNet+, leveraging vision-language models and Prior-guided Attention Block to integrate semantic priors and degraded knowledge for enhancement.

Result: The comprehensive experiments demonstrate that the proposed method achieves superior performance compared to existing LLIE techniques across 10 benchmark datasets.

Conclusion: The HVI color space and HVI-CIDNet+ successfully address color sensitivity issues and restore details in dark regions, making it more effective than current LLIE methods.

Abstract: Low-Light Image Enhancement (LLIE) aims to restore vivid content and details
from corrupted low-light images. However, existing standard RGB (sRGB) color
space-based LLIE methods often produce color bias and brightness artifacts due
to the inherent high color sensitivity. While Hue, Saturation, and Value (HSV)
color space can decouple brightness and color, it introduces significant red
and black noise artifacts. To address this problem, we propose a new color
space for LLIE, namely Horizontal/Vertical-Intensity (HVI), defined by the HV
color map and learnable intensity. The HV color map enforces small distances
for the red coordinates to remove red noise artifacts, while the learnable
intensity compresses the low-light regions to remove black noise artifacts.
Additionally, we introduce the Color and Intensity Decoupling Network+
(HVI-CIDNet+), built upon the HVI color space, to restore damaged content and
mitigate color distortion in extremely dark regions. Specifically, HVI-CIDNet+
leverages abundant contextual and degraded knowledge extracted from low-light
images using pre-trained vision-language models, integrated via a novel
Prior-guided Attention Block (PAB). Within the PAB, latent semantic priors can
promote content restoration, while degraded representations guide precise color
correction, both particularly in extremely dark regions through the
meticulously designed cross-attention fusion mechanism. Furthermore, we
construct a Region Refinement Block that employs convolution for
information-rich regions and self-attention for information-scarce regions,
ensuring accurate brightness adjustments. Comprehensive results from benchmark
experiments demonstrate that the proposed HVI-CIDNet+ outperforms the
state-of-the-art methods on 10 datasets.

</details>


### [113] [Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation](https://arxiv.org/abs/2507.06830)
*Tao Feng,Xianbing Zhao,Zhenhua Chen,Tien Tsin Wong,Hamid Rezatofighi,Gholamreza Haffari,Lizhen Qu*

Main category: cs.CV

TL;DR: The paper introduces a novel framework combining symbolic regression and trajectory-guided image-to-video models to create physically accurate video predictions.


<details>
  <summary>Details</summary>
Motivation: Existing video generation models lack accurate physical alignment and fail to replicate real-world dynamics due to reliance on statistical correlations rather than physical laws.

Method: The framework extracts motion trajectories, uses retrieval-enhanced symbolic regression to discover equations of motion, and applies these to forecast trajectories that guide video generation without fine-tuning.

Result: The method recovers ground-truth equations of motion and improves physical accuracy in generated videos for Classical Mechanics scenarios like spring-mass, pendulums, and projectiles.

Conclusion: This approach effectively enhances physics-grounded video forecasting, outperforming baseline models in physical alignment.

Abstract: Recent advances in diffusion-based and autoregressive video generation models
have achieved remarkable visual realism. However, these models typically lack
accurate physical alignment, failing to replicate real-world dynamics in object
motion. This limitation arises primarily from their reliance on learned
statistical correlations rather than capturing mechanisms adhering to physical
laws. To address this issue, we introduce a novel framework that integrates
symbolic regression (SR) and trajectory-guided image-to-video (I2V) models for
physics-grounded video forecasting. Our approach extracts motion trajectories
from input videos, uses a retrieval-based pre-training mechanism to enhance
symbolic regression, and discovers equations of motion to forecast physically
accurate future trajectories. These trajectories then guide video generation
without requiring fine-tuning of existing models. Evaluated on scenarios in
Classical Mechanics, including spring-mass, pendulums, and projectile motions,
our method successfully recovers ground-truth analytical equations and improves
the physical alignment of generated videos over baseline methods.

</details>


### [114] [Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in Multimodal LLMs](https://arxiv.org/abs/2507.06999)
*Yahan Yu,Yuyang Dong,Masafumi Oyamada*

Main category: cs.CV

TL;DR: The paper introduces the "Deliberate-to-Intuitive reasoning framework" (D2I) to improve reasoning in multimodal LLMs without requiring additional data annotations or complex rewards.


<details>
  <summary>Details</summary>
Motivation: Improving reasoning in multimodal LLMs for complex tasks like mathematical problem-solving while reducing extra annotation and high training costs.

Method: The D2I framework employs deliberate reasoning strategies during training using a rule-based format reward for modality alignment, shifting to intuitive reasoning during evaluation.

Result: D2I demonstrates improved performance over baselines in both in-domain and out-of-domain benchmarks, showing the impact of format reward on transferable reasoning.

Conclusion: D2I reduces reliance on costly annotations and rewards while enhancing reasoning ability, opening pathways to decouple training complexity from test-time flexibility.

Abstract: Reasoning is a key capability for large language models (LLMs), particularly
when applied to complex tasks such as mathematical problem solving. However,
multimodal reasoning research still requires further exploration of modality
alignment and training costs. Many of these approaches rely on additional data
annotation and relevant rule-based rewards to enhance the understanding and
reasoning ability, which significantly increases training costs and limits
scalability. To address these challenges, we propose the
Deliberate-to-Intuitive reasoning framework (D2I) that improves the
understanding and reasoning ability of multimodal LLMs (MLLMs) without extra
annotations and complex rewards. Specifically, our method sets deliberate
reasoning strategies to enhance modality alignment only through the rule-based
format reward during training. While evaluating, the reasoning style shifts to
intuitive, which removes deliberate reasoning strategies during training and
implicitly reflects the model's acquired abilities in the response. D2I
outperforms baselines across both in-domain and out-of-domain benchmarks. Our
findings highlight the role of format reward in fostering transferable
reasoning skills in MLLMs, and inspire directions for decoupling training-time
reasoning depth from test-time response flexibility.

</details>


### [115] [Know Your Attention Maps: Class-specific Token Masking for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2507.06848)
*Joelle Hanna,Damian Borth*

Main category: cs.CV

TL;DR: The paper presents an end-to-end method for Weakly Supervised Semantic Segmentation (WSSS) using a modified Vision Transformer (ViT) to generate pseudo segmentation masks without external modules such as Class Activation Maps.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in weakly supervised semantic segmentation and reduce dependency on external modules and fine-grained labeled data.

Method: The proposed method trains a sparse Vision Transformer with multiple [CLS] tokens using a random masking strategy, which enhances class assignments. At inference, self-attention maps corresponding to predicted labels are aggregated to generate pseudo-masks.

Result: The approach produces interpretable self-attention maps and accurate pseudo-masks. Experiments on standard benchmarks show it outperforms related methods and competes with fully-supervised segmentation models.

Conclusion: The method demonstrates the potential of Vision Transformers in WSSS, significantly reducing the need for detailed labeled data while maintaining performance close to fully-supervised models.

Abstract: Weakly Supervised Semantic Segmentation (WSSS) is a challenging problem that
has been extensively studied in recent years. Traditional approaches often rely
on external modules like Class Activation Maps to highlight regions of interest
and generate pseudo segmentation masks. In this work, we propose an end-to-end
method that directly utilizes the attention maps learned by a Vision
Transformer (ViT) for WSSS. We propose training a sparse ViT with multiple
[CLS] tokens (one for each class), using a random masking strategy to promote
[CLS] token - class assignment. At inference time, we aggregate the different
self-attention maps of each [CLS] token corresponding to the predicted labels
to generate pseudo segmentation masks. Our proposed approach enhances the
interpretability of self-attention maps and ensures accurate class assignments.
Extensive experiments on two standard benchmarks and three specialized datasets
demonstrate that our method generates accurate pseudo-masks, outperforming
related works. Those pseudo-masks can be used to train a segmentation model
which achieves results comparable to fully-supervised models, significantly
reducing the need for fine-grained labeled data.

</details>


### [116] [IAP: Invisible Adversarial Patch Attack through Perceptibility-Aware Localization and Perturbation Optimization](https://arxiv.org/abs/2507.06856)
*Subrat Kishore Dutta,Xiao Zhang*

Main category: cs.CV

TL;DR: This paper introduces IAP, a framework for generating highly invisible adversarial patches that effectively attack computer vision models while remaining imperceptible to humans and automatic defenses.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the issues in existing adversarial patch generation methods, which either fail to perform in targeted attack scenarios or produce patches that are easily noticeable to humans and insufficiently stealthy against automated defenses.

Method: IAP employs perceptibility-aware schemes for patch localization and perturbation optimization. It integrates classwise localization with sensitivity maps to find effective yet inconspicuous patch locations. Additionally, it uses a perceptibility-regularized adversarial loss with color constancy prioritization for optimizing patch invisibility.

Result: Experiments demonstrated that IAP achieves consistent success in targeted attack scenarios while significantly improving the visual stealthiness of patches compared to existing methods. It also bypasses several state-of-the-art patch defenses.

Conclusion: IAP provides a novel solution for creating adversarial patches that are both effective in targeted machine learning attacks and visually imperceptible, overcoming limitations of existing approaches while evading humans and defenses.

Abstract: Despite modifying only a small localized input region, adversarial patches
can drastically change the prediction of computer vision models. However, prior
methods either cannot perform satisfactorily under targeted attack scenarios or
fail to produce contextually coherent adversarial patches, causing them to be
easily noticeable by human examiners and insufficiently stealthy against
automatic patch defenses. In this paper, we introduce IAP, a novel attack
framework that generates highly invisible adversarial patches based on
perceptibility-aware localization and perturbation optimization schemes.
Specifically, IAP first searches for a proper location to place the patch by
leveraging classwise localization and sensitivity maps, balancing the
susceptibility of patch location to both victim model prediction and human
visual system, then employs a perceptibility-regularized adversarial loss and a
gradient update rule that prioritizes color constancy for optimizing invisible
perturbations. Comprehensive experiments across various image benchmarks and
model architectures demonstrate that IAP consistently achieves competitive
attack success rates in targeted settings with significantly improved patch
invisibility compared to existing baselines. In addition to being highly
imperceptible to humans, IAP is shown to be stealthy enough to render several
state-of-the-art patch defenses ineffective.

</details>


### [117] [Longitudinal Study of Facial Biometrics at the BEZ: Temporal Variance Analysis](https://arxiv.org/abs/2507.06858)
*Mathias Schulz,Alexander Spenke,Pia Funk,Florian Blümel,Markus Rohde,Ralph Breithaupt,Gerd Nolden,Norbert Jung,Robert Lange*

Main category: cs.CV

TL;DR: The paper analyzes long-term biometric evaluations over two and a half years using diverse data from 400 participants and highlights daily fluctuations in biometric scores.


<details>
  <summary>Details</summary>
Motivation: To understand variability and reliability in biometric systems by examining data collected over an extended period from a diverse participant pool.

Method: State-of-the-art face recognition algorithms evaluated biometric modalities like face and finger from over 238,000 data sets under controlled conditions.

Result: Daily biometric comparison scores exhibited more fluctuations than changes observed over the entire measurement duration.

Conclusion: Long-term, controlled evaluation is crucial to improving biometric systems and understanding variability in individual biometric characteristics.

Abstract: This study presents findings from long-term biometric evaluations conducted
at the Biometric Evaluation Center (bez). Over the course of two and a half
years, our ongoing research with over 400 participants representing diverse
ethnicities, genders, and age groups were regularly assessed using a variety of
biometric tools and techniques at the controlled testing facilities. Our
findings are based on the General Data Protection Regulation-compliant local
bez database with more than 238.000 biometric data sets categorized into
multiple biometric modalities such as face and finger. We used state-of-the-art
face recognition algorithms to analyze long-term comparison scores. Our results
show that these scores fluctuate more significantly between individual days
than over the entire measurement period. These findings highlight the
importance of testing biometric characteristics of the same individuals over a
longer period of time in a controlled measurement environment and lays the
groundwork for future advancements in biometric data analysis.

</details>


### [118] [SemRaFiner: Panoptic Segmentation in Sparse and Noisy Radar Point Clouds](https://arxiv.org/abs/2507.06906)
*Matthias Zeller,Daniel Casado Herraez,Bengisu Ayan,Jens Behley,Michael Heidingsfeld,Cyrill Stachniss*

Main category: cs.CV

TL;DR: The paper introduces SemRaFiner, an approach for improving panoptic segmentation in sparse radar point clouds to enhance scene understanding.


<details>
  <summary>Details</summary>
Motivation: To address limitations of cameras and LiDAR in adverse weather conditions and their lack of motion information; radar sensors are leveraged but face challenges like sparse and noisy measurements.

Method: Developed SemRaFiner, which optimizes feature extraction and training procedures for radar-based panoptic segmentation, including new data augmentation techniques.

Result: SemRaFiner outperforms state-of-the-art methods in radar-based panoptic segmentation according to experimental results.

Conclusion: The proposed approach effectively deals with sparse radar data, achieving better accuracy and enhancing semantic scene understanding for autonomous vehicles.

Abstract: Semantic scene understanding, including the perception and classification of
moving agents, is essential to enabling safe and robust driving behaviours of
autonomous vehicles. Cameras and LiDARs are commonly used for semantic scene
understanding. However, both sensor modalities face limitations in adverse
weather and usually do not provide motion information. Radar sensors overcome
these limitations and directly offer information about moving agents by
measuring the Doppler velocity, but the measurements are comparably sparse and
noisy. In this paper, we address the problem of panoptic segmentation in sparse
radar point clouds to enhance scene understanding. Our approach, called
SemRaFiner, accounts for changing density in sparse radar point clouds and
optimizes the feature extraction to improve accuracy. Furthermore, we propose
an optimized training procedure to refine instance assignments by incorporating
a dedicated data augmentation. Our experiments suggest that our approach
outperforms state-of-the-art methods for radar-based panoptic segmentation.

</details>


### [119] [Adaptive Part Learning for Fine-Grained Generalized Category Discovery: A Plug-and-Play Enhancement](https://arxiv.org/abs/2507.06928)
*Qiyuan Dai,Hanzhuo Huang,Yu Wu,Sibei Yang*

Main category: cs.CV

TL;DR: Generalized Category Discovery is addressed using an adaptive part discovery and learning method (APL), improving image classification by enhancing object part discrimination and generalization.


<details>
  <summary>Details</summary>
Motivation: To improve the ability to recognize and distinguish unlabeled images from both known and novel classes, while effectively transferring knowledge from labeled data.

Method: This paper introduces APL, which uses shared learnable part queries and DINO priors to discover object parts across similar images. It incorporates an all-min contrastive loss for learning discriminative and generalizable part representations.

Result: APL enhances discriminability and generalization in image classification tasks, significantly improving performance on fine-grained datasets.

Conclusion: The proposed APL method effectively boosts the performance of Generalized Category Discovery frameworks by leveraging better part-based representations and adaptive contrastive learning.

Abstract: Generalized Category Discovery (GCD) aims to recognize unlabeled images from
known and novel classes by distinguishing novel classes from known ones, while
also transferring knowledge from another set of labeled images with known
classes. Existing GCD methods rely on self-supervised vision transformers such
as DINO for representation learning. However, focusing solely on the global
representation of the DINO CLS token introduces an inherent trade-off between
discriminability and generalization. In this paper, we introduce an adaptive
part discovery and learning method, called APL, which generates consistent
object parts and their correspondences across different similar images using a
set of shared learnable part queries and DINO part priors, without requiring
any additional annotations. More importantly, we propose a novel all-min
contrastive loss to learn discriminative yet generalizable part representation,
which adaptively highlights discriminative object parts to distinguish similar
categories for enhanced discriminability while simultaneously sharing other
parts to facilitate knowledge transfer for improved generalization. Our APL can
easily be incorporated into different GCD frameworks by replacing their CLS
token feature with our part representations, showing significant enhancements
on fine-grained datasets.

</details>


### [120] [MCCD: A Multi-Attribute Chinese Calligraphy Character Dataset Annotated with Script Styles, Dynasties, and Calligraphers](https://arxiv.org/abs/2507.06948)
*Yixin Zhao,Yuyi Zhang,Lianwen Jin*

Main category: cs.CV

TL;DR: This paper introduces the Multi-Attribute Chinese Calligraphy Character Dataset (MCCD) to address challenges in recognizing diverse attributes of Chinese calligraphy due to evolving styles, limited datasets, and sparse annotations.


<details>
  <summary>Details</summary>
Motivation: Given the cultural and historical significance of Chinese calligraphy, the absence of comprehensive datasets with attribute annotations restricts in-depth research on styles, dynasties, and calligraphers.

Method: The researchers created MCCD, consisting of 329,715 labeled samples spanning 7,765 categories, with subsets for script styles, dynasties, and calligraphers. They conducted single-task and multi-task recognition experiments to benchmark performance.

Result: Experimental results revealed the interplay between stroke complexity and multi-attribute factors, making recognition tasks significantly challenging. The MCCD offers rich multi-attribute annotations suitable for diverse research applications.

Conclusion: MCCD fills a key gap by offering a comprehensive calligraphy dataset, enabling studies in recognition, writer identification, and character evolution while stimulating advancements in calligraphy research and related fields.

Abstract: Research on the attribute information of calligraphy, such as styles,
dynasties, and calligraphers, holds significant cultural and historical value.
However, the styles of Chinese calligraphy characters have evolved dramatically
through different dynasties and the unique touches of calligraphers, making it
highly challenging to accurately recognize these different characters and their
attributes. Furthermore, existing calligraphic datasets are extremely scarce,
and most provide only character-level annotations without additional attribute
information. This limitation has significantly hindered the in-depth study of
Chinese calligraphy. To fill this gap, we present a novel Multi-Attribute
Chinese Calligraphy Character Dataset (MCCD). The dataset encompasses 7,765
categories with a total of 329,715 isolated image samples of Chinese
calligraphy characters, and three additional subsets were extracted based on
the attribute labeling of the three types of script styles (10 types),
dynasties (15 periods) and calligraphers (142 individuals). The rich
multi-attribute annotations render MCCD well-suited diverse research tasks,
including calligraphic character recognition, writer identification, and
evolutionary studies of Chinese characters. We establish benchmark performance
through single-task and multi-task recognition experiments across MCCD and all
of its subsets. The experimental results demonstrate that the complexity of the
stroke structure of the calligraphic characters, and the interplay between
their different attributes, leading to a substantial increase in the difficulty
of accurate recognition. MCCD not only fills a void in the availability of
detailed calligraphy datasets but also provides valuable resources for
advancing research in Chinese calligraphy and fostering advancements in
multiple fields. The dataset is available at
https://github.com/SCUT-DLVCLab/MCCD.

</details>


### [121] [Pre-Columbian Settlements Shaped Palm Clusters in the Sierra Nevada de Santa Marta, Colombia](https://arxiv.org/abs/2507.06949)
*Sebastian Fajardo,Sina Mohammadi,Jonas Gregorio de Souza,César Ardila,Alan Tapscott Baltar,Shaddai Heidgen,Maria Isabel Mayorga Hernández,Sylvia Mota de Oliveira,Fernando Montejo,Marco Moderato,Vinicius Peripato,Katy Puche,Carlos Reina,Juan Carlos Vargas,Frank W. Takes,Marco Madella*

Main category: cs.CV

TL;DR: The study develops a deep learning method to analyze vegetation patterns using satellite imagery, revealing how ancient human settlements influenced the environment by encouraging palm proliferation, significantly extending our understanding of their ecological impact and settlement scales.


<details>
  <summary>Details</summary>
Motivation: Ancient populations deeply impacted Neotropical forests, but data limitations make it difficult to assess the long-term, fine-scale effects of their forest management practices.

Method: A deep learning model was trained on satellite imagery to identify palm trees, complemented by clustering algorithms to estimate the extent of ancient human influence on vegetation patterns. The approach was applied to analyze palm tree distributions and their correlation with archaeological sites in Colombia.

Result: The analysis demonstrated that palms were more abundant near archaeological sites with significant infrastructure. Ancient human-managed areas may be up to 100 times larger than inferred from archaeological remains alone, showing substantial and lasting ecological impacts.

Conclusion: Pre-Columbian populations significantly shaped local vegetation, promoting palm tree proliferation as part of their environmental management. These findings underscore the value of AI methods in studying ecological and archaeological relationships, enabling insights into human-environment interactions at fine scales.

Abstract: Ancient populations markedly transformed Neotropical forests, yet
understanding the long-term effects of ancient human management, particularly
at high-resolution scales, remains challenging. In this work we propose a new
approach to investigate archaeological areas of influence based on vegetation
signatures. It consists of a deep learning model trained on satellite imagery
to identify palm trees, followed by a clustering algorithm to identify palm
clusters, which are then used to estimate ancient management areas. To assess
the palm distribution in relation to past human activity, we applied the
proposed approach to unique high-resolution satellite imagery data covering 765
km2 of the Sierra Nevada de Santa Marta, Colombia. With this work, we also
release a manually annotated palm tree dataset along with estimated locations
of archaeological sites from ground-surveys and legacy records. Results
demonstrate how palms were significantly more abundant near archaeological
sites showing large infrastructure investment. The extent of the largest palm
cluster indicates that ancient human-managed areas linked to major
infrastructure sites may be up to two orders of magnitude bigger than indicated
by archaeological evidence alone. Our findings suggest that pre-Columbian
populations influenced local vegetation fostering conditions conducive to palm
proliferation, leaving a lasting ecological footprint. This may have lowered
the logistical costs of establishing infrastructure-heavy settlements in
otherwise less accessible locations. Overall, this study demonstrates the
potential of integrating artificial intelligence approaches with new ecological
and archaeological data to identify archaeological areas of interest through
vegetation patterns, revealing fine-scale human-environment interactions.

</details>


### [122] [CheXPO: Preference Optimization for Chest X-ray VLMs with Counterfactual Rationale](https://arxiv.org/abs/2507.06959)
*Xiao Liang,Jiawei Hu,Di Wang,Zhi Ma,Lin Zhao,Ronghan Li,Bo Wan,Quan Wang*

Main category: cs.CV

TL;DR: The paper introduces CheXPO, a strategy to optimize vision-language models for clinical reliability in chest X-ray analysis, addressing issues like hallucinations and prohibitive expert annotations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the reliability of vision-language models in medical applications, especially in chest X-ray diagnostics, while reducing the need for extensive expert annotations.

Method: The method combines confidence-similarity joint mining with counterfactual rationale. It synthesizes a fine-grained multi-task dataset for supervised fine-tuning, uses token-level confidence analysis to identify hard examples, and employs similarity-based retrieval and synthetic rationales for efficient preference optimization.

Result: CheXPO achieves an 8.93% relative performance improvement using only 5% of fine-tuning samples, establishing state-of-the-art results across diverse clinical tasks.

Conclusion: CheXPO offers a scalable and interpretable solution for improving the reliability of vision-language models in radiology applications while utilizing minimal resources.

Abstract: Vision-language models (VLMs) are prone to hallucinations that critically
compromise reliability in medical applications. While preference optimization
can mitigate these hallucinations through clinical feedback, its implementation
faces challenges such as clinically irrelevant training samples, imbalanced
data distributions, and prohibitive expert annotation costs. To address these
challenges, we introduce CheXPO, a Chest X-ray Preference Optimization strategy
that combines confidence-similarity joint mining with counterfactual rationale.
Our approach begins by synthesizing a unified, fine-grained multi-task chest
X-ray visual instruction dataset across different question types for supervised
fine-tuning (SFT). We then identify hard examples through token-level
confidence analysis of SFT failures and use similarity-based retrieval to
expand hard examples for balancing preference sample distributions, while
synthetic counterfactual rationales provide fine-grained clinical preferences,
eliminating the need for additional expert input. Experiments show that CheXPO
achieves 8.93% relative performance gain using only 5% of SFT samples, reaching
state-of-the-art performance across diverse clinical tasks and providing a
scalable, interpretable solution for real-world radiology applications.

</details>


### [123] [Segmentation Regularized Training for Multi-Domain Deep Learning Registration applied to MR-Guided Prostate Cancer Radiotherapy](https://arxiv.org/abs/2507.06966)
*Sudharsan Madhavan,Chengcheng Gui,Lando Bosma,Josiah Simeth,Jue Jiang,Nicolas Cote,Nima Hassan Rezaeian,Himanshu Nagar,Victoria Brennan,Neelam Tyagi,Harini Veeraraghavan*

Main category: cs.CV

TL;DR: This paper develops a deep learning method (ProRSeg) for deformable image registration (DIR) in MR-guided adaptive radiotherapy for various MR domains, showing good accuracy across different structures and suggesting potential for dose accumulation analysis.


<details>
  <summary>Details</summary>
Motivation: Accurate DIR is critical for contour propagation and dose accumulation in MRgART, but domain-specific challenges limit current methods. The study aims to achieve domain-invariant DIR using deep learning.

Method: The ProRSeg method was trained using progressively refined registration and segmentation supervised by a weighted segmentation consistency loss. It was tested on various MR datasets to evaluate domain invariance and accuracy for contour propagation.

Result: ProRSeg showed consistent bladder registration accuracy across domains (DSCs ~0.88), while rectum and CTV registration improved for cross-domain datasets (DSCs ~0.89). Dose accumulation analysis demonstrated compliance with several clinical constraints but raised concerns about upper mean dose thresholds.

Conclusion: ProRSeg evidenced robust performance for multi-domain MR registration, particularly in cross-domain scenarios, and enabled preliminary evaluations of treatment compliance to clinical constraints in prostate cancer radiotherapy patients.

Abstract: Background: Accurate deformable image registration (DIR) is required for
contour propagation and dose accumulation in MR-guided adaptive radiotherapy
(MRgART). This study trained and evaluated a deep learning DIR method for
domain invariant MR-MR registration. Methods: A progressively refined
registration and segmentation (ProRSeg) method was trained with 262 pairs of 3T
MR simulation scans from prostate cancer patients using weighted segmentation
consistency loss. ProRSeg was tested on same- (58 pairs), cross- (72 1.5T MR
Linac pairs), and mixed-domain (42 MRSim-MRL pairs) datasets for contour
propagation accuracy of clinical target volume (CTV), bladder, and rectum. Dose
accumulation was performed for 42 patients undergoing 5-fraction MRgART.
Results: ProRSeg demonstrated generalization for bladder with similar Dice
Similarity Coefficients across domains (0.88, 0.87, 0.86). For rectum and CTV,
performance was domain-dependent with higher accuracy on cross-domain MRL
dataset (DSCs 0.89) versus same-domain data. The model's strong cross-domain
performance prompted us to study the feasibility of using it for dose
accumulation. Dose accumulation showed 83.3% of patients met CTV coverage (D95
>= 40.0 Gy) and bladder sparing (D50 <= 20.0 Gy) constraints. All patients
achieved minimum mean target dose (>40.4 Gy), but only 9.5% remained under
upper limit (<42.0 Gy). Conclusions: ProRSeg showed reasonable multi-domain
MR-MR registration performance for prostate cancer patients with preliminary
feasibility for evaluating treatment compliance to clinical constraints.

</details>


### [124] [A multi-modal dataset for insect biodiversity with imagery and DNA at the trap and individual level](https://arxiv.org/abs/2507.06972)
*Johanna Orsholm,John Quinto,Hannu Autto,Gaia Banelyte,Nicolas Chazot,Jeremy deWaard,Stephanie deWaard,Arielle Farrell,Brendan Furneaux,Bess Hardwick,Nao Ito,Amlan Kar,Oula Kalttopää,Deirdre Kerdraon,Erik Kristensen,Jaclyn McKeown,Tommi Mononen,Ellen Nein,Hanna Rogers,Tomas Roslin,Paula Schmitz,Jayme Sones,Maija Sujala,Amy Thompson,Evgeny V. Zakharov,Iuliia Zarubiieva,Akshita Gupta,Scott C. Lowe,Graham W. Taylor*

Main category: cs.CV

TL;DR: The paper develops the MassID45 dataset to improve the automatic classification of bulk insect samples through a combination of molecular and imaging data, supporting ecological surveys and machine learning advancements.


<details>
  <summary>Details</summary>
Motivation: The severe population decline of insects due to environmental changes and habitat loss necessitates efficient methods to study insect diversity and taxonomy, especially from unsorted bulk samples collected during ecological surveys.

Method: The researchers created the MassID45 dataset that integrates both molecular DNA barcoding and high-resolution imaging. AI-assisted tools and human annotators were utilized for segmenting bulk sample images and assigning taxonomic labels to over 17,000 arthropod specimens.

Result: The study combined taxonomic resolutions from DNA data with visual abundance estimations, generating a dataset that emphasizes tiny object detection and segmentation, expanding applications in taxonomy and ecology.

Conclusion: MassID45 serves as a significant step towards scalable insect community characterization, enabling faster ecological monitoring and fostering innovation in machine learning and ecological research.

Abstract: Insects comprise millions of species, many experiencing severe population
declines under environmental and habitat changes. High-throughput approaches
are crucial for accelerating our understanding of insect diversity, with DNA
barcoding and high-resolution imaging showing strong potential for automatic
taxonomic classification. However, most image-based approaches rely on
individual specimen data, unlike the unsorted bulk samples collected in
large-scale ecological surveys. We present the Mixed Arthropod Sample
Segmentation and Identification (MassID45) dataset for training automatic
classifiers of bulk insect samples. It uniquely combines molecular and imaging
data at both the unsorted sample level and the full set of individual
specimens. Human annotators, supported by an AI-assisted tool, performed two
tasks on bulk images: creating segmentation masks around each individual
arthropod and assigning taxonomic labels to over 17 000 specimens. Combining
the taxonomic resolution of DNA barcodes with precise abundance estimates of
bulk images holds great potential for rapid, large-scale characterization of
insect communities. This dataset pushes the boundaries of tiny object detection
and instance segmentation, fostering innovation in both ecological and machine
learning research.

</details>


### [125] [Free on the Fly: Enhancing Flexibility in Test-Time Adaptation with Online EM](https://arxiv.org/abs/2507.06973)
*Qiyuan Dai,Sibei Yang*

Main category: cs.CV

TL;DR: The paper introduces FreeTTA, a training-free method for test-time adaptation in Vision-Language Models (VLMs), which explicitly models test data distribution to improve predictions under domain shifts.


<details>
  <summary>Details</summary>
Motivation: Challenges in Vision-Language Models arise due to domain shifts and distribution changes in test data, impairing their usability in practical scenarios. Existing test-time adaptation methods are costly or rely on unrealistic assumptions.

Method: FreeTTA utilizes an online EM algorithm that leverages zero-shot predictions from VLMs as priors. It iteratively computes posterior probabilities for test samples and updates parameters without any need for training or historical data.

Result: Experiments across 15 datasets show that FreeTTA outperforms state-of-the-art methods, offering stable and significant improvements in both cross-domain and out-of-distribution settings.

Conclusion: FreeTTA removes restrictive assumptions, delivers better test-time adaptations, and introduces a novel approach of modeling intrinsic relationships among test samples to enhance prediction accuracy.

Abstract: Vision-Language Models (VLMs) have become prominent in open-world image
recognition for their strong generalization abilities. Yet, their effectiveness
in practical applications is compromised by domain shifts and distributional
changes, especially when test data distributions diverge from training data.
Therefore, the paradigm of test-time adaptation (TTA) has emerged, enabling the
use of online off-the-shelf data at test time, supporting independent sample
predictions, and eliminating reliance on test annotations. Traditional TTA
methods, however, often rely on costly training or optimization processes, or
make unrealistic assumptions about accessing or storing historical training and
test data. Instead, this study proposes FreeTTA, a training-free and
universally available method that makes no assumptions, to enhance the
flexibility of TTA. More importantly, FreeTTA is the first to explicitly model
the test data distribution, enabling the use of intrinsic relationships among
test samples to enhance predictions of individual samples without simultaneous
access--a direction not previously explored. FreeTTA achieves these advantages
by introducing an online EM algorithm that utilizes zero-shot predictions from
VLMs as priors to iteratively compute the posterior probabilities of each
online test sample and update parameters. Experiments demonstrate that FreeTTA
achieves stable and significant improvements compared to state-of-the-art
methods across 15 datasets in both cross-domain and out-of-distribution
settings.

</details>


### [126] [DenoiseCP-Net: Efficient Collective Perception in Adverse Weather via Joint LiDAR-Based 3D Object Detection and Denoising](https://arxiv.org/abs/2507.06976)
*Sven Teufel,Dominique Mayer,Jörg Gamerdinger,Oliver Bringmann*

Main category: cs.CV

TL;DR: This paper introduces DenoiseCP-Net, a LiDAR collective perception model tailored for adverse weather, showing effectiveness in noise reduction, bandwidth optimization, and latency management.


<details>
  <summary>Details</summary>
Motivation: The vulnerability of automated vehicle perception systems to environmental and weather-based sensor degradation limits their safety and reliability. Collective perception improves this but is yet to be studied under adverse conditions.

Method: The authors developed DenoiseCP-Net, a multi-task architecture integrating voxel-level noise filtering and object detection within a sparse convolution backbone, and tested it under simulated adverse weather conditions.

Result: DenoiseCP-Net achieves near-perfect noise filtering in adverse weather, reduces bandwidth needs by up to 23.6%, maintains detection accuracy, and improves inference latency.

Conclusion: DenoiseCP-Net successfully addresses key challenges of collective perception in adverse weather, enhancing efficiency and performance in shared autonomous systems.

Abstract: While automated vehicles hold the potential to significantly reduce traffic
accidents, their perception systems remain vulnerable to sensor degradation
caused by adverse weather and environmental occlusions. Collective perception,
which enables vehicles to share information, offers a promising approach to
overcoming these limitations. However, to this date collective perception in
adverse weather is mostly unstudied. Therefore, we conduct the first study of
LiDAR-based collective perception under diverse weather conditions and present
a novel multi-task architecture for LiDAR-based collective perception under
adverse weather. Adverse weather conditions can not only degrade perception
capabilities, but also negatively affect bandwidth requirements and latency due
to the introduced noise that is also transmitted and processed. Denoising prior
to communication can effectively mitigate these issues. Therefore, we propose
DenoiseCP-Net, a novel multi-task architecture for LiDAR-based collective
perception under adverse weather conditions. DenoiseCP-Net integrates
voxel-level noise filtering and object detection into a unified sparse
convolution backbone, eliminating redundant computations associated with
two-stage pipelines. This design not only reduces inference latency and
computational cost but also minimizes communication overhead by removing
non-informative noise. We extended the well-known OPV2V dataset by simulating
rain, snow, and fog using our realistic weather simulation models. We
demonstrate that DenoiseCP-Net achieves near-perfect denoising accuracy in
adverse weather, reduces the bandwidth requirements by up to 23.6% while
maintaining the same detection accuracy and reducing the inference latency for
cooperative vehicles.

</details>


### [127] [MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology Report Generation](https://arxiv.org/abs/2507.06992)
*Qilong Xing,Zikai Song,Youjia Zhang,Na Feng,Junqing Yu,Wei Yang*

Main category: cs.CV

TL;DR: This paper presents MCA-RG, a framework explicitly aligning visual features with medical concepts for improved radiology report generation, tested on MIMIC-CXR and CheXpert Plus datasets.


<details>
  <summary>Details</summary>
Motivation: Challenges in accurately mapping visual features to text and semantic agnostic feature extraction hinder the clinical adoption of LLMs in radiology report generation.

Method: The MCA-RG framework incorporates curated pathology and anatomy concept banks, anatomy-based contrastive learning, matching loss for pathology prioritization, and a feature gating mechanism for enhanced report generation.

Result: MCA-RG outperforms on public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrating its effectiveness in improving radiology report generation.

Conclusion: Aligning visual features with medical concepts is pivotal for improving diagnostic accuracy and facilitating the clinical adoption of AI systems in radiology.

Abstract: Despite significant advancements in adapting Large Language Models (LLMs) for
radiology report generation (RRG), clinical adoption remains challenging due to
difficulties in accurately mapping pathological and anatomical features to
their corresponding text descriptions. Additionally, semantic agnostic feature
extraction further hampers the generation of accurate diagnostic reports. To
address these challenges, we introduce Medical Concept Aligned Radiology Report
Generation (MCA-RG), a knowledge-driven framework that explicitly aligns visual
features with distinct medical concepts to enhance the report generation
process. MCA-RG utilizes two curated concept banks: a pathology bank containing
lesion-related knowledge, and an anatomy bank with anatomical descriptions. The
visual features are aligned with these medical concepts and undergo tailored
enhancement. We further propose an anatomy-based contrastive learning procedure
to improve the generalization of anatomical features, coupled with a matching
loss for pathological features to prioritize clinically relevant regions.
Additionally, a feature gating mechanism is employed to filter out low-quality
concept features. Finally, the visual features are corresponding to individual
medical concepts, and are leveraged to guide the report generation process.
Experiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate
that MCA-RG achieves superior performance, highlighting its effectiveness in
radiology report generation.

</details>


### [128] [Cross-Modality Masked Learning for Survival Prediction in ICI Treated NSCLC Patients](https://arxiv.org/abs/2507.06994)
*Qilong Xing,Zikai Song,Bingxin Gong,Lian Yang,Junqing Yu,Wei Yang*

Main category: cs.CV

TL;DR: This paper introduces a large-scale dataset and a novel fusion framework using CT images and clinical records to improve survival prediction in NSCLC patients undergoing immunotherapy.


<details>
  <summary>Details</summary>
Motivation: To enable personalized treatment planning and improved outcomes for NSCLC patients despite the scarcity of large datasets and effective multi-modal feature fusion strategies.

Method: The method involves a cross-modality masked learning approach with two transformers: a Slice-Depth Transformer for 3D CT imaging features and a Graph-based Transformer for clinical data features, with a masked learning strategy to improve feature fusion.

Result: The proposed framework achieves superior performance in NSCLC survival prediction and sets a new benchmark for multi-modal prognostic models.

Conclusion: The framework effectively addresses challenges in multi-modal feature integration, demonstrating its potential for advancing NSCLC personalized treatment and prognosis.

Abstract: Accurate prognosis of non-small cell lung cancer (NSCLC) patients undergoing
immunotherapy is essential for personalized treatment planning, enabling
informed patient decisions, and improving both treatment outcomes and quality
of life. However, the lack of large, relevant datasets and effective
multi-modal feature fusion strategies pose significant challenges in this
domain. To address these challenges, we present a large-scale dataset and
introduce a novel framework for multi-modal feature fusion aimed at enhancing
the accuracy of survival prediction. The dataset comprises 3D CT images and
corresponding clinical records from NSCLC patients treated with immune
checkpoint inhibitors (ICI), along with progression-free survival (PFS) and
overall survival (OS) data. We further propose a cross-modality masked learning
approach for medical feature fusion, consisting of two distinct branches, each
tailored to its respective modality: a Slice-Depth Transformer for extracting
3D features from CT images and a graph-based Transformer for learning node
features and relationships among clinical variables in tabular data. The fusion
process is guided by a masked modality learning strategy, wherein the model
utilizes the intact modality to reconstruct missing components. This mechanism
improves the integration of modality-specific features, fostering more
effective inter-modality relationships and feature interactions. Our approach
demonstrates superior performance in multi-modal integration for NSCLC survival
prediction, surpassing existing methods and setting a new benchmark for
prognostic models in this context.

</details>


### [129] [GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision Transformers for Whole Slide Image Classification and Captioning](https://arxiv.org/abs/2507.07006)
*S M Taslim Uddin Raju,Md. Milon Islam,Md Rezwanul Haque,Hamdi Altaheri,Fakhri Karray*

Main category: cs.CV

TL;DR: The paper introduces GNN-ViTCap framework for classifying and captioning histopathological microscope images. It addresses challenges like redundant patches and automatic caption generation, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Microscopic WSI are critical for cancer diagnosis, yet they face challenges such as redundant data patches and difficulties in generating associated pathology captions automatically.

Method: The GNN-ViTCap combines feature extraction, dynamic clustering, graph neural networks, and integration with a language model to tackle classification and captioning. It removes redundant patches, builds a graph of embeddings, and fine-tunes a language model for caption generation.

Result: The method achieves high performance metrics, with an F1 score of 0.934 and AUC of 0.963 for classification, and a BLEU-4 score of 0.811 and METEOR score of 0.569 for captioning on two datasets.

Conclusion: GNN-ViTCap demonstrates its superiority over existing methods, offering an efficient and reliable approach for pathology image classification and captioning.

Abstract: Microscopic assessment of histopathology images is vital for accurate cancer
diagnosis and treatment. Whole Slide Image (WSI) classification and captioning
have become crucial tasks in computer-aided pathology. However, microscopic WSI
face challenges such as redundant patches and unknown patch positions due to
subjective pathologist captures. Moreover, generating automatic pathology
captions remains a significant challenge. To address these issues, we introduce
a novel GNN-ViTCap framework for classification and caption generation from
histopathological microscopic images. First, a visual feature extractor
generates patch embeddings. Redundant patches are then removed by dynamically
clustering these embeddings using deep embedded clustering and selecting
representative patches via a scalar dot attention mechanism. We build a graph
by connecting each node to its nearest neighbors in the similarity matrix and
apply a graph neural network to capture both local and global context. The
aggregated image embeddings are projected into the language model's input space
through a linear layer and combined with caption tokens to fine-tune a large
language model. We validate our method on the BreakHis and PatchGastric
datasets. GNN-ViTCap achieves an F1 score of 0.934 and an AUC of 0.963 for
classification, along with a BLEU-4 score of 0.811 and a METEOR score of 0.569
for captioning. Experimental results demonstrate that GNN-ViTCap outperforms
state of the art approaches, offering a reliable and efficient solution for
microscopy based patient diagnosis.

</details>


### [130] [Integrating Pathology Foundation Models and Spatial Transcriptomics for Cellular Decomposition from Histology Images](https://arxiv.org/abs/2507.07013)
*Yutong Sun,Sichen Zhu,Peng Qiu*

Main category: cs.CV

TL;DR: This study proposes a lightweight approach for predicting cell composition in histology images using feature embeddings from pre-trained pathology foundation models, without requiring spatial transcriptomics.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently predicting cellular composition from histology images without the high costs associated with spatial transcriptomics.

Method: The authors use pre-trained pathology foundation models to extract enriched feature embeddings and then train a lightweight multi-layer perceptron (MLP) regressor on cell-type abundance data from cell2location.

Result: The proposed approach achieves accurate predictions of cell-type compositions while being computationally efficient, offering competitive performance compared to existing methods like Hist2Cell.

Conclusion: The method offers a scalable and cost-effective alternative to infer cellular composition from histology images, leveraging pre-trained foundation models and reducing complexity.

Abstract: The rapid development of digital pathology and modern deep learning has
facilitated the emergence of pathology foundation models that are expected to
solve general pathology problems under various disease conditions in one
unified model, with or without fine-tuning. In parallel, spatial
transcriptomics has emerged as a transformative technology that enables the
profiling of gene expression on hematoxylin and eosin (H&E) stained histology
images. Spatial transcriptomics unlocks the unprecedented opportunity to dive
into existing histology images at a more granular, cellular level. In this
work, we propose a lightweight and training-efficient approach to predict
cellular composition directly from H&E-stained histology images by leveraging
information-enriched feature embeddings extracted from pre-trained pathology
foundation models. By training a lightweight multi-layer perceptron (MLP)
regressor on cell-type abundances derived via cell2location, our method
efficiently distills knowledge from pathology foundation models and
demonstrates the ability to accurately predict cell-type compositions from
histology images, without physically performing the costly spatial
transcriptomics. Our method demonstrates competitive performance compared to
existing methods such as Hist2Cell, while significantly reducing computational
complexity.

</details>


### [131] [MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation](https://arxiv.org/abs/2507.07015)
*Hui Li,Pengfei Yang,Juanyang Chen,Le Dong,Yanxin Chen,Quan Wang*

Main category: cs.CV

TL;DR: This paper introduces MST-Distill, a framework to address challenges in cross-modal knowledge distillation using specialized teacher models and adaptive routing, delivering significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Existing cross-modal knowledge distillation techniques face difficulties like distillation path selection and knowledge drift, limiting their effectiveness.

Method: The authors propose the MST-Distill framework, combining a diverse ensemble of teacher models with an instance-level routing network and a masking module to suppress modality-specific discrepancies.

Result: Experiments on five multimodal datasets show that MST-Distill outperforms current state-of-the-art methods in cross-modal distillation tasks.

Conclusion: MST-Distill effectively addresses the challenges of cross-modal knowledge distillation, offering a dynamic, adaptive, and enhanced transfer mechanism.

Abstract: Knowledge distillation as an efficient knowledge transfer technique, has
achieved remarkable success in unimodal scenarios. However, in cross-modal
settings, conventional distillation methods encounter significant challenges
due to data and statistical heterogeneities, failing to leverage the
complementary prior knowledge embedded in cross-modal teacher models. This
paper empirically reveals two critical issues in existing approaches:
distillation path selection and knowledge drift. To address these limitations,
we propose MST-Distill, a novel cross-modal knowledge distillation framework
featuring a mixture of specialized teachers. Our approach employs a diverse
ensemble of teacher models across both cross-modal and multimodal
configurations, integrated with an instance-level routing network that
facilitates adaptive and dynamic distillation. This architecture effectively
transcends the constraints of traditional methods that rely on monotonous and
static teacher models. Additionally, we introduce a plug-in masking module,
independently trained to suppress modality-specific discrepancies and
reconstruct teacher representations, thereby mitigating knowledge drift and
enhancing transfer effectiveness. Extensive experiments across five diverse
multimodal datasets, spanning visual, audio, and text, demonstrate that our
method significantly outperforms existing state-of-the-art knowledge
distillation methods in cross-modal distillation tasks. The source code is
available at https://github.com/Gray-OREO/MST-Distill.

</details>


### [132] [Design and Implementation of an OCR-Powered Pipeline for Table Extraction from Invoices](https://arxiv.org/abs/2507.07029)
*Parshva Dhilankumar Patel*

Main category: cs.CV

TL;DR: The paper introduces an OCR-based pipeline that uses Tesseract OCR and custom logic for accurate table extraction from scanned invoices.


<details>
  <summary>Details</summary>
Motivation: The aim is to solve the challenges of extracting structured tabular data from noisy and non-standard invoice formats to enhance accuracy and consistency in applications like financial workflows.

Method: Tesseract OCR is combined with dynamic preprocessing, table boundary detection, and row-column mapping to design a custom pipeline.

Result: The developed pipeline improves the accuracy and consistency of table data extraction from scanned invoices.

Conclusion: The OCR-powered system has practical real-world applications, improving automated financial processes and aiding digital archiving.

Abstract: This paper presents the design and development of an OCR-powered pipeline for
efficient table extraction from invoices. The system leverages Tesseract OCR
for text recognition and custom post-processing logic to detect, align, and
extract structured tabular data from scanned invoice documents. Our approach
includes dynamic preprocessing, table boundary detection, and row-column
mapping, optimized for noisy and non-standard invoice formats. The resulting
pipeline significantly improves data extraction accuracy and consistency,
supporting real-world use cases such as automated financial workflows and
digital archiving.

</details>


### [133] [Evaluating Large Multimodal Models for Nutrition Analysis: A Benchmark Enriched with Contextual Metadata](https://arxiv.org/abs/2507.07048)
*Bruce Coburn,Jiangpeng He,Megan E. Rollo,Satvinder S. Dhaliwal,Deborah A. Kerr,Fengqing Zhu*

Main category: cs.CV

TL;DR: This study explores how contextual metadata integrated into Large Multimodal Models (LMMs) improves their performance in nutrition analysis from meal images. It analyzes 8 models using a new dataset, ACETADA.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored area of contextual metadata integration and its interactions with reasoning modifiers in enhancing multimodal model performance for nutrition analysis.

Method: The study evaluates 8 LMMs, incorporating GPS-based location, timestamps, and food data as metadata. It also introduces Chain-of-Thought and other reasoning strategies. The new ACETADA dataset is used for experiments.

Result: Incorporating contextual metadata significantly reduces Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) in predicting nutritional values, enhancing performance over image-alone prompts.

Conclusion: Integrating contextual metadata into LMMs leads to more accurate and effective nutrition analysis, showcasing the importance of making such models context-aware.

Abstract: Large Multimodal Models (LMMs) are increasingly applied to meal images for
nutrition analysis. However, existing work primarily evaluates proprietary
models, such as GPT-4. This leaves the broad range of LLMs underexplored.
Additionally, the influence of integrating contextual metadata and its
interaction with various reasoning modifiers remains largely uncharted. This
work investigates how interpreting contextual metadata derived from GPS
coordinates (converted to location/venue type), timestamps (transformed into
meal/day type), and the food items present can enhance LMM performance in
estimating key nutritional values. These values include calories,
macronutrients (protein, carbohydrates, fat), and portion sizes. We also
introduce ACETADA, a new food-image dataset slated for public release. This
open dataset provides nutrition information verified by the dietitian and
serves as the foundation for our analysis. Our evaluation across eight LMMs
(four open-weight and four closed-weight) first establishes the benefit of
contextual metadata integration over straightforward prompting with images
alone. We then demonstrate how this incorporation of contextual information
enhances the efficacy of reasoning modifiers, such as Chain-of-Thought,
Multimodal Chain-of-Thought, Scale Hint, Few-Shot, and Expert Persona.
Empirical results show that integrating metadata intelligently, when applied
through straightforward prompting strategies, can significantly reduce the Mean
Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) in predicted
nutritional values. This work highlights the potential of context-aware LMMs
for improved nutrition analysis.

</details>


### [134] [An AI Approach for Learning the Spectrum of the Laplace-Beltrami Operator](https://arxiv.org/abs/2507.07073)
*Yulin An,Enrique del Castillo*

Main category: cs.CV

TL;DR: The paper proposes a Graph Neural Network framework to efficiently predict the Laplace-Beltrami spectrum of CAD mechanical parts, reducing computation time significantly compared to FEM and maintaining competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: The aim is to address the inefficiency of the FEM method for computing the LB spectrum in scenarios involving large mesh datasets or high-frequency quality control applications, where quick decisions are needed.

Method: The authors design a Graph Neural Network architecture that incorporates geometric features like Gaussian curvature, mean curvature, and principal curvatures from CAD mesh data. They also provide a curated dataset based on the ABC dataset for testing and training.

Result: The proposed method reduces computation time by approximately 5 times compared to FEM while delivering competitive accuracy in predicting the LB spectrum.

Conclusion: This framework demonstrates that the LB spectrum is predictable via geometric deep learning, offering computational savings without compromising accuracy in various applications involving CAD meshes.

Abstract: The spectrum of the Laplace-Beltrami (LB) operator is central in geometric
deep learning tasks, capturing intrinsic properties of the shape of the object
under consideration. The best established method for its estimation, from a
triangulated mesh of the object, is based on the Finite Element Method (FEM),
and computes the top k LB eigenvalues with a complexity of O(Nk), where N is
the number of points. This can render the FEM method inefficient when
repeatedly applied to databases of CAD mechanical parts, or in quality control
applications where part metrology is acquired as large meshes and decisions
about the quality of each part are needed quickly and frequently. As a solution
to this problem, we present a geometric deep learning framework to predict the
LB spectrum efficiently given the CAD mesh of a part, achieving significant
computational savings without sacrificing accuracy, demonstrating that the LB
spectrum is learnable. The proposed Graph Neural Network architecture uses a
rich set of part mesh features - including Gaussian curvature, mean curvature,
and principal curvatures. In addition to our trained network, we make
available, for repeatability, a large curated dataset of real-world mechanical
CAD models derived from the publicly available ABC dataset used for training
and testing. Experimental results show that our method reduces computation time
of the LB spectrum by approximately 5 times over linear FEM while delivering
competitive accuracy.

</details>


### [135] [Reading a Ruler in the Wild](https://arxiv.org/abs/2507.07077)
*Yimu Pan,Manas Mehta,Gwen Sincerbeaux,Jeffery A. Goldstein,Alison D. Gernand,James Z. Wang*

Main category: cs.CV

TL;DR: The paper introduces RulerNet, a deep learning model for accurate real-world scale estimation via keypoint detection, and showcases its superior performance under diverse conditions.


<details>
  <summary>Details</summary>
Motivation: Accurately converting pixel measurements to real-world dimensions is essential but challenging for applications like biomedicine, forensics, and e-commerce, due to reliance on rigid and handcrafted approaches.

Method: RulerNet reformulates ruler detection as a keypoint localization problem, uses distortion-invariant annotations, combines a synthetic-data pipeline for training, and introduces DeepGP for efficient parameter regression without iterative optimization.

Result: RulerNet achieves accurate and consistent scale estimation even in challenging real-world scenarios, demonstrating strong generalization and efficiency.

Conclusion: RulerNet is a robust and generalizable framework for real-world measurements, with potential applications in high-impact domains and integration into automated scale-aware vision systems.

Abstract: Accurately converting pixel measurements into absolute real-world dimensions
remains a fundamental challenge in computer vision and limits progress in key
applications such as biomedicine, forensics, nutritional analysis, and
e-commerce. We introduce RulerNet, a deep learning framework that robustly
infers scale "in the wild" by reformulating ruler reading as a unified
keypoint-detection problem and by representing the ruler with
geometric-progression parameters that are invariant to perspective
transformations. Unlike traditional methods that rely on handcrafted thresholds
or rigid, ruler-specific pipelines, RulerNet directly localizes centimeter
marks using a distortion-invariant annotation and training strategy, enabling
strong generalization across diverse ruler types and imaging conditions while
mitigating data scarcity. We also present a scalable synthetic-data pipeline
that combines graphics-based ruler generation with ControlNet to add
photorealistic context, greatly increasing training diversity and improving
performance. To further enhance robustness and efficiency, we propose DeepGP, a
lightweight feed-forward network that regresses geometric-progression
parameters from noisy marks and eliminates iterative optimization, enabling
real-time scale estimation on mobile or edge devices. Experiments show that
RulerNet delivers accurate, consistent, and efficient scale estimates under
challenging real-world conditions. These results underscore its utility as a
generalizable measurement tool and its potential for integration with other
vision components for automated, scale-aware analysis in high-impact domains. A
live demo is available at https://huggingface.co/spaces/ymp5078/RulerNet-Demo.

</details>


### [136] [Evaluating Attribute Confusion in Fashion Text-to-Image Generation](https://arxiv.org/abs/2507.07079)
*Ziyue Liu,Federico Girella,Yiming Wang,Davide Talon*

Main category: cs.CV

TL;DR: The paper introduces a new metric called Localized VQAScore (L-VQAScore) for better evaluation of Text-to-Image models, specifically focusing on addressing attribute confusion for complex compositional scenarios like fashion.


<details>
  <summary>Details</summary>
Motivation: Existing automated T2I evaluation methods struggle to assess rich entity-attribute semantics, especially in scenarios with attribute confusion, where the wrong entities are associated with correctly depicted attributes.

Method: The paper proposes a Visual Question Answering (VQA) localization strategy targeting individual entities across visual and textual modalities, alongside a localized human evaluation protocol. The L-VQAScore metric combines visual localization with VQA probing to differentiate correct and mis-localized attribute generation.

Result: On a challenging dataset with compositional alignment scenarios, L-VQAScore outperformed state-of-the-art evaluation methods in alignment with human judgments, effectively capturing fine-grained entity-to-attribute associations.

Conclusion: L-VQAScore offers a reliable, scalable alternative to subjective evaluations for assessing T2I models, especially in domains like fashion with complex compositional requirements.

Abstract: Despite the rapid advances in Text-to-Image (T2I) generation models, their
evaluation remains challenging in domains like fashion, involving complex
compositional generation. Recent automated T2I evaluation methods leverage
pre-trained vision-language models to measure cross-modal alignment. However,
our preliminary study reveals that they are still limited in assessing rich
entity-attribute semantics, facing challenges in attribute confusion, i.e.,
when attributes are correctly depicted but associated to the wrong entities. To
address this, we build on a Visual Question Answering (VQA) localization
strategy targeting one single entity at a time across both visual and textual
modalities. We propose a localized human evaluation protocol and introduce a
novel automatic metric, Localized VQAScore (L-VQAScore), that combines visual
localization with VQA probing both correct (reflection) and miss-localized
(leakage) attribute generation. On a newly curated dataset featuring
challenging compositional alignment scenarios, L-VQAScore outperforms
state-of-the-art T2I evaluation methods in terms of correlation with human
judgments, demonstrating its strength in capturing fine-grained
entity-attribute associations. We believe L-VQAScore can be a reliable and
scalable alternative to subjective evaluations.

</details>


### [137] [Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data](https://arxiv.org/abs/2507.07095)
*Ke Fan,Shunlin Lu,Minyue Dai,Runyi Yu,Lixing Xiao,Zhiyang Dou,Junting Dong,Lizhuang Ma,Jingbo Wang*

Main category: cs.CV

TL;DR: The paper introduces MotionMillion, the largest human motion dataset, alongside a robust evaluation benchmark (MotionMillion-Eval) for zero-shot motion generation using textual descriptions, significantly advancing the generalization capabilities of models in this domain.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with zero-shot generalization in text-to-motion due to dataset size limitations and lack an evaluation framework to pinpoint improvement areas.

Method: The authors developed an annotation pipeline to create MotionMillion, a massive motion dataset, and designed MotionMillion-Eval for comprehensive evaluation. They scaled their model to 7B parameters for validation.

Result: The proposed model demonstrated strong generalization to both out-of-domain and complex compositional motions based on text prompts, achieving zero-shot capabilities.

Conclusion: This work significantly advances text-to-motion technology with MotionMillion dataset and MotionMillion-Eval benchmark, setting new standards for zero-shot human motion generation and improving research possibilities in this area.

Abstract: Generating diverse and natural human motion sequences based on textual
descriptions constitutes a fundamental and challenging research area within the
domains of computer vision, graphics, and robotics. Despite significant
advancements in this field, current methodologies often face challenges
regarding zero-shot generalization capabilities, largely attributable to the
limited size of training datasets. Moreover, the lack of a comprehensive
evaluation framework impedes the advancement of this task by failing to
identify directions for improvement. In this work, we aim to push
text-to-motion into a new era, that is, to achieve the generalization ability
of zero-shot. To this end, firstly, we develop an efficient annotation pipeline
and introduce MotionMillion-the largest human motion dataset to date, featuring
over 2,000 hours and 2 million high-quality motion sequences. Additionally, we
propose MotionMillion-Eval, the most comprehensive benchmark for evaluating
zero-shot motion generation. Leveraging a scalable architecture, we scale our
model to 7B parameters and validate its performance on MotionMillion-Eval. Our
results demonstrate strong generalization to out-of-domain and complex
compositional motions, marking a significant step toward zero-shot human motion
generation. The code is available at
https://github.com/VankouF/MotionMillion-Codes.

</details>


### [138] [Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models](https://arxiv.org/abs/2507.07104)
*Tiezheng Zhang,Yitong Li,Yu-cheng Chou,Jieneng Chen,Alan Yuille,Chen Wei,Junfei Xiao*

Main category: cs.CV

TL;DR: This paper proposes VLV, a cost-efficient framework for Vision-Language Models that uses pretrained models to reduce training costs and data dependency while achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the high cost and data requirements involved in training Vision-Language Models with strong captioning capabilities.

Method: The VLV framework integrates pretrained components (vision encoder, T2I diffusion model decoder, and LLM), enforces a bottleneck in the language space, and fine-tunes the LLM to decode language representations.

Result: The resulting VLV model achieves state-of-the-art captioning performance, comparable to leading models, while dramatically reducing training cost and data requirements to under $1,000 USD.

Conclusion: Leveraging pretrained models and single-modal images, the VLV pipeline efficiently produces high-quality captioning results, highlighting its potential for scaling Vision-Language Models cost-effectively.

Abstract: Building state-of-the-art Vision-Language Models (VLMs) with strong
captioning capabilities typically necessitates training on billions of
high-quality image-text pairs, requiring millions of GPU hours. This paper
introduces the Vision-Language-Vision (VLV) auto-encoder framework, which
strategically leverages key pretrained components: a vision encoder, the
decoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large
Language Model (LLM). Specifically, we establish an information bottleneck by
regularizing the language representation space, achieved through freezing the
pretrained T2I diffusion decoder. Our VLV pipeline effectively distills
knowledge from the text-conditioned diffusion model using continuous
embeddings, demonstrating comprehensive semantic understanding via high-quality
reconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the
intermediate language representations into detailed descriptions, we construct
a state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o
and Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and
significantly reduces data requirements; by primarily utilizing single-modal
images for training and maximizing the utility of existing pretrained models
(image encoder, T2I diffusion model, and LLM), it circumvents the need for
massive paired image-text datasets, keeping the total training expenditure
under $1,000 USD.

</details>


### [139] [4KAgent: Agentic Any Image to 4K Super-Resolution](https://arxiv.org/abs/2507.07105)
*Yushen Zuo,Qi Zheng,Mingyang Wu,Xinrui Jiang,Renjie Li,Jian Wang,Yide Zhang,Gengchen Mai,Lihong V. Wang,James Zou,Xiaoyu Wang,Ming-Hsuan Yang,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 4KAgent is a universal image super-resolution system capable of upscaling images to 4K resolution, even from highly degraded inputs, using a modular and agentic architecture.


<details>
  <summary>Details</summary>
Motivation: The paper aims to provide a generalized and effective solution for upscaling images to 4K resolution across diverse domains, addressing limitations in dealing with low-resolution degraded inputs and specialized restoration needs.

Method: The system integrates three components: Profiling to customize the pipeline, a Perception Agent to analyze images using vision-language models, and a Restoration Agent that follows a recursive execution-reflection process for optimal quality. Additionally, it includes a dedicated face restoration pipeline.

Result: 4KAgent delivers state-of-the-art performance across 26 benchmarks in 11 imaging categories, significantly improving perceptual and fidelity metrics in various domains like natural images, medical imaging, and satellite imagery.

Conclusion: 4KAgent sets a new standard for super-resolution tasks, showcasing its versatility and effectiveness across applications, and aims to drive future innovations in vision-centric autonomous agents.

Abstract: We present 4KAgent, a unified agentic super-resolution generalist system
designed to universally upscale any image to 4K resolution (and even higher, if
applied iteratively). Our system can transform images from extremely low
resolutions with severe degradations, for example, highly distorted inputs at
256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three
core components: (1) Profiling, a module that customizes the 4KAgent pipeline
based on bespoke use cases; (2) A Perception Agent, which leverages
vision-language models alongside image quality assessment experts to analyze
the input image and make a tailored restoration plan; and (3) A Restoration
Agent, which executes the plan, following a recursive execution-reflection
paradigm, guided by a quality-driven mixture-of-expert policy to select the
optimal output for each step. Additionally, 4KAgent embeds a specialized face
restoration pipeline, significantly enhancing facial details in portrait and
selfie photos. We rigorously evaluate our 4KAgent across 11 distinct task
categories encompassing a total of 26 diverse benchmarks, setting new
state-of-the-art on a broad spectrum of imaging domains. Our evaluations cover
natural images, portrait photos, AI-generated content, satellite imagery,
fluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and
X-ray, demonstrating superior performance in terms of both perceptual (e.g.,
NIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic
paradigm for low-level vision tasks, we aim to catalyze broader interest and
innovation within vision-centric autonomous agents across diverse research
communities. We will release all the code, models, and results at:
https://4kagent.github.io.

</details>


### [140] [Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor](https://arxiv.org/abs/2507.07106)
*Vatsal Agarwal,Matthew Gwilliam,Gefen Kohavi,Eshan Verma,Daniel Ulbricht,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: This paper explores using pre-trained text-to-image diffusion models as instruction-aware visual encoders to address the limitations of CLIP in fine-grained detail understanding for image-based Q&A tasks. A combined approach using diffusion features and CLIP is proposed and found effective.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the limitation of CLIP as a visual encoder, which struggles to capture fine-grained details in image-based question-answering tasks.

Method: The paper analyzes the internal representations of text-to-image diffusion models, examines their semantic richness and image-text alignment capabilities, and proposes a fusion strategy combining CLIP and conditional diffusion features. It also investigates and mitigates the leakage phenomenon in alignment with LLMs.

Result: The combined approach effectively leverages the strengths of diffusion models and CLIP. Testing on general VQA and MLLM benchmarks shows improved performance, especially in tasks requiring spatial and compositional reasoning.

Conclusion: Diffusion models hold great potential as instruction-aware visual encoders in multimodal systems, particularly by addressing shortcomings of current approaches like CLIP, for enhanced visual understanding in vision-centric tasks.

Abstract: Recent advances in multimodal large language models (MLLMs) have enabled
image-based question-answering capabilities. However, a key limitation is the
use of CLIP as the visual encoder; while it can capture coarse global
information, it often can miss fine-grained details that are relevant to the
input query. To address these shortcomings, this work studies whether
pre-trained text-to-image diffusion models can serve as instruction-aware
visual encoders. Through an analysis of their internal representations, we find
diffusion features are both rich in semantics and can encode strong image-text
alignment. Moreover, we find that we can leverage text conditioning to focus
the model on regions relevant to the input question. We then investigate how to
align these features with large language models and uncover a leakage
phenomenon, where the LLM can inadvertently recover information from the
original diffusion prompt. We analyze the causes of this leakage and propose a
mitigation strategy. Based on these insights, we explore a simple fusion
strategy that utilizes both CLIP and conditional diffusion features. We
evaluate our approach on both general VQA and specialized MLLM benchmarks,
demonstrating the promise of diffusion models for visual understanding,
particularly in vision-centric tasks that require spatial and compositional
reasoning. Our project page can be found
https://vatsalag99.github.io/mustafar/.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [141] [Designing Parallel Algorithms for Community Detection using Arachne](https://arxiv.org/abs/2507.06471)
*Fuhuan Li,Zhihui Du,David A. Bader*

Main category: cs.DC

TL;DR: The paper introduces scalable, parallel implementations of community detection algorithms, achieving significant speedups over existing graph analysis tools.


<details>
  <summary>Details</summary>
Motivation: Efficient and scalable community detection is necessary due to the growing prevalence of graph data in multiple application areas.

Method: Introduced parallelized versions of Label Propagation and Louvain algorithms using the Arachne framework for large-scale graph analysis.

Result: Achieved up to 710x speedup over NetworkX, 75x over igraph, and 12x over NetworKit in their experimental evaluations.

Conclusion: The implementations are faster and scalable, leveraging Arachne framework's capabilities, contributing to open-source tools for advanced graph analysis.

Abstract: The rise of graph data in various fields calls for efficient and scalable
community detection algorithms. In this paper, we present parallel
implementations of two widely used algorithms: Label Propagation and Louvain,
specifically designed to leverage the capabilities of Arachne which is a
Python-accessible, open-source framework for large-scale graph analysis. Our
implementations achieve substantial speedups over existing Python-based tools
like NetworkX and igraph, which lack efficient parallelization, and are
competitive with parallel frameworks such as NetworKit. Experimental results
show that Arachne-based methods outperform these baselines, achieving speedups
of up to 710x over NetworkX, 75x over igraph, and 12x over NetworKit.
Additionally, we analyze the scalability of our implementation under varying
thread counts, demonstrating how different phases contribute to overall
performance gains of the parallel Louvain algorithm. Arachne, including our
community detection implementation, is open-source and available at
https://github.com/Bears-R-Us/arkouda-njit .

</details>


### [142] [Nexus: Taming Throughput-Latency Tradeoff in LLM Serving via Efficient GPU Sharing](https://arxiv.org/abs/2507.06608)
*Xiaoxiang Shi,Colin Cai,Junjia Du,Zhanda Zhu,Xingda Wei,Zhihao Jia*

Main category: cs.DC

TL;DR: This paper proposes Nexus, a system that allows prefill and decode phases of a serving engine to share a single GPU while optimizing resource allocation, improving throughput and reducing latency.


<details>
  <summary>Details</summary>
Motivation: To improve GPU utilization in handling prefill and decode phases while avoiding hardware inefficiencies and latency introduced by current disaggregation methods.

Method: Utilize insights on GPU resource diminishing returns to split and dynamically allocate GPU resources between prefill and decode phases within a single engine to reduce phase interference.

Result: Nexus achieves up to 2.2x higher throughput, 20x lower TTFT, and 2.5x lower TBT compared to vLLM, along with improved performance against SGLang.

Conclusion: Nexus offers an efficient GPU resource allocation strategy, enabling high throughput and low latency while reducing GPU hardware requirements significantly.

Abstract: Current prefill-decode (PD) disaggregation is typically deployed at the level
of entire serving engines, assigning separate GPUs to handle prefill and decode
phases. While effective at reducing latency, this approach demands more
hardware. To improve GPU utilization, Chunked Prefill mixes prefill and decode
requests within the same batch, but introduces phase interference between
prefill and decode.
  While existing PD disaggregation solutions separate the phases across GPUs,
we ask: can the same decoupling be achieved within a single serving engine? The
key challenge lies in managing the conflicting resource requirements of prefill
and decode when they share the same hardware. In this paper, we first show that
chunked prefill requests cause interference with decode requests due to their
distinct requirements for GPU resources. Second, we find that GPU resources
exhibit diminishing returns. Beyond a saturation point, increasing GPU
allocation yields negligible latency improvements. This insight enables us to
split a single GPU's resources and dynamically allocate them to prefill and
decode on the fly, effectively disaggregating the two phases within the same
GPU.
  Across a range of models and workloads, our system Nexus achieves up to 2.2x
higher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM. It also
outperforms SGLang with up to 2x higher throughput, 2x lower TTFT, and 1.7x
lower TBT, and achieves 1.4x higher throughput than vLLM-disaggregation using
only half the number of GPUs.

</details>


### [143] [Towards Efficient and Scalable Distributed Vector Search with RDMA](https://arxiv.org/abs/2507.06653)
*Xiangyu Zhi,Meng Chen,Xiao Yan,Baotong Lu,Hui Li,Qianxi Zhang,Qi Chen,James Cheng*

Main category: cs.DC

TL;DR: CoTra, a scalable distributed system for similarity-based vector search, achieves improved efficiency, scaling, and accuracy through algorithm-system co-designs and system optimizations.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of memory and bandwidth in single-machine vector search systems, especially when handling large, intensive datasets.

Method: Collaborative vector search algorithm is combined with system co-designs (like clustering-based data partitioning and asynchronous execution), and system optimizations (e.g., task scheduling, communication batching).

Result: On real datasets, CoTra achieves 9.8-13.4x scalability using 16 machines and outperforms the best baseline by 2.12-3.58x in query throughput.

Conclusion: CoTra effectively addresses computation-communication trade-offs, scaling vector search across machines while maintaining high performance and accuracy.

Abstract: Similarity-based vector search facilitates many important applications such
as search and recommendation but is limited by the memory capacity and
bandwidth of a single machine due to large datasets and intensive data read. In
this paper, we present CoTra, a system that scales up vector search for
distributed execution. We observe a tension between computation and
communication efficiency, which is the main challenge for good scalability,
i.e., handling the local vectors on each machine independently blows up
computation as the pruning power of vector index is not fully utilized, while
running a global index over all machines introduces rich data dependencies and
thus extensive communication. To resolve such tension, we leverage the fact
that vector search is approximate in nature and robust to asynchronous
execution. In particular, we run collaborative vector search over the machines
with algorithm-system co-designs including clustering-based data partitioning
to reduce communication, asynchronous execution to avoid communication stall,
and task push to reduce network traffic. To make collaborative search
efficient, we introduce a suite of system optimizations including task
scheduling, communication batching, and storage format. We evaluate CoTra on
real datasets and compare with four baselines. The results show that when using
16 machines, the query throughput of CoTra scales to 9.8-13.4x over a single
machine and is 2.12-3.58x of the best-performing baseline at 0.95 recall@10.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [144] [Energy-Efficient Supervised Learning with a Binary Stochastic Forward-Forward Algorithm](https://arxiv.org/abs/2507.06461)
*Risi Jaiswal,Supriyo Datta,Joseph G. Makin*

Main category: cs.LG

TL;DR: The paper presents forward-forward algorithms for binary, stochastic units to address energy consumption challenges in machine learning training, achieving similar performance to real-valued methods while saving energy by about an order of magnitude.


<details>
  <summary>Details</summary>
Motivation: Modern machine learning relies on energy-intensive neural networks, and traditional backpropagation algorithms cause challenges in custom hardware accelerators due to serialization and memory issues.

Method: The authors introduce forward-forward algorithms using binary activations and stochastic units tied with weights, optimized for hardware efficiency leveraging fast binary sampling with p-bits.

Result: Evaluation on datasets like MNIST, Fashion-MNIST, and CIFAR-10 shows competitive performance compared to real-valued methods with significant energy savings.

Conclusion: Forward-forward algorithms provide a promising alternative, enabling hardware optimizations while maintaining performance and drastically reducing energy consumption.

Abstract: Reducing energy consumption has become a pressing need for modern machine
learning, which has achieved many of its most impressive results by scaling to
larger and more energy-consumptive neural networks. Unfortunately, the main
algorithm for training such networks, backpropagation, poses significant
challenges for custom hardware accelerators, due to both its serial
dependencies and the memory footprint needed to store forward activations for
the backward pass. Alternatives to backprop, although less effective, do exist;
here the main computational bottleneck becomes matrix multiplication. In this
study, we derive forward-forward algorithms for binary, stochastic units.
Binarization of the activations transforms matrix multiplications into indexing
operations, which can be executed efficiently in hardware. Stochasticity,
combined with tied weights across units with different biases, bypasses the
information bottleneck imposed by binary units. Furthermore, although slow and
expensive in traditional hardware, binary sampling that is very fast can be
implemented cheaply with p-bits (probabilistic bits), novel devices made up of
unstable magnets. We evaluate our proposed algorithms on the MNIST,
Fashion-MNIST, and CIFAR-10 datasets, showing that its performance is close to
real-valued forward-forward, but with an estimated energy savings of about one
order of magnitude.

</details>


### [145] [Neural Network-Based Parameter Estimation for Non-Autonomous Differential Equations with Discontinuous Signals](https://arxiv.org/abs/2507.06267)
*Hyeontae Jo,Krešimir Josić,Jae Kyoung Kim*

Main category: cs.LG

TL;DR: The paper introduces HADES-NN, a method for accurately estimating parameters of non-autonomous differential equations under abrupt signal changes, using neural networks.


<details>
  <summary>Details</summary>
Motivation: The challenge of fitting non-autonomous differential equation models to data with abruptly changing external signals motivates this study.

Method: The method employs a neural network to approximate the abrupt signal with a smooth function, followed by using this approximation to estimate model parameters.

Result: HADES-NN achieves accurate parameter estimation in cases like circadian clock systems and yeast mating responses influenced by environmental signals.

Conclusion: HADES-NN significantly broadens the range of model systems that can integrate real-world data involving discontinuous signals.

Abstract: Non-autonomous differential equations are crucial for modeling systems
influenced by external signals, yet fitting these models to data becomes
particularly challenging when the signals change abruptly. To address this
problem, we propose a novel parameter estimation method utilizing functional
approximations with artificial neural networks. Our approach, termed Harmonic
Approximation of Discontinuous External Signals using Neural Networks
(HADES-NN), operates in two iterated stages. In the first stage, the algorithm
employs a neural network to approximate the discontinuous signal with a smooth
function. In the second stage, it uses this smooth approximate signal to
estimate model parameters. HADES-NN gives highly accurate and precise parameter
estimates across various applications, including circadian clock systems
regulated by external light inputs measured via wearable devices and the mating
response of yeast to external pheromone signals. HADES-NN greatly extends the
range of model systems that can be fit to real-world measurements.

</details>


### [146] [Sample-Efficient Reinforcement Learning Controller for Deep Brain Stimulation in Parkinson's Disease](https://arxiv.org/abs/2507.06326)
*Harsh Ravivarapu,Gaurav Bagwe,Xiaoyong Yuan,Chunxiu Yu,Lan Zhang*

Main category: cs.LG

TL;DR: SEA-DBS is a sample-efficient actor-critic framework for adaptive deep brain stimulation (aDBS), addressing challenges in reinforcement learning like sample complexity and exploration instability, and is designed to work on resource-constrained neuromodulatory hardware.


<details>
  <summary>Details</summary>
Motivation: Conventional DBS systems are energy-inefficient, lack adaptability to individual neural dynamics, and fail to provide personalized stimulation, necessitating exploration of adaptive, closed-loop solutions like RL-based aDBS.

Method: SEA-DBS integrates a predictive reward model to minimize dependence on real-time feedback and uses Gumbel Softmax for stable and differentiable policy updates in binary action spaces.

Result: SEA-DBS achieves faster convergence, better pathological beta-band power suppression, and resilience against post-training FP16 quantization in simulations of Parkinsonian basal ganglia activity.

Conclusion: SEA-DBS is a viable RL-based solution for real-time, adaptive neuromodulation that addresses key challenges and offers practical deployment on hardware with limited resources.

Abstract: Deep brain stimulation (DBS) is an established intervention for Parkinson's
disease (PD), but conventional open-loop systems lack adaptability, are
energy-inefficient due to continuous stimulation, and provide limited
personalization to individual neural dynamics. Adaptive DBS (aDBS) offers a
closed-loop alternative, using biomarkers such as beta-band oscillations to
dynamically modulate stimulation. While reinforcement learning (RL) holds
promise for personalized aDBS control, existing methods suffer from high sample
complexity, unstable exploration in binary action spaces, and limited
deployability on resource-constrained hardware.
  We propose SEA-DBS, a sample-efficient actor-critic framework that addresses
the core challenges of RL-based adaptive neurostimulation. SEA-DBS integrates a
predictive reward model to reduce reliance on real-time feedback and employs
Gumbel Softmax-based exploration for stable, differentiable policy updates in
binary action spaces. Together, these components improve sample efficiency,
exploration robustness, and compatibility with resource-constrained
neuromodulatory hardware. We evaluate SEA-DBS on a biologically realistic
simulation of Parkinsonian basal ganglia activity, demonstrating faster
convergence, stronger suppression of pathological beta-band power, and
resilience to post-training FP16 quantization. Our results show that SEA-DBS
offers a practical and effective RL-based aDBS framework for real-time,
resource-constrained neuromodulation.

</details>


### [147] [KPFlow: An Operator Perspective on Dynamic Collapse Under Gradient Descent Training of Recurrent Networks](https://arxiv.org/abs/2507.06381)
*James Hazelden,Laura Driscoll,Eli Shlizerman,Eric Shea-Brown*

Main category: cs.LG

TL;DR: This paper develops a decomposition framework for understanding gradient flow in recurrent neural networks, involving a Parameter Operator and a Linearized Flow Propagator.


<details>
  <summary>Details</summary>
Motivation: To provide theoretical tools for better understanding mechanisms shaping learned representations in finite, non-linear recurrent models.

Method: The authors decompose gradient flow into two components: the Parameter Operator (K) and the Linearized Flow Propagator (P), drawing parallels to Neural Tangent Kernel theory and Lyapunov stability.

Result: The decomposition explains the emergence of low-dimensional latent dynamics and enables measuring task alignment in multi-task training. Findings are both theoretically and experimentally validated, with tools released as the 'KPFlow' package.

Conclusion: This work advances the understanding of gradient descent dynamics in non-linear recurrent models and provides practical tools for robust analysis.

Abstract: Gradient Descent (GD) and its variants are the primary tool for enabling
efficient training of recurrent dynamical systems such as Recurrent Neural
Networks (RNNs), Neural ODEs and Gated Recurrent units (GRUs). The dynamics
that are formed in these models exhibit features such as neural collapse and
emergence of latent representations that may support the remarkable
generalization properties of networks. In neuroscience, qualitative features of
these representations are used to compare learning in biological and artificial
systems. Despite recent progress, there remains a need for theoretical tools to
rigorously understand the mechanisms shaping learned representations,
especially in finite, non-linear models. Here, we show that the gradient flow,
which describes how the model's dynamics evolve over GD, can be decomposed into
a product that involves two operators: a Parameter Operator, K, and a
Linearized Flow Propagator, P. K mirrors the Neural Tangent Kernel in
feed-forward neural networks, while P appears in Lyapunov stability and optimal
control theory. We demonstrate two applications of our decomposition. First, we
show how their interplay gives rise to low-dimensional latent dynamics under
GD, and, specifically, how the collapse is a result of the network structure,
over and above the nature of the underlying task. Second, for multi-task
training, we show that the operators can be used to measure how objectives
relevant to individual sub-tasks align. We experimentally and theoretically
validate these findings, providing an efficient Pytorch package, \emph{KPFlow},
implementing robust analysis tools for general recurrent architectures. Taken
together, our work moves towards building a next stage of understanding of GD
learning in non-linear recurrent models.

</details>


### [148] [SymFlux: deep symbolic regression of Hamiltonian vector fields](https://arxiv.org/abs/2507.06342)
*M. A. Evangelista-Alvarado,P. Suárez-Serrato*

Main category: cs.LG

TL;DR: The paper introduces SymFlux, a deep learning model that uses symbolic regression to identify Hamiltonian functions from vector fields, achieving accurate results.


<details>
  <summary>Details</summary>
Motivation: The motivation is to advance automated discovery in Hamiltonian mechanics by enabling the recovery of symbolic Hamiltonian functions from given vector fields.

Method: The paper employs a hybrid CNN-LSTM deep learning architecture within SymFlux to perform symbolic regression. It also introduces new datasets of Hamiltonian vector fields for training and validation.

Result: SymFlux effectively recovers symbolic mathematical expressions of Hamiltonian functions, showcasing high accuracy and utility.

Conclusion: SymFlux represents a step forward in automating the discovery of Hamiltonian functions, offering a robust model for symbolic regression in this field.

Abstract: We present SymFlux, a novel deep learning framework that performs symbolic
regression to identify Hamiltonian functions from their corresponding vector
fields on the standard symplectic plane. SymFlux models utilize hybrid CNN-LSTM
architectures to learn and output the symbolic mathematical expression of the
underlying Hamiltonian. Training and validation are conducted on newly
developed datasets of Hamiltonian vector fields, a key contribution of this
work. Our results demonstrate the model's effectiveness in accurately
recovering these symbolic expressions, advancing automated discovery in
Hamiltonian mechanics.

</details>


### [149] [DecoyDB: A Dataset for Graph Contrastive Learning in Protein-Ligand Binding Affinity Prediction](https://arxiv.org/abs/2507.06366)
*Yupu Zhang,Zelin Xu,Tingsong Xiao,Gustavo Seabra,Yanjun Li,Chenglong Li,Zhe Jiang*

Main category: cs.LG

TL;DR: The paper introduces DecoyDB, a specialized dataset for self-supervised graph contrastive learning (GCL) on protein-ligand binding affinity prediction, and demonstrates its effectiveness in boosting model performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limited availability of large-scale, high-quality labeled datasets for predicting protein-ligand binding affinity, a key challenge in drug discovery.

Method: The authors create DecoyDB, a structure-aware dataset with labeled complexes, including negative pairs and RMSD annotations, and develop a tailored GCL framework for pre-training graph neural networks.

Result: Models pre-trained with DecoyDB demonstrated better accuracy, efficiency in using labeled data, and generalizability compared to alternatives.

Conclusion: DecoyDB and the proposed GCL framework significantly improve the prediction of protein-ligand binding affinity, bridging limitations in current datasets and methodologies.

Abstract: Predicting the binding affinity of protein-ligand complexes plays a vital
role in drug discovery. Unfortunately, progress has been hindered by the lack
of large-scale and high-quality binding affinity labels. The widely used
PDBbind dataset has fewer than 20K labeled complexes. Self-supervised learning,
especially graph contrastive learning (GCL), provides a unique opportunity to
break the barrier by pre-training graph neural network models based on vast
unlabeled complexes and fine-tuning the models on much fewer labeled complexes.
However, the problem faces unique challenges, including a lack of a
comprehensive unlabeled dataset with well-defined positive/negative complex
pairs and the need to design GCL algorithms that incorporate the unique
characteristics of such data. To fill the gap, we propose DecoyDB, a
large-scale, structure-aware dataset specifically designed for self-supervised
GCL on protein-ligand complexes. DecoyDB consists of high-resolution ground
truth complexes (less than 2.5 Angstrom) and diverse decoy structures with
computationally generated binding poses that range from realistic to suboptimal
(negative pairs). Each decoy is annotated with a Root Mean Squared Deviation
(RMSD) from the native pose. We further design a customized GCL framework to
pre-train graph neural networks based on DecoyDB and fine-tune the models with
labels from PDBbind. Extensive experiments confirm that models pre-trained with
DecoyDB achieve superior accuracy, label efficiency, and generalizability.

</details>


### [150] [Deep-Learning-Based Pre-Layout Parasitic Capacitance Prediction on SRAM Designs](https://arxiv.org/abs/2507.06549)
*Shan Shen,Dingcheng Yang,Yuyang Xie,Chunyan Pei,Wenjian Yu,Bei Yu*

Main category: cs.LG

TL;DR: The paper proposes a deep-learning-based 2-stage model to predict parasitics in SRAM circuits at pre-layout stages, achieving better accuracy and simulation efficiency.


<details>
  <summary>Details</summary>
Motivation: Designing energy-efficient SRAM in System-on-Chips (SoCs) requires managing discrepancies between pre-layout and post-layout simulations caused by parasitic effects, which result in difficulties converging design parameters.

Method: The authors developed a 2-stage model combining a Graph Neural Network (GNN) classifier and Multi-Layer Perceptron (MLP) regressors. They used Focal Loss to address class imbalance and integrated subcircuit information into graphs to capture SRAM schematic hierarchy.

Result: Their approach achieved up to a 19X reduction in parasitic prediction error and boosted simulation processes by up to 598X compared to state-of-the-art models.

Conclusion: The proposed model reliably enables pre-layout parasitic-aware simulation for SRAM designs, improving design accuracy while drastically reducing simulation time.

Abstract: To achieve higher system energy efficiency, SRAM in SoCs is often customized.
The parasitic effects cause notable discrepancies between pre-layout and
post-layout circuit simulations, leading to difficulty in converging design
parameters and excessive design iterations. Is it possible to well predict the
parasitics based on the pre-layout circuit, so as to perform parasitic-aware
pre-layout simulation? In this work, we propose a deep-learning-based 2-stage
model to accurately predict these parasitics in pre-layout stages. The model
combines a Graph Neural Network (GNN) classifier and Multi-Layer Perceptron
(MLP) regressors, effectively managing class imbalance of the net parasitics in
SRAM circuits. We also employ Focal Loss to mitigate the impact of abundant
internal net samples and integrate subcircuit information into the graph to
abstract the hierarchical structure of schematics. Experiments on 4 real SRAM
designs show that our approach not only surpasses the state-of-the-art model in
parasitic prediction by a maximum of 19X reduction of error but also
significantly boosts the simulation process by up to 598X speedup.

</details>


### [151] [FedPhD: Federated Pruning with Hierarchical Learning of Diffusion Models](https://arxiv.org/abs/2507.06449)
*Qianyu Long,Qiyuan Wang,Christos Anagnostopoulos,Daning Bi*

Main category: cs.LG

TL;DR: The paper proposes FedPhD, a method for efficiently training Diffusion Models in Federated Learning environments, addressing challenges like high communication costs and data heterogeneity.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of high communication costs and data heterogeneity in training Diffusion Models within Federated Learning setups.

Method: FedPhD uses Hierarchical FL with homogeneity-aware model aggregation, selection policy, and distributed structured pruning to enhance efficiency and handle heterogeneity.

Result: Experiments show FedPhD improves model performance by at least 34% in FID scores, reduces communication costs by up to 88%, and uses only 56% of resources compared to baseline methods.

Conclusion: FedPhD effectively tackles FL challenges for training DMs, offering superior performance and efficiency over existing methods.

Abstract: Federated Learning (FL), as a distributed learning paradigm, trains models
over distributed clients' data. FL is particularly beneficial for distributed
training of Diffusion Models (DMs), which are high-quality image generators
that require diverse data. However, challenges such as high communication costs
and data heterogeneity persist in training DMs similar to training Transformers
and Convolutional Neural Networks. Limited research has addressed these issues
in FL environments. To address this gap and challenges, we introduce a novel
approach, FedPhD, designed to efficiently train DMs in FL environments. FedPhD
leverages Hierarchical FL with homogeneity-aware model aggregation and
selection policy to tackle data heterogeneity while reducing communication
costs. The distributed structured pruning of FedPhD enhances computational
efficiency and reduces model storage requirements in clients. Our experiments
across multiple datasets demonstrate that FedPhD achieves high model
performance regarding Fr\'echet Inception Distance (FID) scores while reducing
communication costs by up to $88\%$. FedPhD outperforms baseline methods
achieving at least a $34\%$ improvement in FID, while utilizing only $56\%$ of
the total computation and communication resources.

</details>


### [152] [The Riemannian Geometry associated to Gradient Flows of Linear Convolutional Networks](https://arxiv.org/abs/2507.06367)
*El Mehdi Achour,Kathlén Kohn,Holger Rauhut*

Main category: cs.LG

TL;DR: The paper investigates the gradient flow of deep linear convolutional networks and demonstrates that it can always be represented as a Riemannian gradient flow on function space, regardless of initialization, under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the geometric structure of the learning process in deep linear convolutional networks and generalize prior work on gradient flow for function space representations in neural networks.

Method: The authors analyzed the gradient flow on parameter space and established its representation as a Riemannian gradient flow in function space. The study extends the prior results of fully connected networks to convolutional networks under specific conditions.

Result: The gradient flow for learning deep linear convolutional networks is shown to be equivalent to a Riemannian gradient flow on function space without needing a balanced initialization (except for specific cases involving 1-dimensional convolutions with stride conditions).

Conclusion: Learning in deep linear convolutional networks has an inherent Riemannian geometric structure on function space that depends on initialization, providing insights into the training dynamics and optimization in these systems.

Abstract: We study geometric properties of the gradient flow for learning deep linear
convolutional networks. For linear fully connected networks, it has been shown
recently that the corresponding gradient flow on parameter space can be written
as a Riemannian gradient flow on function space (i.e., on the product of weight
matrices) if the initialization satisfies a so-called balancedness condition.
We establish that the gradient flow on parameter space for learning linear
convolutional networks can be written as a Riemannian gradient flow on function
space regardless of the initialization. This result holds for $D$-dimensional
convolutions with $D \geq 2$, and for $D =1$ it holds if all so-called strides
of the convolutions are greater than one. The corresponding Riemannian metric
depends on the initialization.

</details>


### [153] [A Single Merging Suffices: Recovering Server-based Learning Performance in Decentralized Learning](https://arxiv.org/abs/2507.06542)
*Tongtian Zhu,Tianyu Zhang,Mingze Wang,Zhanpeng Zhou,Can Wang*

Main category: cs.LG

TL;DR: The paper explores how scheduling communication in decentralized learning can improve performance, showing that concentrating communication in later stages and performing a single global merging can match centralized training's results.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of limited peer-to-peer communication which hampers decentralized learning performance and to explore optimal communication strategies for improved training outcomes.

Method: The researchers empirically investigated communication scheduling strategies, focusing on late-stage synchronization and global merging, and developed a theoretical framework to explain its effects on convergence and generalization.

Result: They found that concentrating communication budgets in later training stages, as well as performing a single fully connected communication (global merging), preserves mergeability and matches server-based training. Theoretical analysis showed decentralized SGD can converge faster than centralized mini-batch SGD.

Conclusion: The study overturns beliefs about decentralized learning's inefficiency under data heterogeneity and low communication, offering insights into leveraging model merging and loss landscape properties for improved scalability and generalization.

Abstract: Decentralized learning provides a scalable alternative to traditional
parameter-server-based training, yet its performance is often hindered by
limited peer-to-peer communication. In this paper, we study how communication
should be scheduled over time, including determining when and how frequently
devices synchronize. Our empirical results show that concentrating
communication budgets in the later stages of decentralized training markedly
improves global generalization. Surprisingly, we uncover that fully connected
communication at the final step, implemented by a single global merging, is
sufficient to match the performance of server-based training. We further show
that low communication in decentralized learning preserves the
\textit{mergeability} of local models throughout training. Our theoretical
contributions, which explains these phenomena, are first to establish that the
globally merged model of decentralized SGD can converge faster than centralized
mini-batch SGD. Technically, we novelly reinterpret part of the discrepancy
among local models, which were previously considered as detrimental noise, as
constructive components that accelerate convergence. This work challenges the
common belief that decentralized learning generalizes poorly under data
heterogeneity and limited communication, while offering new insights into model
merging and neural network loss landscapes.

</details>


### [154] [Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation](https://arxiv.org/abs/2507.06380)
*Habibur Rahaman,Atri Chatterjee,Swarup Bhunia*

Main category: cs.LG

TL;DR: WINGs is a framework for reducing memory use in neural networks by dynamically generating weights and using PCA and SVR for compression, with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Modern neural networks require substantial memory, which poses challenges for resource-constrained environments. This work aims to address memory inefficiency in storing synaptic weights without losing model accuracy.

Method: WINGs utilizes principal component analysis (PCA) for dimensionality reduction and lightweight support vector regression (SVR) for weight prediction and compression, along with sensitivity-aware techniques for CNNs.

Result: The framework achieves 53x compression for FC layers, 28x for AlexNet on MNIST, and 18x for AlexNet on CIFAR-10, with only a 1-2% accuracy loss.

Conclusion: WINGs significantly reduces memory requirements while maintaining performance, enhancing throughput, energy efficiency, and security—especially for edge applications.

Abstract: Complex neural networks require substantial memory to store a large number of
synaptic weights. This work introduces WINGs (Automatic Weight Generator for
Secure and Storage-Efficient Deep Learning Models), a novel framework that
dynamically generates layer weights in a fully connected neural network (FC)
and compresses the weights in convolutional neural networks (CNNs) during
inference, significantly reducing memory requirements without sacrificing
accuracy. WINGs framework uses principal component analysis (PCA) for
dimensionality reduction and lightweight support vector regression (SVR) models
to predict layer weights in the FC networks, removing the need for storing
full-weight matrices and achieving substantial memory savings. It also
preferentially compresses the weights in low-sensitivity layers of CNNs using
PCA and SVR with sensitivity analysis. The sensitivity-aware design also offers
an added level of security, as any bit-flip attack with weights in compressed
layers has an amplified and readily detectable effect on accuracy. WINGs
achieves 53x compression for the FC layers and 28x for AlexNet with MNIST
dataset, and 18x for Alexnet with CIFAR-10 dataset with 1-2% accuracy loss.
This significant reduction in memory results in higher throughput and lower
energy for DNN inference, making it attractive for resource-constrained edge
applications.

</details>


### [155] [SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference](https://arxiv.org/abs/2507.06567)
*Qian Chen,Xianhao Chen,Kaibin Huang*

Main category: cs.LG

TL;DR: This paper presents a method to optimize the caching of experts in Mixture-of-Experts (MoE) models across edge servers, minimizing inference latency under storage constraints.


<details>
  <summary>Details</summary>
Motivation: Current Mixture-of-Experts (MoE) models require significant storage for the numerous expert networks they use, making deployment on edge devices challenging.

Method: The study introduces caching optimization algorithms: a greedy-based method with a $(1 - 1/e)$-approximation for $K=1$, and a successive greedy decomposition coupled with dynamic programming for $K\geq1$. An accelerated version using max-convolution is also developed.

Result: Simulation results demonstrate that the proposed methods significantly reduce inference latency for several MoE models when compared to existing baseline approaches.

Conclusion: Optimized expert caching techniques can effectively reduce latency and make MoE models more practical for distributed edge environments.

Abstract: Mixture-of-Experts (MoE) models improve the scalability of large language
models (LLMs) by activating only a small subset of relevant experts per input.
However, the sheer number of expert networks in an MoE model introduces a
significant storage burden for an edge device. To address this challenge, we
consider a scenario where experts are dispersed within an edge network for
distributed inference. Based on the popular Top-$K$ expert selection strategy,
we formulate a latency minimization problem by optimizing expert caching on
edge servers under storage constraints. When $K=1$, the problem reduces to a
monotone submodular maximization problem with knapsack constraints, for which
we design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.
For the general case where $K\geq1$, expert co-activation within the same MoE
layer introduces non-submodularity, causing greedy methods to be ineffective.
To tackle this issue, we propose a successive greedy decomposition method to
decompose the original problem into a series of subproblems, with each being
solved by a dynamic programming approach. Furthermore, we design an accelerated
algorithm based on the max-convolution technique to obtain the approximate
solution with a provable guarantee in polynomial time. Simulation results on
various MoE models demonstrate that our method significantly reduces inference
latency compared to existing baselines.

</details>


### [156] [DICE: Data Influence Cascade in Decentralized Learning](https://arxiv.org/abs/2507.06931)
*Tongtian Zhu,Wenhao Li,Can Wang,Fengxiang He*

Main category: cs.LG

TL;DR: This paper proposes DICE, a framework to estimate data influence cascades in decentralized networks, addressing fair contribution attribution challenges in decentralized learning.


<details>
  <summary>Details</summary>
Motivation: The need to incentivize participation in decentralized learning by fairly attributing node contributions amidst challenges arising from localized connections and influence cascades.

Method: DICE estimates influence cascades through tractable approximations over network components (data, topology, loss landscape), enabling fair contribution attribution.

Result: DICE enables a better understanding of influence cascades, supports use cases like collaborator selection, and offers tools for identifying malicious nodes.

Conclusion: DICE provides a novel framework for addressing influence cascades in decentralized learning, offering theoretical and practical advancements for collaboration and security.

Abstract: Decentralized learning offers a promising approach to crowdsource data
consumptions and computational workloads across geographically distributed
compute interconnected through peer-to-peer networks, accommodating the
exponentially increasing demands. However, proper incentives are still in
absence, considerably discouraging participation. Our vision is that a fair
incentive mechanism relies on fair attribution of contributions to
participating nodes, which faces non-trivial challenges arising from the
localized connections making influence ``cascade'' in a decentralized network.
To overcome this, we design the first method to estimate \textbf{D}ata
\textbf{I}nfluence \textbf{C}ascad\textbf{E} (DICE) in a decentralized
environment. Theoretically, the framework derives tractable approximations of
influence cascade over arbitrary neighbor hops, suggesting the influence
cascade is determined by an interplay of data, communication topology, and the
curvature of loss landscape. DICE also lays the foundations for applications
including selecting suitable collaborators and identifying malicious behaviors.
Project page is available at https://raiden-zhu.github.io/blog/2025/DICE/.

</details>


### [157] [Detection of Intelligent Tampering in Wireless Electrocardiogram Signals Using Hybrid Machine Learning](https://arxiv.org/abs/2507.06402)
*Siddhant Deshpande,Yalemzerf Getnet,Waltenegus Dargie*

Main category: cs.LG

TL;DR: The paper explores the use of CNN, ResNet, and hybrid Transformer-CNN models for tamper detection in ECG signals, achieving high accuracy. Additionally, it evaluates Siamese networks for identity verification based on ECG data.


<details>
  <summary>Details</summary>
Motivation: To address the growing need for protecting ECG signal integrity against tampering in wireless health monitoring and authentication systems.

Method: The researchers emulate six tampering strategies on ECG signals converted to time-frequency domain representation using continuous wavelet transform (CWT). They train and test CNN, ResNet, and hybrid models on data collected from 54 subjects during daily activities outside clinical settings.

Result: The proposed models achieved exceptional accuracy (above 99.5%) for fragmented manipulation scenarios and consistently reliable performance (98% accuracy) for subtle manipulations. The Siamese network demonstrated an average accuracy of 98.3%, while the hybrid CNN-Transformer Siamese model achieved 100% accuracy for identity verification.

Conclusion: Hybrid deep learning models, particularly CNN-Transformer-based architectures, are highly effective for tamper detection and identity verification in ECG-based systems, providing robust performance against signal manipulations and ensuring reliable authentication.

Abstract: With the proliferation of wireless electrocardiogram (ECG) systems for health
monitoring and authentication, protecting signal integrity against tampering is
becoming increasingly important. This paper analyzes the performance of CNN,
ResNet, and hybrid Transformer-CNN models for tamper detection. It also
evaluates the performance of a Siamese network for ECG based identity
verification. Six tampering strategies, including structured segment
substitutions and random insertions, are emulated to mimic real world attacks.
The one-dimensional ECG signals are transformed into a two dimensional
representation in the time frequency domain using the continuous wavelet
transform (CWT). The models are trained and evaluated using ECG data from 54
subjects recorded in four sessions 2019 to 2025 outside of clinical settings
while the subjects performed seven different daily activities. Experimental
results show that in highly fragmented manipulation scenarios, CNN,
FeatCNN-TranCNN, FeatCNN-Tran and ResNet models achieved an accuracy exceeding
99.5 percent . Similarly, for subtle manipulations (for example, 50 percent
from A and 50 percent from B and, 75 percent from A and 25 percent from B
substitutions) our FeatCNN-TranCNN model demonstrated consistently reliable
performance, achieving an average accuracy of 98 percent . For identity
verification, the pure Transformer-Siamese network achieved an average accuracy
of 98.30 percent . In contrast, the hybrid CNN-Transformer Siamese model
delivered perfect verification performance with 100 percent accuracy.

</details>


### [158] [Bridging Data Gaps of Rare Conditions in ICU: A Multi-Disease Adaptation Approach for Clinical Prediction](https://arxiv.org/abs/2507.06432)
*Mingcheng Zhu,Yu Liu,Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: The paper presents KnowRare, a deep learning framework designed for predicting clinical outcomes in rare ICU conditions by addressing data scarcity and condition heterogeneity and demonstrating superior performance over existing models.


<details>
  <summary>Details</summary>
Motivation: Rare ICU conditions are underserved in AI-driven critical care due to challenges like data scarcity and heterogeneity. Addressing this gap is crucial for effective clinical decision-making.

Method: KnowRare employs a domain adaptation-based deep learning framework, combining self-supervised pre-training to learn condition-agnostic representations and a condition knowledge graph to adapt knowledge from similar conditions.

Result: KnowRare outperformed state-of-the-art models and existing ICU scoring systems (APACHE IV and IV-a) across five clinical prediction tasks using two ICU datasets. It demonstrated generalization and adaptability in diverse scenarios.

Conclusion: KnowRare is a robust and practical framework for improving clinical decisions and care for rare ICU conditions by overcoming limitations related to data scarcity and heterogeneity.

Abstract: Artificial Intelligence has revolutionised critical care for common
conditions. Yet, rare conditions in the intensive care unit (ICU), including
recognised rare diseases and low-prevalence conditions in the ICU, remain
underserved due to data scarcity and intra-condition heterogeneity. To bridge
such gaps, we developed KnowRare, a domain adaptation-based deep learning
framework for predicting clinical outcomes for rare conditions in the ICU.
KnowRare mitigates data scarcity by initially learning condition-agnostic
representations from diverse electronic health records through self-supervised
pre-training. It addresses intra-condition heterogeneity by selectively
adapting knowledge from clinically similar conditions with a developed
condition knowledge graph. Evaluated on two ICU datasets across five clinical
prediction tasks (90-day mortality, 30-day readmission, ICU mortality,
remaining length of stay, and phenotyping), KnowRare consistently outperformed
existing state-of-the-art models. Additionally, KnowRare demonstrated superior
predictive performance compared to established ICU scoring systems, including
APACHE IV and IV-a. Case studies further demonstrated KnowRare's flexibility in
adapting its parameters to accommodate dataset-specific and task-specific
characteristics, its generalisation to common conditions under limited data
scenarios, and its rationality in selecting source conditions. These findings
highlight KnowRare's potential as a robust and practical solution for
supporting clinical decision-making and improving care for rare conditions in
the ICU.

</details>


### [159] [eegFloss: A Python package for refining sleep EEG recordings using machine learning models](https://arxiv.org/abs/2507.06433)
*Niloy Sikder,Paul Zerr,Mahdad Jafarzadeh Esfahani,Martin Dresler,Matthias Krauledat*

Main category: cs.LG

TL;DR: The paper introduces eegFloss, an open-source Python package that uses the eegUsability model to automatically detect artifacts in sleep EEG data, improving sleep study analysis.


<details>
  <summary>Details</summary>
Motivation: Sleep studies rely heavily on EEG data, but errors from artifacts can compromise results. Addressing artifact detection is critical for enhancing the reliability and accuracy of automatic sleep staging.

Method: The authors developed eegUsability, a machine learning model trained on artifact-labeled sleep EEG data from 15 participants using the Zmax headband. The model identifies artifact segments and integrates with the eegFloss tool.

Result: eegUsability achieved strong classification metrics (F1-score ~0.85, Cohens kappa ~0.78) with a recall rate of ~94% in detecting usable EEG data. eegFloss also provides additional sleep analysis features.

Conclusion: eegFloss improves sleep study analysis by filtering out artifacts, supporting automatic time-in-bed detection, and enhancing the accuracy of sleep statistics. It addresses key challenges in EEG artifact management for large-scale sleep research.

Abstract: Electroencephalography (EEG) allows monitoring of brain activity, providing
insights into the functional dynamics of various brain regions and their roles
in cognitive processes. EEG is a cornerstone in sleep research, serving as the
primary modality of polysomnography, the gold standard in the field. However,
EEG signals are prone to artifacts caused by both internal (device-specific)
factors and external (environmental) interferences. As sleep studies are
becoming larger, most rely on automatic sleep staging, a process highly
susceptible to artifacts, leading to erroneous sleep scores. This paper
addresses this challenge by introducing eegFloss, an open-source Python package
to utilize eegUsability, a novel machine learning (ML) model designed to detect
segments with artifacts in sleep EEG recordings. eegUsability has been trained
and evaluated on manually artifact-labeled EEG data collected from 15
participants over 127 nights using the Zmax headband. It demonstrates solid
overall classification performance (F1-score is approximately 0.85, Cohens
kappa is 0.78), achieving a high recall rate of approximately 94% in
identifying channel-wise usable EEG data, and extends beyond Zmax.
Additionally, eegFloss offers features such as automatic time-in-bed detection
using another ML model named eegMobility, filtering out certain artifacts, and
generating hypnograms and sleep statistics. By addressing a fundamental
challenge faced by most sleep studies, eegFloss can enhance the precision and
rigor of their analysis as well as the accuracy and reliability of their
outcomes.

</details>


### [160] [Instance-Wise Monotonic Calibration by Constrained Transformation](https://arxiv.org/abs/2507.06516)
*Yunrui Zhang,Gustavo Batista,Salil S. Kanhere*

Main category: cs.LG

TL;DR: This paper introduces novel post-hoc calibration methods for deep neural network probabilities, focusing on monotonicity, expressiveness, and robustness.


<details>
  <summary>Details</summary>
Motivation: Miscalibrated probabilities in deep neural networks often lead to overconfident predictions, and most existing calibration methods fail to ensure monotonicity.

Method: The method involves a constrained calibration map parameterized linearly with respect to the number of classes, formulated as a constrained optimization problem.

Result: The proposed methods demonstrate state-of-the-art performance across various datasets and neural network models, surpassing existing calibration techniques.

Conclusion: The approach ensures accurate and robust probability calibration with interpretability while being computationally efficient, addressing the shortcomings of previous methods.

Abstract: Deep neural networks often produce miscalibrated probability estimates,
leading to overconfident predictions. A common approach for calibration is
fitting a post-hoc calibration map on unseen validation data that transforms
predicted probabilities. A key desirable property of the calibration map is
instance-wise monotonicity (i.e., preserving the ranking of probability
outputs). However, most existing post-hoc calibration methods do not guarantee
monotonicity. Previous monotonic approaches either use an under-parameterized
calibration map with limited expressive ability or rely on black-box neural
networks, which lack interpretability and robustness. In this paper, we propose
a family of novel monotonic post-hoc calibration methods, which employs a
constrained calibration map parameterized linearly with respect to the number
of classes. Our proposed approach ensures expressiveness, robustness, and
interpretability while preserving the relative ordering of the probability
output by formulating the proposed calibration map as a constrained
optimization problem. Our proposed methods achieve state-of-the-art performance
across datasets with different deep neural network models, outperforming
existing calibration methods while being data and computation-efficient. Our
code is available at
https://github.com/YunruiZhang/Calibration-by-Constrained-Transformation

</details>


### [161] [Can Interpretation Predict Behavior on Unseen Data?](https://arxiv.org/abs/2507.06445)
*Victoria R. Li,Jenny Kaufmann,Martin Wattenberg,David Alvarez-Melis,Naomi Saphra*

Main category: cs.LG

TL;DR: The paper studies the use of interpretability in predicting unseen out-of-distribution (OOD) model behavior using attention patterns in Transformer models.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the gap in interpretability research where predicting model responses to unseen input data is underexplored.

Method: Hundreds of Transformer models were trained on a synthetic classification task, and correlational analysis was used to study the link between attention patterns and OOD behavior.

Result: Observational tools from interpretability successfully predict OOD model performance, especially hierarchical generalization based on in-distribution attention patterns.

Conclusion: The study provides a novel proof-of-concept suggesting interpretability can be a viable approach for predicting OOD behavior, encouraging further research in this area.

Abstract: Interpretability research often aims to predict how a model will respond to
targeted interventions on specific mechanisms. However, it rarely predicts how
a model will respond to unseen input data. This paper explores the promises and
challenges of interpretability as a tool for predicting out-of-distribution
(OOD) model behavior. Specifically, we investigate the correspondence between
attention patterns and OOD generalization in hundreds of Transformer models
independently trained on a synthetic classification task. These models exhibit
several distinct systematic generalization rules OOD, forming a diverse
population for correlational analysis. In this setting, we find that simple
observational tools from interpretability can predict OOD performance. In
particular, when in-distribution attention exhibits hierarchical patterns, the
model is likely to generalize hierarchically on OOD data -- even when the
rule's implementation does not rely on these hierarchical patterns, according
to ablation tests. Our findings offer a proof-of-concept to motivate further
interpretability work on predicting unseen model behavior.

</details>


### [162] [AdaDPIGU: Differentially Private SGD with Adaptive Clipping and Importance-Based Gradient Updates for Deep Neural Networks](https://arxiv.org/abs/2507.06525)
*Huiqi Zhang,Fang Xie*

Main category: cs.LG

TL;DR: This paper proposes AdaDPIGU, a differentially private SGD framework that uses importance-based gradient updates for deep learning, achieving high accuracy with improved efficiency under privacy constraints.


<details>
  <summary>Details</summary>
Motivation: Existing differentially private SGD methods struggle in high-dimensional settings because noise increases with dimensionality, leading to performance degradation.

Method: AdaDPIGU applies differentially private Gaussian mechanisms to estimate parameter importance, prunes low-importance coordinates, and uses coordinate-wise adaptive clipping for sparse and noise-efficient gradient updates. It ensures $(\varepsilon, \delta)$-differential privacy and convergence.

Result: Experiments demonstrate AdaDPIGU's efficacy, with MNIST test accuracy of 99.12% at $\epsilon = 8$ (near non-private performance) and CIFAR-10 accuracy of 73.21% at $\epsilon = 4$ (exceeding non-private baseline of 71.12%).

Conclusion: AdaDPIGU achieves significant accuracy improvements under privacy constraints, showcasing the potential of adaptive sparsification to enhance both privacy and utility in differentially private deep learning.

Abstract: Differential privacy has been proven effective for stochastic gradient
descent; however, existing methods often suffer from performance degradation in
high-dimensional settings, as the scale of injected noise increases with
dimensionality. To tackle this challenge, we propose AdaDPIGU--a new
differentially private SGD framework with importance-based gradient updates
tailored for deep neural networks. In the pretraining stage, we apply a
differentially private Gaussian mechanism to estimate the importance of each
parameter while preserving privacy. During the gradient update phase, we prune
low-importance coordinates and introduce a coordinate-wise adaptive clipping
mechanism, enabling sparse and noise-efficient gradient updates. Theoretically,
we prove that AdaDPIGU satisfies $(\varepsilon, \delta)$-differential privacy
and retains convergence guarantees. Extensive experiments on standard
benchmarks validate the effectiveness of AdaDPIGU. All results are reported
under a fixed retention ratio of 60%. On MNIST, our method achieves a test
accuracy of 99.12% under a privacy budget of $\epsilon = 8$, nearly matching
the non-private model. Remarkably, on CIFAR-10, it attains 73.21% accuracy at
$\epsilon = 4$, outperforming the non-private baseline of 71.12%, demonstrating
that adaptive sparsification can enhance both privacy and utility.

</details>


### [163] [Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models](https://arxiv.org/abs/2507.06458)
*Arjun Banerjee,David Martinez,Camille Dang,Ethan Tam*

Main category: cs.LG

TL;DR: The paper introduces a framework to label neurons in Protein Language Models (PLMs) with biological descriptions and proposes a new method for neuron-guided protein generation.


<details>
  <summary>Details</summary>
Motivation: Understanding the internal neuron representations in Protein Language Models (PLMs) is crucial but remains poorly understood. This motivates the need for scalable and biologically meaningful neuron-labeling methods.

Method: The researchers developed an automated framework that assigns natural language biological descriptions to PLM neurons. They also introduced a neuron activation-guided steering approach for generating proteins with specific biochemical and structural properties.

Result: The framework effectively labeled hundreds of thousands of neurons, revealing selective sensitivity to biochemical/structural traits. The neuron-guided method enabled generating proteins with target properties, while scaling analysis showed structured neuron distributions and insights into PLM scaling laws.

Conclusion: Automated neuron labeling provides insights into the biological functions of PLMs, while the neuron steering technique demonstrates practical protein design applications. This advances our understanding of PLM functionality and scaling.

Abstract: Protein language models (PLMs) encode rich biological information, yet their
internal neuron representations are poorly understood. We introduce the first
automated framework for labeling every neuron in a PLM with biologically
grounded natural language descriptions. Unlike prior approaches relying on
sparse autoencoders or manual annotation, our method scales to hundreds of
thousands of neurons, revealing individual neurons are selectively sensitive to
diverse biochemical and structural properties. We then develop a novel neuron
activation-guided steering method to generate proteins with desired traits,
enabling convergence to target biochemical properties like molecular weight and
instability index as well as secondary and tertiary structural motifs,
including alpha helices and canonical Zinc Fingers. We finally show that
analysis of labeled neurons in different model sizes reveals PLM scaling laws
and a structured neuron space distribution.

</details>


### [164] [Steps Adaptive Decay DPSGD: Enhancing Performance on Imbalanced Datasets with Differential Privacy with HAM10000](https://arxiv.org/abs/2507.06619)
*Xiaobo Huang,Fang Xie*

Main category: cs.LG

TL;DR: This paper addresses data leakage in medical image classification with imbalanced datasets by proposing SAD-DPSGD, a novel mechanism for privacy-preserving training that improves performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Data leakage in medical image classification, especially on small, imbalanced datasets, degrades model performance as privacy-preserving methods like noise addition fail to effectively handle minority classes.

Method: SAD-DPSGD introduces a linear decaying mechanism to adjust noise and clipping thresholds in differential privacy training, prioritizing minority classes early in the training process to avoid suboptimal solutions.

Result: Experiments demonstrate that SAD-DPSGD improves accuracy by 2.15% over Auto-DPSGD on the HAM10000 dataset under specific privacy parameters.

Conclusion: The proposed SAD-DPSGD method successfully mitigates the challenges of data leakage and imbalanced dataset issues in medical image classification, achieving more robust and accurate models.

Abstract: When applying machine learning to medical image classification, data leakage
is a critical issue. Previous methods, such as adding noise to gradients for
differential privacy, work well on large datasets like MNIST and CIFAR-100, but
fail on small, imbalanced medical datasets like HAM10000. This is because the
imbalanced distribution causes gradients from minority classes to be clipped
and lose crucial information, while majority classes dominate. This leads the
model to fall into suboptimal solutions early. To address this, we propose
SAD-DPSGD, which uses a linear decaying mechanism for noise and clipping
thresholds. By allocating more privacy budget and using higher clipping
thresholds in the initial training phases, the model avoids suboptimal
solutions and enhances performance. Experiments show that SAD-DPSGD outperforms
Auto-DPSGD on HAM10000, improving accuracy by 2.15% under $\epsilon = 3.0$ ,
$\delta = 10^{-3}$.

</details>


### [165] [SoftSignSGD(S3): An Enhanced Optimizer for Practical DNN Training and Loss Spikes Minimization Beyond Adam](https://arxiv.org/abs/2507.06464)
*Hanyang Peng,Shuang Qin,Yue Yu,Fangqing Jiang,Hui Wang,Wen Gao*

Main category: cs.LG

TL;DR: The paper introduces SignSoftSGD (S3), a new optimization method that improves upon Adam optimizer's robustness and limitations, achieving faster convergence and better training stability.


<details>
  <summary>Details</summary>
Motivation: Understanding and addressing the strengths and limitations of Adam optimizer which is widely used for training deep neural networks.

Method: Propose SignSoftSGD (S3) with three innovations: a generalized momentum update, bounded updates to simplify hyperparameter tuning, and an equivalent Nesterov's accelerated gradient module for faster convergence.

Result: S3 achieves optimal convergence rate, avoids loss spikes even with higher learning rates, and demonstrates superior performance and efficiency compared to AdamW across multiple vision and language tasks.

Conclusion: SignSoftSGD presents a robust and efficient alternative to traditional optimizers, combining faster convergence with enhanced stability and achieving comparable or better final task performance within fewer training steps.

Abstract: Adam has proven remarkable successful in training deep neural networks, but
the mechanisms underlying its empirical successes and limitations remain
underexplored. In this study, we demonstrate that the effectiveness of Adam
stems largely from its similarity to SignSGD in robustly handling large
gradient fluctuations, yet it is also vulnerable to destabilizing loss spikes
due to its uncontrolled update scaling. To enhance the advantage of Adam and
mitigate its limitation, we propose SignSoftSGD (S3), a novel optimizer with
three key innovations. \emph{First}, S3 generalizes the sign-like update by
employing a flexible $p$-th order momentum ($p \geq 1$) in the denominator,
departing from the conventional second-order momentum (variance)
preconditioning. This design enables enhanced performance while achieving
stable training even with aggressive learning rates. \emph{Second}, S3
minimizes the occurrences of loss spikes through unified exponential moving
average coefficients for numerator and denominator momenta, which inherently
bound updates to $[-1, 1]$ and simplify hyperparameter tuning. \emph{Third}, S3
incorporates an equivalent Nesterov's accelerated gradient(NAG) module,
accelerating convergence without memory overhead. Theoretically, we prove that
S3 achieves the optimal convergence rate of
$O\left(\frac{1}{T^{\sfrac{1}{4}}}\right)$ for general nonconvex stochastic
optimization under weak assumptions. Extensive experiments across a range of
vision and language tasks show that \textsf{\small S3} not only converges more
rapidly and improves performance but also rarely experiences loss spikes, even
with a \textbf{$\bm{10 \times}$} larger learning rate. In fact, S3 delivers
performance comparable to or better than AdamW with \textbf{$2 \times$} the
training steps, establishing its efficacy in both efficiency and final task
performance.

</details>


### [166] [Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models](https://arxiv.org/abs/2507.06466)
*Aaron Dharna,Cong Lu,Jeff Clune*

Main category: cs.LG

TL;DR: The paper introduces Foundation-Model Self-Play (FMSP), leveraging foundation models to improve self-play. The framework overcomes limitations of traditional self-play by enhancing solution diversity and quality through methods like vFMSP, NSSP, and QDSP.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional self-play algorithms, which often result in locally optimal or non-diverse solutions, by exploring the potential of foundation models to enable more creative and high-quality strategy discovery.

Method: The paper presents three approaches: 1) Vanilla Foundation-Model Self-Play (vFMSP) for competitive refinement of policies; 2) Novelty-Search Self-Play (NSSP) for building diverse strategies; 3) Quality-Diversity Self-Play (QDSP) for combining high-quality and diverse policy exploration.

Result: FMSPs were evaluated in the Car Tag and Gandalf simulations, where they outperformed human-designed strategies in strategy quality and successfully identified and patched vulnerabilities in LLM defenses.

Conclusion: FMSPs demonstrate significant potential in improving self-play using foundation models, enabling exploration beyond local optima and creating diverse, high-quality strategies for various applications.

Abstract: Multi-agent interactions have long fueled innovation, from natural
predator-prey dynamics to the space race. Self-play (SP) algorithms try to
harness these dynamics by pitting agents against ever-improving opponents,
thereby creating an implicit curriculum toward learning high-quality solutions.
However, SP often fails to produce diverse solutions and can get stuck in
locally optimal behaviors. We introduce Foundation-Model Self-Play (FMSP), a
new direction that leverages the code-generation capabilities and vast
knowledge of foundation models (FMs) to overcome these challenges by leaping
across local optima in policy space. We propose a family of approaches: (1)
\textbf{Vanilla Foundation-Model Self-Play (vFMSP)} continually refines agent
policies via competitive self-play; (2) \textbf{Novelty-Search Self-Play
(NSSP)} builds a diverse population of strategies, ignoring performance; and
(3) the most promising variant, \textbf{Quality-Diveristy Self-Play (QDSP)},
creates a diverse set of high-quality policies by combining the diversity of
NSSP and refinement of vFMSP. We evaluate FMSPs in Car Tag, a
continuous-control pursuer-evader setting, and in Gandalf, a simple AI safety
simulation in which an attacker tries to jailbreak an LLM's defenses. In Car
Tag, FMSPs explore a wide variety of reinforcement learning, tree search, and
heuristic-based methods, to name just a few. In terms of discovered policy
quality, \ouralgo and vFMSP surpass strong human-designed strategies. In
Gandalf, FMSPs can successfully automatically red-team an LLM, breaking through
and jailbreaking six different, progressively stronger levels of defense.
Furthermore, FMSPs can automatically proceed to patch the discovered
vulnerabilities. Overall, FMSPs represent a promising new research frontier of
improving self-play with foundation models, opening fresh paths toward more
creative and open-ended strategy discovery

</details>


### [167] [Mathematical artificial data for operator learning](https://arxiv.org/abs/2507.06752)
*Heng Wu,Benzhuo Lu*

Main category: cs.LG

TL;DR: The paper introduces the Mathematical Artificial Data (MAD) framework, which integrates physical laws and data-driven learning to solve differential equations (DEs) without costly training data.


<details>
  <summary>Details</summary>
Motivation: Current methods for solving DEs are limited by a dependence on labeled datasets and trade-offs between efficiency and accuracy.

Method: The MAD framework generates synthetic data using the intrinsic mathematical structure of DEs, removing reliance on experimental or simulated data.

Result: MAD is shown to deliver generalizability, superior efficiency, and accuracy in solving 2D parametric DE problems.

Conclusion: MAD offers a promising universal approach for physics-informed machine learning in scientific computing, effectively addressing challenges in multi-parameter systems.

Abstract: Machine learning has emerged as a transformative tool for solving
differential equations (DEs), yet prevailing methodologies remain constrained
by dual limitations: data-driven methods demand costly labeled datasets while
model-driven techniques face efficiency-accuracy trade-offs. We present the
Mathematical Artificial Data (MAD) framework, a new paradigm that integrates
physical laws with data-driven learning to facilitate large-scale operator
discovery. By exploiting DEs' intrinsic mathematical structure to generate
physics-embedded analytical solutions and associated synthetic data, MAD
fundamentally eliminates dependence on experimental or simulated training data.
This enables computationally efficient operator learning across multi-parameter
systems while maintaining mathematical rigor. Through numerical demonstrations
spanning 2D parametric problems where both the boundary values and source term
are functions, we showcase MAD's generalizability and superior
efficiency/accuracy across various DE scenarios. This
physics-embedded-data-driven framework and its capacity to handle complex
parameter spaces gives it the potential to become a universal paradigm for
physics-informed machine intelligence in scientific computing.

</details>


### [168] [Robust and Safe Traffic Sign Recognition using N-version with Weighted Voting](https://arxiv.org/abs/2507.06907)
*Linyun Gao,Qiang Wen,Fumio Machida*

Main category: cs.LG

TL;DR: This paper addresses the vulnerability of traffic sign recognition systems in autonomous vehicles to adversarial attacks. It introduces an N-version machine learning framework with a weighted soft voting mechanism to enhance safety and robustness.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the critical challenge of ensuring safety in autonomous driving, specifically addressing adversarial attacks on traffic sign recognition systems, which could jeopardize driving safety.

Method: The authors propose an N-version machine learning framework that uses Failure Mode and Effects Analysis (FMEA) to dynamically assign safety-aware weights to ensemble outputs. Robustness was tested against adversarial attacks such as FGSM and PGD.

Result: Experimental results reveal that the NVML framework significantly boosts the robustness and safety of traffic sign recognition systems under adversarial conditions.

Conclusion: The study concludes that the NVML framework improves the safety and reliability of traffic sign recognition, serving as a promising approach to mitigate adversarial risks in autonomous driving.

Abstract: Autonomous driving is rapidly advancing as a key application of machine
learning, yet ensuring the safety of these systems remains a critical
challenge. Traffic sign recognition, an essential component of autonomous
vehicles, is particularly vulnerable to adversarial attacks that can compromise
driving safety. In this paper, we propose an N-version machine learning (NVML)
framework that integrates a safety-aware weighted soft voting mechanism. Our
approach utilizes Failure Mode and Effects Analysis (FMEA) to assess potential
safety risks and assign dynamic, safety-aware weights to the ensemble outputs.
We evaluate the robustness of three-version NVML systems employing various
voting mechanisms against adversarial samples generated using the Fast Gradient
Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks. Experimental
results demonstrate that our NVML approach significantly enhances the
robustness and safety of traffic sign recognition systems under adversarial
conditions.

</details>


### [169] [Mitigating Message Imbalance in Fraud Detection with Dual-View Graph Representation Learning](https://arxiv.org/abs/2507.06469)
*Yudan Song,Yuecen Wei,Yuhang Lu,Qingyun Sun,Minglai Shao,Li-e Wang,Chunming Hu,Xianxian Li,Xingcheng Fu*

Main category: cs.LG

TL;DR: The paper introduces a method, MimbFD, to enhance fraud detection using graph representation learning by addressing imbalanced message transmission caused by fraudsters' topological obfuscation and class imbalance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to deal with the challenges of imbalanced transmission of global topological information and the risk of losing node-specific information in graph neural networks (GNNs) used for fraud detection, stemming from fraudsters' deceptive behaviors.

Method: The paper proposes a dual-view graph representation learning method called MimbFD, comprising a topological message reachability module for improved node representation and a local confounding debiasing module to balance class influence.

Result: The proposed method was experimentally validated on three public fraud detection datasets, demonstrating superior performance in identifying fraudulent behavior.

Conclusion: MimbFD effectively addresses the challenges of topological obfuscation and class imbalance in fraud detection tasks, offering enhanced performance and reliable node representation.

Abstract: Graph representation learning has become a mainstream method for fraud
detection due to its strong expressive power, which focuses on enhancing node
representations through improved neighborhood knowledge capture. However, the
focus on local interactions leads to imbalanced transmission of global
topological information and increased risk of node-specific information being
overwhelmed during aggregation due to the imbalance between fraud and benign
nodes. In this paper, we first summarize the impact of topology and class
imbalance on downstream tasks in GNN-based fraud detection, as the problem of
imbalanced supervisory messages is caused by fraudsters' topological behavior
obfuscation and identity feature concealment. Based on statistical validation,
we propose a novel dual-view graph representation learning method to mitigate
Message imbalance in Fraud Detection(MimbFD). Specifically, we design a
topological message reachability module for high-quality node representation
learning to penetrate fraudsters' camouflage and alleviate insufficient
propagation. Then, we introduce a local confounding debiasing module to adjust
node representations, enhancing the stable association between node
representations and labels to balance the influence of different classes.
Finally, we conducted experiments on three public fraud datasets, and the
results demonstrate that MimbFD exhibits outstanding performance in fraud
detection.

</details>


### [170] [Mutual Information Free Topological Generalization Bounds via Stability](https://arxiv.org/abs/2507.06775)
*Mario Tuci,Lennart Bastian,Benjamin Dupuis,Nassir Navab,Tolga Birdal,Umut Şimşekli*

Main category: cs.LG

TL;DR: The paper proposes interpretable topological generalization bounds for stochastic optimization, avoiding intractable mutual information terms and showcasing the importance of trajectory stability.


<details>
  <summary>Details</summary>
Motivation: Current generalization error bounds in stochastic optimization rely on complex, intractable terms tied to mutual information, which limits their applicability in practical algorithms.

Method: The authors introduce a novel framework based on trajectory stability, extending the notion of hypothesis set stability, and derive generalization bounds using topological data analysis (TDA) concepts tied to parameter space trajectory complexity.

Result: The derived generalization bounds relate error to trajectory stability and TDA complexity metrics. Experiments confirm that topological terms play a critical role, especially with large training datasets.

Conclusion: The proposed framework allows practical insight into the generalization performance of stochastic optimization algorithms while confirming the significance of geometric trajectory analysis.

Abstract: Providing generalization guarantees for stochastic optimization algorithms is
a major challenge in modern learning theory. Recently, several studies
highlighted the impact of the geometry of training trajectories on the
generalization error, both theoretically and empirically. Among these works, a
series of topological generalization bounds have been proposed, relating the
generalization error to notions of topological complexity that stem from
topological data analysis (TDA). Despite their empirical success, these bounds
rely on intricate information-theoretic (IT) terms that can be bounded in
specific cases but remain intractable for practical algorithms (such as ADAM),
potentially reducing the relevance of the derived bounds. In this paper, we
seek to formulate comprehensive and interpretable topological generalization
bounds free of intractable mutual information terms. To this end, we introduce
a novel learning theoretic framework that departs from the existing strategies
via proof techniques rooted in algorithmic stability. By extending an existing
notion of \textit{hypothesis set stability}, to \textit{trajectory stability},
we prove that the generalization error of trajectory-stable algorithms can be
upper bounded in terms of (i) TDA quantities describing the complexity of the
trajectory of the optimizer in the parameter space, and (ii) the trajectory
stability parameter of the algorithm. Through a series of experimental
evaluations, we demonstrate that the TDA terms in the bound are of great
importance, especially as the number of training samples grows. This ultimately
forms an explanation of the empirical success of the topological generalization
bounds.

</details>


### [171] [FedDifRC: Unlocking the Potential of Text-to-Image Diffusion Models in Heterogeneous Federated Learning](https://arxiv.org/abs/2507.06482)
*Huan Wang,Haoran Li,Huaming Chen,Jun Yan,Jiahua Shi,Jun Shen*

Main category: cs.LG

TL;DR: This paper introduces a novel methodology, FedDifRC, which integrates diffusion models into federated learning to address data heterogeneity challenges, enhance convergence, and improve model performance.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the challenge of data heterogeneity in federated learning, where biased local data across participants impedes model convergence and overall performance.

Method: FedDifRC leverages diffusion models to provide guidance using text-driven diffusion contrasting and noise-driven regularization. It utilizes contrastive learning strategies through diffusion model feedback and imposes regularization by aligning local instances with denoising representations.

Result: The proposed FedDifRC paradigm demonstrated improved handling of data heterogeneity and validated its effectiveness experimentally in various scenarios. It proved efficient both in supervised and self-supervised implementations.

Conclusion: FedDifRC effectively mitigates federated learning issues caused by data heterogeneity, leveraging diffusion model representations. It shows promise for enhancing privacy-sensitive collaborative training paradigms.

Abstract: Federated learning aims at training models collaboratively across
participants while protecting privacy. However, one major challenge for this
paradigm is the data heterogeneity issue, where biased data preferences across
multiple clients, harming the model's convergence and performance. In this
paper, we first introduce powerful diffusion models into the federated learning
paradigm and show that diffusion representations are effective steers during
federated training. To explore the possibility of using diffusion
representations in handling data heterogeneity, we propose a novel
diffusion-inspired Federated paradigm with Diffusion Representation
Collaboration, termed FedDifRC, leveraging meaningful guidance of diffusion
models to mitigate data heterogeneity. The key idea is to construct text-driven
diffusion contrasting and noise-driven diffusion regularization, aiming to
provide abundant class-related semantic information and consistent convergence
signals. On the one hand, we exploit the conditional feedback from the
diffusion model for different text prompts to build a text-driven contrastive
learning strategy. On the other hand, we introduce a noise-driven consistency
regularization to align local instances with diffusion denoising
representations, constraining the optimization region in the feature space. In
addition, FedDifRC can be extended to a self-supervised scheme without relying
on any labeled data. We also provide a theoretical analysis for FedDifRC to
ensure convergence under non-convex objectives. The experiments on different
scenarios validate the effectiveness of FedDifRC and the efficiency of crucial
components.

</details>


### [172] [Scalable Gaussian Processes: Advances in Iterative Methods and Pathwise Conditioning](https://arxiv.org/abs/2507.06839)
*Jihao Andreas Lin*

Main category: cs.LG

TL;DR: This dissertation advances Gaussian processes for large-scale applications using iterative methods and pathwise conditioning to enhance scalability and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Gaussian processes are limited in their scalability when applied to large datasets, and this paper seeks to address this limitation by leveraging modern hardware and computational techniques.

Method: The paper combines iterative methods and pathwise conditioning, reformulating expensive computations into linear systems and utilizing matrix multiplication for efficient execution on modern hardware.

Result: The approach enables Gaussian processes to handle larger datasets by reducing memory requirements and optimizing computations for massively-parallel hardware.

Conclusion: Synergistically integrating iterative methods with pathwise conditioning offers a scalable and hardware-efficient framework for leveraging Gaussian processes in large-scale applications.

Abstract: Gaussian processes are a powerful framework for uncertainty-aware function
approximation and sequential decision-making. Unfortunately, their classical
formulation does not scale gracefully to large amounts of data and modern
hardware for massively-parallel computation, prompting many researchers to
develop techniques which improve their scalability. This dissertation focuses
on the powerful combination of iterative methods and pathwise conditioning to
develop methodological contributions which facilitate the use of Gaussian
processes in modern large-scale settings. By combining these two techniques
synergistically, expensive computations are expressed as solutions to systems
of linear equations and obtained by leveraging iterative linear system solvers.
This drastically reduces memory requirements, facilitating application to
significantly larger amounts of data, and introduces matrix multiplication as
the main computational operation, which is ideal for modern hardware.

</details>


### [173] [MoFE-Time: Mixture of Frequency Domain Experts for Time-Series Forecasting Models](https://arxiv.org/abs/2507.06502)
*Yiwen Liu,Chenyu Zhang,Junjie Song,Siqi Chen,Sun Yin,Zihan Wang,Lingming Zeng,Yuji Cao,Junming Jiao*

Main category: cs.LG

TL;DR: The paper introduces MoFE-Time, a time series forecasting model that integrates time and frequency domain features with a Mixture of Experts (MoE) network to achieve state-of-the-art results on benchmark datasets and real-world data.


<details>
  <summary>Details</summary>
Motivation: Address the shortcomings of existing time series forecasting models by effectively modeling both time and frequency characteristics, and transferring prior pattern knowledge in a pretraining-finetuning paradigm.

Method: Propose MoFE-Time, a model based on the Mixture of Experts (MoE) network, using frequency and time cells as experts. Incorporates pretraining-finetuning for knowledge transfer across datasets with varying periodicity.

Result: Achieved state-of-the-art performance on six public benchmarks, reducing MSE by 6.95% and MAE by 6.02% compared to Time-MoE. Demonstrated strong results on a proprietary business dataset (NEV-sales).

Conclusion: MoFE-Time effectively integrates time and frequency features for better time series forecasting and shows significant improvements in both standard benchmarks and practical applications.

Abstract: As a prominent data modality task, time series forecasting plays a pivotal
role in diverse applications. With the remarkable advancements in Large
Language Models (LLMs), the adoption of LLMs as the foundational architecture
for time series modeling has gained significant attention. Although existing
models achieve some success, they rarely both model time and frequency
characteristics in a pretraining-finetuning paradigm leading to suboptimal
performance in predictions of complex time series, which requires both modeling
periodicity and prior pattern knowledge of signals. We propose MoFE-Time, an
innovative time series forecasting model that integrates time and frequency
domain features within a Mixture of Experts (MoE) network. Moreover, we use the
pretraining-finetuning paradigm as our training framework to effectively
transfer prior pattern knowledge across pretraining and finetuning datasets
with different periodicity distributions. Our method introduces both frequency
and time cells as experts after attention modules and leverages the MoE routing
mechanism to construct multidimensional sparse representations of input
signals. In experiments on six public benchmarks, MoFE-Time has achieved new
state-of-the-art performance, reducing MSE and MAE by 6.95% and 6.02% compared
to the representative methods Time-MoE. Beyond the existing evaluation
benchmarks, we have developed a proprietary dataset, NEV-sales, derived from
real-world business scenarios. Our method achieves outstanding results on this
dataset, underscoring the effectiveness of the MoFE-Time model in practical
commercial applications.

</details>


### [174] [Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy](https://arxiv.org/abs/2507.06969)
*Bogdan Kulynych,Juan Felipe Gomez,Georgios Kaissis,Jamie Hayes,Borja Balle,Flavio du Pin Calmon,Jean Louis Raisaro*

Main category: cs.LG

TL;DR: This paper refines how Differentially Private mechanisms are assessed by unifying risk bounds for privacy attacks, showing consistent and tunable interpretations across various risks.


<details>
  <summary>Details</summary>
Motivation: Current methods to measure privacy risks in DP mechanisms are overly pessimistic and inconsistent, creating challenges in balancing protection and utility.

Method: The authors use the hypothesis-testing interpretation of DP ($f$-DP) to develop unified risk bounds for re-identification, attribute inference, and data reconstruction attacks.

Result: Their approach yields tighter risk assessments compared to $
\varepsilon-DP$, Rényi DP, and concentrated DP, reducing required noise by 20% while achieving significant accuracy improvements.

Conclusion: This framework improves the interpretability and calibration of DP mechanisms, providing practitioners with better tools for balancing privacy and utility trade-offs.

Abstract: Differentially private (DP) mechanisms are difficult to interpret and
calibrate because existing methods for mapping standard privacy parameters to
concrete privacy risks -- re-identification, attribute inference, and data
reconstruction -- are both overly pessimistic and inconsistent. In this work,
we use the hypothesis-testing interpretation of DP ($f$-DP), and determine that
bounds on attack success can take the same unified form across
re-identification, attribute inference, and data reconstruction risks. Our
unified bounds are (1) consistent across a multitude of attack settings, and
(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary
(including worst-case) levels of baseline risk. Empirically, our results are
tighter than prior methods using $\varepsilon$-DP, R\'enyi DP, and concentrated
DP. As a result, calibrating noise using our bounds can reduce the required
noise by 20% at the same risk level, which yields, e.g., more than 15pp
accuracy increase in a text classification task. Overall, this unifying
perspective provides a principled framework for interpreting and calibrating
the degree of protection in DP against specific levels of re-identification,
attribute inference, or data reconstruction risk.

</details>


### [175] [Direct Regret Optimization in Bayesian Optimization](https://arxiv.org/abs/2507.06529)
*Fengxue Zhang,Yuxin Chen*

Main category: cs.LG

TL;DR: This paper introduces a non-myopic approach to Bayesian optimization (BO) using a decision transformer to minimize multi-step regret.


<details>
  <summary>Details</summary>
Motivation: Conventional BO methods are myopic and rely on separate, hand-crafted acquisition functions and surrogate models, leaving room for improvement in optimizing expensive black-box functions.

Method: An ensemble of Gaussian Processes with varying hyperparameters generates simulated BO trajectories for various acquisition functions; these are used to train a decision transformer offline, which subsequently guides real evaluations with limited refinement.

Result: The proposed method outperforms traditional BO baselines, achieving lower regret and showing robust performance in high-dimensional and noisy contexts.

Conclusion: The approach demonstrates the effectiveness of combining non-myopic strategies, simulated training, and a decision transformer in improving BO for challenging scenarios.

Abstract: Bayesian optimization (BO) is a powerful paradigm for optimizing expensive
black-box functions. Traditional BO methods typically rely on separate
hand-crafted acquisition functions and surrogate models for the underlying
function, and often operate in a myopic manner. In this paper, we propose a
novel direct regret optimization approach that jointly learns the optimal model
and non-myopic acquisition by distilling from a set of candidate models and
acquisitions, and explicitly targets minimizing the multi-step regret. Our
framework leverages an ensemble of Gaussian Processes (GPs) with varying
hyperparameters to generate simulated BO trajectories, each guided by an
acquisition function chosen from a pool of conventional choices, until a
Bayesian early stop criterion is met. These simulated trajectories, capturing
multi-step exploration strategies, are used to train an end-to-end decision
transformer that directly learns to select next query points aimed at improving
the ultimate objective. We further adopt a dense training--sparse learning
paradigm: The decision transformer is trained offline with abundant simulated
data sampled from ensemble GPs and acquisitions, while a limited number of real
evaluations refine the GPs online. Experimental results on synthetic and
real-world benchmarks suggest that our method consistently outperforms BO
baselines, achieving lower simple regret and demonstrating more robust
exploration in high-dimensional or noisy settings.

</details>


### [176] [Transferable Parasitic Estimation via Graph Contrastive Learning and Label Rebalancing in AMS Circuits](https://arxiv.org/abs/2507.06535)
*Shan Shen,Shenglu Hua,Jiajun Zou,Jiawei Liu,Jianwang Zhai,Chuan Shi,Wenjian Yu*

Main category: cs.LG

TL;DR: The paper introduces CircuitGCL, a novel graph contrastive learning framework for better representation learning in Analog-Mixed Signal (AMS) circuits, addressing challenges like data scarcity, label imbalance, and circuit diversity.


<details>
  <summary>Details</summary>
Motivation: To improve graph representation learning for AMS circuits, which is constrained by limited data, unbalanced labels, and high diversity in circuit designs, making it difficult to achieve robust and transferable representations.

Method: A graph contrastive learning framework named CircuitGCL is proposed, utilizing hyperspherical representation scattering for topology-invariant node embeddings and balancing label disparities using combined loss functions (balanced MSE and softmax cross-entropy).

Result: CircuitGCL achieves significant performance improvements on tasks such as parasitic capacitance estimation and ground capacitance classification, surpassing state-of-the-art methods with substantial gains in $R^2$ (33.64%-44.20%) for edge regression and F1-scores (0.9x-2.1x) for node classification.

Conclusion: CircuitGCL effectively enhances representation learning in AMS circuits, demonstrating superior transferability and robustness in experiments, providing a promising approach for related tasks.

Abstract: Graph representation learning on Analog-Mixed Signal (AMS) circuits is
crucial for various downstream tasks, e.g., parasitic estimation. However, the
scarcity of design data, the unbalanced distribution of labels, and the
inherent diversity of circuit implementations pose significant challenges to
learning robust and transferable circuit representations. To address these
limitations, we propose CircuitGCL, a novel graph contrastive learning
framework that integrates representation scattering and label rebalancing to
enhance transferability across heterogeneous circuit graphs. CircuitGCL employs
a self-supervised strategy to learn topology-invariant node embeddings through
hyperspherical representation scattering, eliminating dependency on large-scale
data. Simultaneously, balanced mean squared error (MSE) and softmax
cross-entropy (bsmCE) losses are introduced to mitigate label distribution
disparities between circuits, enabling robust and transferable parasitic
estimation. Evaluated on parasitic capacitance estimation (edge-level task) and
ground capacitance classification (node-level task) across TSMC 28nm AMS
designs, CircuitGCL outperforms all state-of-the-art (SOTA) methods, with the
$R^2$ improvement of $33.64\% \sim 44.20\%$ for edge regression and F1-score
gain of $0.9\times \sim 2.1\times$ for node classification. Our code is
available at
\href{https://anonymous.4open.science/r/CircuitGCL-099B/README.md}{here}.

</details>


### [177] [Few-shot Learning on AMS Circuits and Its Application to Parasitic Capacitance Prediction](https://arxiv.org/abs/2507.06538)
*Shan Shen,Yibin Zhang,Hector Rodriguez Rodriguez,Wenjian Yu*

Main category: cs.LG

TL;DR: This paper introduces CircuitGPS, a few-shot method for AMS circuit parasitic effect prediction, demonstrating improved accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: AMS circuit design suffers from limited integrated circuit design data, restricting the application of deep learning. This work aims to develop a graph representation learning method to address parasitic effect prediction in AMS circuits with minimal data.

Method: CircuitGPS represents circuit netlists as heterogeneous graphs, pre-trains on link prediction, and fine-tunes using edge regression. It employs a small-hop sampling technique for subgraph construction, a hybrid graph Transformer for embedding learning, and a low-cost positional encoding for structural and positional information.

Result: CircuitGPS improves coupling existence accuracy by over 20% and reduces capacitance estimation error (MAE) by at least 0.067. It also demonstrates scalability via zero-shot learning.

Conclusion: CircuitGPS enhances parasitic effect prediction in AMS circuits leveraging graph representation learning, few-shot data efficiency, and scalability across diverse designs. The approach offers insights into graph model development.

Abstract: Graph representation learning is a powerful method to extract features from
graph-structured data, such as analog/mixed-signal (AMS) circuits. However,
training deep learning models for AMS designs is severely limited by the
scarcity of integrated circuit design data. In this work, we present
CircuitGPS, a few-shot learning method for parasitic effect prediction in AMS
circuits. The circuit netlist is represented as a heterogeneous graph, with the
coupling capacitance modeled as a link. CircuitGPS is pre-trained on link
prediction and fine-tuned on edge regression. The proposed method starts with a
small-hop sampling technique that converts a link or a node into a subgraph.
Then, the subgraph embeddings are learned with a hybrid graph Transformer.
Additionally, CircuitGPS integrates a low-cost positional encoding that
summarizes the positional and structural information of the sampled subgraph.
CircuitGPS improves the accuracy of coupling existence by at least 20\% and
reduces the MAE of capacitance estimation by at least 0.067 compared to
existing methods. Our method demonstrates strong inherent scalability, enabling
direct application to diverse AMS circuit designs through zero-shot learning.
Furthermore, the ablation studies provide valuable insights into graph models
for representation learning.

</details>


### [178] [The Primacy of Magnitude in Low-Rank Adaptation](https://arxiv.org/abs/2507.06558)
*Zicheng Zhang,Haoran Li,Yifeng Zhang,Guoqiang Gong,Jiaxing Wang,Pengzhang Liu,Qixia Jiang,Junxing Hu*

Main category: cs.LG

TL;DR: The paper introduces LoRAM, a new initialization scheme for Low-Rank Adaptation (LoRA), which matches the performance of computationally-heavy spectral initialization methods while remaining efficient.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the inefficiency of recent spectral methods in LoRA that, despite improving performance, come with additional computational and storage costs.

Method: The proposed LoRAM method derives weight magnitude updates as key to LoRA's performance and employs a deterministic 'Basis & Basis' scheme, which simulates spectral benefits using pretrained weight magnitudes.

Result: Experiments demonstrate that LoRAM performs as well as or better than spectral methods across various benchmarks, maintaining the efficiency of LoRA.

Conclusion: The LoRAM initialization scheme simplifies and improves upon spectral methods, making it a robust and efficient alternative for parameter-efficient model tuning.

Abstract: Low-Rank Adaptation (LoRA) offers a parameter-efficient paradigm for tuning
large models. While recent spectral initialization methods improve convergence
and performance over the naive "Noise & Zeros" scheme, their extra
computational and storage overhead undermines efficiency. In this paper, we
establish update magnitude as the fundamental driver of LoRA performance and
propose LoRAM, a magnitude-driven "Basis & Basis" initialization scheme that
matches spectral methods without their inefficiencies. Our key contributions
are threefold: (i) Magnitude of weight updates determines convergence. We prove
low-rank structures intrinsically bound update magnitudes, unifying
hyperparameter tuning in learning rate, scaling factor, and initialization as
mechanisms to optimize magnitude regulation. (ii) Spectral initialization
succeeds via magnitude amplification. We demystify that the presumed
knowledge-driven benefit of the spectral component essentially arises from the
boost in the weight update magnitude. (iii) A novel and compact initialization
strategy, LoRAM, scales deterministic orthogonal bases using pretrained weight
magnitudes to simulate spectral gains. Extensive experiments show that LoRAM
serves as a strong baseline, retaining the full efficiency of LoRA while
matching or outperforming spectral initialization across benchmarks.

</details>


### [179] [From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via Progressive Optimization](https://arxiv.org/abs/2507.06573)
*Xinjie Chen,Minpeng Liao,Guoxin Chen,Chengxi Li,Biao Fu,Kai Fan,Xinggao Liu*

Main category: cs.LG

TL;DR: This paper introduces LPPO, a reinforcement learning framework aimed at improving large language models' abilities by efficiently using a small set of high-quality data through progressive optimization.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of improving reasoning capabilities of large language models by focusing on efficiently leveraging high-quality demonstrations rather than simply scaling data volume.

Method: The paper proposes two key techniques: (1) Prefix-guided sampling, which uses partial solutions as guiding hints for challenging problems; and (2) Learning-progress weighting, which dynamically adjusts the importance of training samples based on their contribution to the model's progression.

Result: The proposed methods outperform existing baselines on mathematical reasoning tasks, achieving faster learning convergence and a higher performance ceiling.

Conclusion: The LPPO framework effectively enhances reasoning capabilities in large language models by optimizing the use of limited high-quality data through innovative sampling and weighting strategies.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently advanced
the reasoning capabilities of large language models (LLMs). While prior work
has emphasized algorithmic design, data curation, and reward shaping, we
investigate RLVR from a sample-centric perspective and introduce LPPO
(Learning-Progress and Prefix-guided Optimization), a framework of progressive
optimization techniques. Our work addresses a critical question: how to best
leverage a small set of trusted, high-quality demonstrations, rather than
simply scaling up data volume. First, motivated by how hints aid human
problem-solving, we propose prefix-guided sampling, an online data augmentation
method that incorporates partial solution prefixes from expert demonstrations
to guide the policy, particularly for challenging instances. Second, inspired
by how humans focus on important questions aligned with their current
capabilities, we introduce learning-progress weighting, a dynamic strategy that
adjusts each training sample's influence based on model progression. We
estimate sample-level learning progress via an exponential moving average of
per-sample pass rates, promoting samples that foster learning and
de-emphasizing stagnant ones. Experiments on mathematical-reasoning benchmarks
demonstrate that our methods outperform strong baselines, yielding faster
convergence and a higher performance ceiling.

</details>


### [180] [Learning controllable dynamics through informative exploration](https://arxiv.org/abs/2507.06582)
*Peter N. Loxley,Friedrich T. Sommer*

Main category: cs.LG

TL;DR: The paper proposes using 'predicted information gain' to identify and explore the most informative regions in environments with unknown, controllable dynamics.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of understanding environments with unknown dynamics where explicit models are unavailable, by efficiently learning these dynamics through exploration.

Method: Used 'predicted information gain' as an information measure, combined with reinforcement learning techniques, to develop exploration policies that identify informative regions in the environment.

Result: Achieved reliable estimates of underlying controllable dynamics and demonstrated superior results compared to myopic exploration approaches.

Conclusion: The proposed approach improves exploration efficiency and aids in better understanding of unknown controllable dynamics using information measures and reinforcement learning.

Abstract: Environments with controllable dynamics are usually understood in terms of
explicit models. However, such models are not always available, but may
sometimes be learned by exploring an environment. In this work, we investigate
using an information measure called "predicted information gain" to determine
the most informative regions of an environment to explore next. Applying
methods from reinforcement learning allows good suboptimal exploring policies
to be found, and leads to reliable estimates of the underlying controllable
dynamics. This approach is demonstrated by comparing with several myopic
exploration approaches.

</details>


### [181] [Generalization in Reinforcement Learning for Radio Access Networks](https://arxiv.org/abs/2507.06602)
*Burak Demirel,Yu Wang,Cristian Tatino,Pablo Soldati*

Main category: cs.LG

TL;DR: A generalization-focused RL framework for RAN control improves throughput and spectral efficiency across various settings while addressing computational complexities.


<details>
  <summary>Details</summary>
Motivation: Rule-based RRM algorithms underperform in dynamic RAN environments; RL solutions face generalization challenges.

Method: Uses attention-based graph representations, domain randomization, distributed data generation, centralized training aligned with O-RAN principles.

Result: Achieved ~10% and >20% improvements in throughput and spectral efficiency, outperforming baselines in different traffic benchmarks.

Conclusion: The framework paves the way for AI-native 6G RAN by utilizing scalable generalizable RL agents across diverse network scenarios.

Abstract: Modern RAN operate in highly dynamic and heterogeneous environments, where
hand-tuned, rule-based RRM algorithms often underperform. While RL can surpass
such heuristics in constrained settings, the diversity of deployments and
unpredictable radio conditions introduce major generalization challenges.
Data-driven policies frequently overfit to training conditions, degrading
performance in unseen scenarios. To address this, we propose a
generalization-centered RL framework for RAN control that: (i) encodes cell
topology and node attributes via attention-based graph representations; (ii)
applies domain randomization to broaden the training distribution; and (iii)
distributes data generation across multiple actors while centralizing training
in a cloud-compatible architecture aligned with O-RAN principles. Although
generalization increases computational and data-management complexity, our
distributed design mitigates this by scaling data collection and training
across diverse network conditions. Applied to downlink link adaptation in five
5G benchmarks, our policy improves average throughput and spectral efficiency
by ~10% over an OLLA baseline (10% BLER target) in full-buffer MIMO/mMIMO and
by >20% under high mobility. It matches specialized RL in full-buffer traffic
and achieves up to 4- and 2-fold gains in eMBB and mixed-traffic benchmarks,
respectively. In nine-cell deployments, GAT models offer 30% higher throughput
over MLP baselines. These results, combined with our scalable architecture,
offer a path toward AI-native 6G RAN using a single, generalizable RL agent.

</details>


### [182] [Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation](https://arxiv.org/abs/2507.06613)
*Anshuk Uppal,Yuhta Takida,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: The paper introduces a generative modeling framework that balances disentanglement and generation quality using a novel VAE and a diffusion model.


<details>
  <summary>Details</summary>
Motivation: Existing generative models struggle to achieve both disentangled latent representations and high-quality reconstruction, due to trade-offs imposed by techniques like the $\beta$-VAE framework.

Method: The authors propose a VAE with a new loss function to learn latent representations controlled by $\beta$, and a non-linear diffusion model to enhance reconstruction quality while retaining disentanglement.

Result: The framework achieves a balance between disentanglement and reconstruction quality, supports consistent transitions in latent spaces, and functions as a standalone generative model.

Conclusion: The proposed method improves upon the trade-off between disentanglement and reconstruction, successfully combining high-quality sample generation and interpretability.

Abstract: Disentangled and interpretable latent representations in generative models
typically come at the cost of generation quality. The $\beta$-VAE framework
introduces a hyperparameter $\beta$ to balance disentanglement and
reconstruction quality, where setting $\beta > 1$ introduces an information
bottleneck that favors disentanglement over sharp, accurate reconstructions. To
address this trade-off, we propose a novel generative modeling framework that
leverages a range of $\beta$ values to learn multiple corresponding latent
representations. First, we obtain a slew of representations by training a
single variational autoencoder (VAE), with a new loss function that controls
the information retained in each latent representation such that the higher
$\beta$ value prioritize disentanglement over reconstruction fidelity. We then,
introduce a non-linear diffusion model that smoothly transitions latent
representations corresponding to different $\beta$ values. This model denoises
towards less disentangled and more informative representations, ultimately
leading to (almost) lossless representations, enabling sharp reconstructions.
Furthermore, our model supports sample generation without input images,
functioning as a standalone generative model. We evaluate our framework in
terms of both disentanglement and generation quality. Additionally, we observe
smooth transitions in the latent spaces with respect to changes in $\beta$,
facilitating consistent manipulation of generated outputs.

</details>


### [183] [Efficient Multi-Task Reinforcement Learning with Cross-Task Policy Guidance](https://arxiv.org/abs/2507.06615)
*Jinmin He,Kai Li,Yifan Zang,Haobo Fu,Qiang Fu,Junliang Xing,Jian Cheng*

Main category: cs.LG

TL;DR: The paper introduces Cross-Task Policy Guidance (CTPG), a framework to improve multi-task reinforcement learning by using proficient policies to guide unmastered tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods in multi-task reinforcement learning focus mainly on parameter sharing but ignore leveraging cross-task similarities through explicit policy guidance.

Method: The proposed CTPG framework uses guide policies to select task-specific behavior policies for better environment interactions and integrates filtering and blocking mechanisms for efficiency.

Result: CTPG, when combined with existing approaches, significantly boosts performance on manipulation and locomotion benchmarks.

Conclusion: Incorporating explicit cross-task policy guidance using CTPG provides a complementary and effective way to enhance multi-task reinforcement learning.

Abstract: Multi-task reinforcement learning endeavors to efficiently leverage shared
information across various tasks, facilitating the simultaneous learning of
multiple tasks. Existing approaches primarily focus on parameter sharing with
carefully designed network structures or tailored optimization procedures.
However, they overlook a direct and complementary way to exploit cross-task
similarities: the control policies of tasks already proficient in some skills
can provide explicit guidance for unmastered tasks to accelerate skills
acquisition. To this end, we present a novel framework called Cross-Task Policy
Guidance (CTPG), which trains a guide policy for each task to select the
behavior policy interacting with the environment from all tasks' control
policies, generating better training trajectories. In addition, we propose two
gating mechanisms to improve the learning efficiency of CTPG: one gate filters
out control policies that are not beneficial for guidance, while the other gate
blocks tasks that do not necessitate guidance. CTPG is a general framework
adaptable to existing parameter sharing approaches. Empirical evaluations
demonstrate that incorporating CTPG with these approaches significantly
enhances performance in manipulation and locomotion benchmarks.

</details>


### [184] [UniOD: A Universal Model for Outlier Detection across Diverse Domains](https://arxiv.org/abs/2507.06624)
*Dazhi Fu,Jicong Fan*

Main category: cs.LG

TL;DR: The paper introduces UniOD, a universal outlier detection framework that uses labeled datasets to train a single model for diverse domains, avoiding hyperparameter tuning and costly training specific to each dataset.


<details>
  <summary>Details</summary>
Motivation: Current outlier detection methods are cumbersome due to dataset-specific hyperparameter tuning and expensive model training, necessitating a more universal and efficient solution.

Method: UniOD converts datasets into graphs, creates consistent node features, and frames the outlier detection problem as a node-classification task using knowledge from labeled datasets.

Result: UniOD outperforms 15 state-of-the-art baselines across 15 benchmark datasets, showcasing its generalizability and effectiveness.

Conclusion: UniOD provides an efficient, accurate, and universal solution for outlier detection, reducing computational cost, improving convenience, and generalizing to unseen domains.

Abstract: Outlier detection (OD) seeks to distinguish inliers and outliers in
completely unlabeled datasets and plays a vital role in science and
engineering. Most existing OD methods require troublesome dataset-specific
hyperparameter tuning and costly model training before they can be deployed to
identify outliers. In this work, we propose UniOD, a universal OD framework
that leverages labeled datasets to train a single model capable of detecting
outliers of datasets from diverse domains. Specifically, UniOD converts each
dataset into multiple graphs, produces consistent node features, and frames
outlier detection as a node-classification task, and is able to generalize to
unseen domains. As a result, UniOD avoids effort on model selection and
hyperparameter tuning, reduces computational cost, and effectively utilizes the
knowledge from historical datasets, which improves the convenience and accuracy
in real applications. We evaluate UniOD on 15 benchmark OD datasets against 15
state-of-the-art baselines, demonstrating its effectiveness.

</details>


### [185] [Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning](https://arxiv.org/abs/2507.06628)
*Jinmin He,Kai Li,Yifan Zang,Haobo Fu,Qiang Fu,Junliang Xing,Jian Cheng*

Main category: cs.LG

TL;DR: The paper proposes GO-Skill, a method for offline multi-task reinforcement learning that uses reusable skills to enhance task performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in offline multi-task reinforcement learning related to inefficient knowledge sharing across tasks.

Method: GO-Skill extracts reusable skills using a goal-oriented process, employs vector quantization for a discrete skill library, and integrates skills through hierarchical policy learning.

Result: Experiments on robotic tasks in MetaWorld show GO-Skill's effectiveness and versatility.

Conclusion: GO-Skill improves knowledge transfer and task performance in reinforcement learning via reusable skill abstraction and hierarchical coordination.

Abstract: Offline multi-task reinforcement learning aims to learn a unified policy
capable of solving multiple tasks using only pre-collected task-mixed datasets,
without requiring any online interaction with the environment. However, it
faces significant challenges in effectively sharing knowledge across tasks.
Inspired by the efficient knowledge abstraction observed in human learning, we
propose Goal-Oriented Skill Abstraction (GO-Skill), a novel approach designed
to extract and utilize reusable skills to enhance knowledge transfer and task
performance. Our approach uncovers reusable skills through a goal-oriented
skill extraction process and leverages vector quantization to construct a
discrete skill library. To mitigate class imbalances between broadly applicable
and task-specific skills, we introduce a skill enhancement phase to refine the
extracted skills. Furthermore, we integrate these skills using hierarchical
policy learning, enabling the construction of a high-level policy that
dynamically orchestrates discrete skills to accomplish specific tasks.
Extensive experiments on diverse robotic manipulation tasks within the
MetaWorld benchmark demonstrate the effectiveness and versatility of GO-Skill.

</details>


### [186] [Prevention of Overfitting on Mesh-Structured Data Regressions with a Modified Laplace Operator](https://arxiv.org/abs/2507.06631)
*Enda D. V. Bigarella*

Main category: cs.LG

TL;DR: This paper presents a method to combat overfitting in data regressions through Laplace-operator derivatives, reducing oscillations and entropy in mesh-like data structures.


<details>
  <summary>Details</summary>
Motivation: To address overfitting in data regression tasks, particularly by leveraging mathematical properties of mesh-like data structures.

Method: Compute Laplace-operator second-order derivatives to quantify entropy. Use staggered meshes for training data evaluation and hyperparameter optimization by minimizing oscillations.

Result: The method effectively reduces oscillations and entropy, allowing training on all data points without traditional train-test splits.

Conclusion: The Laplace-operator method enhances model reliability by mitigating overfitting, providing an alternative surrogate metric for testing based on diffusion properties.

Abstract: This document reports on a method for detecting and preventing overfitting on
data regressions, herein applied to mesh-like data structures. The mesh
structure allows for the straightforward computation of the Laplace-operator
second-order derivatives in a finite-difference fashion for noiseless data.
Derivatives of the training data are computed on the original training mesh to
serve as a true label of the entropy of the training data. Derivatives of the
trained data are computed on a staggered mesh to identify oscillations in the
interior of the original training mesh cells. The loss of the Laplace-operator
derivatives is used for hyperparameter optimisation, achieving a reduction of
unwanted oscillation through the minimisation of the entropy of the trained
model. In this setup, testing does not require the splitting of points from the
training data, and training is thus directly performed on all available
training points. The Laplace operator applied to the trained data on a
staggered mesh serves as a surrogate testing metric based on diffusion
properties.

</details>


### [187] [Deep Disentangled Representation Network for Treatment Effect Estimation](https://arxiv.org/abs/2507.06650)
*Hui Meng,Keping Yang,Xuyu Peng,Bo Zheng*

Main category: cs.LG

TL;DR: This paper proposes a novel algorithm incorporating soft disentanglement and importance sampling for estimating individual-level treatment effects, showing superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of estimating individual-level treatment effects from observational data, crucial in domains like healthcare and education, but hindered by the lack of robust disentanglement methods.

Method: A new algorithm utilizes a mixture of experts with multi-head attention, a linear orthogonal regularizer for soft decomposition of covariates, and importance sampling for selection bias correction.

Result: Extensive experiments on semi-synthetic and real-world datasets demonstrate that the proposed method exceeds the performance of current state-of-the-art techniques.

Conclusion: The new approach effectively models causal relationships with disentangled representations and reduces bias, advancing the estimation of treatment effects in observational studies.

Abstract: Estimating individual-level treatment effect from observational data is a
fundamental problem in causal inference and has attracted increasing attention
in the fields of education, healthcare, and public policy.In this work, we
concentrate on the study of disentangled representation methods that have shown
promising outcomes by decomposing observed covariates into instrumental,
confounding, and adjustment factors. However, most of the previous work has
primarily revolved around generative models or hard decomposition methods for
covariates, which often struggle to guarantee the attainment of precisely
disentangled factors. In order to effectively model different causal
relationships, we propose a novel treatment effect estimation algorithm that
incorporates a mixture of experts with multi-head attention and a linear
orthogonal regularizer to softly decompose the pre-treatment variables, and
simultaneously eliminates selection bias via importance sampling re-weighting
techniques. We conduct extensive experiments on both public semi-synthetic and
real-world production datasets. The experimental results clearly demonstrate
that our algorithm outperforms the state-of-the-art methods focused on
individual treatment effects.

</details>


### [188] [Federated Learning Inspired Fuzzy Systems: Decentralized Rule Updating for Privacy and Scalable Decision Making](https://arxiv.org/abs/2507.06652)
*Arthur Alexander Lim,Zhen Bin It,Jovan Bowen Heng,Tee Hui Teo*

Main category: cs.LG

TL;DR: The paper explores how integrating machine learning and federated learning with fuzzy systems can address uncertainties in technology, but acknowledges further research is needed.


<details>
  <summary>Details</summary>
Motivation: Fuzzy systems help manage uncertainty but need advancements to enhance their functionality. Drawing from innovations in machine learning and federated learning can optimize their application.

Method: The paper proposes leveraging aspects of machine learning (ML) and federated learning (FL), such as using federated updates to modify fuzzy rules, to improve fuzzy systems.

Result: The conceptual improvements suggested indicate promising potential for enhancing fuzzy system performance by incorporating ML and FL frameworks.

Conclusion: The paper concludes that while the proposed concepts are compelling, more research is necessary to assess their effectiveness and actualize these enhancements in practice.

Abstract: Fuzzy systems are a way to allow machines, systems and frameworks to deal
with uncertainty, which is not possible in binary systems that most computers
use. These systems have already been deployed for certain use cases, and fuzzy
systems could be further improved as proposed in this paper. Such technologies
to draw inspiration from include machine learning and federated learning.
Machine learning is one of the recent breakthroughs of technology and could be
applied to fuzzy systems to further improve the results it produces. Federated
learning is also one of the recent technologies that have huge potential, which
allows machine learning training to improve by reducing privacy risk, reducing
burden on networking infrastructure, and reducing latency of the latest model.
Aspects from federated learning could be used to improve federated learning,
such as applying the idea of updating the fuzzy rules that make up a key part
of fuzzy systems, to further improve it over time. This paper discusses how
these improvements would be implemented in fuzzy systems, and how it would
improve fuzzy systems. It also discusses certain limitations on the potential
improvements. It concludes that these proposed ideas and improvements require
further investigation to see how far the improvements are, but the potential is
there to improve fuzzy systems.

</details>


### [189] [Heterogeneous Graph Neural Networks for Short-term State Forecasting in Power Systems across Domains and Time Scales: A Hydroelectric Power Plant Case Study](https://arxiv.org/abs/2507.06694)
*Raffael Theiler,Olga Fink*

Main category: cs.LG

TL;DR: The paper presents a methodology using Heterogeneous Graph Attention Networks to overcome challenges in short-term state forecasting for power systems that involve multiple domains, achieving a 35.5% improvement in accuracy compared to baselines.


<details>
  <summary>Details</summary>
Motivation: Modern power systems exhibit variability due to renewable and distributed energy resources and span multiple physical domains, making accurate short-term state forecasting crucial for operational stability and efficient decision-making.

Method: The authors propose the use of Heterogeneous Graph Attention Networks to model both intra-domain (homogeneous) and inter-domain (heterogeneous) relationships between sensor data sourced from hydraulic and electrical domains with differing temporal dynamics.

Result: The proposed method surpasses traditional baselines by 35.5% in normalized root mean square error, proving its efficacy.

Conclusion: Heterogeneous Graph Attention Networks are effective in handling multi-domain, multi-rate power system state forecasting and offer significant improvements in prediction accuracy, making them suitable for modern power systems.

Abstract: Accurate short-term state forecasting is essential for efficient and stable
operation of modern power systems, especially in the context of increasing
variability introduced by renewable and distributed energy resources. As these
systems evolve rapidly, it becomes increasingly important to reliably predict
their states in the short term to ensure operational stability, support control
decisions, and enable interpretable monitoring of sensor and machine behavior.
Modern power systems often span multiple physical domains - including
electrical, mechanical, hydraulic, and thermal - posing significant challenges
for modeling and prediction. Graph Neural Networks (GNNs) have emerged as a
promising data-driven framework for system state estimation and state
forecasting in such settings. By leveraging the topological structure of sensor
networks, GNNs can implicitly learn inter-sensor relationships and propagate
information across the network. However, most existing GNN-based methods are
designed under the assumption of homogeneous sensor relationships and are
typically constrained to a single physical domain. This limitation restricts
their ability to integrate and reason over heterogeneous sensor data commonly
encountered in real-world energy systems, such as those used in energy
conversion infrastructure. In this work, we propose the use of Heterogeneous
Graph Attention Networks to address these limitations. Our approach models both
homogeneous intra-domain and heterogeneous inter-domain relationships among
sensor data from two distinct physical domains - hydraulic and electrical -
which exhibit fundamentally different temporal dynamics. Experimental results
demonstrate that our method significantly outperforms conventional baselines on
average by 35.5% in terms of normalized root mean square error, confirming its
effectiveness in multi-domain, multi-rate power system state forecasting.

</details>


### [190] [Value from Observations: Towards Large-Scale Imitation Learning via Self-Improvement](https://arxiv.org/abs/2507.06701)
*Michael Bloesch,Markus Wulfmeier,Philemon Brakel,Todor Davchev,Martina Zambelli,Jost Tobias Springenberg,Abbas Abdolmaleki,William F Whitney,Nicolas Heess,Roland Hafner,Martin Riedmiller*

Main category: cs.LG

TL;DR: The paper proposes improvements in Imitation Learning from Observation (IfO) by focusing on nuanced data distributions and introducing methods to learn iteratively via self-improvement.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current IfO research which relies on idealized and bimodal-quality data distributions, making results less applicable to complex real-world scenarios.

Method: The paper adapts RL-based imitation learning for action-free demonstrations, leveraging a value function to transfer knowledge between expert and non-expert data.

Result: Comprehensive evaluations reveal the relationship between data distributions and algorithm applicability, exposing the limitations of existing IfO methods.

Conclusion: The study provides insights for creating robust and practical IfO techniques, enabling scalable behavior learning through iterative self-improvement.

Abstract: Imitation Learning from Observation (IfO) offers a powerful way to learn
behaviors at large-scale: Unlike behavior cloning or offline reinforcement
learning, IfO can leverage action-free demonstrations and thus circumvents the
need for costly action-labeled demonstrations or reward functions. However,
current IfO research focuses on idealized scenarios with mostly bimodal-quality
data distributions, restricting the meaningfulness of the results. In contrast,
this paper investigates more nuanced distributions and introduces a method to
learn from such data, moving closer to a paradigm in which imitation learning
can be performed iteratively via self-improvement. Our method adapts RL-based
imitation learning to action-free demonstrations, using a value function to
transfer information between expert and non-expert data. Through comprehensive
evaluation, we delineate the relation between different data distributions and
the applicability of algorithms and highlight the limitations of established
methods. Our findings provide valuable insights for developing more robust and
practical IfO techniques on a path to scalable behaviour learning.

</details>


### [191] [PINN-Obs: Physics-Informed Neural Network-Based Observer for Nonlinear Dynamical Systems](https://arxiv.org/abs/2507.06712)
*Ayoub Farkane,Mohamed Boutayeb,Mustapha Oudani,Mounir Ghogho*

Main category: cs.LG

TL;DR: The paper introduces an adaptive physics-informed neural network-based approach (PINN-Obs) to improve state estimation in nonlinear systems with noisy and partial measurements, showing superior accuracy and robustness compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address challenges in accurately estimating states in nonlinear dynamical systems when measurements are partial and noisy, surpassing limitations of traditional model-based methods that rely on explicit transformations or linearization.

Method: The paper proposes a physics-informed neural network-based observer (PINN-Obs) that integrates system dynamics and sensor data into the learning process while adaptively learning an optimal gain matrix, supported by theoretical convergence analysis.

Result: Extensive simulations on various nonlinear systems, including an induction motor and satellite system, showed that PINN-Obs performs better in accuracy, robustness, and adaptability compared to existing observer designs.

Conclusion: PINN-Obs is presented as a robust and accurate framework for state estimation in nonlinear systems, offering theoretical convergence guarantees and practical advantages over traditional approaches.

Abstract: State estimation for nonlinear dynamical systems is a critical challenge in
control and engineering applications, particularly when only partial and noisy
measurements are available. This paper introduces a novel Adaptive
Physics-Informed Neural Network-based Observer (PINN-Obs) for accurate state
estimation in nonlinear systems. Unlike traditional model-based observers,
which require explicit system transformations or linearization, the proposed
framework directly integrates system dynamics and sensor data into a
physics-informed learning process. The observer adaptively learns an optimal
gain matrix, ensuring convergence of the estimated states to the true system
states. A rigorous theoretical analysis establishes formal convergence
guarantees, demonstrating that the proposed approach achieves uniform error
minimization under mild observability conditions. The effectiveness of PINN-Obs
is validated through extensive numerical simulations on diverse nonlinear
systems, including an induction motor model, a satellite motion system, and
benchmark academic examples. Comparative experimental studies against existing
observer designs highlight its superior accuracy, robustness, and adaptability.

</details>


### [192] [Robust Deep Network Learning of Nonlinear Regression Tasks by Parametric Leaky Exponential Linear Units (LELUs) and a Diffusion Metric](https://arxiv.org/abs/2507.06765)
*Enda D. V. Bigarella*

Main category: cs.LG

TL;DR: The paper introduces the "Leaky Exponential Linear Unit" activation function, which is smooth and trainable, addressing overfitting and sensitivity issues in neural networks.


<details>
  <summary>Details</summary>
Motivation: To improve performance in large neural networks by addressing limitations of traditional activation functions (e.g., vanishing gradients, discontinuities) and measuring overfitting using a new metric.

Method: The authors propose a novel activation function called "Leaky Exponential Linear Unit" with non-zero gradients and a diffusion-loss metric to quantify overfitting.

Result: The smooth Leaky Exponential Linear Unit shows improved model performance compared to other activation functions, particularly in mitigating overfitting and sensitivity to parameters.

Conclusion: Smooth and well-designed activation functions positively affect large neural networks' performance, and the proposed methods offer concrete improvements over existing approaches.

Abstract: This document proposes a parametric activation function (ac.f.) aimed at
improving multidimensional nonlinear data regression. It is a established
knowledge that nonlinear ac.f.'s are required for learning nonlinear datasets.
This work shows that smoothness and gradient properties of the ac.f. further
impact the performance of large neural networks in terms of overfitting and
sensitivity to model parameters. Smooth but vanishing-gradient ac.f.'s such as
ELU or SiLU have limited performance and non-smooth ac.f.'s such as RELU and
Leaky-RELU further impart discontinuity in the trained model. Improved
performance is demonstrated with a smooth "Leaky Exponential Linear Unit", with
non-zero gradient that can be trained. A novel diffusion-loss metric is also
proposed to gauge the performance of the trained models in terms of
overfitting.

</details>


### [193] [Learning safe, constrained policies via imitation learning: Connection to Probabilistic Inference and a Naive Algorithm](https://arxiv.org/abs/2507.06780)
*George Papadopoulos,George A. Vouros*

Main category: cs.LG

TL;DR: The paper presents a method for imitation learning to optimize maximum entropy policies while adhering to constraints from expert demonstrations, validated via theoretical and experimental results.


<details>
  <summary>Details</summary>
Motivation: To develop a method for learning constrained policies from expert trajectories while optimizing entropy, connecting this to rigorous probabilistic reinforcement learning frameworks.

Method: Combines a probabilistic inference framework with reinforcement learning objectives, optimizing using dual gradient descent for stability and efficiency.

Result: The method learns constraint-compliant policies, performs well across diverse constraint settings, and shows generalization capabilities.

Conclusion: The approach successfully integrates entropy maximization with constraint adherence, offering a robust framework for imitation learning in complex scenarios.

Abstract: This article introduces an imitation learning method for learning maximum
entropy policies that comply with constraints demonstrated by expert
trajectories executing a task. The formulation of the method takes advantage of
results connecting performance to bounds for the KL-divergence between
demonstrated and learned policies, and its objective is rigorously justified
through a connection to a probabilistic inference framework for reinforcement
learning, incorporating the reinforcement learning objective and the objective
to abide by constraints in an entropy maximization setting. The proposed
algorithm optimizes the learning objective with dual gradient descent,
supporting effective and stable training. Experiments show that the proposed
method can learn effective policy models for constraints-abiding behaviour, in
settings with multiple constraints of different types, accommodating different
modalities of demonstrated behaviour, and with abilities to generalize.

</details>


### [194] [Speech Tokenizer is Key to Consistent Representation](https://arxiv.org/abs/2507.06802)
*Wonjin Jung,Sungil Kang,Dong-Yeon Cho*

Main category: cs.LG

TL;DR: The paper introduces a novel speech tokenizer that captures both linguistic and acoustic information, improving speech processing across multiple applications.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from limitations in recent speech tokenization approaches, which focus on semantic content but fail to adequately encode critical acoustic features like prosody and emotion.

Method: The authors propose an advanced tokenization method that encodes both linguistic and acoustic information, ensuring high-quality speech representation while maintaining prosodic and emotional content.

Result: The proposed tokenizer demonstrates effectiveness in diverse tasks, including speech coding, voice conversion, emotion recognition, and multimodal language modeling, without requiring task-specific training.

Conclusion: This novel approach provides a versatile and impactful tool for AI-driven speech processing, enhancing its fidelity and applicability across different domains.

Abstract: Speech tokenization is crucial in digital speech processing, converting
continuous speech signals into discrete units for various computational tasks.
This paper introduces a novel speech tokenizer with broad applicability across
downstream tasks. While recent advances in residual vector quantization (RVQ)
have incorporated semantic elements, they often neglect critical acoustic
features. We propose an advanced approach that simultaneously encodes both
linguistic and acoustic information, preserving prosodic and emotional content.
Our method significantly enhances speech representation fidelity across diverse
applications. Empirical evaluations demonstrate its effectiveness in speech
coding, voice conversion, emotion recognition, and multimodal language
modeling, without requiring additional training. This versatility underscores
its potential as a key tool for advancing AI-driven speech processing.

</details>


### [195] [Intrinsic Training Signals for Federated Learning Aggregation](https://arxiv.org/abs/2507.06813)
*Cosimo Fiorini,Matteo Mosconi,Pietro Buzzega,Riccardo Salami,Simone Calderara*

Main category: cs.LG

TL;DR: LIVAR is a federated learning (FL) method leveraging existing training signals for model training and merging, offering state-of-the-art results without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Improve efficiency and effectiveness in federated model aggregation without requiring modifications to network architectures or loss functions.

Method: Introduces two techniques: (i) Variance-weighted classifier aggregation using feature statistics, (ii) LoRA merging guided by SHAP-based explainability applied to update patterns.

Result: Achieves state-of-the-art performance across benchmarks while enabling seamless integration with current FL methods.

Conclusion: LIVAR sets a new standard for efficient FL model merging with no added architectural complexity, relying solely on existing training signals for its operations.

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients while preserving data privacy. While existing approaches
for aggregating client-specific classification heads and adapted backbone
parameters require architectural modifications or loss function changes, our
method uniquely leverages intrinsic training signals already available during
standard optimization. We present LIVAR (Layer Importance and VARiance-based
merging), which introduces: i) a variance-weighted classifier aggregation
scheme using naturally emergent feature statistics, and ii) an
explainability-driven LoRA merging technique based on SHAP analysis of existing
update parameter patterns. Without any architectural overhead, LIVAR achieves
state-of-the-art performance on multiple benchmarks while maintaining seamless
integration with existing FL methods. This work demonstrates that effective
model merging can be achieved solely through existing training signals,
establishing a new paradigm for efficient federated model aggregation. The code
will be made publicly available upon acceptance.

</details>


### [196] [Comprehensive Evaluation of Prototype Neural Networks](https://arxiv.org/abs/2507.06819)
*Philipp Schlinge,Steffen Meinert,Martin Atzmueller*

Main category: cs.LG

TL;DR: This paper analyzes prominent prototype models like ProtoPNet and PIPNet using both existing and new interpretability metrics, tested across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Understanding and improving the interpretability and effectiveness of prototype models for explainable AI and interpretable machine learning.

Method: The authors assess prototype models using a comprehensive metric set, extend this evaluation with new proposed metrics, and test the models on varied datasets including fine-grained and multi-label classification.

Result: Prototype models showed varying performance across diverse datasets, and the newly proposed metrics provided additional insights into model interpretability.

Conclusion: The paper enhances the evaluation of prototype models for XAI, providing an open-source library for applying and extending interpretability metrics.

Abstract: Prototype models are an important method for explainable artificial
intelligence (XAI) and interpretable machine learning. In this paper, we
perform an in-depth analysis of a set of prominent prototype models including
ProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive
set of metrics. In addition to applying standard metrics from literature, we
propose several new metrics to further complement the analysis of model
interpretability. In our experimentation, we apply the set of prototype models
on a diverse set of datasets including fine-grained classification, Non-IID
settings and multi-label classification to further contrast the performance.
Furthermore, we also provide our code as an open-source library, which
facilitates simple application of the metrics itself, as well as extensibility
- providing the option for easily adding new metrics and models.
https://github.com/uos-sis/quanproto

</details>


### [197] [HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for Emotion Distribution Learning](https://arxiv.org/abs/2507.06821)
*Chuhang Zheng,Chunwei Tian,Jie Wen,Daoqiang Zhang,Qi Zhu*

Main category: cs.LG

TL;DR: This paper introduces a multi-modal emotion distribution learning framework, HeLo, to better mine the heterogeneity in multi-modal emotional data and exploit correlation among mixed emotions.


<details>
  <summary>Details</summary>
Motivation: Current methods for multi-modal emotion distribution learning struggle with mining data heterogeneity across modalities and fail to fully exploit semantic correlations between emotions.

Method: HeLo uses cross-attention for data fusion, an optimal transport module to analyze heterogeneity between physiological and behavioral data, and a learnable label embedding approach optimized by correlation matrix alignment for improved emotion recognition.

Result: Experiments on two public datasets show that HeLo delivers superior performance in emotion distribution learning compared to existing methods.

Conclusion: The proposed HeLo framework effectively addresses challenges in multi-modal emotion recognition by leveraging cross-modal data fusion and exploring label correlations for improved identification of mixed emotions.

Abstract: Multi-modal emotion recognition has garnered increasing attention as it plays
a significant role in human-computer interaction (HCI) in recent years. Since
different discrete emotions may exist at the same time, compared with
single-class emotion recognition, emotion distribution learning (EDL) that
identifies a mixture of basic emotions has gradually emerged as a trend.
However, existing EDL methods face challenges in mining the heterogeneity among
multiple modalities. Besides, rich semantic correlations across arbitrary basic
emotions are not fully exploited. In this paper, we propose a multi-modal
emotion distribution learning framework, named HeLo, aimed at fully exploring
the heterogeneity and complementary information in multi-modal emotional data
and label correlation within mixed basic emotions. Specifically, we first adopt
cross-attention to effectively fuse the physiological data. Then, an optimal
transport (OT)-based heterogeneity mining module is devised to mine the
interaction and heterogeneity between the physiological and behavioral
representations. To facilitate label correlation learning, we introduce a
learnable label embedding optimized by correlation matrix alignment. Finally,
the learnable label embeddings and label correlation matrices are integrated
with the multi-modal representations through a novel label correlation-driven
cross-attention mechanism for accurate emotion distribution learning.
Experimental results on two publicly available datasets demonstrate the
superiority of our proposed method in emotion distribution learning.

</details>


### [198] [Artificial Generals Intelligence: Mastering Generals.io with Reinforcement Learning](https://arxiv.org/abs/2507.06825)
*Matej Straka,Martin Schmid*

Main category: cs.LG

TL;DR: The paper presents a real-time strategy (RTS) game benchmark based on Generals.io, offering high performance and compatibility with popular AI libraries, along with a state-of-the-art learning agent.


<details>
  <summary>Details</summary>
Motivation: To create a challenging yet accessible environment for advancing research in multi-agent reinforcement learning, utilizing the Generals.io game framework.

Method: The authors integrated Generals.io with Gymnasium and PettingZoo for scalability and created a competitive agent using supervised pre-training, self-play, potential-based reward shaping, and memory features.

Result: The reference agent achieved a top 0.003% ranking on the 1v1 human leaderboard within 36 hours of training with an H100 GPU.

Conclusion: The presented RTS benchmark and high-performing agent provide an effective platform for testing and advancing multi-agent reinforcement learning techniques.

Abstract: We introduce a real-time strategy game environment built on Generals.io, a
game that hosts thousands of active players each week across multiple game
formats. Our environment is fully compatible with Gymnasium and PettingZoo,
capable of running thousands of frames per second on commodity hardware. Our
reference agent -- trained with supervised pre-training and self-play -- hits
the top 0.003\% of the 1v1 human leaderboard after just 36 hours on a single
H100 GPU. To accelerate learning, we incorporate potential-based reward shaping
and memory features. Our contributions -- a modular RTS benchmark and a
competitive, state-of-the-art baseline agent -- provide an accessible yet
challenging platform for advancing multi-agent reinforcement learning research.

</details>


### [199] [DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models](https://arxiv.org/abs/2507.06853)
*Liang Wang,Yu Rong,Tingyang Xu,Zhenyi Zhong,Zhiyuan Liu,Pengju Wang,Deli Zhao,Qiang Liu,Shu Wu,Liang Wang*

Main category: cs.LG

TL;DR: DiffSpectra is a novel framework that uses diffusion models to directly infer both 2D and 3D molecular structures from multi-modal spectral data with high accuracy, offering significant advancements over traditional and existing AI-based approaches.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the limitations of traditional expert-based and retrieval-based methods in molecular structure elucidation, which lack scalability and fail to generalize to novel molecules, while addressing the shortcomings of existing machine learning models that do not fully account for 3D geometry or diverse spectral modalities.

Method: The paper introduces DiffSpectra, a generative framework that uses diffusion models to formulate structure elucidation as a conditional generation task. It employs a SE(3)-equivariant architecture, Diffusion Molecule Transformer, to capture topological and geometric information, and a transformer-based spectral encoder, SpecFormer, to integrate multi-modal spectral dependencies.

Result: DiffSpectra achieved a top-1 accuracy of 16.01% and a top-20 accuracy of 96.86% in recovering exact molecular structures through sampling. The success is significantly attributed to 3D geometric modeling, SpecFormer pre-training, and multi-modal spectral conditioning.

Conclusion: DiffSpectra demonstrates that spectrum-conditioned diffusion modeling has strong potential for molecular structure elucidation, offering accuracy and flexibility that surpass previous techniques. This framework represents a significant step forward by unifying multi-modal spectral reasoning and joint 2D/3D generative modeling for de novo molecular structure prediction.

Abstract: Molecular structure elucidation from spectra is a foundational problem in
chemistry, with profound implications for compound identification, synthesis,
and drug development. Traditional methods rely heavily on expert interpretation
and lack scalability. Pioneering machine learning methods have introduced
retrieval-based strategies, but their reliance on finite libraries limits
generalization to novel molecules. Generative models offer a promising
alternative, yet most adopt autoregressive SMILES-based architectures that
overlook 3D geometry and struggle to integrate diverse spectral modalities. In
this work, we present DiffSpectra, a generative framework that directly infers
both 2D and 3D molecular structures from multi-modal spectral data using
diffusion models. DiffSpectra formulates structure elucidation as a conditional
generation process. Its denoising network is parameterized by Diffusion
Molecule Transformer, an SE(3)-equivariant architecture that integrates
topological and geometric information. Conditioning is provided by SpecFormer,
a transformer-based spectral encoder that captures intra- and inter-spectral
dependencies from multi-modal spectra. Extensive experiments demonstrate that
DiffSpectra achieves high accuracy in structure elucidation, recovering exact
structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through
sampling. The model benefits significantly from 3D geometric modeling,
SpecFormer pre-training, and multi-modal conditioning. These results highlight
the effectiveness of spectrum-conditioned diffusion modeling in addressing the
challenge of molecular structure elucidation. To our knowledge, DiffSpectra is
the first framework to unify multi-modal spectral reasoning and joint 2D/3D
generative modeling for de novo molecular structure elucidation.

</details>


### [200] [Episodic Contextual Bandits with Knapsacks under Conversion Models](https://arxiv.org/abs/2507.06859)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: The paper addresses an online decision-making problem involving contextual bandit-with-knapsack setups across repeated episodes with non-stationary contexts, proposing a novel algorithm with sub-linear regret.


<details>
  <summary>Details</summary>
Motivation: To improve decision-making strategies in dynamic applications like pricing and auctions, where resources and contexts change episodically, aiming for efficiency in settings with complex and unbounded state spaces.

Method: The authors design an online algorithm leveraging a confidence bound oracle to achieve sub-linear regret, overcoming challenges like non-stationarity and unlimited contexts.

Result: The proposed algorithm demonstrates improved regret performance, especially when unlabeled feature data is available, filling a novel gap in the contextual BwK literature.

Conclusion: This work enhances the theoretical foundations for episodic decision-making frameworks by reducing regret in challenging scenarios and broadening applicability to dynamic resource environments.

Abstract: We study an online setting, where a decision maker (DM) interacts with
contextual bandit-with-knapsack (BwK) instances in repeated episodes. These
episodes start with different resource amounts, and the contexts' probability
distributions are non-stationary in an episode. All episodes share the same
latent conversion model, which governs the random outcome contingent upon a
request's context and an allocation decision. Our model captures applications
such as dynamic pricing on perishable resources with episodic replenishment,
and first price auctions in repeated episodes with different starting budgets.
We design an online algorithm that achieves a regret sub-linear in $T$, the
number of episodes, assuming access to a \emph{confidence bound oracle} that
achieves an $o(T)$-regret. Such an oracle is readily available from existing
contextual bandit literature. We overcome the technical challenge with
arbitrarily many possible contexts, which leads to a reinforcement learning
problem with an unbounded state space. Our framework provides improved regret
bounds in certain settings when the DM is provided with unlabeled feature data,
which is novel to the contextual BwK literature.

</details>


### [201] [Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model](https://arxiv.org/abs/2507.06892)
*Jing Liang,Hongyao Tang,Yi Ma,Jinyi Liu,Yan Zheng,Shuyue Hu,Lei Bai,Jianye Hao*

Main category: cs.LG

TL;DR: This paper introduces ReMix, a method to enhance Large Language Models (LLMs) using off-policy RL for reduced computation costs, achieving efficient training and superior performance in math reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing Reinforcement Finetuning (RFT) methods are predominantly on-policy RL, which limits their utilization of past data, resulting in high computation and time costs.

Method: ReMix incorporates three components: Mix-policy proximal policy gradient for higher Update-To-Data ratio, KL-Convex policy constraint for balancing stability and flexibility, and Policy reincarnation for transitioning from early-stage learning to steady improvement.

Result: Experimental evaluation demonstrates SOTA-level performance across math reasoning benchmarks, while dramatically reducing training costs by 30x to 450x compared to prior models.

Conclusion: ReMix effectively combines on-policy and off-policy RL methods to address computational inefficiency and achieves both economic and high-performance scaling in training LLMs.

Abstract: Reinforcement Learning (RL) has demonstrated its potential to improve the
reasoning ability of Large Language Models (LLMs). One major limitation of most
existing Reinforcement Finetuning (RFT) methods is that they are on-policy RL
in nature, i.e., data generated during the past learning process is not fully
utilized. This inevitably comes at a significant cost of compute and time,
posing a stringent bottleneck on continuing economic and efficient scaling. To
this end, we launch the renaissance of off-policy RL and propose Reincarnating
Mix-policy Proximal Policy Gradient (ReMix), a general approach to enable
on-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix
consists of three major components: (1) Mix-policy proximal policy gradient
with an increased Update-To-Data (UTD) ratio for efficient training; (2)
KL-Convex policy constraint to balance the trade-off between stability and
flexibility; (3) Policy reincarnation to achieve a seamless transition from
efficient early-stage learning to steady asymptotic improvement. In our
experiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base
models. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with
0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B
model) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math
reasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and
MATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level
performance with an over 30x to 450x reduction in training cost in terms of
rollout data volume. In addition, we reveal insightful findings via
multifaceted analysis, including the implicit preference for shorter responses
due to the Whipping Effect of off-policy discrepancy, the collapse mode of
self-reflection behavior under the presence of severe off-policyness, etc.

</details>


### [202] [Horizontal and Vertical Federated Causal Structure Learning via Higher-order Cumulants](https://arxiv.org/abs/2507.06888)
*Wei Chen,Wanyang Gu,Linjun Peng,Ruichu Cai,Zhifeng Hao,Kun Zhang*

Main category: cs.LG

TL;DR: The paper introduces an algorithm for federated causal discovery addressing horizontal and vertical federated settings using higher-order cumulants.


<details>
  <summary>Details</summary>
Motivation: Common federated causal discovery methods mainly focus on horizontal federated settings and fail in real-world scenarios when clients contain distinct variable sets, leading to spurious causal links.

Method: The authors propose using higher-order cumulants to aggregate information across clients and construct global cumulant estimates for recursive causal structure identification.

Result: The algorithm effectively reconstructs causal graphs and estimates causal strengths, achieving superior experimental results on synthetic and real-world data.

Conclusion: The proposed method advances federated causal discovery by addressing incomplete variable sets in clients and demonstrates efficacy in generating accurate causal relationships.

Abstract: Federated causal discovery aims to uncover the causal relationships between
entities while protecting data privacy, which has significant importance and
numerous applications in real-world scenarios. Existing federated causal
structure learning methods primarily focus on horizontal federated settings.
However, in practical situations, different clients may not necessarily contain
data on the same variables. In a single client, the incomplete set of variables
can easily lead to spurious causal relationships, thereby affecting the
information transmitted to other clients. To address this issue, we
comprehensively consider causal structure learning methods under both
horizontal and vertical federated settings. We provide the identification
theories and methods for learning causal structure in the horizontal and
vertical federal setting via higher-order cumulants. Specifically, we first
aggregate higher-order cumulant information from all participating clients to
construct global cumulant estimates. These global estimates are then used for
recursive source identification, ultimately yielding a global causal strength
matrix. Our approach not only enables the reconstruction of causal graphs but
also facilitates the estimation of causal strength coefficients. Our algorithm
demonstrates superior performance in experiments conducted on both synthetic
data and real-world data.

</details>


### [203] [Designing Adaptive Algorithms Based on Reinforcement Learning for Dynamic Optimization of Sliding Window Size in Multi-Dimensional Data Streams](https://arxiv.org/abs/2507.06901)
*Abolfazl Zarghani,Sadegh Abedi*

Main category: cs.LG

TL;DR: This paper introduces RL-Window, a reinforcement learning-based method for dynamically optimizing sliding window sizes in multi-dimensional data streams, outperforming traditional methods in accuracy, robustness, and efficiency.


<details>
  <summary>Details</summary>
Motivation: The study addresses challenges in processing multi-dimensional data streams such as high velocity, unbounded nature, and dynamic changes like concept drift or bursty patterns, where traditional fixed-size sliding window techniques fall short.

Method: The proposed method, RL-Window, uses a reinforcement learning framework that applies a Dueling Deep Q-Network (DQN) with prioritized experience replay, allowing for adaptive adjustment of sliding window sizes based on stream characteristics.

Result: Experiments on benchmark datasets demonstrate that RL-Window achieves superior performance in classification accuracy, drift robustness, and computational efficiency compared to state-of-the-art methods like ADWIN and CNN-Adaptive.

Conclusion: RL-Window is an adaptable and robust solution for managing sliding window sizes in real-time multi-dimensional data streams, making it suitable for diverse applications like IoT and real-time analytics.

Abstract: Multi-dimensional data streams, prevalent in applications like IoT, financial
markets, and real-time analytics, pose significant challenges due to their high
velocity, unbounded nature, and complex inter-dimensional dependencies. Sliding
window techniques are critical for processing such streams, but fixed-size
windows struggle to adapt to dynamic changes like concept drift or bursty
patterns. This paper proposes a novel reinforcement learning (RL)-based
approach to dynamically optimize sliding window sizes for multi-dimensional
data streams. By formulating window size selection as an RL problem, we enable
an agent to learn an adaptive policy based on stream characteristics, such as
variance, correlations, and temporal trends. Our method, RL-Window, leverages a
Dueling Deep Q-Network (DQN) with prioritized experience replay to handle
non-stationarity and high-dimensionality. Evaluations on benchmark datasets
(UCI HAR, PAMAP2, Yahoo! Finance Stream) demonstrate that RL-Window outperforms
state-of-the-art methods like ADWIN and CNN-Adaptive in classification
accuracy, drift robustness, and computational efficiency. Additional
qualitative analyses, extended metrics (e.g., energy efficiency, latency), and
a comprehensive dataset characterization further highlight its adaptability and
stability, making it suitable for real-time applications.

</details>


### [204] [What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models](https://arxiv.org/abs/2507.06952)
*Keyon Vafa,Peter G. Chang,Ashesh Rambachan,Sendhil Mullainathan*

Main category: cs.LG

TL;DR: The paper evaluates foundation models through a technique called inductive bias probes and finds that they succeed in training tasks but fail to generalize to deeper domain understanding.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address whether foundation models truly capture domain understanding beyond their training tasks, reflecting deeper structural insights like Newtonian mechanics from observational data.

Method: The authors introduce inductive bias probes, evaluating foundation models on synthetic datasets derived from postulated world models to assess alignment with deeper inductive biases.

Result: Foundation models excel at their training tasks but fail to develop inductive biases towards underlying world models. Models trained on orbital trajectories fail to apply Newtonian mechanics in new tasks.

Conclusion: Foundation models tend to develop task-specific heuristics rather than generalized domain understanding, demonstrating limitations in adapting to new tasks that require deeper structural reasoning.

Abstract: Foundation models are premised on the idea that sequence prediction can
uncover deeper domain understanding, much like how Kepler's predictions of
planetary motion later led to the discovery of Newtonian mechanics. However,
evaluating whether these models truly capture deeper structure remains a
challenge. We develop a technique for evaluating foundation models that
examines how they adapt to synthetic datasets generated from some postulated
world model. Our technique measures whether the foundation model's inductive
bias aligns with the world model, and so we refer to it as an inductive bias
probe. Across multiple domains, we find that foundation models can excel at
their training tasks yet fail to develop inductive biases towards the
underlying world model when adapted to new tasks. We particularly find that
foundation models trained on orbital trajectories consistently fail to apply
Newtonian mechanics when adapted to new physics tasks. Further analysis reveals
that these models behave as if they develop task-specific heuristics that fail
to generalize.

</details>


### [205] [Noisy PDE Training Requires Bigger PINNs](https://arxiv.org/abs/2507.06967)
*Sebastien Andre-Sloan,Anirbit Mukherjee,Matthew Colbrook*

Main category: cs.LG

TL;DR: The paper investigates the conditions under which Physics-Informed Neural Networks (PINNs) can effectively minimize empirical risk in the presence of noisy data, providing a lower bound on network size and showing empirical validation.


<details>
  <summary>Details</summary>
Motivation: The study seeks to address the lack of theoretical understanding about when PINNs can effectively achieve low empirical risk in scenarios with noisy supervision data.

Method: The authors derive theoretical bounds on the size of PINNs needed to minimize empirical risk below the data's noise variance and provide empirical evidence to support their findings, with a case study on the Hamilton-Jacobi-Bellman PDE.

Result: A lower bound on the neural network size required for PINNs to achieve low empirical risk was proposed and validated empirically, demonstrating that merely increasing noisy labels does not always reduce risk.

Conclusion: The work highlights the importance of adequate network parameterization in noisy data scenarios and lays a foundation for understanding the training of PINNs under such conditions.

Abstract: Physics-Informed Neural Networks (PINNs) are increasingly used to approximate
solutions of partial differential equations (PDEs), especially in high
dimensions. In real-world applications, data samples are noisy, so it is
important to know when a predictor can still achieve low empirical risk.
However, little is known about the conditions under which a PINN can do so
effectively. We prove a lower bound on the size of neural networks required for
the supervised PINN empirical risk to fall below the variance of noisy
supervision labels. Specifically, if a predictor achieves an empirical risk
$O(\eta)$ below $\sigma^2$ (variance of supervision data), then necessarily
$d_N\log d_N\gtrsim N_s \eta^2$, where $N_s$ is the number of samples and $d_N$
is the number of trainable parameters of the PINN. A similar constraint applies
to the fully unsupervised PINN setting when boundary labels are sampled
noisily. Consequently, increasing the number of noisy supervision labels alone
does not provide a ``free lunch'' in reducing empirical risk. We also show
empirically that PINNs can indeed achieve empirical risks below $\sigma^2$
under such conditions. As a case study, we investigate PINNs applied to the
Hamilton--Jacobi--Bellman (HJB) PDE. Our findings lay the groundwork for
quantitatively understanding the parameter requirements for training PINNs in
the presence of noise.

</details>


### [206] [A Principled Framework for Multi-View Contrastive Learning](https://arxiv.org/abs/2507.06979)
*Panagiotis Koromilas,Efthymios Georgiou,Giorgos Bouritsas,Theodoros Giannakopoulos,Mihalis A. Nicolaou,Yannis Panagakis*

Main category: cs.LG

TL;DR: This paper introduces two novel loss functions (MV-InfoNCE and MV-DHEL) to tackle limitations in contrastive learning when dealing with multiple data views.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing contrastive learning methods when handling multiple augmented views, such as conflicting objectives, lack of interaction modeling, and challenges in scalability and generalization.

Method: The paper proposes MV-InfoNCE, extending InfoNCE to consider all view interactions in a single term, and MV-DHEL, which decouples alignment and uniformity while scaling interactions for increased views.

Result: Empirical tests on datasets like ImageNet1K show superior performance for these methods, effectively scaling with view multiplicity and supporting applications in multimodal data.

Conclusion: The proposed loss functions address critical limitations of existing methods, providing scalable and theoretically grounded solutions that enhance multi-view contrastive learning and mitigate dimensionality collapse.

Abstract: Contrastive Learning (CL), a leading paradigm in Self-Supervised Learning
(SSL), typically relies on pairs of data views generated through augmentation.
While multiple augmentations per instance (more than two) improve
generalization in supervised learning, current CL methods handle additional
views suboptimally by simply aggregating different pairwise objectives. This
approach suffers from four critical limitations: (L1) it utilizes multiple
optimization terms per data point resulting to conflicting objectives, (L2) it
fails to model all interactions across views and data points, (L3) it inherits
fundamental limitations (e.g. alignment-uniformity coupling) from pairwise CL
losses, and (L4) it prevents fully realizing the benefits of increased view
multiplicity observed in supervised settings. We address these limitations
through two novel loss functions: MV-InfoNCE, which extends InfoNCE to
incorporate all possible view interactions simultaneously in one term per data
point, and MV-DHEL, which decouples alignment from uniformity across views
while scaling interaction complexity with view multiplicity. Both approaches
are theoretically grounded - we prove they asymptotically optimize for
alignment of all views and uniformity, providing principled extensions to
multi-view contrastive learning. Our empirical results on ImageNet1K and three
other datasets demonstrate that our methods consistently outperform existing
multi-view approaches and effectively scale with increasing view multiplicity.
We also apply our objectives to multimodal data and show that, in contrast to
other contrastive objectives, they can scale beyond just two modalities. Most
significantly, ablation studies reveal that MV-DHEL with five or more views
effectively mitigates dimensionality collapse by fully utilizing the embedding
space, thereby delivering multi-view benefits observed in supervised learning.

</details>


### [207] [Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing](https://arxiv.org/abs/2507.06996)
*Eunbyeol Cho,Jiyoun Kim,Minjae Lee,Sungjin Park,Edward Choi*

Main category: cs.LG

TL;DR: The paper introduces RawMed, a framework for generating realistic multi-table, time-series synthetic electronic health record (EHR) datasets, outperforming previous methods in fidelity and utility.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address privacy and regulatory issues in utilizing sensitive electronic health record (EHR) data by creating synthetic datasets that mirror the complexity of real EHR data.

Method: RawMed employs text-based representation and compression techniques to synthesize multi-table, time-series EHR data while preserving complex structures and temporal dynamics. Additionally, it introduces an evaluation framework for assessing distributional similarity, inter-table relationships, temporal dynamics, and privacy.

Result: RawMed demonstrates superior performance compared to baseline models in terms of fidelity and utility, as validated on two open-source EHR datasets.

Conclusion: RawMed sets a new standard for synthetic EHR data generation, providing a practical and more realistic alternative for healthcare research while addressing critical privacy concerns.

Abstract: Electronic Health Records (EHR) are time-series relational databases that
record patient interactions and medical events over time, serving as a critical
resource for healthcare research and applications. However, privacy concerns
and regulatory restrictions limit the sharing and utilization of such sensitive
data, necessitating the generation of synthetic EHR datasets. Unlike previous
EHR synthesis methods, which typically generate medical records consisting of
expert-chosen features (e.g. a few vital signs or structured codes only), we
introduce RawMed, the first framework to synthesize multi-table, time-series
EHR data that closely resembles raw EHRs. Using text-based representation and
compression techniques, RawMed captures complex structures and temporal
dynamics with minimal preprocessing. We also propose a new evaluation framework
for multi-table time-series synthetic EHRs, assessing distributional
similarity, inter-table relationships, temporal dynamics, and privacy.
Validated on two open-source EHR datasets, RawMed outperforms baseline models
in fidelity and utility. The code is available at
https://github.com/eunbyeol-cho/RawMed.

</details>


### [208] [Exact Evaluation of the Accuracy of Diffusion Models for Inverse Problems with Gaussian Data Distributions](https://arxiv.org/abs/2507.07008)
*Emile Pierret,Bruno Galerne*

Main category: cs.LG

TL;DR: The paper examines the accuracy of diffusion models as Bayesian priors for deblurring tasks, analyzing the gap between theoretical and actual performance using Wasserstein distance.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address uncertainties surrounding the performance of diffusion models in solving inverse problems like deblurring.

Method: They investigate the Gaussian data distribution in the context of deblurring, using Wasserstein distance to measure discrepancies between the ideal and actual solutions provided by diffusion models.

Result: The study quantifies the accuracy of diffusion models, enabling a comparative analysis of various algorithms.

Conclusion: Diffusion models as priors allow for performance analysis in inverse problems, but understanding the discrepancy with theoretical resolutions is key for improvement and comparison.

Abstract: Used as priors for Bayesian inverse problems, diffusion models have recently
attracted considerable attention in the literature. Their flexibility and high
variance enable them to generate multiple solutions for a given task, such as
inpainting, super-resolution, and deblurring. However, several unresolved
questions remain about how well they perform. In this article, we investigate
the accuracy of these models when applied to a Gaussian data distribution for
deblurring. Within this constrained context, we are able to precisely analyze
the discrepancy between the theoretical resolution of inverse problems and
their resolution obtained using diffusion models by computing the exact
Wasserstein distance between the distribution of the diffusion model sampler
and the ideal distribution of solutions to the inverse problem. Our findings
allow for the comparison of different algorithms from the literature.

</details>


### [209] [On-Device Training of PV Power Forecasting Models in a Smart Meter for Grid Edge Intelligence](https://arxiv.org/abs/2507.07016)
*Jian Huang,Yongli Zhu,Linna Xu,Zhe Zheng,Wenpeng Cui,Mingyang Sun*

Main category: cs.LG

TL;DR: This paper examines on-device machine learning model training for smart meters with limited resources, focusing on photovoltaic power forecasting.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need for grid-edge intelligence and the benefits of on-device model training for enhancing smart meter functionality.

Method: The paper introduces key technical steps for on-device training and explores two machine learning models (gradient boosting tree and recurrent neural network) with mixed- and reduced-precision training schemes tailored for resource-limited devices.

Result: Experimental results show that it is feasible and cost-effective to implement grid-edge intelligence on current advanced metering infrastructures.

Conclusion: On-device model training is a practical and economic solution for achieving intelligent functionalities in resource-limited grid-edge devices like smart meters.

Abstract: In this paper, an edge-side model training study is conducted on a
resource-limited smart meter. The motivation of grid-edge intelligence and the
concept of on-device training are introduced. Then, the technical preparation
steps for on-device training are described. A case study on the task of
photovoltaic power forecasting is presented, where two representative machine
learning models are investigated: a gradient boosting tree model and a
recurrent neural network model. To adapt to the resource-limited situation in
the smart meter, "mixed"- and "reduced"-precision training schemes are also
devised. Experiment results demonstrate the feasibility of economically
achieving grid-edge intelligence via the existing advanced metering
infrastructures.

</details>


### [210] [PLAME: Leveraging Pretrained Language Models to Generate Enhanced Protein Multiple Sequence Alignments](https://arxiv.org/abs/2507.07032)
*Hanqun Cao,Xinyi Zhou,Zijun Gao,Chenyu Wang,Xin Gao,Zhi Zhang,Chunbin Gu,Ge Liu,Pheng-Ann Heng*

Main category: cs.LG

TL;DR: PLAME is a new method using protein language model embeddings to improve protein structure prediction, especially for low-homology and orphan proteins.


<details>
  <summary>Details</summary>
Motivation: Traditional protein structure prediction models rely heavily on multiple sequence alignments (MSAs), which are ineffective for low-homology proteins and orphan proteins due to sparse evolutionary information.

Method: PLAME incorporates pretrained evolutionary embeddings, a conservation-diversity loss for improving MSA quality, a novel MSA screening method, and a sequence quality metric for evaluation of MSAs.

Result: PLAME demonstrated state-of-the-art performance on AlphaFold2 benchmarks for low-homology proteins, and consistent improvement was shown on AlphaFold3. Validation and case studies further support its effectiveness.

Conclusion: PLAME addresses limitations of MSA-dependent predictions, enhancing protein folding quality for challenging cases and providing a bridge to high speed and accuracy similar to ESMFold and AlphaFold2.

Abstract: Protein structure prediction is essential for drug discovery and
understanding biological functions. While recent advancements like AlphaFold
have achieved remarkable accuracy, most folding models rely heavily on multiple
sequence alignments (MSAs) to boost prediction performance. This dependency
limits their effectiveness on low-homology proteins and orphan proteins, where
MSA information is sparse or unavailable. To address this limitation, we
propose PLAME, a novel MSA design model that leverages evolutionary embeddings
from pretrained protein language models. Unlike existing methods, PLAME
introduces pretrained representations to enhance evolutionary information and
employs a conservation-diversity loss to enhance generation quality.
Additionally, we propose a novel MSA selection method to effectively screen
high-quality MSAs and improve folding performance. We also propose a sequence
quality assessment metric that provides an orthogonal perspective to evaluate
MSA quality. On the AlphaFold2 benchmark of low-homology and orphan proteins,
PLAME achieves state-of-the-art performance in folding enhancement and sequence
quality assessment, with consistent improvements demonstrated on AlphaFold3.
Ablation studies validate the effectiveness of the MSA selection method, while
extensive case studies on various protein types provide insights into the
relationship between AlphaFold's prediction quality and MSA characteristics.
Furthermore, we demonstrate that PLAME can serve as an adapter achieving
AlphaFold2-level accuracy with the ESMFold's inference speed.

</details>


### [211] [Self-Supervised Learning at the Edge: The Cost of Labeling](https://arxiv.org/abs/2507.07033)
*Roberto Pereira,Fernanda Famá,Asal Rangrazi,Marco Miozzo,Charalampos Kalalas,Paolo Dini*

Main category: cs.LG

TL;DR: This paper explores energy-efficient contrastive learning techniques tailored for resource-constrained edge devices, achieving competitive performance with up to 4X reduced resource consumption.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of deploying contrastive and self-supervised learning methods, which often require significant data and computational resources, on energy and resource-limited edge devices.

Method: The authors evaluate contrastive and self-supervised learning techniques under limited computational, data, and energy budgets, exploring performance-energy trade-offs. Semi-supervised learning is also considered to minimize energy costs for data labeling.

Result: Tailored SSL strategies demonstrated the potential to achieve robust representation learning with reduced energy and resource consumption by up to 4X, maintaining competitive performance.

Conclusion: Self-supervised and contrastive learning methods can be adapted for resource-constrained edge devices, offering significant efficiency gains without compromising performance.

Abstract: Contrastive learning (CL) has recently emerged as an alternative to
traditional supervised machine learning solutions by enabling rich
representations from unstructured and unlabeled data. However, CL and, more
broadly, self-supervised learning (SSL) methods often demand a large amount of
data and computational resources, posing challenges for deployment on
resource-constrained edge devices. In this work, we explore the feasibility and
efficiency of SSL techniques for edge-based learning, focusing on trade-offs
between model performance and energy efficiency. In particular, we analyze how
different SSL techniques adapt to limited computational, data, and energy
budgets, evaluating their effectiveness in learning robust representations
under resource-constrained settings. Moreover, we also consider the energy
costs involved in labeling data and assess how semi-supervised learning may
assist in reducing the overall energy consumed to train CL models. Through
extensive experiments, we demonstrate that tailored SSL strategies can achieve
competitive performance while reducing resource consumption by up to 4X,
underscoring their potential for energy-efficient learning at the edge.

</details>


### [212] [An Ensemble Embedding Approach for Improving Semantic Caching Performance in LLM-based Systems](https://arxiv.org/abs/2507.07061)
*Shervin Ghaffari,Zohre Bahranifard,Mohammad Akbari*

Main category: cs.LG

TL;DR: This paper proposes an ensemble embedding approach to improve semantic caching in large language models, showing significant gains in efficiency and accuracy compared to single-model frameworks.


<details>
  <summary>Details</summary>
Motivation: Existing semantic caching systems rely on single embedding models, which fail to adequately capture diverse semantic relationships in real-world queries.

Method: The approach involves combining multiple embedding models through a trained meta-encoder to better detect semantic similarity. Evaluations are conducted using the QQP dataset with metrics such as cache hit ratio, cache miss ratio, token savings, and response time.

Result: The ensemble approach achieves a 92% cache hit ratio and an 85% accuracy for rejecting non-equivalent queries, proving more effective than single-model systems.

Conclusion: Ensemble embeddings enhance semantic caching performance, delivering improved efficiency and accuracy in LLM systems with reduced computational overhead.

Abstract: Semantic caching enhances the efficiency of large language model (LLM)
systems by identifying semantically similar queries, storing responses once,
and serving them for subsequent equivalent requests. However, existing semantic
caching frameworks rely on single embedding models for query representation,
which limits their ability to capture the diverse semantic relationships
present in real-world query distributions. This paper presents an ensemble
embedding approach that combines multiple embedding models through a trained
meta-encoder to improve semantic similarity detection in LLM caching systems.
We evaluate our method using the Quora Question Pairs (QQP) dataset, measuring
cache hit ratios, cache miss ratios, token savings, and response times. Our
ensemble approach achieves a 92\% cache hit ratio for semantically equivalent
queries while maintaining an 85\% accuracy in correctly rejecting
non-equivalent queries as cache misses. These results demonstrate that ensemble
embedding methods significantly outperform single-model approaches in
distinguishing between semantically similar and dissimilar queries, leading to
more effective caching performance and reduced computational overhead in
LLM-based systems.

</details>


### [213] [Addressing Imbalanced Domain-Incremental Learning through Dual-Balance Collaborative Experts](https://arxiv.org/abs/2507.07100)
*Lan Li,Da-Wei Zhou,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.LG

TL;DR: The paper addresses challenges in Domain-Incremental Learning (DIL) with imbalanced data by introducing the Dual-Balance Collaborative Experts (DCE) framework, improving handling of class imbalance and distribution shifts.


<details>
  <summary>Details</summary>
Motivation: To tackle the limitations of existing DIL methods in handling intra-domain class imbalance and cross-domain class distribution shifts, which impact the adaptability and stability of models in continual learning environments.

Method: The proposed DCE framework comprises a frequency-aware expert group guided by distinct loss functions for frequency-specific feature learning, paired with a dynamic expert selector using balanced Gaussian sampling, to effectively address the trade-offs between preserving knowledge and adapting to new data.

Result: Experimental evaluations on four benchmark datasets verify that the DCE framework achieves state-of-the-art performance in addressing class imbalance and distribution shifts in DIL scenarios.

Conclusion: The DCE framework enhances the ability of models to learn effectively in dynamic and imbalanced environments, providing a robust solution to the challenges in DIL.

Abstract: Domain-Incremental Learning (DIL) focuses on continual learning in
non-stationary environments, requiring models to adjust to evolving domains
while preserving historical knowledge. DIL faces two critical challenges in the
context of imbalanced data: intra-domain class imbalance and cross-domain class
distribution shifts. These challenges significantly hinder model performance,
as intra-domain imbalance leads to underfitting of few-shot classes, while
cross-domain shifts require maintaining well-learned many-shot classes and
transferring knowledge to improve few-shot class performance in old domains. To
overcome these challenges, we introduce the Dual-Balance Collaborative Experts
(DCE) framework. DCE employs a frequency-aware expert group, where each expert
is guided by specialized loss functions to learn features for specific
frequency groups, effectively addressing intra-domain class imbalance.
Subsequently, a dynamic expert selector is learned by synthesizing
pseudo-features through balanced Gaussian sampling from historical class
statistics. This mechanism navigates the trade-off between preserving many-shot
knowledge of previous domains and leveraging new data to improve few-shot class
performance in earlier tasks. Extensive experimental results on four benchmark
datasets demonstrate DCE's state-of-the-art performance.

</details>


### [214] [Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful](https://arxiv.org/abs/2507.07101)
*Martin Marek,Sanae Lotfi,Aditya Somasundaram,Andrew Gordon Wilson,Micah Goldblum*

Main category: cs.LG

TL;DR: The work revisits small batch sizes for language model training, proposing a scaling rule for Adam hyperparameters, demonstrating stability and efficiency improvements with batch sizes as small as one.


<details>
  <summary>Details</summary>
Motivation: Current practices in language model training favor larger batch sizes, which often trade instability issues for increased computational demands. The researchers aim to explore and address these challenges with smaller batch sizes.

Method: The authors propose a rule for scaling Adam optimizer hyperparameters tailored for small batch sizes, test their framework with batch size reductions down to one and evaluate stability and performance.

Result: Small batch sizes offer enhanced stability and robustness, achieve comparable or superior per-FLOP performance, and enable simplified vanilla SGD training without momentum or optimizer state in some cases.

Conclusion: Small batch sizes are viable and effective for robust language model pretraining and fine-tuning, reducing reliance on gradient accumulation, and providing practical guidance for both single and distributed training setups.

Abstract: Conventional wisdom dictates that small batch sizes make language model
pretraining and fine-tuning unstable, motivating gradient accumulation, which
trades off the number of optimizer steps for a proportional increase in batch
size. While it is common to decrease the learning rate for smaller batch sizes,
other hyperparameters are often held fixed. In this work, we revisit small
batch sizes all the way down to batch size one, and we propose a rule for
scaling Adam hyperparameters to small batch sizes. We find that small batch
sizes (1) train stably, (2) are consistently more robust to hyperparameter
choices, (3) achieve equal or better per-FLOP performance than larger batch
sizes, and (4) notably enable stable language model training with vanilla SGD,
even without momentum, despite storing no optimizer state. Building on these
results, we provide practical recommendations for selecting a batch size and
setting optimizer hyperparameters. We further recommend against gradient
accumulation unless training on multiple devices with multiple model replicas,
bottlenecked by inter-device bandwidth.

</details>


### [215] [Does Data Scaling Lead to Visual Compositional Generalization?](https://arxiv.org/abs/2507.07102)
*Arnas Uselis,Andrea Dittadi,Seong Joon Oh*

Main category: cs.LG

TL;DR: Scaling data and model sizes alone does not guarantee compositional generalization; diversity in data is key to compositional understanding in vision models.


<details>
  <summary>Details</summary>
Motivation: To assess whether contemporary vision models can achieve compositional understanding and how data scaling impacts this capability.

Method: Conducted controlled experiments that varied data scale, concept diversity, and combination coverage to analyze their impact on compositional generalization.

Result: Found that compositional generalization is driven by data diversity and combinatorial coverage, not merely by data scale. Representational structure enables perfect generalization from fewer observed combinations.

Conclusion: Highlighting the importance of diverse datasets and representational structure to improve compositional generalization in vision models.

Abstract: Compositional understanding is crucial for human intelligence, yet it remains
unclear whether contemporary vision models exhibit it. The dominant machine
learning paradigm is built on the premise that scaling data and model sizes
will improve out-of-distribution performance, including compositional
generalization. We test this premise through controlled experiments that
systematically vary data scale, concept diversity, and combination coverage. We
find that compositional generalization is driven by data diversity, not mere
data scale. Increased combinatorial coverage forces models to discover a
linearly factored representational structure, where concepts decompose into
additive components. We prove this structure is key to efficiency, enabling
perfect generalization from few observed combinations. Evaluating pretrained
models (DINO, CLIP), we find above-random yet imperfect performance, suggesting
partial presence of this structure. Our work motivates stronger emphasis on
constructing diverse datasets for compositional generalization, and considering
the importance of representational structure that enables efficient
compositional learning. Code available at
https://github.com/oshapio/visual-compositional-generalization.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [216] [gigiProfiler: Diagnosing Performance Issues by Uncovering Application Resource Bottlenecks](https://arxiv.org/abs/2507.06452)
*Yigong Hu,Haodong Zheng,Yicheng Liu,Dedong Xie,Youliang Huang,Baris Kasikci*

Main category: cs.PF

TL;DR: Traditional profiling methods fail to capture application-level resource contention. This paper introduces OmniResource Profiling, implemented in gigiProfiler, to address these limitations by combining system and application-level tracing.


<details>
  <summary>Details</summary>
Motivation: Diagnosing software performance bottlenecks, especially in complex applications with custom resource management policies, remains challenging due to the limitations of traditional profiling methods.

Method: The authors propose OmniResource Profiling, realized via gigiProfiler, which employs a hybrid LLM-static analysis approach to trace application-level resources and diagnose bottleneck causes during buggy executions.

Result: The proposed method successfully identified performance bottlenecks in all evaluated cases and diagnosed the root causes of two previously undetected issues across 12 real-world performance scenarios.

Conclusion: OmniResource Profiling, implemented in gigiProfiler, demonstrates effectiveness in diagnosing complex performance bottlenecks by integrating system and application-level tracing, offering significant utility for developers.

Abstract: Diagnosing performance bottlenecks in modern software is essential yet
challenging, particularly as applications become more complex and rely on
custom resource management policies. While traditional profilers effectively
identify execution bottlenecks by tracing system-level metrics, they fall short
when it comes to application-level resource contention caused by waiting for
application-level events. In this work, we introduce OmniResource Profiling, a
performance analysis approach that integrates system-level and
application-level resource tracing to diagnose resource bottlenecks
comprehensively. gigiProfiler, our realization of OmniResource Profiling, uses
a hybrid LLM-static analysis approach to identify application-defined resources
offline and analyze their impact on performance during buggy executions to
uncover the performance bottleneck. gigiProfiler then samples and records
critical variables related to these bottleneck resources during buggy execution
and compares their value with those from normal executions to identify the root
causes. We evaluated gigiProfiler on 12 real-world performance issues across
five applications. gigiProfiler accurately identified performance bottlenecks
in all cases. gigiProfiler also successfully diagnosed the root causes of two
newly emerged, previously undiagnosed problems, with the findings confirmed by
developers.

</details>


### [217] [Uncertainty Quantification as a Complementary Latent Health Indicator for Remaining Useful Life Prediction on Turbofan Engines](https://arxiv.org/abs/2507.06672)
*Lucas Thil,Jesse Read,Rim Kaddah,Guillaume Florent Doquet*

Main category: cs.PF

TL;DR: This paper introduces a framework integrating uncertainty quantification into autoencoder-based health indicators (HIs) for improved Remaining Useful Life (RUL) prediction.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for constructing Health Indicators (HIs) often fail to address uncertainties, which hinder the accuracy of predictions like Remaining Useful Life (RUL).

Method: The authors propose a method that separates aleatoric and epistemic uncertainties within autoencoder latent spaces using standard and variational autoencoders combined with a machine learning model for RUL prediction.

Result: When benchmarked on the NASA C-MAPSS dataset, the framework outperformed traditional HI-based methods and competitive RUL estimation models.

Conclusion: Uncertainty quantification significantly enhances health assessment and predictive accuracy, proving its utility in constructing better Health Indicators.

Abstract: Health Indicators (HIs) are essential for predicting system failures in
predictive maintenance. While methods like RaPP (Reconstruction along Projected
Pathways) improve traditional HI approaches by leveraging autoencoder latent
spaces, their performance can be hindered by both aleatoric and epistemic
uncertainties. In this paper, we propose a novel framework that integrates
uncertainty quantification into autoencoder-based latent spaces, enhancing
RaPP-generated HIs. We demonstrate that separating aleatoric uncertainty from
epistemic uncertainty and cross combining HI information is the driver of
accuracy improvements in Remaining Useful Life (RUL) prediction. Our method
employs both standard and variational autoencoders to construct these HIs,
which are then used to train a machine learning model for RUL prediction.
Benchmarked on the NASA C-MAPSS turbofan dataset, our approach outperforms
traditional HI-based methods and end-to-end RUL prediction models and is
competitive with RUL estimation methods. These results underscore the
importance of uncertainty quantification in health assessment and showcase its
significant impact on predictive performance when incorporated into the HI
construction process.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [218] [Pyrosome: Verified Compilation for Modular Metatheory](https://arxiv.org/abs/2507.06360)
*Dustin Jamner,Gabriel Kammer,Ritam Nag,Adam Chlipala*

Main category: cs.PL

TL;DR: Pyrosome introduces a generic framework for creating modular and extensible compilers, facilitating semantic reasoning and verified language extensions.


<details>
  <summary>Details</summary>
Motivation: Address limitations in semantic reasoning tied to specific language and compiler structures by creating a fully extensible framework.

Method: Developed in Coq, Pyrosome uses dependently sorted equational theories for semantics, enabling reusable compiler-correctness proofs via type-checking and equational reasoning.

Result: Demonstrated through a case study on building an incremental System F compiler, showcasing extensible multipass compilation and handling substructural typing and imperative features.

Conclusion: Pyrosome simplifies the process of extending languages and compilers, promoting modular verification and reusable correctness proofs for new language features.

Abstract: We present Pyrosome, a generic framework for modular language metatheory that
embodies a novel approach to extensible semantics and compilation, implemented
in Coq. Common techniques for semantic reasoning are often tied to the specific
structures of the languages and compilers that they support. In Pyrosome,
verified compilers are fully extensible, meaning that to extend a language
(even with a new kind of effect) simply requires defining and verifying the
compilation of the new feature, reusing the old correctness theorem for all
other cases. The novel enabling idea is an inductive formulation of equivalence
preservation that supports the addition of new rules to the source language,
target language, and compiler.
  Pyrosome defines a formal, deeply embedded notion of programming languages
with semantics given by dependently sorted equational theories, so all
compiler-correctness proofs boil down to type-checking and equational
reasoning. We support vertical composition of any compilers expressed in our
framework in addition to feature extension. As a case study, we present a
multipass compiler from System F with simple references, through CPS
translation and closure conversion. Specifically, we demonstrate how we can
build such a compiler incrementally by starting with a compiler for simply
typed lambda-calculus and adding natural numbers, the unit type, recursive
functions, and a global heap, then extending judgments with a type environment
and adding type abstraction, all while reusing the original theorems. We also
present a linear version of the simply typed CPS pass and compile a small
imperative language to the simply typed target to show how Pyrosome handles
substructural typing and imperative features.

</details>


### [219] [Fast Collection Operations from Indexed Stream Fusion](https://arxiv.org/abs/2507.06456)
*Scott Kovach,Praneeth Kolichala,Kyle A. Miller,David Broman,Fredrik Kjolstad*

Main category: cs.PL

TL;DR: The paper introduces a method for traversing and combining associative collection data structures without needing specialized compiler infrastructure, leveraging indexed streams.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies and complexity of existing methods for traversing and combining associative collections, ensuring composability without dependency on specialized compiler tools.

Method: The approach uses indexed streams to perform complex joins over input collections while avoiding intermediate allocations. Implementations are provided for Lean, Morphic, and Rust languages.

Result: The library achieves efficient traversal and combination of data structures and includes a mechanized proof of functional correctness in Lean.

Conclusion: This system delivers efficient, composable techniques for associative collections processing, validated through implementation and correctness proof in multiple programming environments.

Abstract: We present a system of efficient methods for traversing and combining
associative collection data structures. A distinguishing feature of the system
is that, like traditional sequential iterator libraries, it does not require
specialized compiler infrastructure or staged compilation for efficiency and
composability. By using a representation based on indexed streams, the library
can express complex joins over input collections while using no intermediate
allocations. We implement the library for the Lean, Morphic, and Rust
programming languages and provide a mechanized proof of functional correctness
in Lean.

</details>


### [220] [Finding Compiler Bugs through Cross-Language Code Generator and Differential Testing](https://arxiv.org/abs/2507.06584)
*Qiong Feng,Xiaotian Ma,Ziyuan Feng,Marat Akhin,Wei Song,Peng Liang*

Main category: cs.PL

TL;DR: The paper introduces CrossLangFuzzer, a framework for identifying and diagnosing compiler bugs in cross-language compilation scenarios using a universal intermediate representation and mutation techniques.


<details>
  <summary>Details</summary>
Motivation: Despite efforts to verify single-language compilation correctness, cross-language compilation remains largely unexplored, risking reliability issues in multi-language environments.

Method: The authors developed CrossLangFuzzer, incorporating a universal IR for JVM-based languages and using three mutation techniques—LangShuffler, FunctionRemoval, and TypeChanger—to generate and evaluate diverse test programs.

Result: The framework uncovered 10 Kotlin, 4 Groovy, 7 Scala 3, 2 Scala 2, and 1 Java compiler bugs, with the TypeChanger mutator being the most effective in detecting 11 bugs.

Conclusion: This work is the first to address compiler correctness in cross-language scenarios, offering insights for improving compiler reliability in multi-language settings.

Abstract: Compilers play a central role in translating high-level code into executable
programs, making their correctness essential for ensuring code safety and
reliability. While extensive research has focused on verifying the correctness
of compilers for single-language compilation, the correctness of cross-language
compilation - which involves the interaction between two languages and their
respective compilers - remains largely unexplored. To fill this research gap,
we propose CrossLangFuzzer, a novel framework that introduces a universal
intermediate representation (IR) for JVM-based languages and automatically
generates cross-language test programs with diverse type parameters and complex
inheritance structures. After generating the initial IR, CrossLangFuzzer
applies three mutation techniques - LangShuffler, FunctionRemoval, and
TypeChanger - to enhance program diversity. By evaluating both the original and
mutated programs across multiple compiler versions, CrossLangFuzzer
successfully uncovered 10 confirmed bugs in the Kotlin compiler, 4 confirmed
bugs in the Groovy compiler, 7 confirmed bugs in the Scala 3 compiler, 2
confirmed bugs in the Scala 2 compiler, and 1 confirmed bug in the Java
compiler. Among all mutators, TypeChanger is the most effective, detecting 11
of the 24 compiler bugs. Furthermore, we analyze the symptoms and root causes
of cross-compilation bugs, examining the respective responsibilities of
language compilers when incorrect behavior occurs during cross-language
compilation. To the best of our knowledge, this is the firstwork specifically
focused on identifying and diagnosing compiler bugs in cross-language
compilation scenarios. Our research helps to understand these challenges and
contributes to improving compiler correctness in multi-language environments.

</details>


### [221] [Sound Interval-Based Synthesis for Probabilistic Programs](https://arxiv.org/abs/2507.06939)
*Guilherme Espada,Alcides Fonseca*

Main category: cs.PL

TL;DR: This paper introduces a type system and synthesis algorithm for the automatic generation of valid probabilistic programs, outperforming existing methods in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: To make probabilistic programming more accessible to domain practitioners by automating the model selection process, removing the necessity for advanced statistical expertise.

Method: The authors propose a type system to statically reject invalid probabilistic programs, a type-directed synthesis algorithm for generating type-safe programs, and a heuristic search strategy for the vast program search space.

Result: The proposed method outperforms a type-agnostic random search and the data-guided DaPPer approach, particularly for complex probabilistic programs.

Conclusion: This approach significantly improves the efficiency and accessibility of probabilistic program synthesis, enabling previously challenging techniques like Genetic Programming to be utilized effectively.

Abstract: Probabilistic programming has become a standard practice to model stochastic
events and learn about the behavior of nature in different scientific contexts,
ranging from Genetics and Ecology to Linguistics and Psychology. However,
domain practitioners (such as biologists) also need to be experts in statistics
in order to select which probabilistic model is suitable for a given particular
problem, relying then on probabilistic inference engines such as Stan, Pyro or
Edward to fine-tune the parameters of that particular model. Probabilistic
Programming would be more useful if the model selection is made automatic,
without requiring statistics expertise from the end user. Automatically
selecting the model is challenging because of the large search space of
probabilistic programs needed to be explored, because the fact that most of
that search space contains invalid programs, and because invalid programs may
only be detected in some executions, due to its probabilistic nature. We
propose a type system to statically reject invalid probabilistic programs, a
type-directed synthesis algorithm that guarantees that generated programs are
type-safe by construction, and an heuristic search procedure to handle the vast
search space. We collect a number of probabilistic programs from the
literature, and use them to compare our method with both a type-agnostic random
search, and a data-guided method from the literature (DaPPer). Our results show
that our technique both outperforms random search and DaPPer, specially on more
complex programs. This drastic performance difference in synthesis allows for
fast sampling of programs and enables techniques that previously suffered from
the complexity of synthesis, such as Genetic Programming, to be applied.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [222] [Solving the Constrained Random Disambiguation Path Problem via Lagrangian Relaxation and Graph Reduction](https://arxiv.org/abs/2507.06346)
*Li Zhou,Elvan Ceyhan*

Main category: cs.RO

TL;DR: The paper introduces a method to solve a resource-constrained path-planning problem in uncertain environments using a novel algorithm COLOGR, which outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of efficient path planning in uncertain environments where a navigating agent must account for ambiguous obstacles under resource constraints.

Method: The proposed method formulates the problem as a Weight-Constrained Shortest Path Problem (WCSPP) with risk-adjusted edge costs and uses the COLOGR algorithm, which combines Lagrangian relaxation and a Two-Phase Vertex Elimination (TPVE) procedure to optimize the solution.

Result: The COLOGR algorithm achieves zero duality gap under many conditions, improves computational efficiency, and consistently outperforms greedy baselines in simulations with various obstacle densities, sensor accuracies, and risk models.

Conclusion: COLOGR is a robust and efficient method for solving constrained decision-making problems under uncertainty, with broad applicability in network design, mobility planning, and related fields.

Abstract: We study a resource-constrained variant of the Random Disambiguation Path
(RDP) problem, a generalization of the Stochastic Obstacle Scene (SOS) problem,
in which a navigating agent must reach a target in a spatial environment
populated with uncertain obstacles. Each ambiguous obstacle may be
disambiguated at a (possibly) heterogeneous resource cost, subject to a global
disambiguation budget. We formulate this constrained planning problem as a
Weight-Constrained Shortest Path Problem (WCSPP) with risk-adjusted edge costs
that incorporate probabilistic blockage and traversal penalties. To solve it,
we propose a novel algorithmic framework-COLOGR-combining Lagrangian relaxation
with a two-phase vertex elimination (TPVE) procedure. The method prunes
infeasible and suboptimal paths while provably preserving the optimal solution,
and leverages dual bounds to guide efficient search. We establish correctness,
feasibility guarantees, and surrogate optimality under mild assumptions. Our
analysis also demonstrates that COLOGR frequently achieves zero duality gap and
offers improved computational complexity over prior constrained path-planning
methods. Extensive simulation experiments validate the algorithm's robustness
across varying obstacle densities, sensor accuracies, and risk models,
consistently outperforming greedy baselines and approaching offline-optimal
benchmarks. The proposed framework is broadly applicable to stochastic network
design, mobility planning, and constrained decision-making under uncertainty.

</details>


### [223] [Mapping the Catacombs: An Underwater Cave Segment of the Devil's Eye System](https://arxiv.org/abs/2507.06397)
*Michalis Chatzispyrou,Luke Horgan,Hyunkil Hwang,Harish Sathishchandra,Monika Roznere,Alberto Quattrini Li,Philippos Mordohai,Ioannis Rekleitis*

Main category: cs.RO

TL;DR: This paper introduces a cost-effective method for mapping underwater caves using action cameras combined with a dive computer, validated through comparisons with manual surveys.


<details>
  <summary>Details</summary>
Motivation: Underwater caves are vital for freshwater management, archaeology, and hydrogeology, but effective mapping tools are lacking.

Method: The paper uses inexpensive action cameras and a dive computer to estimate camera trajectories, enabling sparse point clouds and dense reconstructions. A VI-SLAM package (SVIn2) supports trajectory data, while COLMAP is used for global optimization of dense reconstructions. Manual surveys provide additional validation.

Result: The framework successfully mapped a section of the Devil's Eye cave system, producing dense, photorealistic 3D structures of specific areas and critical cave mapping features.

Conclusion: The proposed method demonstrates that low-cost tools, combined with visual/inertial algorithms, can effectively map underwater caves, offering significant potential for broader application.

Abstract: This paper presents a framework for mapping underwater caves. Underwater
caves are crucial for fresh water resource management, underwater archaeology,
and hydrogeology. Mapping the cave's outline and dimensions, as well as
creating photorealistic 3D maps, is critical for enabling a better
understanding of this underwater domain. In this paper, we present the mapping
of an underwater cave segment (the catacombs) of the Devil's Eye cave system at
Ginnie Springs, FL. We utilized a set of inexpensive action cameras in
conjunction with a dive computer to estimate the trajectories of the cameras
together with a sparse point cloud. The resulting reconstructions are utilized
to produce a one-dimensional retract of the cave passages in the form of the
average trajectory together with the boundaries (top, bottom, left, and right).
The use of the dive computer enables the observability of the z-dimension in
addition to the roll and pitch in a visual/inertial framework (SVIn2). In
addition, the keyframes generated by SVIn2 together with the estimated camera
poses for select areas are used as input to a global optimization (bundle
adjustment) framework -- COLMAP -- in order to produce a dense reconstruction
of those areas. The same cave segment is manually surveyed using the MNemo V2
instrument, providing an additional set of measurements validating the proposed
approach. It is worth noting that with the use of action cameras, the primary
components of a cave map can be constructed. Furthermore, with the utilization
of a global optimization framework guided by the results of VI-SLAM package
SVIn2, photorealistic dense 3D representations of selected areas can be
reconstructed.

</details>


### [224] [Learning to Evaluate Autonomous Behaviour in Human-Robot Interaction](https://arxiv.org/abs/2507.06404)
*Matteo Tiezzi,Tommaso Apicella,Carlos Cardenas-Perez,Giovanni Fregonese,Stefano Dafarra,Pietro Morerio,Daniele Pucci,Alessio Del Bue*

Main category: cs.RO

TL;DR: This paper introduces a novel evaluation framework using a Neural Meta Evaluator (NeME) to analyze robot trajectory performance, addressing reproducibility and complexity challenges in Human-Robot Interaction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create a reproducible and systematic way to evaluate humanoid robot performance, overcoming limitations of traditional success rate metrics that don't fully capture trajectory complexities.

Method: A Neural Meta Evaluator (NeME), a deep learning-based classifier, is trained on robot joint trajectories to evaluate imitation learning methods and compare robot policy performance without human involvement.

Result: The proposed method was validated on the ergoCub robot, showing greater alignment with success rates and outperforming baseline approaches in systematically evaluating multimodal imitation learning.

Conclusion: The framework provides a reproducible, insightful approach to comparing the performance of imitation learning in complex human-robot interaction tasks.

Abstract: Evaluating and comparing the performance of autonomous Humanoid Robots is
challenging, as success rate metrics are difficult to reproduce and fail to
capture the complexity of robot movement trajectories, critical in Human-Robot
Interaction and Collaboration (HRIC). To address these challenges, we propose a
general evaluation framework that measures the quality of Imitation Learning
(IL) methods by focusing on trajectory performance. We devise the Neural Meta
Evaluator (NeME), a deep learning model trained to classify actions from robot
joint trajectories. NeME serves as a meta-evaluator to compare the performance
of robot control policies, enabling policy evaluation without requiring human
involvement in the loop. We validate our framework on ergoCub, a humanoid
robot, using teleoperation data and comparing IL methods tailored to the
available platform. The experimental results indicate that our method is more
aligned with the success rate obtained on the robot than baselines, offering a
reproducible, systematic, and insightful means for comparing the performance of
multimodal imitation learning approaches in complex HRI tasks.

</details>


### [225] [Evaluating Robots Like Human Infants: A Case Study of Learned Bipedal Locomotion](https://arxiv.org/abs/2507.06426)
*Devin Crowley,Whitney G. Cole,Christina M. Hospodar,Ruiting Shen,Karen E. Adolph,Alan Fern*

Main category: cs.RO

TL;DR: This research adapts methods from developmental psychology to study how training regimens affect learned behaviors in the bipedal robot, Cassie, comparing them to infant walking development.


<details>
  <summary>Details</summary>
Motivation: Understanding learned robot behaviors is often coarse and unsystematic, offering limited insights into training impacts or behavioral complexity. Drawing parallels to developmental psychology can enhance systematic evaluations.

Method: The study adopts reinforcement learning to train a simulated bipedal robot, Cassie, under training regimens inspired by infant walking studies. Evaluation methods mirror controlled psychology experiments.

Result: The research highlights differing behavioral outcomes from various training regimens and draws comparisons between Cassie's development and infant walking behavior.

Conclusion: The study underscores the value of interdisciplinary approaches for comprehensively analyzing learned robot behaviors and opens avenues for more systematic research on training impacts.

Abstract: Typically, learned robot controllers are trained via relatively unsystematic
regimens and evaluated with coarse-grained outcome measures such as average
cumulative reward. The typical approach is useful to compare learning
algorithms but provides limited insight into the effects of different training
regimens and little understanding about the richness and complexity of learned
behaviors. Likewise, human infants and other animals are "trained" via
unsystematic regimens, but in contrast, developmental psychologists evaluate
their performance in highly-controlled experiments with fine-grained measures
such as success, speed of walking, and prospective adjustments. However, the
study of learned behavior in human infants is limited by the practical
constraints of training and testing babies. Here, we present a case study that
applies methods from developmental psychology to study the learned behavior of
the simulated bipedal robot Cassie. Following research on infant walking, we
systematically designed reinforcement learning training regimens and tested the
resulting controllers in simulated environments analogous to those used for
babies--but without the practical constraints. Results reveal new insights into
the behavioral impact of different training regimens and the development of
Cassie's learned behaviors relative to infants who are learning to walk. This
interdisciplinary baby-robot approach provides inspiration for future research
designed to systematically test effects of training on the development of
complex learned robot behaviors.

</details>


### [226] [Failure Forecasting Boosts Robustness of Sim2Real Rhythmic Insertion Policies](https://arxiv.org/abs/2507.06519)
*Yuhan Liu,Xinyu Zhang,Haonan Chang,Abdeslam Boularias*

Main category: cs.RO

TL;DR: This paper develops a sim-to-real framework to address high-precision Rhythmic Insertion Tasks (RIT) using reinforcement learning and failure forecasting.


<details>
  <summary>Details</summary>
Motivation: Challenges in maintaining millimeter-level accuracy and consistent performance in repetitive robotic insertion tasks under complex conditions.

Method: A sim-to-real framework that combines a reinforcement-learning-based insertion policy with a failure forecasting module using 6D pose tracking and retry mechanisms.

Result: Experiments show high success rates in both simulated and real-world environments with robust performance over long repetitive tasks.

Conclusion: The proposed method improves precision and repeatability for robotic insertion tasks while addressing failure recovery effectively.

Abstract: This paper addresses the challenges of Rhythmic Insertion Tasks (RIT), where
a robot must repeatedly perform high-precision insertions, such as screwing a
nut into a bolt with a wrench. The inherent difficulty of RIT lies in achieving
millimeter-level accuracy and maintaining consistent performance over multiple
repetitions, particularly when factors like nut rotation and friction introduce
additional complexity. We propose a sim-to-real framework that integrates a
reinforcement learning-based insertion policy with a failure forecasting
module. By representing the wrench's pose in the nut's coordinate frame rather
than the robot's frame, our approach significantly enhances sim-to-real
transferability. The insertion policy, trained in simulation, leverages
real-time 6D pose tracking to execute precise alignment, insertion, and
rotation maneuvers. Simultaneously, a neural network predicts potential
execution failures, triggering a simple recovery mechanism that lifts the
wrench and retries the insertion. Extensive experiments in both simulated and
real-world environments demonstrate that our method not only achieves a high
one-time success rate but also robustly maintains performance over long-horizon
repetitive tasks.

</details>


### [227] [KLEIYN : A Quadruped Robot with an Active Waist for Both Locomotion and Wall Climbing](https://arxiv.org/abs/2507.06562)
*Keita Yoneda,Kento Kawaharazuka,Temma Suzuki,Takahiro Hattori,Kei Okada*

Main category: cs.RO

TL;DR: This paper presents the quadruped robot KLEIYN, incorporating a waist joint and using reinforcement learning (RL) with Contact-Guided Curriculum Learning (CGCL) to achieve efficient vertical locomotion, including chimney climbing.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of enabling autonomous locomotion for quadruped robots on rough terrains with significant height variations, which require stable vertical movement capabilities.

Method: The researchers developed the quadruped robot KLEIYN equipped with a waist joint and introduced Contact-Guided Curriculum Learning (CGCL) for RL-based locomotion control, focusing on enabling chimney climbing on narrow walls.

Result: KLEIYN achieved successful climbing on walls between 800 mm and 1000 mm in width at an average speed of 150 mm/s, demonstrating a 50-fold speed improvement compared to conventional robots.

Conclusion: The addition of a waist joint significantly enhances climbing performance, particularly on narrow structures, marking a notable step forward in enabling vertical locomotion for quadruped robots through RL.

Abstract: In recent years, advancements in hardware have enabled quadruped robots to
operate with high power and speed, while robust locomotion control using
reinforcement learning (RL) has also been realized. As a result, expectations
are rising for the automation of tasks such as material transport and
exploration in unknown environments. However, autonomous locomotion in rough
terrains with significant height variations requires vertical movement, and
robots capable of performing such movements stably, along with their control
methods, have not yet been fully established. In this study, we developed the
quadruped robot KLEIYN, which features a waist joint, and aimed to expand
quadruped locomotion by enabling chimney climbing through RL. To facilitate the
learning of vertical motion, we introduced Contact-Guided Curriculum Learning
(CGCL). As a result, KLEIYN successfully climbed walls ranging from 800 mm to
1000 mm in width at an average speed of 150 mm/s, 50 times faster than
conventional robots. Furthermore, we demonstrated that the introduction of a
waist joint improves climbing performance, particularly enhancing tracking
ability on narrow walls.

</details>


### [228] [SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in Urban Environments](https://arxiv.org/abs/2507.06564)
*Tianshun Li,Tianyi Huai,Zhen Li,Yichun Gao,Haoang Li,Xinhu Zheng*

Main category: cs.RO

TL;DR: SkyVLN is a UAV framework combining vision-and-language navigation (VLN) with Nonlinear Model Predictive Control (NMPC), enabling autonomous navigation in dynamic 3D urban environments.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional UAV navigation methods in interpreting and executing spatial instructions in complex and dynamic urban spaces.

Method: Integrates Large Language Models (LLMs) for interpreting natural language and visual data, multimodal agents with history-path memory, spatial verbalizer, and NMPC for obstacle avoidance and trajectory management, tested in AirSim simulation.

Result: Experiments show improved navigation efficiency and success rates, especially in new and unfamiliar environments.

Conclusion: SkyVLN offers a robust method for UAV autonomy with enhanced navigation capabilities, paving the way for deployment in diverse urban scenarios.

Abstract: Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across
various sectors, driven by their mobility and adaptability. This paper
introduces SkyVLN, a novel framework integrating vision-and-language navigation
(VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in
complex urban environments. Unlike traditional navigation methods, SkyVLN
leverages Large Language Models (LLMs) to interpret natural language
instructions and visual observations, enabling UAVs to navigate through dynamic
3D spaces with improved accuracy and robustness. We present a multimodal
navigation agent equipped with a fine-grained spatial verbalizer and a history
path memory mechanism. These components allow the UAV to disambiguate spatial
contexts, handle ambiguous instructions, and backtrack when necessary. The
framework also incorporates an NMPC module for dynamic obstacle avoidance,
ensuring precise trajectory tracking and collision prevention. To validate our
approach, we developed a high-fidelity 3D urban simulation environment using
AirSim, featuring realistic imagery and dynamic urban elements. Extensive
experiments demonstrate that SkyVLN significantly improves navigation success
rates and efficiency, particularly in new and unseen environments.

</details>


### [229] [AI Space Cortex: An Experimental System for Future Era Space Exploration](https://arxiv.org/abs/2507.06574)
*Thomas Touma,Ersin Daş,Erica Tevere,Martin Feather,Ksenia Kolcio,Maurice Prather,Alberto Candela,Ashish Goel,Erik Kramer,Hari Nayar,Lorraine Fesq,Joel W. Burdick*

Main category: cs.RO

TL;DR: The paper presents REASIMO, an AI-assisted autonomy framework addressing operational challenges in Ocean world missions like Europa and Enceladus.


<details>
  <summary>Details</summary>
Motivation: Ocean world missions face operational constraints from communication delays, limited power, and harsh environmental factors which complicate completing mission objectives.

Method: REASIMO employs an AI-driven framework using advanced technologies to enable anomaly detection, recovery, and pre-trained autonomous behaviors instead of traditional hard-coded logic.

Result: Testing of REASIMO occurred on a lander-manipulator at NASA's JPL, showcasing robust autonomous sampling under simulated Ocean world surface conditions.

Conclusion: REASIMO highlights the potential of AI-assisted autonomy to overcome operational limitations, paving the way for effective Ocean world mission execution.

Abstract: Our Robust, Explainable Autonomy for Scientific Icy Moon Operations (REASIMO)
effort contributes to NASA's Concepts for Ocean worlds Life Detection
Technology (COLDTech) program, which explores science platform technologies for
ocean worlds such as Europa and Enceladus. Ocean world missions pose
significant operational challenges. These include long communication lags,
limited power, and lifetime limitations caused by radiation damage and hostile
conditions. Given these operational limitations, onboard autonomy will be vital
for future Ocean world missions. Besides the management of nominal lander
operations, onboard autonomy must react appropriately in the event of
anomalies. Traditional spacecraft rely on a transition into 'safe-mode' in
which non-essential components and subsystems are powered off to preserve
safety and maintain communication with Earth. For a severely time-limited Ocean
world mission, resolutions to these anomalies that can be executed without
Earth-in-the-loop communication and associated delays are paramount for
completion of the mission objectives and science goals. To address these
challenges, the REASIMO effort aims to demonstrate a robust level of
AI-assisted autonomy for such missions, including the ability to detect and
recover from anomalies, and to perform missions based on pre-trained behaviors
rather than hard-coded, predetermined logic like all prior space missions. We
developed an AI-assisted, personality-driven, intelligent framework for control
of an Ocean world mission by combining a mix of advanced technologies. To
demonstrate the capabilities of the framework, we perform tests of autonomous
sampling operations on a lander-manipulator testbed at the NASA Jet Propulsion
Laboratory, approximating possible surface conditions such a mission might
encounter.

</details>


### [230] [Growing Trees with an Agent: Accelerating RRTs with Learned, Multi-Step Episodic Exploration](https://arxiv.org/abs/2507.06605)
*Xinyu Wu*

Main category: cs.RO

TL;DR: The paper proposes Episodic RRT (ERRT), a hybrid motion planning framework driven by Deep Reinforcement Learning (DRL) to address inefficiencies in sampling-based planners like RRTs.


<details>
  <summary>Details</summary>
Motivation: Traditional sampling-based planners like RRTs struggle in cluttered or high-dimensional spaces due to undirected random sampling, leading to inefficiencies in collision checks and exploration.

Method: ERRT uses episodes generated by a DRL agent as a substitute for random points, enabling directional and focused exploration and generating locally valid connected paths without expensive collision checks.

Result: The proposed ERRT framework demonstrated superior performance with quicker computations, significantly higher success rates, reduced collision checks, shorter paths, and better anytime performance compared to traditional methods like RRT and RRT* in varied environments.

Conclusion: ERRT fundamentally reshapes sampling-based motion planning by leveraging learned exploratory episodes for efficient, directed exploration, addressing critical limitations of classical RRT frameworks in high-dimensional and cluttered spaces.

Abstract: Classical sampling-based motion planners like the RRTs suffer from
inefficiencies, particularly in cluttered or high-dimensional spaces, due to
their reliance on undirected, random sampling. This paper introduces the
Episodic RRT, a novel hybrid planning framework that replaces the primitive of
a random point with a learned, multi-step "exploratory episode" generated by a
Deep Reinforcement Learning agent. By making the DRL agent the engine of
exploration, ERRT transforms the search process from a diffuse, volumetric
expansion into a directed, branch-like growth. This paradigm shift yields key
advantages: it counters the curse of dimensionality with focused exploration,
minimizes expensive collision checks by proactively proposing locally valid
paths, and improves connectivity by generating inherently connected path
segments. We demonstrate through extensive empirical evaluation across 2D, 3D,
and 6D environments that ERRT and its variants consistently and significantly
outperform their classical counterparts. In a challenging 6D robotic arm
scenario, ERRT achieves a 98% success rate compared to 19% for RRT, is up to
107x faster, reduces collision checks by over 99.6%, and finds initial paths
that are nearly 50% shorter. Furthermore, its asymptotically optimal variant,
ERRT*, demonstrates vastly superior anytime performance, refining solutions to
near-optimality up to 29x faster than standard RRT* in 3D environments. Code:
https://xinyuwuu.github.io/Episodic_RRT/.

</details>


### [231] [Q-STAC: Q-Guided Stein Variational Model Predictive Actor-Critic](https://arxiv.org/abs/2507.06625)
*Shizhe Cai,Jayadeep Jacob,Zeya Yin,Fabio Ramos*

Main category: cs.RO

TL;DR: This paper introduces Q-STAC, which combines model predictive control (MPC) and reinforcement learning (RL) to enhance sample efficiency, safety, and robustness.


<details>
  <summary>Details</summary>
Motivation: Standard deep reinforcement learning often struggles with extensive data needs, long-horizon planning, and safety constraints, while traditional MPC is limited by locally optimal solutions and challenging cost function design.

Method: Q-STAC integrates Bayesian MPC with actor-critic RL using constrained Stein Variational Gradient Descent (SVGD), leveraging system dynamics and learned Q-values for safe and efficient control.

Result: Experiments show Q-STAC achieves better sample efficiency, robustness, and control optimality over advanced RL methods in 2D navigation and robotics tasks.

Conclusion: Q-STAC successfully bridges MPC and RL approaches, offering expressive policies, safety guarantees, and improved performance metrics.

Abstract: Deep reinforcement learning has shown remarkable success in continuous
control tasks, yet often requires extensive training data, struggles with
complex, long-horizon planning, and fails to maintain safety constraints during
operation. Meanwhile, Model Predictive Control (MPC) offers explainability and
constraint satisfaction, but typically yields only locally optimal solutions
and demands careful cost function design. This paper introduces the Q-guided
STein variational model predictive Actor-Critic (Q-STAC), a novel framework
that bridges these approaches by integrating Bayesian MPC with actor-critic
reinforcement learning through constrained Stein Variational Gradient Descent
(SVGD). Our method optimizes control sequences directly using learned Q-values
as objectives, eliminating the need for explicit cost function design while
leveraging known system dynamics to enhance sample efficiency and ensure
control signals remain within safe boundaries. Extensive experiments on 2D
navigation and robotic manipulation tasks demonstrate that Q-STAC achieves
superior sample efficiency, robustness, and optimality compared to
state-of-the-art algorithms, while maintaining the high expressiveness of
policy distributions. Experiment videos are available on our website:
https://sites.google.com/view/q-stac

</details>


### [232] [Multi-Task Multi-Agent Reinforcement Learning via Skill Graphs](https://arxiv.org/abs/2507.06690)
*Guobin Zhu,Rui Zhou,Wenkang Ji,Hongyin Zhang,Donglin Wang,Shiyu Zhao*

Main category: cs.RO

TL;DR: This paper introduces a hierarchical multi-task multi-agent reinforcement learning (MT-MARL) approach using a skill graph for better knowledge transfer across unrelated tasks.


<details>
  <summary>Details</summary>
Motivation: Current multi-task MARL methods struggle with unrelated tasks and have limited ability for efficient knowledge transfer.

Method: The authors propose a hierarchical system with a high-level skill graph and a low-level MARL algorithm, ensuring the high-level layer is trained independently from the low-level module.

Result: Extensive experiments show that the proposed approach outperforms existing hierarchical MAPPO algorithms, showcasing superior task handling and knowledge transfer.

Conclusion: The hierarchical MT-MARL approach, utilizing skill graphs, proves effective in addressing unrelated tasks and enhancing adaptability across multiple scenarios.

Abstract: Multi-task multi-agent reinforcement learning (MT-MARL) has recently gained
attention for its potential to enhance MARL's adaptability across multiple
tasks. However, it is challenging for existing multi-task learning methods to
handle complex problems, as they are unable to handle unrelated tasks and
possess limited knowledge transfer capabilities. In this paper, we propose a
hierarchical approach that efficiently addresses these challenges. The
high-level module utilizes a skill graph, while the low-level module employs a
standard MARL algorithm. Our approach offers two contributions. First, we
consider the MT-MARL problem in the context of unrelated tasks, expanding the
scope of MTRL. Second, the skill graph is used as the upper layer of the
standard hierarchical approach, with training independent of the lower layer,
effectively handling unrelated tasks and enhancing knowledge transfer
capabilities. Extensive experiments are conducted to validate these advantages
and demonstrate that the proposed method outperforms the latest hierarchical
MAPPO algorithms. Videos and code are available at
https://github.com/WindyLab/MT-MARL-SG

</details>


### [233] [Integrating Perceptions: A Human-Centered Physical Safety Model for Human-Robot Interaction](https://arxiv.org/abs/2507.06700)
*Pranav Pandey,Ramviyas Parasuraman,Prashant Doshi*

Main category: cs.RO

TL;DR: This paper develops a personalized safety model for human-robot interaction by incorporating a parameter ($\rho$) to consider individual safety perceptions influenced by emotional states, trust, and robot behavior.


<details>
  <summary>Details</summary>
Motivation: Traditional models for physical safety in human-robot interaction do not account for subjective safety perceptions, which are influenced by individual traits and contextual factors.

Method: The authors introduced a parameterized general safety model and conducted a series of human-subject studies in simulated rescue scenarios to investigate the relationship between emotional state, trust, robot behavior, and perceived safety.

Result: Results show that personalization parameter ($\rho$) reveals individual differences shaped by trust, emotional states, and user roles. Predictable robot behavior enhances perceived safety, while exposure to certain roles can decrease it over time.

Conclusion: Integrating psychological and behavioral elements into safety models is essential for designing more trustworthy human-robot systems, especially in safety-critical domains.

Abstract: Ensuring safety in human-robot interaction (HRI) is essential to foster user
trust and enable the broader adoption of robotic systems. Traditional safety
models primarily rely on sensor-based measures, such as relative distance and
velocity, to assess physical safety. However, these models often fail to
capture subjective safety perceptions, which are shaped by individual traits
and contextual factors. In this paper, we introduce and analyze a parameterized
general safety model that bridges the gap between physical and perceived safety
by incorporating a personalization parameter, $\rho$, into the safety
measurement framework to account for individual differences in safety
perception. Through a series of hypothesis-driven human-subject studies in a
simulated rescue scenario, we investigate how emotional state, trust, and robot
behavior influence perceived safety. Our results show that $\rho$ effectively
captures meaningful individual differences, driven by affective responses,
trust in task consistency, and clustering into distinct user types.
Specifically, our findings confirm that predictable and consistent robot
behavior as well as the elicitation of positive emotional states, significantly
enhance perceived safety. Moreover, responses cluster into a small number of
user types, supporting adaptive personalization based on shared safety models.
Notably, participant role significantly shapes safety perception, and repeated
exposure reduces perceived safety for participants in the casualty role,
emphasizing the impact of physical interaction and experiential change. These
findings highlight the importance of adaptive, human-centered safety models
that integrate both psychological and behavioral dimensions, offering a pathway
toward more trustworthy and effective HRI in safety-critical domains.

</details>


### [234] [Spatial-Temporal Aware Visuomotor Diffusion Policy Learning](https://arxiv.org/abs/2507.06710)
*Zhenyang Liu,Yikai Wang,Kuanning Wang,Longfei Liang,Xiangyang Xue,Yanwei Fu*

Main category: cs.RO

TL;DR: The paper introduces DP4, a visual imitation learning framework that integrates spatiotemporal awareness into diffusion-based policies, achieving improved robotic task success rates.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing visual imitation learning methods, which rely on trajectory cloning and struggle to capture 3D spatial and 4D spatiotemporal relationships essential for real-world deployment.

Method: DP4 uses a dynamic Gaussian world model to facilitate the learning of 3D spatial and 4D spatiotemporal perceptions. It constructs and predicts 3D scenes from single-view RGB-D observations to optimize trajectory generation while modeling spatial and temporal dependencies.

Result: Experiments with 17 simulation tasks and 3 real-world tasks show that DP4 outperforms existing methods, with an average success rate improvement of 16.4% (Adroit), 14% (DexArt), 6.45% (RLBench) in simulations, and 8.6% in real-world tasks.

Conclusion: The proposed DP4 method significantly enhances the spatial and temporal modeling capabilities of visual imitation learning, demonstrating superior performance in both simulated and real-world robotic tasks.

Abstract: Visual imitation learning is effective for robots to learn versatile tasks.
However, many existing methods rely on behavior cloning with supervised
historical trajectories, limiting their 3D spatial and 4D spatiotemporal
awareness. Consequently, these methods struggle to capture the 3D structures
and 4D spatiotemporal relationships necessary for real-world deployment. In
this work, we propose 4D Diffusion Policy (DP4), a novel visual imitation
learning method that incorporates spatiotemporal awareness into diffusion-based
policies. Unlike traditional approaches that rely on trajectory cloning, DP4
leverages a dynamic Gaussian world model to guide the learning of 3D spatial
and 4D spatiotemporal perceptions from interactive environments. Our method
constructs the current 3D scene from a single-view RGB-D observation and
predicts the future 3D scene, optimizing trajectory generation by explicitly
modeling both spatial and temporal dependencies. Extensive experiments across
17 simulation tasks with 173 variants and 3 real-world robotic tasks
demonstrate that the 4D Diffusion Policy (DP4) outperforms baseline methods,
improving the average simulation task success rate by 16.4% (Adroit), 14%
(DexArt), and 6.45% (RLBench), and the average real-world robotic task success
rate by 8.6%.

</details>


### [235] [LOVON: Legged Open-Vocabulary Object Navigator](https://arxiv.org/abs/2507.06747)
*Daojie Peng,Jiahang Cao,Qiang Zhang,Jun Ma*

Main category: cs.RO

TL;DR: The paper introduces LOVON, a framework combining large language models and open-vocabulary vision for effective robotic object navigation in open-world environments.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional robotic navigation methods in integrating task planning and object detection for executing long-horizon and complex tasks.

Method: The LOVON framework incorporates hierarchical task planning via large language models and open-vocabulary visual detection models, alongside visual stabilization techniques like Laplacian Variance Filtering and robust execution logic.

Result: LOVON successfully completes long-sequence tasks involving open-vocabulary target detection and navigation, supported by extensive evaluations and experiments on legged robots.

Conclusion: LOVON enhances robot navigation in dynamic environments, demonstrating adaptability, robustness, and compatibility across multiple robotic platforms.

Abstract: Object navigation in open-world environments remains a formidable and
pervasive challenge for robotic systems, particularly when it comes to
executing long-horizon tasks that require both open-world object detection and
high-level task planning. Traditional methods often struggle to integrate these
components effectively, and this limits their capability to deal with complex,
long-range navigation missions. In this paper, we propose LOVON, a novel
framework that integrates large language models (LLMs) for hierarchical task
planning with open-vocabulary visual detection models, tailored for effective
long-range object navigation in dynamic, unstructured environments. To tackle
real-world challenges including visual jittering, blind zones, and temporary
target loss, we design dedicated solutions such as Laplacian Variance Filtering
for visual stabilization. We also develop a functional execution logic for the
robot that guarantees LOVON's capabilities in autonomous navigation, task
adaptation, and robust task completion. Extensive evaluations demonstrate the
successful completion of long-sequence tasks involving real-time detection,
search, and navigation toward open-vocabulary dynamic targets. Furthermore,
real-world experiments across different legged robots (Unitree Go2, B2, and
H1-2) showcase the compatibility and appealing plug-and-play feature of LOVON.

</details>


### [236] [Distributed Fault-Tolerant Multi-Robot Cooperative Localization in Adversarial Environments](https://arxiv.org/abs/2507.06750)
*Tohid Kargar Tasooji,Ramviyas Parasuraman*

Main category: cs.RO

TL;DR: The paper introduces a robust distributed algorithm for cooperative localization in multi-robot systems, addressing adversarial challenges with improved communication efficiency and fault tolerance.


<details>
  <summary>Details</summary>
Motivation: Adversarial disruptions, such as sensor manipulation and communication jamming, undermine the effectiveness of traditional multi-robot localization methods, necessitating a resilient framework.

Method: Proposes a fault-tolerant cooperative localization framework utilizing an adaptive event-triggered communication strategy, supported by theoretical analysis of convergence and stability.

Result: The proposed algorithm outperforms existing methods in localization accuracy and communication efficiency under adversarial conditions, validated through Robotarium-based experiments.

Conclusion: The framework offers enhanced scalability, reliability, and fault tolerance for multi-robot systems, making it suitable for deployment in adversarial and challenging real-world environments.

Abstract: In multi-robot systems (MRS), cooperative localization is a crucial task for
enhancing system robustness and scalability, especially in GPS-denied or
communication-limited environments. However, adversarial attacks, such as
sensor manipulation, and communication jamming, pose significant challenges to
the performance of traditional localization methods. In this paper, we propose
a novel distributed fault-tolerant cooperative localization framework to
enhance resilience against sensor and communication disruptions in adversarial
environments. We introduce an adaptive event-triggered communication strategy
that dynamically adjusts communication thresholds based on real-time sensing
and communication quality. This strategy ensures optimal performance even in
the presence of sensor degradation or communication failure. Furthermore, we
conduct a rigorous analysis of the convergence and stability properties of the
proposed algorithm, demonstrating its resilience against bounded adversarial
zones and maintaining accurate state estimation. Robotarium-based experiment
results show that our proposed algorithm significantly outperforms traditional
methods in terms of localization accuracy and communication efficiency,
particularly in adversarial settings. Our approach offers improved scalability,
reliability, and fault tolerance for MRS, making it suitable for large-scale
deployments in real-world, challenging environments.

</details>


### [237] [Stream Function-Based Navigation for Complex Quadcopter Obstacle Avoidance](https://arxiv.org/abs/2507.06787)
*Sean Smith,Emmanuel Witrant,Ya-Jun Pan*

Main category: cs.RO

TL;DR: The paper presents a navigation system combining vortex panel methods and model predictive control for obstacle avoidance in complex environments, tested using drones.


<details>
  <summary>Details</summary>
Motivation: The study aims to address limitations in obstacle avoidance systems, particularly dealing with close-proximity and fast-moving obstacles in complex, dynamic environments.

Method: The system integrates vortex panel methods for flow simulation and model predictive controllers utilizing higher-order control barrier functions. Minimum bounding ellipses and adaptive Kalman filters enhance obstacle modeling and prediction.

Result: The approach is validated through simulation and real-world experiments using drones equipped with LiDAR sensors.

Conclusion: The combined system effectively handles dynamic obstacles, demonstrating its feasibility for real-time navigation.

Abstract: This article presents a novel stream function-based navigational control
system for obstacle avoidance, where obstacles are represented as
two-dimensional (2D) rigid surfaces in inviscid, incompressible flows. The
approach leverages the vortex panel method (VPM) and incorporates safety
margins to control the stream function and flow properties around virtual
surfaces, enabling navigation in complex, partially observed environments using
real-time sensing. To address the limitations of the VPM in managing relative
distance and avoiding rapidly accelerating obstacles at close proximity, the
system integrates a model predictive controller (MPC) based on higher-order
control barrier functions (HOCBF). This integration incorporates VPM trajectory
generation, state estimation, and constraint handling into a receding-horizon
optimization problem. The 2D rigid surfaces are enclosed using minimum bounding
ellipses (MBEs), while an adaptive Kalman filter (AKF) captures and predicts
obstacle dynamics, propagating these estimates into the MPC-HOCBF for rapid
avoidance maneuvers. Evaluation is conducted using a PX4-powered Clover drone
Gazebo simulator and real-time experiments involving a COEX Clover quadcopter
equipped with a 360 degree LiDAR sensor.

</details>


### [238] [Hierarchical Reinforcement Learning for Articulated Tool Manipulation with Multifingered Hand](https://arxiv.org/abs/2507.06822)
*Wei Xu,Yanchao Zhao,Weichao Guo,Xinjun Sheng*

Main category: cs.RO

TL;DR: The paper focuses on using reinforcement learning for robotic hands to manipulate complex articulated tools like tweezers. It proposes a framework that combines hierarchical policies and affordance estimation to perform precise tool-based tasks.


<details>
  <summary>Details</summary>
Motivation: To enable robotic hands to effectively manipulate articulated tools, which dynamically change shape and present unique challenges compared to rigid tools.

Method: A hierarchical goal-conditioned reinforcement learning framework is developed, consisting of a low-level policy for tool configuration and a high-level policy for task control. An encoder trained on point clouds estimates tool affordance states, while a privilege-informed heuristic policy aids training efficiency.

Result: The framework was validated via real-world experiments, achieving a 70.8% success rate in manipulating a tweezer-like tool to grasp various objects.

Conclusion: The study demonstrates that reinforcement learning can significantly improve the manipulation capabilities of robotic hands for articulated tools, highlighting its potential for advanced automation tasks.

Abstract: Manipulating articulated tools, such as tweezers or scissors, has rarely been
explored in previous research. Unlike rigid tools, articulated tools change
their shape dynamically, creating unique challenges for dexterous robotic
hands. In this work, we present a hierarchical, goal-conditioned reinforcement
learning (GCRL) framework to improve the manipulation capabilities of
anthropomorphic robotic hands using articulated tools. Our framework comprises
two policy layers: (1) a low-level policy that enables the dexterous hand to
manipulate the tool into various configurations for objects of different sizes,
and (2) a high-level policy that defines the tool's goal state and controls the
robotic arm for object-picking tasks. We employ an encoder, trained on
synthetic pointclouds, to estimate the tool's affordance states--specifically,
how different tool configurations (e.g., tweezer opening angles) enable
grasping of objects of varying sizes--from input point clouds, thereby enabling
precise tool manipulation. We also utilize a privilege-informed heuristic
policy to generate replay buffer, improving the training efficiency of the
high-level policy. We validate our approach through real-world experiments,
showing that the robot can effectively manipulate a tweezer-like tool to grasp
objects of diverse shapes and sizes with a 70.8 % success rate. This study
highlights the potential of RL to advance dexterous robotic manipulation of
articulated tools.

</details>


### [239] [Friction Estimation for In-Hand Planar Motion](https://arxiv.org/abs/2507.06824)
*Gabriel Arslan Waltersson,Yiannis Karayiannidis*

Main category: cs.RO

TL;DR: The paper introduces a method for estimating friction and contact radius during in-hand sliding manipulation, tested in simulation and real-world settings.


<details>
  <summary>Details</summary>
Motivation: To improve understanding and control of robotic in-hand sliding manipulation by estimating contact properties such as friction and contact radius in real-time.

Method: Utilizes tactile measurements of contact forces and sliding velocities for estimating contact properties, with a heuristic to address fast slip-stick dynamics.

Result: The proposed method shows practical applicability, validated through simulation and real-world experiments.

Conclusion: The method effectively enhances real-time estimation of contact properties during manipulation tasks, potentially improving robotic handling of objects.

Abstract: This paper presents a method for online estimation of contact properties
during in-hand sliding manipulation with a parallel gripper. We estimate the
static and Coulomb friction as well as the contact radius from tactile
measurements of contact forces and sliding velocities. The method is validated
in both simulation and real-world experiments. Furthermore, we propose a
heuristic to deal with fast slip-stick dynamics which can adversely affect the
estimation.

</details>


### [240] [Toward a Full-Stack Co-Simulation Platform for Testing of Automated Driving Systems](https://arxiv.org/abs/2507.06884)
*Dong Bi,Yongqi Zhao,Zhengguo Gu,Tomislav Mihalj,Jia Hu,Arno Eichberger*

Main category: cs.RO

TL;DR: The paper presents a full-stack toolchain for virtual testing of automated driving systems, enabling automatic scenario generation and validation using real-world datasets in a simulation environment.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing simulation toolchains, which struggle to combine automated scenario generation and advanced driving capabilities.

Method: A full-stack toolchain is developed, integrating automatic scenario generation from real-world data and co-simulation using CarMaker, ROS, and Apollo.

Result: Simulation results show the effectiveness of the proposed toolchain for testing automated driving systems.

Conclusion: The toolchain provides an efficient solution for the validation of automated driving systems, advancing their development through virtual testing.

Abstract: Virtual testing has emerged as an effective approach to accelerate the
deployment of automated driving systems. Nevertheless, existing simulation
toolchains encounter difficulties in integrating rapid, automated scenario
generation with simulation environments supporting advanced automated driving
capabilities. To address this limitation, a full-stack toolchain is presented,
enabling automatic scenario generation from real-world datasets and efficient
validation through a co-simulation platform based on CarMaker, ROS, and Apollo.
The simulation results demonstrate the effectiveness of the proposed toolchain.
A demonstration video showcasing the toolchain is available at the provided
link: https://youtu.be/taJw_-CmSiY.

</details>


### [241] [ULC: A Unified and Fine-Grained Controller for Humanoid Loco-Manipulation](https://arxiv.org/abs/2507.06905)
*Wandong Sun,Luying Feng,Baoshi Cao,Yang Liu,Yaochu Jin,Zongwu Xie*

Main category: cs.RO

TL;DR: The paper introduces the Unified Loco-Manipulation Controller (ULC), a single control policy for humanoid robots that integrates locomotion and manipulation capabilities, overcoming the limitations of traditional hierarchical approaches.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current hierarchical control methods in humanoid robots, which separate locomotion and manipulation policies and suffer from coordination constraints.

Method: The authors developed the Unified Loco-Manipulation Controller (ULC) using techniques like sequence skill acquisition, residual action modeling, command interpolation, random delay release, load randomization, and center-of-gravity tracking for stability and robustness.

Result: ULC was validated on the Unitree G1 humanoid robot, showcasing better tracking accuracy, broader workspace coverage, and robustness compared to existing disentangled control methods.

Conclusion: ULC proves that a unified control policy can integrate locomotion and manipulation effectively, achieving precise dual-arm tracking and whole-body coordination for complex tasks, surpassing the performance of traditional methods.

Abstract: Loco-Manipulation for humanoid robots aims to enable robots to integrate
mobility with upper-body tracking capabilities. Most existing approaches adopt
hierarchical architectures that decompose control into isolated upper-body
(manipulation) and lower-body (locomotion) policies. While this decomposition
reduces training complexity, it inherently limits coordination between
subsystems and contradicts the unified whole-body control exhibited by humans.
We demonstrate that a single unified policy can achieve a combination of
tracking accuracy, large workspace, and robustness for humanoid
loco-manipulation. We propose the Unified Loco-Manipulation Controller (ULC), a
single-policy framework that simultaneously tracks root velocity, root height,
torso rotation, and dual-arm joint positions in an end-to-end manner, proving
the feasibility of unified control without sacrificing performance. We achieve
this unified control through key technologies: sequence skill acquisition for
progressive learning complexity, residual action modeling for fine-grained
control adjustments, command polynomial interpolation for smooth motion
transitions, random delay release for robustness to deploy variations, load
randomization for generalization to external disturbances, and
center-of-gravity tracking for providing explicit policy gradients to maintain
stability. We validate our method on the Unitree G1 humanoid robot with 3-DOF
(degrees-of-freedom) waist. Compared with strong baselines, ULC shows better
tracking performance to disentangled methods and demonstrating larger workspace
coverage. The unified dual-arm tracking enables precise manipulation under
external loads while maintaining coordinated whole-body control for complex
loco-manipulation tasks.

</details>


### [242] [Bounomodes: the grazing ox algorithm for exploration of clustered anomalies](https://arxiv.org/abs/2507.06960)
*Samuel Matloob,Ayan Dutta,O. Patrick Kreidl,Swapnonel Roy,Ladislau Bölöni*

Main category: cs.RO

TL;DR: This paper introduces 'bounomodes' algorithms for Informative Path Planning (IPP), combining uniform area coverage and targeted exploration of anomaly clusters, with deep reinforcement learning driving the exploration.


<details>
  <summary>Details</summary>
Motivation: To improve anomaly detection by prioritizing exploration of clustered anomalies rather than using uniform area coverage approaches.

Method: Proposes the 'bounomodes' framework, which alternates between geometric-based uniform area coverage and deep reinforcement learning for anomaly cluster exploration.

Result: The experimental evaluation shows that the proposed approach outperforms traditional baselines in detecting clustered anomalies efficiently.

Conclusion: The 'bounomodes' framework is an effective solution for IPP in environments with clustered anomalies, balancing uniform coverage with focused exploration.

Abstract: A common class of algorithms for informative path planning (IPP) follows
boustrophedon ("as the ox turns") patterns, which aim to achieve uniform area
coverage. However, IPP is often applied in scenarios where anomalies, such as
plant diseases, pollution, or hurricane damage, appear in clusters. In such
cases, prioritizing the exploration of anomalous regions over uniform coverage
is beneficial. This work introduces a class of algorithms referred to as
bounom\=odes ("as the ox grazes"), which alternates between uniform
boustrophedon sampling and targeted exploration of detected anomaly clusters.
While uniform sampling can be designed using geometric principles, close
exploration of clusters depends on the spatial distribution of anomalies and
must be learned. In our implementation, the close exploration behavior is
learned using deep reinforcement learning algorithms. Experimental evaluations
demonstrate that the proposed approach outperforms several established
baselines.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [243] [Quality attributes of test cases and test suites -- importance & challenges from practitioners' perspectives](https://arxiv.org/abs/2507.06343)
*Huynh Khanh Vi Tran,Nauman bin Ali,Michael Unterkalmsteiner,Jürgen Börstler,Panagiota Chatzipetrou*

Main category: cs.SE

TL;DR: This paper explores practitioners' views on the relative importance and challenges of quality attributes in test cases and suites through an industrial survey.


<details>
  <summary>Details</summary>
Motivation: To better understand the relative importance of software testing quality attributes and identify challenges faced by practitioners in ensuring these attributes.

Method: An industrial survey was conducted using a questionnaire based on a literature review, with professionals identified via LinkedIn to ensure a wide and heterogeneous sample.

Result: 354 responses were analyzed, highlighting Fault Detection, Usability, Maintainability, Reliability, and Coverage as the most valued attributes. Challenges include inadequate definitions, lack of metrics, unclear review processes, and insufficient external support.

Conclusion: The study highlights where practitioners need enhanced support and offers guidance for researchers and companies to improve the quality of test cases and suites.

Abstract: Context: The quality of the test suites and the constituent test cases
significantly impacts confidence in software testing. While research has
identified several quality attributes of test cases and test suites, there is a
need for a better understanding of their relative importance in practice.
Objective: We investigate practitioners' perceptions regarding the relative
importance of quality attributes of test cases and test suites and the
challenges they face in ensuring the perceived important quality attributes.
Method: We conducted an industrial survey using a questionnaire based on the
quality attributes identified in an extensive literature review. We used a
sampling strategy that leverages LinkedIn to draw a large and heterogeneous
sample of professionals with experience in software testing. Results: We
collected 354 responses from practitioners with a wide range of experience. We
found that the majority of practitioners rated Fault Detection, Usability,
Maintainability, Reliability, and Coverage to be the most important quality
attributes. Resource Efficiency, Reusability, and Simplicity received the most
divergent opinions, which, according to our analysis, depend on the
software-testing contexts. We identified common challenges that apply to the
important attributes, namely inadequate definition, lack of useful metrics,
lack of an established review process, and lack of external support.
Conclusion: The findings point out where practitioners actually need further
support with respect to achieving high-quality test cases and test suites under
different software testing contexts. The findings can serve as a guideline for
academic researchers when looking for research directions on the topic. The
findings can also be used to encourage companies to provide more support to
practitioners to achieve high-quality test cases and test suites.

</details>


### [244] [A proposal and assessment of an improved heuristic for the Eager Test smell detection](https://arxiv.org/abs/2507.06354)
*Huynh Khanh Vi Tran,Nauman bin Ali,Michael Unterkalmsteiner,Jürgen Börstler*

Main category: cs.SE

TL;DR: The paper addresses inadequacies in detecting the Eager Test smell in unit testing by reviewing past studies, proposing a new heuristic, and demonstrating its improved effectiveness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the detection rules for the Eager Test smell, as existing approaches lack precision and lead to disagreement in detection outcomes.

Method: The authors reviewed 56 studies, proposed a new definition and heuristic for the Eager Test smell, and manually applied it to 300 Java test cases to evaluate its performance.

Result: The study highlighted inconsistencies in existing detection rules and showcased that their heuristic could identify patterns missed by current approaches.

Conclusion: The proposed heuristic provides a more precise way of identifying the Eager Test smell, addressing practitioner concerns and surpassing existing detection methods in accuracy.

Abstract: Context: The evidence for the prevalence of test smells at the unit testing
level has relied on the accuracy of detection tools, which have seen intense
research in the last two decades. The Eager Test smell, one of the most
prevalent, is often identified using simplified detection rules that
practitioners find inadequate. Objective: We aim to improve the rules for
detecting the Eager Test smell. Method: We reviewed the literature on test
smells to analyze the definitions and detection rules of the Eager Test smell.
We proposed a novel, unambiguous definition of the test smell and a heuristic
to address the limitations of the existing rules. We evaluated our heuristic
against existing detection rules by manually applying it to 300 unit test cases
in Java. Results: Our review identified 56 relevant studies. We found that
inadequate interpretations of original definitions of the Eager Test smell led
to imprecise detection rules, resulting in a high level of disagreement in
detection outcomes. Also, our heuristic detected patterns of eager and
non-eager tests that existing rules missed. Conclusion: Our heuristic captures
the essence of the Eager Test smell more precisely; hence, it may address
practitioners' concerns regarding the adequacy of existing detection rules.

</details>


### [245] [Evaluating Efficiency and Novelty of LLM-Generated Code for Graph Analysis](https://arxiv.org/abs/2507.06463)
*Atieh Barati Nia,Mohammad Dindoost,David A. Bader*

Main category: cs.SE

TL;DR: The study evaluates eight state-of-the-art Large Language Models (LLMs) for generating efficient C implementations of graph algorithms, focusing on performance benchmarks and algorithm integration.


<details>
  <summary>Details</summary>
Motivation: Investigate whether LLMs can generate efficient and optimized C code for graph analysis routines, which require strict runtime and memory constraints.

Method: Two benchmarking approaches were used: assessing the ability of LLMs to generate efficient algorithms compared to existing ones and evaluating their ability to integrate algorithms into benchmarks.

Result: Claude Sonnet 4 Extended outperformed other LLMs in generating ready-to-use and efficient graph algorithms, surpassing human-written baselines in tasks like triangle counting.

Conclusion: Contemporary LLMs are effective at optimizing existing algorithms and integrating them into benchmarks but are less capable of inventing novel graph techniques.

Abstract: Large Language Models (LLMs) are increasingly used to automate software
development, yet most prior evaluations focus on functional correctness or
high-level languages such as Python. We present the first systematic study of
LLMs' ability to generate efficient C implementations of graph-analysis
routines--code that must satisfy the stringent runtime and memory constraints.
Eight state-of-the-art models (OpenAI ChatGPT o3 and o4-mini-high, Anthropic
Claude 4 Sonnet and Sonnet Extended, Google Gemini 2.5 Flash and Pro, xAI Grok
3-Think, and DeepSeek DeepThink R1) are benchmarked by two distinct approaches.
The first approach checks the ability of LLMs in generating an algorithm
outperforming other present algorithms in the benchmark. The second approach
evaluates the ability of LLMs to generate graph algorithms for integration into
the benchmark. Results show that Claude Sonnet 4 Extended achieves the best
result in the case of ready-to-use code generation and efficiency,
outperforming human-written baselines in triangle counting. The study confirms
that contemporary LLMs excel at optimizing and integrating established
algorithms but not inventing novel techniques. We provide prompts, the first
approach's generated code, and measurement scripts to foster reproducible
research.

</details>


### [246] [Issue Tracking Ecosystems: Context and Best Practices](https://arxiv.org/abs/2507.06704)
*Lloyd Montgomery*

Main category: cs.SE

TL;DR: The paper explores the overlooked intersections of Issue Tracking Systems (ITSs) and their broader ecosystems, called Issue Tracking Ecosystems (ITEs), emphasizing a context-rich understanding to tackle complexity and diversity.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the challenges faced by software engineering organizations in managing complex and diverse Issue Tracking Ecosystems (ITEs), which are poorly understood despite forming a cornerstone of modern SE workflows.

Method: The author conducted interviews with practitioners and performed archival analysis on diverse Issue Tracking Systems (ITSs) to investigate the ecosystem-level challenges and context-dependent issues.

Result: Findings revealed that problems within ITEs are highly context-specific, and existing ITS solutions lack a cohesive, comparable, and context-enriched framework.

Conclusion: The work highlights the need for more aligned and context-specific solutions within ITEs and presents the Best Practice Ontology for ITEs as a proposed approach to address these gaps.

Abstract: Issue Tracking Systems (ITSs), such as GitHub and Jira, are popular tools
that support Software Engineering (SE) organisations through the management of
``issues'', which represent different SE artefacts such as requirements,
development tasks, and maintenance items. ITSs also support internal linking
between issues, and external linking to other tools and information sources.
This provides SE organisations key forms of documentation, including forwards
and backwards traceability (e.g., Feature Requests linked to sprint releases
and code commits linked to Bug Reports). An Issue Tracking Ecosystem (ITE) is
the aggregate of the central ITS and the related SE artefacts, stakeholders,
and processes -- with an emphasis on how these contextual factors interact with
the ITS. The quality of ITEs is central to the success of these organisations
and their software products. There are challenges, however, within ITEs,
including complex networks of interlinked artefacts and diverse workflows.
While ITSs have been the subject of study in SE research for decades, ITEs as a
whole need further exploration.
  In this thesis, I undertake the challenge of understanding ITEs at a broader
level, addressing these questions regarding complexity and diversity. I
interviewed practitioners and performed archival analysis on a diverse set of
ITSs. These analyses revealed the context-dependent nature of ITE problems,
highlighting the need for context-specific ITE research. While previous work
has produced many solutions to specific ITS problems, these solutions are not
consistently framed in a context-rich and comparable way, leading to a desire
for more aligned solutions across research and practice. To address this
emergent information and lack of alignment, I created the Best Practice
Ontology for ITEs. <... truncated due to arXiv abstract character limit ...>

</details>


### [247] [Leveraging LLMs for Semantic Conflict Detection via Unit Test Generation](https://arxiv.org/abs/2507.06762)
*Nathalia Barbosa,Paulo Borba,Léuson Da Silva*

Main category: cs.SE

TL;DR: The paper discusses using large language models (LLMs), specifically Code Llama 70B, to improve semantic conflict detection in collaborative software development by enhancing unit test generation in the SMAT tool.


<details>
  <summary>Details</summary>
Motivation: Traditional merge tools fail to detect semantic conflicts in software development, and tools like SMAT, relying on unit test generation, suffer from high false negatives due to limitations of existing test tools. This paper investigates whether LLMs can enhance test generation to address these challenges.

Method: The study integrates Code Llama 70B into SMAT for unit test generation, explores different strategies (interaction methods, prompt contents, and parameter settings), and evaluates its conflict detection effectiveness on two datasets--a smaller benchmark and a larger real-world sample.

Result: LLM-based test generation demonstrates potential for better detecting semantic conflicts, though it remains computationally intensive and challenging for complex real-world scenarios.

Conclusion: While challenges remain, LLMs show promise in advancing semantic conflict detection in software development, offering a pathway to improving collaborative code integration tools like SMAT.

Abstract: Semantic conflicts arise when a developer introduces changes to a codebase
that unintentionally affect the behavior of changes integrated in parallel by
other developers. Traditional merge tools are unable to detect such conflicts,
so complementary tools like SMAT have been proposed. SMAT relies on generating
and executing unit tests: if a test fails on the base version, passes on a
developer's modified version, but fails again after merging with another
developer's changes, a semantic conflict is indicated. While SMAT is effective
at detecting conflicts, it suffers from a high rate of false negatives, partly
due to the limitations of unit test generation tools such as Randoop and
Evosuite. To investigate whether large language models (LLMs) can overcome
these limitations, we propose and integrate a new test generation tool based on
Code Llama 70B into SMAT. We explore the model's ability to generate tests
using different interaction strategies, prompt contents, and parameter
configurations. Our evaluation uses two samples: a benchmark with simpler
systems from related work, and a more significant sample based on complex,
real-world systems. We assess the effectiveness of the new SMAT extension in
detecting conflicts. Results indicate that, although LLM-based test generation
remains challenging and computationally expensive in complex scenarios, there
is promising potential for improving semantic conflict detection.
  --
  Conflitos sem^anticos surgem quando um desenvolvedor introduz mudan\c{c}as em
uma base de c\'odigo que afetam, de forma n~ao intencional, o comportamento de
altera\c{c}~oes integradas em paralelo por outros desenvolvedores. Ferramentas
tradicionais de merge n~ao conseguem detectar esse tipo de conflito, por isso
ferramentas complementares como o SMAT foram propostas. O SMAT depende da
gera\c{c}~ao e execu\c{c}~ao de testes de unidade: se um teste falha na vers~ao
base, passa na vers~ao modificada por um desenvolvedor, mas volta a falhar
ap\'os o merge com as mudan\c{c}as de outro desenvolvedor, um conflito
sem^antico \'e identificado. Embora o SMAT seja eficaz na detec\c{c}~ao de
conflitos, apresenta alta taxa de falsos negativos, em parte devido \`as
limita\c{c}~oes das ferramentas de gera\c{c}~ao de testes como Randoop e
Evosuite. Para investigar se modelos de linguagem de grande porte (LLMs) podem
superar essas limita\c{c}~oes, propomos e integramos ao SMAT uma nova
ferramenta de gera\c{c}~ao de testes baseada no Code Llama 70B. Exploramos a
capacidade do modelo de gerar testes utilizando diferentes estrat\'egias de
intera\c{c}~ao, conte\'udos de prompts e configura\c{c}~oes de par^ametros.
Nossa avalia\c{c}~ao utiliza duas amostras: um benchmark com sistemas mais
simples, usados em trabalhos relacionados, e uma amostra mais significativa
baseada em sistemas complexos e reais. Avaliamos a efic\'acia da nova extens~ao
do SMAT na detec\c{c}~ao de conflitos. Os resultados indicam que, embora a
gera\c{c}~ao de testes por LLM em cen\'arios complexos ainda seja desafiadora e
custosa computacionalmente, h\'a potencial promissor para aprimorar a
detec\c{c}~ao de conflitos sem^anticos.

</details>


### [248] [Formalization of the AADL Run-Time Services with Time](https://arxiv.org/abs/2507.06881)
*Brian R Larson,Ehsan Ahmad*

Main category: cs.SE

TL;DR: This paper expands the AADL standard by introducing time modeling in its formal semantics and enhancing its Run-Time Services to support behavior specifications.


<details>
  <summary>Details</summary>
Motivation: To address the lack of time modeling and expand Run-Time Services in the AADL standard, enabling better support for behavior specifications of cyber-physical systems.

Method: The paper extends formal semantics of AADL using a modal logic defined by a Kripke structure, integrates timing, and expands RTS to accommodate reactive state-transition machines.

Result: Enhanced RTS with time support for state-transition machine behavior written in BLESS, demonstrated via implementation in HAMR.

Conclusion: The extended formalization with integrated time and improved RTS enables more robust specification and execution of cyber-physical systems within the AADL framework.

Abstract: The Architecture Analysis & Design Language (AADL) is an architecture
description language for design of cyber-physical systems--machines controlled
by software. The AADL standard, SAE International AS5506D, describes Run-Time
Services (RTS) to be provided to execute AADL models in accordance with
semantics defined by the standard. The RTS of primary concern are transport
services and timing services. Although, the study presented in [1] sets a
foundation for the formal semantics of AADL, but without modeling time. This
paper extends and simplifies this formalization using a modal logic defined by
a Kripke structure, to explicitly include time. The RTS defined in the AADL
standard are also expanded to support reactive state-transition machines of the
Behavior Specification annex standard language (BA) and its closely-related,
formally-defined counterpart, the Behavior Language for Embedded Systems with
Software (BLESS). An example of AADL RTS with time, implemented by the High
Assurance Modeling and Rapid Engineering for Embedded Systems (HAMR) for
state-transition machine behavior written in BLESS, is also presented.

</details>


### [249] [Are They All Good? Evaluating the Quality of CoTs in LLM-based Code Generation](https://arxiv.org/abs/2507.06980)
*Binquan Zhang,Li Zhang,Zhiwen Luo,Yuxin Du,Fang Liu,Song Wang,Lin Shi*

Main category: cs.SE

TL;DR: The paper investigates the quality of chain-of-thought (CoT) reasoning in large language models (LLMs) for code generation, analyzing factors affecting performance and suggesting CoT improvement techniques.


<details>
  <summary>Details</summary>
Motivation: To understand and evaluate the quality of intermediate reasoning steps (CoTs) generated by LLMs, as they play a crucial role in producing reliable and correct code.

Method: The authors analyzed 1,023 failed code samples across two benchmarks, examined 210 CoT-code pairs, and refined unsatisfactory CoTs using LLM-driven prompting methods to evaluate performance improvements.

Result: Key findings include: (1) 53.60% of CoT issues arise from external factors like unclear requirements, while 40.10% are due to LLM misinterpreting prompts. (2) Even correct CoTs can lead to code errors (18.5%) and flawed CoTs can still produce correct code (11.90%). (3) Refining CoTs is effective if provided with detailed problem descriptions.

Conclusion: Improving the quality of CoTs and addressing external and internal challenges can enhance LLM reliability and reasoning in code generation tasks.

Abstract: Large language models (LLMs) have demonstrated impressive performance in code
generation, particularly when augmented with chain-of-thought (CoT) prompting
techniques. They break down requirements into intermediate reasoning steps,
which act as design rationales to guide LLMs in writing code like human
programmers. Thus, the quality of these steps is crucial for ensuring the
correctness and reliability of the generated code. However, little is known
about the quality of CoT generated by LLMs. To what extent can we trust the
thoughts generated by LLMs? How good are they? This paper empirically explores
the external and internal factors of why LLMs generate unsatisfactory CoTs by
analyzing 1,023 failed code samples on two widely used code generation
benchmarks. We also evaluate their impact on code generation performance by
analyzing 210 CoT-code pairs and refining the unsatisfied CoTs by prompting
LLMs. Our study reveals three key findings: (1) External factors (53.60%), such
as unclear requirements and lack of context, mainly affect CoT quality, while
internal factors (40.10%) stem from LLMs' misunderstanding prompts. (2) Even
when CoTs are correct, 18.5% of the generated code contains errors due to
instruction-following issues; conversely, 11.90% of correct code is paired with
flawed CoTs. (3) Refining low-quality CoTs is feasible, i.e., LLMs improve when
given detailed problem descriptions. These findings highlight key challenges in
CoT-based code generation and suggest directions for improving LLM reasoning
and reliability.

</details>


### [250] [Exploring Fairness Interventions in Open Source Projects](https://arxiv.org/abs/2507.07026)
*Sadia Afrin Mim,Fatema Tuz Zohra,Justin Smith,Brittany Johnson*

Main category: cs.SE

TL;DR: The paper examines the adoption and maintenance of open-source machine learning fairness interventions, finding limited real-world application despite their importance in addressing bias.


<details>
  <summary>Details</summary>
Motivation: The increasing use of biased ML models in critical areas like criminal justice and healthcare necessitates fairness interventions to mitigate adverse societal impacts.

Method: Researchers compiled a dataset of 62 open-source fairness interventions, analyzed their features, and assessed their active maintenance status.

Result: 32% of the fairness interventions were actively maintained within the past year, and half provide both bias detection and mitigation capabilities, predominantly during inprocessing.

Conclusion: Real-world adoption of fairness interventions remains limited, though their value in mitigating bias and promoting fairness in ML models is clear.

Abstract: The deployment of biased machine learning (ML) models has resulted in adverse
effects in crucial sectors such as criminal justice and healthcare. To address
these challenges, a diverse range of machine learning fairness interventions
have been developed, aiming to mitigate bias and promote the creation of more
equitable models. Despite the growing availability of these interventions,
their adoption in real-world applications remains limited, with many
practitioners unaware of their existence. To address this gap, we
systematically identified and compiled a dataset of 62 open source fairness
interventions and identified active ones. We conducted an in-depth analysis of
their specifications and features to uncover considerations that may drive
practitioner preference and to identify the software interventions actively
maintained in the open source ecosystem. Our findings indicate that 32% of
these interventions have been actively maintained within the past year, and 50%
of them offer both bias detection and mitigation capabilities, mostly during
inprocessing.

</details>


### [251] [5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient Design Framework for Individual and SME LLM Usage](https://arxiv.org/abs/2507.07045)
*Ugur Ari*

Main category: cs.SE

TL;DR: The paper introduces the 5C Prompt Contract framework for efficient and systematic prompt design, addressing token and cognitive overhead.


<details>
  <summary>Details</summary>
Motivation: To address challenges in traditional prompt engineering, including inefficiency and complexity, especially for those with limited AI resources.

Method: Propose the 5C framework defining five components for prompt design: Character, Cause, Constraint, Contingency, Calibration.

Result: Experimental studies show the 5C framework improves token efficiency while delivering consistent outputs across diverse LLM frameworks.

Conclusion: The 5C framework is practical, interpretable, and enhances reliability and creative flexibility, making it suitable for resource-constrained entities.

Abstract: The progression from traditional prompt engineering to a more rigorous
discipline of prompt design marks a pivotal shift in human-LLM interaction. As
Large Language Models (LLMs) become increasingly embedded in mission-critical
applications, there emerges a pressing need for frameworks that are not only
explicit and systematic but also minimal enough to remain practical and broadly
accessible. While many existing approaches address prompt structuring through
elaborate Domain-Specific Languages (DSLs) or multi-layered templates, such
methods can impose significant token and cognitive overhead, potentially
constraining the model's creative capacity. In this context, we propose the 5C
Prompt Contract, a framework that distills prompt design into five intuitive
components: Character, Cause, Constraint, Contingency, and Calibration. This
minimal cognitive schema explicitly integrates fallback and output optimization
directives, fostering reliable, interpretable, and creatively flexible AI
interactions. Experimental results demonstrate that the 5C framework
consistently achieves superior input token efficiency while maintaining rich
and consistent outputs across diverse LLM architectures (OpenAI, Anthropic,
DeepSeek, and Gemini), making it particularly suited for individuals and
Small-to-Medium Enterprises (SMEs) with limited AI engineering resources.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [252] [Quantifying Uncertainty in Error Consistency: Towards Reliable Behavioral Comparison of Classifiers](https://arxiv.org/abs/2507.06645)
*Thomas Klein,Sascha Meyen,Wieland Brendel,Felix A. Wichmann,Kristof Meding*

Main category: q-bio.NC

TL;DR: The paper enhances the error consistency (EC) metric used in ML model benchmarking by introducing bootstrapping for confidence intervals and proposing a computational model, improving the reliability of conclusions in NeuroAI benchmarking.


<details>
  <summary>Details</summary>
Motivation: Benchmarking progresses machine learning by comparing models. Error consistency (EC) is key for assessing how ML models align with human behavior, but lacks reliability due to omission of confidence intervals.

Method: The authors propose bootstrapping techniques to calculate EC confidence intervals and develop a computational model linking EC to response copying probabilities between classifiers.

Result: The new methods reveal that several previously reported differences between deep vision models in NeuroAI are statistically insignificant, highlighting issues in prior benchmarking conclusions.

Conclusion: The proposed methodologies improve the reliability and rigor of ML model benchmarking by providing tools to detect statistically significant behavioral differences, enabling more robust comparisons.

Abstract: Benchmarking models is a key factor for the rapid progress in machine
learning (ML) research. Thus, further progress depends on improving
benchmarking metrics. A standard metric to measure the behavioral alignment
between ML models and human observers is error consistency (EC). EC allows for
more fine-grained comparisons of behavior than other metrics such as e.g.
accuracy, and has been used in the influential Brain-Score benchmark to rank
different DNNs by their behavioral consistency with humans. Previously, EC
values have been reported without confidence intervals. However, empirically
measured EC values are typically noisy -- thus, without confidence intervals,
valid benchmarking conclusions are problematic. Here we improve on standard EC
in two ways: First, we show how to obtain confidence intervals for EC using a
bootstrapping technique, allowing us to derive significance tests for EC.
Second, we propose a new computational model relating the EC between two
classifiers to the implicit probability that one of them copies responses from
the other. This view of EC allows us to give practical guidance to scientists
regarding the number of trials required for sufficiently powerful, conclusive
experiments. Finally, we use our methodology to revisit popular
NeuroAI-results. We find that while the general trend of behavioral differences
between humans and machines holds up to scrutiny, many reported differences
between deep vision models are statistically insignificant. Our methodology
enables researchers to design adequately powered experiments that can reliably
detect behavioral differences between models, providing a foundation for more
rigorous benchmarking of behavioral alignment.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [253] [On the Hardness of Unsupervised Domain Adaptation: Optimal Learners and Information-Theoretic Perspective](https://arxiv.org/abs/2507.06552)
*Zhiyi Dong,Zixuan Liu,Yongyi Mao*

Main category: stat.ML

TL;DR: The paper analyzes unsupervised domain adaptation (UDA) under covariate shift, introduces Posterior Target Label Uncertainty (PTLU) to quantify UDA difficulty, and demonstrates its advantages over existing measures.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the uncertainty and hardness in UDA by focusing on realistic scenarios that deviate from classical worst-case analyses.

Method: The authors define PTLU, an information-theoretic metric capturing entropy in target label predictions, and prove it serves as a lower-bound for UDA learner risk.

Result: PTLU effectively quantifies the difficulty of UDA learning and offers better evaluation benchmarks compared to traditional metrics.

Conclusion: PTLU and its empirical counterpart (EPTLU) provide meaningful proxies for assessing UDA difficulty, potentially improving learning strategies under covariate shifts.

Abstract: This paper studies the hardness of unsupervised domain adaptation (UDA) under
covariate shift. We model the uncertainty that the learner faces by a
distribution $\pi$ in the ground-truth triples $(p, q, f)$ -- which we call a
UDA class -- where $(p, q)$ is the source -- target distribution pair and $f$
is the classifier. We define the performance of a learner as the overall target
domain risk, averaged over the randomness of the ground-truth triple. This
formulation couples the source distribution, the target distribution and the
classifier in the ground truth, and deviates from the classical worst-case
analyses, which pessimistically emphasize the impact of hard but rare UDA
instances. In this formulation, we precisely characterize the optimal learner.
The performance of the optimal learner then allows us to define the learning
difficulty for the UDA class and for the observed sample. To quantify this
difficulty, we introduce an information-theoretic quantity -- Posterior Target
Label Uncertainty (PTLU) -- along with its empirical estimate (EPTLU) from the
sample , which capture the uncertainty in the prediction for the target domain.
Briefly, PTLU is the entropy of the predicted label in the target domain under
the posterior distribution of ground-truth classifier given the observed source
and target samples. By proving that such a quantity serves to lower-bound the
risk of any learner, we suggest that these quantities can be used as proxies
for evaluating the hardness of UDA learning. We provide several examples to
demonstrate the advantage of PTLU, relative to the existing measures, in
evaluating the difficulty of UDA learning.

</details>


### [254] [Semi-parametric Functional Classification via Path Signatures Logistic Regression](https://arxiv.org/abs/2507.06637)
*Pengcheng Zeng,Siyuan Jiang*

Main category: stat.ML

TL;DR: The paper introduces PSLR for effective classification of vector-valued functional data, addressing challenges from irregular sampling.


<details>
  <summary>Details</summary>
Motivation: Classical functional logistic regression models struggle with linear limitations and irregular sampling in functional data.

Method: It uses truncated path signatures to create a finite-dimensional, basis-free representation that captures nonlinear and cross-channel dependencies.

Result: PSLR surpasses traditional models in accuracy, robustness, and interpretability, particularly under irregular sampling.

Conclusion: Integrating rough path theory into functional data analysis enhances performance and resolves limitations of traditional methods.

Abstract: We propose Path Signatures Logistic Regression (PSLR), a semi-parametric
framework for classifying vector-valued functional data with scalar covariates.
Classical functional logistic regression models rely on linear assumptions and
fixed basis expansions, which limit flexibility and degrade performance under
irregular sampling. PSLR overcomes these issues by leveraging truncated path
signatures to construct a finite-dimensional, basis-free representation that
captures nonlinear and cross-channel dependencies. By embedding trajectories as
time-augmented paths, PSLR extracts stable, geometry-aware features that are
robust to sampling irregularity without requiring a common time grid, while
still preserving subject-specific timing patterns. We establish theoretical
guarantees for the existence and consistent estimation of the optimal
truncation order, along with non-asymptotic risk bounds. Experiments on
synthetic and real-world datasets show that PSLR outperforms traditional
functional classifiers in accuracy, robustness, and interpretability,
particularly under non-uniform sampling schemes. Our results highlight the
practical and theoretical benefits of integrating rough path theory into modern
functional data analysis.

</details>


### [255] [Fast Gaussian Processes under Monotonicity Constraints](https://arxiv.org/abs/2507.06677)
*Chao Zhang,Jasper M. Everink,Jakob Sauer Jørgensen*

Main category: stat.ML

TL;DR: The paper proposes a virtual point-based framework for constrained Gaussian process (GP) models under monotonicity constraints, enabling efficient sampling using regularized linear randomize-then-optimize (RLRTO). Improvements in computational efficiency are demonstrated using a Python implementation.


<details>
  <summary>Details</summary>
Motivation: Gaussian processes are powerful surrogate models, but integrating prior knowledge like monotonicity is computationally difficult in high dimensions. This research seeks to enhance model fidelity while addressing computational challenges.

Method: The paper introduces a novel virtual point-based framework using regularized linear randomize-then-optimize (RLRTO). It also updates existing methods by replacing Gibbs sampling with the No U-Turn Sampler (NUTS) for efficiency.

Result: The methods achieve comparable predictive performance but significantly improve computational efficiency, especially with the RLRTO method, as validated on synthetic functions and differential equation systems.

Conclusion: The proposed approaches offer an efficient and practical framework for constrained GP modeling. The implementation allows for versatile use in various applied computational problems.

Abstract: Gaussian processes (GPs) are widely used as surrogate models for complicated
functions in scientific and engineering applications. In many cases, prior
knowledge about the function to be approximated, such as monotonicity, is
available and can be leveraged to improve model fidelity. Incorporating such
constraints into GP models enhances predictive accuracy and reduces
uncertainty, but remains a computationally challenging task for
high-dimensional problems. In this work, we present a novel virtual point-based
framework for building constrained GP models under monotonicity constraints,
based on regularized linear randomize-then-optimize (RLRTO), which enables
efficient sampling from a constrained posterior distribution by means of
solving randomized optimization problems. We also enhance two existing virtual
point-based approaches by replacing Gibbs sampling with the No U-Turn Sampler
(NUTS) for improved efficiency. A Python implementation of these methods is
provided and can be easily applied to a wide range of problems. This
implementation is then used to validate the approaches on approximating a range
of synthetic functions, demonstrating comparable predictive performance between
all considered methods and significant improvements in computational efficiency
with the two NUTS methods and especially with the RLRTO method. The framework
is further applied to construct surrogate models for systems of differential
equations.

</details>


### [256] [Adaptive collaboration for online personalized distributed learning with heterogeneous clients](https://arxiv.org/abs/2507.06844)
*Constantin Philippenko,Batiste Le Bars,Kevin Scaman,Laurent Massoulié*

Main category: stat.ML

TL;DR: The paper addresses online personalized decentralized learning, introducing a dynamic gradient-based collaboration method for variance reduction while managing bias.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of collaboratively accelerating local training among statistically heterogeneous clients while minimizing gradient variance and the potential bias introduced by collaborations.

Method: A gradient-based collaboration criterion is introduced to dynamically select collaborators with similar gradients. The approach builds on a refined theoretical foundation of the All-for-one algorithm with detailed excess loss bounds for various types of objective functions.

Result: The proposed approaches are theoretically supported and experimentally validated, showing that one variant retains the optimality of the All-for-one algorithm while effectively reducing variance.

Conclusion: The gradient-based collaboration method enhances decentralized learning by mitigating bias, reducing gradient variance, and preserving algorithmic optimality, with broad applicability demonstrated through diverse datasets.

Abstract: We study the problem of online personalized decentralized learning with $N$
statistically heterogeneous clients collaborating to accelerate local training.
An important challenge in this setting is to select relevant collaborators to
reduce gradient variance while mitigating the introduced bias. To tackle this,
we introduce a gradient-based collaboration criterion, allowing each client to
dynamically select peers with similar gradients during the optimization
process. Our criterion is motivated by a refined and more general theoretical
analysis of the All-for-one algorithm, proved to be optimal in Even et al.
(2022) for an oracle collaboration scheme. We derive excess loss upper-bounds
for smooth objective functions, being either strongly convex, non-convex, or
satisfying the Polyak-Lojasiewicz condition; our analysis reveals that the
algorithm acts as a variance reduction method where the speed-up depends on a
sufficient variance. We put forward two collaboration methods instantiating the
proposed general schema; and we show that one variant preserves the optimality
of All-for-one. We validate our results with experiments on synthetic and real
datasets.

</details>


### [257] [Conformal Prediction for Long-Tailed Classification](https://arxiv.org/abs/2507.06867)
*Tiffany Ding,Jean-Baptiste Fermanian,Joseph Salmon*

Main category: stat.ML

TL;DR: The paper addresses the challenge of making prediction sets suitable for classification problems with long-tailed class distributions, proposing two methods to achieve both reasonable prediction set sizes and good class-conditional coverage.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the problem of handling long-tailed class distributions in classification tasks, like plant identification, where prediction methods often trade off small set sizes for poor coverage of rare classes or large sets with good coverage.

Method: Two methods are proposed: (1) Prevalence-adjusted softmax to target macro-coverage, a relaxed form of class-conditional coverage, and (2) Label-weighted conformal prediction to interpolate between marginal and class-conditional coverage.

Result: The proposed methods were evaluated on two long-tailed image datasets, Pl@ntNet and iNaturalist, with substantial improvements in balancing class-conditional coverage and prediction set size.

Conclusion: The methods provide a way to address challenges in long-tailed classification problems by improving marginal coverage while maintaining practical set sizes, making them useful for real-world tasks.

Abstract: Many real-world classification problems, such as plant identification, have
extremely long-tailed class distributions. In order for prediction sets to be
useful in such settings, they should (i) provide good class-conditional
coverage, ensuring that rare classes are not systematically omitted from the
prediction sets, and (ii) be a reasonable size, allowing users to easily verify
candidate labels. Unfortunately, existing conformal prediction methods, when
applied to the long-tailed setting, force practitioners to make a binary choice
between small sets with poor class-conditional coverage or sets with very good
class-conditional coverage but that are extremely large. We propose methods
with guaranteed marginal coverage that smoothly trade off between set size and
class-conditional coverage. First, we propose a conformal score function,
prevalence-adjusted softmax, that targets a relaxed notion of class-conditional
coverage called macro-coverage. Second, we propose a label-weighted conformal
prediction method that allows us to interpolate between marginal and
class-conditional conformal prediction. We demonstrate our methods on Pl@ntNet
and iNaturalist, two long-tailed image datasets with 1,081 and 8,142 classes,
respectively.

</details>


### [258] [Distribution-free inference for LightGBM and GLM with Tweedie loss](https://arxiv.org/abs/2507.06921)
*Alokesh Manna,Aditya Vikram Sett,Dipak K. Dey,Yuwen Gu,Elizabeth D. Schifano,Jichao He*

Main category: stat.ML

TL;DR: The paper proposes new non-conformity measures for GLMs and GBMs in insurance claims modeling, demonstrating improved prediction interval performance with locally weighted Pearson residuals.


<details>
  <summary>Details</summary>
Motivation: To improve accuracy in predicting insurance claims by accounting for prediction uncertainty using conformal predictive inference methods.

Method: Developed and tested new non-conformity measures for GLMs and GBMs (e.g., regularized Tweedie GLM regression and LightGBM with Tweedie loss) in the context of insurance claims data.

Result: Demonstrated that locally weighted Pearson residuals for LightGBM yielded prediction intervals with nominal coverage and minimized average interval width in simulations.

Conclusion: The proposed methods effectively quantify prediction uncertainty in insurance claims data, improving reliability and efficiency of predictive models.

Abstract: Prediction uncertainty quantification is a key research topic in recent years
scientific and business problems. In insurance industries
(\cite{parodi2023pricing}), assessing the range of possible claim costs for
individual drivers improves premium pricing accuracy. It also enables insurers
to manage risk more effectively by accounting for uncertainty in accident
likelihood and severity. In the presence of covariates, a variety of
regression-type models are often used for modeling insurance claims, ranging
from relatively simple generalized linear models (GLMs) to regularized GLMs to
gradient boosting models (GBMs). Conformal predictive inference has arisen as a
popular distribution-free approach for quantifying predictive uncertainty under
relatively weak assumptions of exchangeability, and has been well studied under
the classic linear regression setting. In this work, we propose new
non-conformity measures for GLMs and GBMs with GLM-type loss. Using regularized
Tweedie GLM regression and LightGBM with Tweedie loss, we demonstrate conformal
prediction performance with these non-conformity measures in insurance claims
data. Our simulation results favor the use of locally weighted Pearson
residuals for LightGBM over other methods considered, as the resulting
intervals maintained the nominal coverage with the smallest average width.

</details>


### [259] [Off-Policy Evaluation Under Nonignorable Missing Data](https://arxiv.org/abs/2507.06961)
*Han Wang,Yang Xu,Wenbin Lu,Rui Song*

Main category: stat.ML

TL;DR: The paper studies Off-Policy Evaluation (OPE) under conditions of missing data, proposing a new unbiased estimation method.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding and methods for accurate OPE under conditions of missing data in real-world applications.

Method: The authors propose an inverse probability weighted value estimator combined with statistical inference to handle missing data in OPE.

Result: The proposed estimator provides more reliable and unbiased value estimates, especially under ignorable missingness, as shown in numerical experiments.

Conclusion: The findings offer a robust theoretical and empirical approach to maintain consistency in OPE, even in scenarios with missing data.

Abstract: Off-Policy Evaluation (OPE) aims to estimate the value of a target policy
using offline data collected from potentially different policies. In real-world
applications, however, logged data often suffers from missingness. While OPE
has been extensively studied in the literature, a theoretical understanding of
how missing data affects OPE results remains unclear. In this paper, we
investigate OPE in the presence of monotone missingness and theoretically
demonstrate that the value estimates remain unbiased under ignorable
missingness but can be biased under nonignorable (informative) missingness. To
retain the consistency of value estimation, we propose an inverse probability
weighted value estimator and conduct statistical inference to quantify the
uncertainty of the estimates. Through a series of numerical experiments, we
empirically demonstrate that our proposed estimator yields a more reliable
value inference under missing data.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [260] [Graph-Based Complexity Metrics for Multi-Agent Curriculum Learning: A Validated Approach to Task Ordering in Cooperative Coordination Environments](https://arxiv.org/abs/2507.07074)
*Farhaan Ebadulla,Dharini Hindlatti,Srinivaasan NS,Apoorva VH,Ayman Aftab*

Main category: cs.MA

TL;DR: The paper introduces a graph-based complexity metric for task sequencing in multi-agent reinforcement learning (MARL) and validates its effectiveness in guiding curriculum learning and improving performance in cooperative tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of curriculum design and task sequencing in multi-agent reinforcement learning, especially for cooperative coordination, by introducing a principled complexity metric.

Method: The authors propose a graph-based coordination complexity metric that incorporates agent dependency entropy, spatial interference patterns, and goal overlap analysis to assess task difficulty. They validate the metric empirically and use it within a curriculum learning framework tested with MADDPG in specific MARL environments.

Result: The complexity metric shows a high correlation (rho = 0.952, p < 0.001) between predicted task difficulty and empirical difficulty. The framework achieves a 56-fold performance improvement in a tight coordination task (MultiWalker) and systematic task progression in cooperative navigation (Simple Spread).

Conclusion: The proposed metric is effective for multi-agent curriculum design. Coordination tightness is identified as a key factor in curriculum learning effectiveness, benefiting environments that require strict inter-agent coordination. This contributes to practical multi-robot coordination.

Abstract: Multi-agent reinforcement learning (MARL) faces significant challenges in
task sequencing and curriculum design, particularly for cooperative
coordination scenarios. While curriculum learning has demonstrated success in
single-agent domains, principled approaches for multi-agent coordination remain
limited due to the absence of validated task complexity metrics. This approach
presents a graph-based coordination complexity metric that integrates agent
dependency entropy, spatial interference patterns, and goal overlap analysis to
predict task difficulty in multi-agent environments. The complexity metric
achieves strong empirical validation with rho = 0.952 correlation (p < 0.001)
between predicted complexity and empirical difficulty determined by random
agent performance evaluation. This approach evaluates the curriculum learning
framework using MADDPG across two distinct coordination environments: achieving
56x performance improvement in tight coordination tasks (MultiWalker) and
demonstrating systematic task progression in cooperative navigation (Simple
Spread). Through systematic analysis, coordination tightness emerges as a
predictor of curriculum learning effectiveness, where environments requiring
strict agent interdependence benefit substantially from structured progression.
This approach provides a validated complexity metric for multi-agent curriculum
design and establishes empirical guidelines for multi-robot coordination
applications.

</details>


### [261] [A Survey of Multi Agent Reinforcement Learning: Federated Learning and Cooperative and Noncooperative Decentralized Regimes](https://arxiv.org/abs/2507.06278)
*Kemboi Cheruiyot,Nickson Kiprotich,Vyacheslav Kungurtsev,Kennedy Mugo,Vivian Mwirigi,Marvin Ngesa*

Main category: cs.MA

TL;DR: The paper surveys AI agent interaction structures: Federal RL, Decentralized RL, and Noncooperative RL.


<details>
  <summary>Details</summary>
Motivation: The study was motivated by the increasing research interest in autonomous agents interacting in various multi-agent environments.

Method: The authors conducted a comprehensive survey of three main frameworks: Federal RL, Decentralized RL, and Noncooperative RL, examining their structures, theories, and performance.

Result: They summarized advancements, identified structural commonalities, theoretical guarantees, and performance limitations in the surveyed domains.

Conclusion: This survey consolidates recent progress in multi-agent reinforcement learning, aiding in understanding different interaction topologies and their implications.

Abstract: The increasing interest in research and innovation towards the development of
autonomous agents presents a number of complex yet important scenarios of
multiple AI Agents interacting with each other in an environment. The
particular setting can be understood as exhibiting three possibly topologies of
interaction - centrally coordinated cooperation, ad-hoc interaction and
cooperation, and settings with noncooperative incentive structures. This
article presents a comprehensive survey of all three domains, defined under the
formalism of Federal Reinforcement Learning (RL), Decentralized RL, and
Noncooperative RL, respectively. Highlighting the structural similarities and
distinctions, we review the state of the art in these subjects, primarily
explored and developed only recently in the literature. We include the
formulations as well as known theoretical guarantees and highlights and
limitations of numerical performance.

</details>


### [262] [Gradientsys: A Multi-Agent LLM Scheduler with ReAct Orchestration](https://arxiv.org/abs/2507.06520)
*Xinyuan Song,Zeyu Wang,Siyi Wu,Tianyu Shi,Lynn Ai*

Main category: cs.MA

TL;DR: Gradientsys introduces a next-gen multi-agent scheduling system using advanced AI agent coordination and dynamic planning for improved task handling, reducing latency and costs.


<details>
  <summary>Details</summary>
Motivation: To enhance AI multi-agent coordination for efficient task management with advanced scheduling and observability.

Method: Utilizes MCP, a ReAct planning loop, and LLM-powered scheduler for intelligent and efficient agent orchestration.

Result: Higher task success rates, reduced latency, and lower API costs demonstrated in GAIA benchmark experiments.

Conclusion: Gradientsys showcases superior multi-agent coordination capabilities compared to existing frameworks, emphasizing flexibility, performance, and cost efficiency.

Abstract: We present Gradientsys, a next-generation multi-agent scheduling framework
that coordinates diverse specialized AI agents using a typed Model-Context
Protocol (MCP) and a ReAct-based dynamic planning loop. At its core,
Gradientsys employs an LLM-powered scheduler for intelligent one-to-many task
dispatch, enabling parallel execution of heterogeneous agents such as PDF
parsers, web search modules, GUI controllers, and web builders. The framework
supports hybrid synchronous/asynchronous execution, respects agent capacity
constraints, and incorporates a robust retry-and-replan mechanism to handle
failures gracefully. To promote transparency and trust, Gradientsys includes an
observability layer streaming real-time agent activity and intermediate
reasoning via Server-Sent Events (SSE). We offer an architectural overview and
evaluate Gradientsys against existing frameworks in terms of extensibility,
scheduling topology, tool reusability, parallelism, and observability.
Experiments on the GAIA general-assistant benchmark show that Gradientsys
achieves higher task success rates with reduced latency and lower API costs
compared to a MinionS-style baseline, demonstrating the strength of its
LLM-driven multi-agent orchestration.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [263] [OpenDPDv2: A Unified Learning and Optimization Framework for Neural Network Digital Predistortion](https://arxiv.org/abs/2507.06849)
*Yizhuo Wu,Ang Li,Chang Gao*

Main category: eess.SP

TL;DR: This paper introduces OpenDPDv2, a framework for energy-efficient Digital Predistortion (DPD) for RF power amplifiers using neural networks, achieving significant performance improvements while reducing energy consumption.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the high energy consumption and parameter reliance of traditional neural network-based digital predistortion (DPD), particularly for wideband RF power amplifiers with complex signals, without compromising linearization performance.

Method: The paper introduces OpenDPDv2, including the novel TRes-DeltaGRU-DPD algorithm, fixed-point quantization, and exploitation of dynamic temporal sparsity in input signals and hidden neurons.

Result: The FP32 TRes-DeltaGRU-DPD model achieves -59.4 dBc ACPR and -42.1 dBc EVM, with energy reduction of 4.5X and performance of -50.3 dBc ACPR and -35.2 dBc EVM maintained with 56% temporal sparsity in a 3.5 GHz GaN Doherty RF PA.

Conclusion: OpenDPDv2 successfully combines high linearization performance with significant energy efficiency. Its open-source implementation contributes to accessible advancements in DPD for RF systems.

Abstract: Neural network (NN)-based Digital Predistortion (DPD) stands out in improving
signal quality in wideband radio frequency (RF) power amplifiers (PAs)
employing complex modulation. However, NN DPDs usually rely on a large number
of parameters for effective linearization and can significantly contribute to
the energy consumption of the digital back-end in RF systems. This paper
presents OpenDPDv2, a unified framework for PA modeling, DPD learning, and
model optimization to reduce power consumption while maintaining high
linearization performance. The optimization techniques feature a novel DPD
algorithm, TRes-DeltaGRU, alongside two energy-efficient methods. The
top-performing 32-bit floating-point (FP32) TRes-DeltaGRU-DPD model achieves an
Adjacent Channel Power Ratio (ACPR) of -59.4 dBc and Error Vector Magnitude
(EVM) of -42.1 dBc. By exploiting fixed-point quantization and dynamic temporal
sparsity of input signals and hidden neurons, the inference energy of our model
can be reduced by 4.5X while still maintaining -50.3 dBc ACPR and -35.2 dB EVM
with 56% temporal sparsity. This was evaluated using a TM3.1a 200 MHz bandwidth
256-QAM OFDM signal applied to a 3.5 GHz GaN Doherty RF PA. OpenDPDv2 code,
datasets, and documentation are publicly accessible at:
https://github.com/lab-emi/OpenDPD.

</details>


### [264] [Federated Learning-based MARL for Strengthening Physical-Layer Security in B5G Networks](https://arxiv.org/abs/2507.06997)
*Deemah H. Tashman,Soumaya Cherkaoui,Walaa Hamouda*

Main category: eess.SP

TL;DR: This paper applies a federated learning approach using multi-agent reinforcement learning to bolster physical-layer security in beyond 5G multi-cellular networks, comparing deep Q-network (DQN) and Reinforce deep policy gradient (RDPG) methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring physical-layer security in multi-cellular networks, particularly in combating attempts by eavesdroppers to intercept sensitive communication.

Method: Each base station functions as a deep reinforcement learning agent, employing federated learning to maximize secrecy rates while sharing network parameters but not private user data. Two approaches, DQN and RDPG, are implemented and compared.

Result: The RDPG method exhibited faster convergence compared to DQN and surpassed the performance of a distributed DRL approach, highlighting a trade-off between security and computational complexity.

Conclusion: Federated learning-based MARL strategies introduce an effective solution for improving physical-layer security in beyond 5G networks, with RDPG as the preferred method due to its rapid convergence and effective results.

Abstract: This paper explores the application of a federated learning-based multi-agent
reinforcement learning (MARL) strategy to enhance physical-layer security (PLS)
in a multi-cellular network within the context of beyond 5G networks. At each
cell, a base station (BS) operates as a deep reinforcement learning (DRL) agent
that interacts with the surrounding environment to maximize the secrecy rate of
legitimate users in the presence of an eavesdropper. This eavesdropper attempts
to intercept the confidential information shared between the BS and its
authorized users. The DRL agents are deemed to be federated since they only
share their network parameters with a central server and not the private data
of their legitimate users. Two DRL approaches, deep Q-network (DQN) and
Reinforce deep policy gradient (RDPG), are explored and compared. The results
demonstrate that RDPG converges more rapidly than DQN. In addition, we
demonstrate that the proposed method outperforms the distributed DRL approach.
Furthermore, the outcomes illustrate the trade-off between security and
complexity.

</details>


### [265] [How to Bridge the Sim-to-Real Gap in Digital Twin-Aided Telecommunication Networks](https://arxiv.org/abs/2507.07067)
*Clement Ruah,Houssem Sifaou,Osvaldo Simeone,Bashir M. Al-Hashimi*

Main category: eess.SP

TL;DR: This paper addresses the challenge of training AI models in telecommunications by utilizing data generated by digital twins, and explores methods to mitigate the simulation-to-reality gap.


<details>
  <summary>Details</summary>
Motivation: The scarcity of deployment-specific data in telecommunications hinders AI training, and real data collection is costly while available datasets lack the necessary operational variability.

Method: The authors evaluated two strategies: calibrating digital twins with real-world data and using sim-to-real gap-aware training strategies, specifically Bayesian learning and prediction-powered inference.

Result: Two distinct methods, Bayesian learning and prediction-powered inference, were assessed for handling discrepancies between simulated and real-world data.

Conclusion: Digital twins can supplement AI training datasets, but effectively bridging the simulation-to-reality gap requires calibration with real data and sim-to-real-aware training methods.

Abstract: Training effective artificial intelligence models for telecommunications is
challenging due to the scarcity of deployment-specific data. Real data
collection is expensive, and available datasets often fail to capture the
unique operational conditions and contextual variability of the network
environment. Digital twinning provides a potential solution to this problem, as
simulators tailored to the current network deployment can generate
site-specific data to augment the available training datasets. However, there
is a need to develop solutions to bridge the inherent simulation-to-reality
(sim-to-real) gap between synthetic and real-world data. This paper reviews
recent advances on two complementary strategies: 1) the calibration of digital
twins (DTs) through real-world measurements, and 2) the use of sim-to-real
gap-aware training strategies to robustly handle residual discrepancies between
digital twin-generated and real data. For the latter, we evaluate two
conceptually distinct methods that model the sim-to-real gap either at the
level of the environment via Bayesian learning or at the level of the training
loss via prediction-powered inference.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [266] [Robust Containerization of the High Angular Resolution Functional Imaging (HARFI) Pipeline](https://arxiv.org/abs/2507.07010)
*Zhiyuan Li,Kurt G. Schilling,Bennett A. Landman*

Main category: physics.med-ph

TL;DR: This paper introduces a containerized version of the High Angular Resolution Functional Imaging (HARFI) pipeline for studying functional activity in white matter using fMRI.


<details>
  <summary>Details</summary>
Motivation: To facilitate broader exploration of functional white matter architecture and reduce technical barriers to using the HARFI pipeline.

Method: A containerized implementation of the HARFI pipeline using odd-order spherical harmonics to analyze high angular resolution functional correlations in white matter.

Result: The containerized HARFI pipeline offers an efficient, reproducible, and accessible tool for analyzing functional white matter across multiple datasets.

Conclusion: This work supports reproducible research and enables deeper insights into functional white matter by eliminating technical complexities in using HARFI.

Abstract: Historically, functional magnetic resonance imaging (fMRI) of the brain has
focused primarily on gray matter, particularly the cortical gray matter and
associated nuclei. However, recent work has demonstrated that functional
activity in white matter also plays a meaningful role in both cognition and
learning. In previous work, we introduced the High Angular Resolution
Functional Imaging (HARFI) pipeline, which demonstrated both local and global
patterns of functional correlation in white matter. Notably, HARFI enabled
exploration of asymmetric voxel-wise correlation using odd-order spherical
harmonics. Although the original implementation of HARFI was released via
GitHub, adoption was limited due to the technical complexity of running the
source code. In this work, we present a robust and efficient containerized
version of the HARFI pipeline, enabling seamless execution across multiple
public datasets. Our goal is to facilitate broader and deeper exploration of
functional white matter architecture, especially through the lens of high
angular resolution functional correlations. The key innovation of this work is
the containerized implementation, which we have made available under a
permissive open-source license to support reproducible and accessible research
practices.

</details>


### [267] [Magneto-radiative modelling and artificial neural network optimization of biofluid flow in a stenosed arterial domain](https://arxiv.org/abs/2507.06273)
*S P Shivakumar,Gunisetty Ramasekhar,P Nimmy,Sujesh Areekara,L Thanuja,T V Smitha,S Devanathan,Ganesh R Naik,K V Nagaraja*

Main category: physics.med-ph

TL;DR: This study explores the flow properties of Casson-Maxwell nanofluids through a stenosed artery and their potential for advanced, targeted drug delivery.


<details>
  <summary>Details</summary>
Motivation: To address the inefficacy of traditional cardiovascular treatments and encourage sustainable healthcare technologies aligned with UN SDGs 3, 4, 9, and 17.

Method: The paper employs fluid dynamics analyses and forecasts heat flow using the Levenberg-Marquardt backpropagation technique, incorporating parameters such as nanoparticle volume fractions and the Maxwell parameter.

Result: Casson-Maxwell nanofluids show improved drug residence time and heat transfer dynamics, with specific trends identified for nanoparticle types and parameter sensitivities.

Conclusion: Findings highlight the potential of Casson-Maxwell nanofluids for targeted drug delivery and interdisciplinary innovation in sustainable healthcare and fluid dynamics.

Abstract: The increasing complexity of cardiovascular diseases and limitations in
traditional healing methods mandate the invention of new drug delivery systems
that assure targeted, effective, and regulated treatments, contributing
directly to UN SDGs 3 and 9, thereby encouraging the utilization of sustainable
medical technologies in healthcare. This study investigates the flow of a
Casson-Maxwell nanofluid through a stenosed arterial domain. The quantities,
such as skin friction and heat transfer rate, are analysed in detail. The
Casson-Maxwell fluid shows a lower velocity profile than the Casson fluids,
which indicates the improved residence time for efficient drug delivery. The
heat transfer rate shows an increase with higher volume fractions of copper and
aluminium oxide nanoparticles and a decrease with higher volume fractions of
silver nanoparticles. The skin friction coefficient decreases by 219% with a
unit increase in the Maxwell parameter, whereas it increases by 66.1% with a
unit rise in the Casson parameter. This work supports SDGs 4 and 17 by
fostering interdisciplinary learning and collaboration in fluid dynamics and
healthcare innovation. Additionally, the rate of heat flow was forecasted (with
an overall R-value of 0.99457) using the Levenberg-Marquardt backpropagation
training scheme under the influence of magneto-radiative, linear heat source
and Casson-Maxwell parameters along with the tri-metallic nanoparticle volume
fractions. It is also observed that the drag coefficient is most sensitive to
the changes in the Maxwell parameter.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [268] [X-ray transferable polyrepresentation learning](https://arxiv.org/abs/2507.06264)
*Weronika Hryniewska-Guzik,Przemyslaw Biecek*

Main category: eess.IV

TL;DR: This paper introduces a method called polyrepresentation, integrating multiple modalities for improved feature extraction and performance, particularly in image-related solutions.


<details>
  <summary>Details</summary>
Motivation: Machine learning algorithms depend on quality feature extraction, but generalizing feature extraction effectively to unseen datasets remains a critical challenge.

Method: The authors propose polyrepresentation, integrating multiple representations from diverse sources (e.g., Siamese Networks, self-supervised models, radiomic features).

Result: Polyrepresentation outperforms single representations, demonstrated specifically with X-ray images, and transfers effectively to smaller datasets.

Conclusion: Polyrepresentation is resource-efficient, transferable, and applicable beyond medical imaging to other domains, highlighting its broad potential.

Abstract: The success of machine learning algorithms is inherently related to the
extraction of meaningful features, as they play a pivotal role in the
performance of these algorithms. Central to this challenge is the quality of
data representation. However, the ability to generalize and extract these
features effectively from unseen datasets is also crucial. In light of this, we
introduce a novel concept: the polyrepresentation. Polyrepresentation
integrates multiple representations of the same modality extracted from
distinct sources, for example, vector embeddings from the Siamese Network,
self-supervised models, and interpretable radiomic features. This approach
yields better performance metrics compared to relying on a single
representation. Additionally, in the context of X-ray images, we demonstrate
the transferability of the created polyrepresentation to a smaller dataset,
underscoring its potential as a pragmatic and resource-efficient approach in
various image-related solutions. It is worth noting that the concept of
polyprepresentation on the example of medical data can also be applied to other
domains, showcasing its versatility and broad potential impact.

</details>


### [269] [Photometric Stereo using Gaussian Splatting and inverse rendering](https://arxiv.org/abs/2507.06684)
*Matéo Ducastel,David Tschumperlé,Yvain Quéau*

Main category: eess.IV

TL;DR: The paper explores using Gaussian Splatting for calibrated photometric stereo, offering an interpretable 3D scene reconstruction method.


<details>
  <summary>Details</summary>
Motivation: To improve 3D scene reconstruction in calibrated photometric stereo with interpretable parameterization using modern inverse rendering techniques.

Method: The authors leverage Gaussian Splatting formalism for parameterizing and optimizing 3D scene reconstruction in photometric stereo, simplifying the light representation model.

Result: This approach demonstrates the feasibility and potential advantages of Gaussian Splatting in solving calibrated photometric stereo problems.

Conclusion: Gaussian Splatting offers a promising framework for photometric stereo by combining interpretability and optimization efficiency.

Abstract: Recent state-of-the-art algorithms in photometric stereo rely on neural
networks and operate either through prior learning or inverse rendering
optimization. Here, we revisit the problem of calibrated photometric stereo by
leveraging recent advances in 3D inverse rendering using the Gaussian Splatting
formalism. This allows us to parameterize the 3D scene to be reconstructed and
optimize it in a more interpretable manner. Our approach incorporates a
simplified model for light representation and demonstrates the potential of the
Gaussian Splatting rendering engine for the photometric stereo problem.

</details>


### [270] [Mamba Goes HoME: Hierarchical Soft Mixture-of-Experts for 3D Medical Image Segmentation](https://arxiv.org/abs/2507.06363)
*Szymon Płotka,Maciej Chrabaszcz,Gizem Mert,Ewa Szczurek,Arkadiusz Sitek*

Main category: eess.IV

TL;DR: The paper presents Hierarchical Soft Mixture-of-Experts (HoME), a novel model for 3D medical image segmentation, with improved performance and efficiency across diverse imaging modalities.


<details>
  <summary>Details</summary>
Motivation: To address challenges in efficient 3D medical image processing and managing variability across different modalities and data quality.

Method: Proposed a two-level token-routing method called Hierarchical Soft Mixture-of-Experts (HoME), involving local and global Soft Mixture-of-Experts (SMoE) layers for segmenting 3D medical images, built on the Mamba state-space model (SSM) backbone.

Result: HoME achieved state-of-the-art segmentation performance across datasets covering the three most common 3D medical imaging modalities.

Conclusion: HoME's hierarchical approach, with local and global expert refinement, significantly improves medical image segmentation's efficiency and generalizability.

Abstract: In recent years, artificial intelligence has significantly advanced medical
image segmentation. However, challenges remain, including efficient 3D medical
image processing across diverse modalities and handling data variability. In
this work, we introduce Hierarchical Soft Mixture-of-Experts (HoME), a
two-level token-routing layer for efficient long-context modeling, specifically
designed for 3D medical image segmentation. Built on the Mamba state-space
model (SSM) backbone, HoME enhances sequential modeling through sparse,
adaptive expert routing. The first stage employs a Soft Mixture-of-Experts
(SMoE) layer to partition input sequences into local groups, routing tokens to
specialized per-group experts for localized feature extraction. The second
stage aggregates these outputs via a global SMoE layer, enabling cross-group
information fusion and global context refinement. This hierarchical design,
combining local expert routing with global expert refinement improves
generalizability and segmentation performance, surpassing state-of-the-art
results across datasets from the three most commonly used 3D medical imaging
modalities and data quality.

</details>


### [271] [Capsule-ConvKAN: A Hybrid Neural Approach to Medical Image Classification](https://arxiv.org/abs/2507.06417)
*Laura Pituková,Peter Sinčák,László József Kovács*

Main category: eess.IV

TL;DR: The study compares four neural architectures, introducing Capsule-ConvKAN, which achieves the best classification accuracy (91.21%) on biomedical images.


<details>
  <summary>Details</summary>
Motivation: Traditional convolutional models encounter challenges in representing complex spatial patterns and feature hierarchies, especially in medical image analysis.

Method: The researchers proposed Capsule-ConvKAN, combining Capsule and Convolutional Kolmogorov--Arnold Networks. They compared its performance against three other neural architectures using a histopathological image dataset.

Result: Capsule-ConvKAN outperformed other models, achieving the highest classification accuracy of 91.21% on biomedical image data evaluation.

Conclusion: The novel Capsule-ConvKAN model effectively addresses traditional model limitations and excels in biomedical image classification, showing its potential for challenging real-world applications.

Abstract: This study conducts a comprehensive comparison of four neural network
architectures: Convolutional Neural Network, Capsule Network, Convolutional
Kolmogorov--Arnold Network, and the newly proposed Capsule--Convolutional
Kolmogorov--Arnold Network. The proposed Capsule-ConvKAN architecture combines
the dynamic routing and spatial hierarchy capabilities of Capsule Network with
the flexible and interpretable function approximation of Convolutional
Kolmogorov--Arnold Networks. This novel hybrid model was developed to improve
feature representation and classification accuracy, particularly in challenging
real-world biomedical image data. The architectures were evaluated on a
histopathological image dataset, where Capsule-ConvKAN achieved the highest
classification performance with an accuracy of 91.21\%. The results demonstrate
the potential of the newly introduced Capsule-ConvKAN in capturing spatial
patterns, managing complex features, and addressing the limitations of
traditional convolutional models in medical image classification.

</details>


### [272] [Mitigating Multi-Sequence 3D Prostate MRI Data Scarcity through Domain Adaptation using Locally-Trained Latent Diffusion Models for Prostate Cancer Detection](https://arxiv.org/abs/2507.06384)
*Emerson P. Grabke,Babak Taati,Masoom A. Haider*

Main category: eess.IV

TL;DR: This paper introduces CCELLA++, an improved latent diffusion model for generating synthetic biparametric MRI, enhancing classifier performance beyond real MRI data, especially for inter-institutional domain shifts.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in medical image analysis due to limited data availability, specifically by improving prostate cancer detection using synthetic biparametric MRI and addressing inter-institutional variability.

Method: CCELLA++ generates simultaneous synthetic biparametric MRI sequences (AxT2, HighB, ADC) and evaluates its impact on classifiers through domain adaptation experiments using internal and external datasets.

Result: CCELLA++ demonstrated enhanced fidelity for HighB and ADC sequences and improved classifier performance notably under domain adaptation scenarios, outperforming real MRI data in classifier pretraining.

Conclusion: Synthetic biparametric MRI from CCELLA++ improves classifier generalization and performance under limited dataset conditions, showcasing its potential for medical imaging applications.

Abstract: Objective: Latent diffusion models (LDMs) could mitigate data scarcity
challenges affecting machine learning development for medical image
interpretation. The recent CCELLA LDM improved prostate cancer detection
performance using synthetic MRI for classifier training but was limited to the
axial T2-weighted (AxT2) sequence, did not investigate inter-institutional
domain shift, and prioritized radiology over histopathology outcomes. We
propose CCELLA++ to address these limitations and improve clinical utility.
Methods: CCELLA++ expands CCELLA for simultaneous biparametric prostate MRI
(bpMRI) generation, including the AxT2, high b-value diffusion series (HighB)
and apparent diffusion coefficient map (ADC). Domain adaptation was
investigated by pretraining classifiers on real or LDM-generated synthetic data
from an internal institution, followed with fine-tuning on progressively
smaller fractions of an out-of-distribution, external dataset. Results:
CCELLA++ improved 3D FID for HighB and ADC but not AxT2 (0.013, 0.012, 0.063
respectively) sequences compared to CCELLA (0.060). Classifier pretraining with
CCELLA++ bpMRI outperformed real bpMRI in AP and AUC for all domain adaptation
scenarios. CCELLA++ pretraining achieved highest classifier performance below
50% (n=665) external dataset volume. Conclusion: Synthetic bpMRI generated by
our method can improve downstream classifier generalization and performance
beyond real bpMRI or CCELLA-generated AxT2-only images. Future work should seek
to quantify medical image sample quality, balance multi-sequence LDM training,
and condition the LDM with additional information. Significance: The proposed
CCELLA++ LDM can generate synthetic bpMRI that outperforms real data for domain
adaptation with a limited target institution dataset. Our code is available at
https://github.com/grabkeem/CCELLA-plus-plus

</details>


### [273] [Speckle2Self: Self-Supervised Ultrasound Speckle Reduction Without Clean Data](https://arxiv.org/abs/2507.06828)
*Xuesong Li,Nassir Navab,Zhongliang Jiang*

Main category: eess.IV

TL;DR: The paper introduces Speckle2Self, a self-supervised algorithm designed to reduce tissue-dependent speckle noise in medical ultrasound images using single noisy observations.


<details>
  <summary>Details</summary>
Motivation: Speckle noise in ultrasound imaging significantly degrades image quality, and existing denoising methods are unsuitable due to the unique, tissue-dependent nature of US speckle.

Method: Speckle2Self uses a multi-scale perturbation (MSP) operation to introduce tissue-dependent variations in speckle patterns across different scales, modeling the clean image as a low-rank signal and isolating sparse noise.

Result: The method is extensively validated against both traditional filter-based and state-of-the-art learning-based denoising algorithms, showing robust performance on simulated and real human carotid ultrasound images, as well as strong generalization across various US machines.

Conclusion: Speckle2Self effectively addresses the challenge of speckle noise reduction in ultrasound imaging, offering a promising solution adaptable to unseen domains while relying solely on single noisy observations.

Abstract: Image denoising is a fundamental task in computer vision, particularly in
medical ultrasound (US) imaging, where speckle noise significantly degrades
image quality. Although recent advancements in deep neural networks have led to
substantial improvements in denoising for natural images, these methods cannot
be directly applied to US speckle noise, as it is not purely random. Instead,
US speckle arises from complex wave interference within the body
microstructure, making it tissue-dependent. This dependency means that
obtaining two independent noisy observations of the same scene, as required by
pioneering Noise2Noise, is not feasible. Additionally, blind-spot networks also
cannot handle US speckle noise due to its high spatial dependency. To address
this challenge, we introduce Speckle2Self, a novel self-supervised algorithm
for speckle reduction using only single noisy observations. The key insight is
that applying a multi-scale perturbation (MSP) operation introduces
tissue-dependent variations in the speckle pattern across different scales,
while preserving the shared anatomical structure. This enables effective
speckle suppression by modeling the clean image as a low-rank signal and
isolating the sparse noise component. To demonstrate its effectiveness,
Speckle2Self is comprehensively compared with conventional filter-based
denoising algorithms and SOTA learning-based methods, using both realistic
simulated US images and human carotid US images. Additionally, data from
multiple US machines are employed to evaluate model generalization and
adaptability to images from unseen domains. \textit{Code and datasets will be
released upon acceptance.

</details>


### [274] [Attention-Enhanced Deep Learning Ensemble for Breast Density Classification in Mammography](https://arxiv.org/abs/2507.06410)
*Peyman Sharifian,Xiaotong Hong,Alireza Karimian,Mehdi Amini,Hossein Arabi*

Main category: eess.IV

TL;DR: This study proposes an automated deep learning system for classifying breast density into low and high categories, achieving strong performance metrics using ensemble techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in breast density assessment, which is a vital factor in breast cancer risk and detection. High breast density poses both a diagnostic challenge and a cancer risk factor.

Method: The study utilized deep learning, employing four convolutional neural networks (ResNet18, ResNet50, EfficientNet-B0, DenseNet121) with channel attention mechanisms. It also introduced a Combined Focal Label Smoothing Loss and employed advanced preprocessing methods like CLAHE and data augmentation. An ensemble voting approach was used to combine model predictions.

Result: The system achieved high performance metrics, with an AUC of 0.963 and an F1-score of 0.952, outperforming individual models.

Conclusion: This automated system shows potential for standardizing breast density assessments, enhancing cancer detection efficiency, improving screening precision, and reducing variability among radiologists.

Abstract: Breast density assessment is a crucial component of mammographic
interpretation, with high breast density (BI-RADS categories C and D)
representing both a significant risk factor for developing breast cancer and a
technical challenge for tumor detection. This study proposes an automated deep
learning system for robust binary classification of breast density (low: A/B
vs. high: C/D) using the VinDr-Mammo dataset. We implemented and compared four
advanced convolutional neural networks: ResNet18, ResNet50, EfficientNet-B0,
and DenseNet121, each enhanced with channel attention mechanisms. To address
the inherent class imbalance, we developed a novel Combined Focal Label
Smoothing Loss function that integrates focal loss, label smoothing, and
class-balanced weighting. Our preprocessing pipeline incorporated advanced
techniques, including contrast-limited adaptive histogram equalization (CLAHE)
and comprehensive data augmentation. The individual models were combined
through an optimized ensemble voting approach, achieving superior performance
(AUC: 0.963, F1-score: 0.952) compared to any single model. This system
demonstrates significant potential to standardize density assessments in
clinical practice, potentially improving screening efficiency and early cancer
detection rates while reducing inter-observer variability among radiologists.

</details>


### [275] [Airway Segmentation Network for Enhanced Tubular Feature Extraction](https://arxiv.org/abs/2507.06581)
*Qibiao Wu,Yagang Wang,Qian Zhang*

Main category: eess.IV

TL;DR: The study introduces TfeNet, a novel neural network architecture for better airway segmentation in CT images, featuring unique tree-like structures, and achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Manual airway segmentation in CT images is time-consuming and expertise-dependent. Automating this task is crucial for faster bronchoscopic navigation and robotic system deployment.

Method: The paper proposes TfeNet, which uses a direction-aware convolution operation to adjust sampling positions of convolution kernels, complemented by a tubular feature fusion module for enhanced focus on fine airway structures.

Result: TfeNet outperforms existing methods, achieving a 94.95% overall score on the largest airway segmentation dataset ATM22 and demonstrating superior results on the lung fibrosis dataset AIIB23.

Conclusion: TfeNet is able to provide accurate and continuous airway structure predictions, addressing the challenges of fine and intricate airway segmentation better than existing techniques.

Abstract: Manual annotation of airway regions in computed tomography images is a
time-consuming and expertise-dependent task. Automatic airway segmentation is
therefore a prerequisite for enabling rapid bronchoscopic navigation and the
clinical deployment of bronchoscopic robotic systems. Although convolutional
neural network methods have gained considerable attention in airway
segmentation, the unique tree-like structure of airways poses challenges for
conventional and deformable convolutions, which often fail to focus on fine
airway structures, leading to missed segments and discontinuities. To address
this issue, this study proposes a novel tubular feature extraction network,
named TfeNet. TfeNet introduces a novel direction-aware convolution operation
that first applies spatial rotation transformations to adjust the sampling
positions of linear convolution kernels. The deformed kernels are then
represented as line segments or polylines in 3D space. Furthermore, a tubular
feature fusion module (TFFM) is designed based on asymmetric convolution and
residual connection strategies, enhancing the network's focus on subtle airway
structures. Extensive experiments conducted on one public dataset and two
datasets used in airway segmentation challenges demonstrate that the proposed
TfeNet achieves more accuracy and continuous airway structure predictions
compared with existing methods. In particular, TfeNet achieves the highest
overall score of 94.95% on the current largest airway segmentation dataset,
Airway Tree Modeling(ATM22), and demonstrates advanced performance on the lung
fibrosis dataset(AIIB23). The code is available at
https://github.com/QibiaoWu/TfeNet.

</details>


### [276] [Fast Equivariant Imaging: Acceleration for Unsupervised Learning via Augmented Lagrangian and Auxiliary PnP Denoisers](https://arxiv.org/abs/2507.06764)
*Guixian Xu,Jinglai Li,Junqi Tang*

Main category: eess.IV

TL;DR: The paper proposes a new unsupervised framework, Fast Equivariant Imaging (FEI), which accelerates and enhances the training of imaging networks without needing ground-truth data.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies and performance limitations in the vanilla Equivariant Imaging paradigm for training imaging networks.

Method: Reformulating the Equivariant Imaging optimization problem with Lagrange multipliers and incorporating plug-and-play denoisers in a novel unsupervised training scheme called FEI.

Result: The proposed FEI framework achieves a 10x acceleration and improved performance over standard Equivariant Imaging in training U-Net with the CT100 dataset for X-ray CT reconstruction.

Conclusion: Fast Equivariant Imaging (FEI) is more efficient and generalizes better than the vanilla Equivariant Imaging, making it a superior method for unsupervised training of deep imaging networks.

Abstract: We propose Fast Equivariant Imaging (FEI), a novel unsupervised learning
framework to efficiently train deep imaging networks without ground-truth data.
From the perspective of reformulating the Equivariant Imaging based
optimization problem via the method of Lagrange multipliers and utilizing
plug-and-play denoisers, this novel unsupervised scheme shows superior
efficiency and performance compared to vanilla Equivariant Imaging paradigm. In
particular, our PnP-FEI scheme achieves an order-of-magnitude (10x)
acceleration over standard EI on training U-Net with CT100 dataset for X-ray CT
reconstruction, with improved generalization performance.

</details>


### [277] [SimCortex: Collision-free Simultaneous Cortical Surfaces Reconstruction](https://arxiv.org/abs/2507.06955)
*Kaveh Moradkhani,R Jarrett Rushmore,Sylvain Bouix*

Main category: eess.IV

TL;DR: SimCortex is a deep learning framework for accurate cortical surface reconstruction from MRI that avoids structural issues like overlaps and topological defects.


<details>
  <summary>Details</summary>
Motivation: Current cortical surface reconstruction methods struggle with complex geometries and topological issues, leading to inaccuracies in neuroanatomical analyses.

Method: The proposed method first segments MRI images into nine tissue classes, generates collision-free initial surface meshes, and applies topology-preserving deformations using stationary velocity fields.

Result: SimCortex reduces surface overlaps and self-intersections more effectively than existing techniques, while maintaining high geometric accuracy.

Conclusion: SimCortex represents a significant advancement in cortical surface reconstruction by ensuring topology preservation and surpassing current methods in accuracy.

Abstract: Accurate cortical surface reconstruction from magnetic resonance imaging
(MRI) data is crucial for reliable neuroanatomical analyses. Current methods
have to contend with complex cortical geometries, strict topological
requirements, and often produce surfaces with overlaps, self-intersections, and
topological defects. To overcome these shortcomings, we introduce SimCortex, a
deep learning framework that simultaneously reconstructs all brain surfaces
(left/right white-matter and pial) from T1-weighted(T1w) MRI volumes while
preserving topological properties. Our method first segments the T1w image into
a nine-class tissue label map. From these segmentations, we generate
subject-specific, collision-free initial surface meshes. These surfaces serve
as precise initializations for subsequent multiscale diffeomorphic
deformations. Employing stationary velocity fields (SVFs) integrated via
scaling-and-squaring, our approach ensures smooth, topology-preserving
transformations with significantly reduced surface collisions and
self-intersections. Evaluations on standard datasets demonstrate that SimCortex
dramatically reduces surface overlaps and self-intersections, surpassing
current methods while maintaining state-of-the-art geometric accuracy.

</details>


### [278] [Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning](https://arxiv.org/abs/2507.07011)
*Daniel Onah,Ravish Desai*

Main category: eess.IV

TL;DR: The paper introduces Deep Brain Net combining EfficientNetB0 and ResNet50 architectures for accurate and efficient brain tumor detection via MRI.


<details>
  <summary>Details</summary>
Motivation: Challenges in achieving high accuracy and computational efficiency in automated brain tumor detection using MRI.

Method: Integrating EfficientNetB0 for efficiency and ResNet50 for deeper network capabilities, employing transfer learning.

Result: Achieved improved accuracy (88%), weighted F1 score (88.75%), and macro AUC ROC score (98.17%) on MRI datasets.

Conclusion: Deep Brain Net is robust, efficient, and holds clinical potential for aiding radiologists in brain tumor diagnosis.

Abstract: In recent years, deep learning has shown great promise in the automated
detection and classification of brain tumors from MRI images. However,
achieving high accuracy and computational efficiency remains a challenge. In
this research, we propose Deep Brain Net, a novel deep learning system designed
to optimize performance in the detection of brain tumors. The model integrates
the strengths of two advanced neural network architectures which are
EfficientNetB0 and ResNet50, combined with transfer learning to improve
generalization and reduce training time. The EfficientNetB0 architecture
enhances model efficiency by utilizing mobile inverted bottleneck blocks, which
incorporate depth wise separable convolutions. This design significantly
reduces the number of parameters and computational cost while preserving the
ability of models to learn complex feature representations. The ResNet50
architecture, pre trained on large scale datasets like ImageNet, is fine tuned
for brain tumor classification. Its use of residual connections allows for
training deeper networks by mitigating the vanishing gradient problem and
avoiding performance degradation. The integration of these components ensures
that the proposed system is both computationally efficient and highly accurate.
Extensive experiments performed on publicly available MRI datasets demonstrate
that Deep Brain Net consistently outperforms existing state of the art methods
in terms of classification accuracy, precision, recall, and computational
efficiency. The result is an accuracy of 88 percent, a weighted F1 score of
88.75 percent, and a macro AUC ROC score of 98.17 percent which demonstrates
the robustness and clinical potential of Deep Brain Net in assisting
radiologists with brain tumor diagnosis.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [279] [Multi-Queue SSD I/O Modeling & Its Implications for Data Structure Design](https://arxiv.org/abs/2507.06349)
*Erin Ransom,Andrew Lim,Michael Mitzenmacher*

Main category: cs.DS

TL;DR: This paper introduces the MQSSD model, which accurately represents modern multi-queue SSDs' performance to optimize LSM-tree-based storage engines.


<details>
  <summary>Details</summary>
Motivation: Existing storage performance models, such as the DAM model, are inadequate for capturing the advancements of modern storage technology, particularly multi-queue SSDs.

Method: The authors identify performance-critical aspects of multi-queue SSDs and propose the MQSSD model. They validate its application through experiments on RocksDB.

Result: Experiments show that leveraging concurrent access based on MQSSD insights improves optimization for RocksDB's storage engine.

Conclusion: The MQSSD model is a more accurate abstraction for modern SSDs, enabling better optimization strategies and deeper insights into storage engine design.

Abstract: Understanding the performance profiles of storage devices and how best to
utilize them has always been non-trivial due to factors such as seek times,
caching, scheduling, concurrent access, flash wear-out, and garbage collection.
However, analytical frameworks that provide simplified abstractions of storage
performance can still be accurate enough to evaluate external memory algorithms
and data structures at the design stage. For example, the Disk Access Machine
(DAM) model assumes that a storage device transfers data in fixed-size blocks
of size B and that all transfers have unit latency. This abstraction is already
sufficient to explain some of the benefits of data structures such as B-trees
and Log-Structured Merge trees (LSM trees); however, storage technology
advances have significantly reduced current models' accuracy and utility.
  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new
storage abstraction. This model builds upon previous models and aims to more
accurately represent the performance characteristics of modern storage
hardware. We identify key performance-critical aspects of modern multi-queue
solid-state drives on which we base our model and demonstrate these
characteristics on actual hardware. We then show how our model can be applied
to LSM-tree-based storage engines to optimize them for modern storage hardware.
We highlight that leveraging concurrent access is crucial for fully utilizing
the high throughput of multi-queue SSDs, enabling designs that may appear
counterintuitive under traditional paradigms We then validate these insights
through experiments using Facebook's LSM-tree-based key-value store, RocksDB.
We conclude that the MQSSD model offers a more accurate abstraction of modern
hardware than previous models, allowing for greater insight and optimization.

</details>


### [280] [Prediction-Augmented Mechanism Design for Weighted Facility Location](https://arxiv.org/abs/2507.06509)
*Yangguang Shi,Zhenyu Xue*

Main category: cs.DS

TL;DR: This paper introduces a prediction-augmented algorithmic framework for strategyproof weighted facility location problems (FLP) that balances consistency and robustness guarantees in settings with agents having non-uniform weights.


<details>
  <summary>Details</summary>
Motivation: The study addresses the limitations of existing strategyproof mechanisms in facility location problems, particularly under the assumption that all agents are equally important, which is unrealistic in many practical scenarios.

Method: The authors use a reduction technique that identifies a subset of representative instances to map other locations onto these, allowing for a strategyproof mechanism with bounded consistency and robustness guarantees. They also explore the balance between these guarantees using parameter tuning.

Result: The paper proves a strategyproof mechanism with mathematical bounds for consistency and robustness in weighted settings and demonstrates that certain guarantees are unattainable even with perfect predictions.

Conclusion: This research extends the theoretical framework of facility location problems to weighted scenarios, providing a balance between consistency and robustness while outlining the limitations in achieving ideal outcomes.

Abstract: Facility location is fundamental in operations research, mechanism design,
and algorithmic game theory, with applications ranging from urban
infrastructure planning to distributed systems. Recent research in this area
has focused on augmenting classic strategyproof mechanisms with predictions to
achieve an improved performance guarantee against the uncertainty under the
strategic environment. Previous work has been devoted to address the trade-off
obstacle of balancing the consistency (near-optimality under accurate
predictions) and robustness (bounded inefficiency under poor predictions)
primarily in the unweighted setting, assuming that all agents have the same
importance. However, this assumption may not be true in some practical
scenarios, leading to research of weighted facility location problems.
  The major contribution of the current work is to provide a prediction
augmented algorithmic framework for balancing the consistency and robustness
over strategic agents with non-uniform weights. In particular, through a
reduction technique that identifies a subset of \emph{representative} instances
and maps the other given locations to the representative ones, we prove that
there exists a \emph{strategyproof} mechanism achieving a bounded consistency
guarantee of $\frac{\sqrt{(1+c)^2W^2_{\min}+(1-c)^2W^2_{\max}}}{(1+c)W_{\min}}$
and a bounded robustness guarantee of
$\frac{\sqrt{(1-c)^2W^2_{\min}+(1+c)^2W^2_{\max}}}{(1-c)W_{\min}}$ in weighted
settings, where $c$ can be viewed as a parameter to make a trade-off between
the consistency and robustness and $W_{\min}$ and $W_{\max}$ denote the minimum
and maximum agents' weight. We also proved that there is no strategyproof
deterministic mechanism that reach $1$-consistency and $O\left( n \cdot
\frac{W_{\max}}{W_{\min}} \right)$-robustness in weighted FLP, even with fully
predictions of all agents.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [281] [Machine Learning based Enterprise Financial Audit Framework and High Risk Identification](https://arxiv.org/abs/2507.06266)
*Tingyu Yuan,Xi Zhang,Xuanjing Chen*

Main category: q-fin.RM

TL;DR: The paper proposes an AI-driven audit framework to address challenges in financial auditing, evaluates three machine learning models, and concludes that Random Forest performs best for fraud and risk detection.


<details>
  <summary>Details</summary>
Motivation: Manual auditing struggles with large-scale data, evolving fraud tactics, and complex business operations. Hence, AI-driven solutions for financial audits are explored to improve efficiency and compliance.

Method: The research analyzes data from Big Four accounting firms (2020-2025), assessing three machine learning algorithms (SVM, Random Forest, KNN) for fraud detection using metrics like F1-score, accuracy, and recall.

Result: Random Forest emerged as the top-performing model, achieving an F1-score of 0.9012 and highlighting features like audit frequency, past violations, and workload as key predictors.

Conclusion: AI-driven auditing (particularly using Random Forest) is valuable for fraud detection and risk management, offering improved efficiency and predictive insight for modern enterprise operations.

Abstract: In the face of global economic uncertainty, financial auditing has become
essential for regulatory compliance and risk mitigation. Traditional manual
auditing methods are increasingly limited by large data volumes, complex
business structures, and evolving fraud tactics. This study proposes an
AI-driven framework for enterprise financial audits and high-risk
identification, leveraging machine learning to improve efficiency and accuracy.
Using a dataset from the Big Four accounting firms (EY, PwC, Deloitte, KPMG)
from 2020 to 2025, the research examines trends in risk assessment, compliance
violations, and fraud detection. The dataset includes key indicators such as
audit project counts, high-risk cases, fraud instances, compliance breaches,
employee workload, and client satisfaction, capturing both audit behaviors and
AI's impact on operations. To build a robust risk prediction model, three
algorithms - Support Vector Machine (SVM), Random Forest (RF), and K-Nearest
Neighbors (KNN) - are evaluated. SVM uses hyperplane optimization for complex
classification, RF combines decision trees to manage high-dimensional,
nonlinear data with resistance to overfitting, and KNN applies distance-based
learning for flexible performance. Through hierarchical K-fold cross-validation
and evaluation using F1-score, accuracy, and recall, Random Forest achieves the
best performance, with an F1-score of 0.9012, excelling in identifying fraud
and compliance anomalies. Feature importance analysis reveals audit frequency,
past violations, employee workload, and client ratings as key predictors. The
study recommends adopting Random Forest as a core model, enhancing features via
engineering, and implementing real-time risk monitoring. This research
contributes valuable insights into using machine learning for intelligent
auditing and risk management in modern enterprises.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [282] [A Collectivist, Economic Perspective on AI](https://arxiv.org/abs/2507.06268)
*Michael I. Jordan*

Main category: cs.CY

TL;DR: The paper advocates for integrating economic and social principles into the design of information technology systems, emphasizing human-centric approaches and social welfare over solely cognitive perspectives.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the dominance of cognitive and data-driven perspectives in current technological development, arguing that social and cultural aspects of intelligence are overlooked.

Method: The paper suggests blending computational and inferential concepts with social and economic principles to establish system-level designs that prioritize social welfare.

Result: The proposed approach paves the way for creating technology that is inherently human-centric and accounts for societal implications in its design.

Conclusion: A new human-centric engineering field can emerge by prioritizing social welfare and blending technical and societal considerations.

Abstract: Information technology is in the midst of a revolution in which omnipresent
data collection and machine learning are impacting the human world as never
before. The word "intelligence" is being used as a North Star for the
development of this technology, with human cognition viewed as a baseline. This
view neglects the fact that humans are social animals, and that much of our
intelligence is social and cultural in origin. A related issue is that the
current view treats the social consequences of technology as an afterthought.
The path forward is not merely more data and compute, and not merely more
attention paid to cognitive or symbolic representations, but a thorough
blending of economic and social concepts with computational and inferential
concepts, in the service of system-level designs in which social welfare is a
first-class citizen, and with the aspiration that a new human-centric
engineering field will emerge.

</details>


### [283] [The Emotional Alignment Design Policy](https://arxiv.org/abs/2507.06263)
*Eric Schwitzgebel,Jeff Sebo*

Main category: cs.CY

TL;DR: The paper proposes the Emotional Alignment Design Policy, which ensures AI elicits appropriate emotional responses reflecting its capacities and moral status, but practical challenges exist.


<details>
  <summary>Details</summary>
Motivation: To address the need for designing artificial entities that elicit appropriate emotional reactions in users proportional to the systems' actual capacities and moral status.

Method: Exploration of challenges inherent in implementing the Emotional Alignment Design Policy, such as respecting user autonomy, managing public disagreement, and addressing user assumptions.

Result: The authors highlight several ethical and practical dilemmas, such as navigating emotional responses and moral status, creating a nuanced framework for emotional alignment.

Conclusion: Emotional alignment is an important but complex design policy for artificial entities, requiring careful handling of ethical, practical, and perceptual challenges.

Abstract: According to what we call the Emotional Alignment Design Policy, artificial
entities should be designed to elicit emotional reactions from users that
appropriately reflect the entities' capacities and moral status, or lack
thereof. This principle can be violated in two ways: by designing an artificial
system that elicits stronger or weaker emotional reactions than its capacities
and moral status warrant (overshooting or undershooting), or by designing a
system that elicits the wrong type of emotional reaction (hitting the wrong
target). Although presumably attractive, practical implementation faces several
challenges including: How can we respect user autonomy while promoting
appropriate responses? How should we navigate expert and public disagreement
and uncertainty about facts and values? What if emotional alignment seems to
require creating or destroying entities with moral status? To what extent
should designs conform to versus attempt to alter user assumptions and
attitudes?

</details>


### [284] [The Prompt War: How AI Decides on a Military Intervention](https://arxiv.org/abs/2507.06277)
*Maxim Chupilkin*

Main category: cs.CY

TL;DR: This study explores factors driving AI decisions for military interventions, finding that high domestic support and probable success are the strongest predictors.


<details>
  <summary>Details</summary>
Motivation: To analyze the key factors influencing AI-driven decisions for military interventions, given the rising use of AI in war games and planning.

Method: Conducted a conjoint experiment using AI models on 640 vignettes, each run 100 times, to systematically examine AI decisions on military intervention.

Result: AI decisions are most influenced by strong domestic support and likelihood of success, with costs like deaths and economic effects having less impact. The findings are consistent across AI platforms.

Conclusion: AI decisions follow identifiable patterns, primarily favoring interventions with domestic backing and high chances of success over costs and secondary factors.

Abstract: Which factors determine AI propensity for military intervention? While the
use of AI in war games and military planning is growing exponentially, the
simple analysis of key drivers embedded in the models has not yet been done.
This paper does a simple conjoint experiment proposing a model to decide on
military intervention in 640 vignettes where each was run for 100 times
allowing to explore AI decision on military intervention systematically. The
analysis finds that largest predictors of AI decision to intervene are high
domestic support and high probability of success. Costs such as international
condemnation, military deaths, civilian deaths, and negative economic effect
are statistically significant, but their effect is around half of domestic
support and probability of victory. Closing window of opportunity only reaches
statistical significance in interaction with other factors. The results are
remarkably consistent across scenarios and across different models (OpenAI GPT,
Anthropic Claude, Google Gemini) suggesting a pattern in AI decision-making.

</details>


### [285] [Too Human to Model:The Uncanny Valley of LLMs in Social Simulation -- When Generative Language Agents Misalign with Modelling Principles](https://arxiv.org/abs/2507.06310)
*Yongchao Zeng,Calum Brown,Mark Rounsevell*

Main category: cs.CY

TL;DR: This paper critiques the use of large language models (LLMs) in social simulations, highlighting how their detailed and expressive capabilities can hinder abstraction and interpretability in modeling. Five dilemmas are identified, suggesting the need for careful application of LLM agents.


<details>
  <summary>Details</summary>
Motivation: To critically evaluate the utility of LLMs in social simulations, especially in terms of their compatibility with the epistemic requirements of modeling such as abstraction and interpretability.

Method: A thought experiment is conducted to convert the Bass diffusion model into an LLM-based variant, revealing key challenges and dilemmas in applying LLM agents for modeling.

Result: The study uncovers five dilemmas: temporal resolution mismatch, challenges in balancing intervention and spontaneity, tension between natural conversations and rule-based instructions, role consistency issues, and the difficulty in understanding emergent patterns due to verbose outputs.

Conclusion: While LLMs enhance realism in social simulations, their human-like expressiveness may obscure the clarification of social mechanisms. The paper calls for repositioning LLM agents to contexts where linguistic nuances are central and emergent system-level patterns are not the focus.

Abstract: Large language models (LLMs) have been increasingly used to build agents in
social simulation because of their impressive abilities to generate fluent,
contextually coherent dialogues. Such abilities can enhance the realism of
models. However, the pursuit of realism is not necessarily compatible with the
epistemic foundation of modelling. We argue that LLM agents, in many regards,
are too human to model: they are too expressive, detailed and intractable to be
consistent with the abstraction, simplification, and interpretability typically
demanded by modelling. Through a model-building thought experiment that
converts the Bass diffusion model to an LLM-based variant, we uncover five core
dilemmas: a temporal resolution mismatch between natural conversation and
abstract time steps; the need for intervention in conversations while avoiding
undermining spontaneous agent outputs; the temptation to introduce rule-like
instructions in prompts while maintaining conversational naturalness; the
tension between role consistency and role evolution across time; and the
challenge of understanding emergence, where system-level patterns become
obscured by verbose micro textual outputs. These dilemmas steer the LLM agents
towards an uncanny valley: not abstract enough to clarify underlying social
mechanisms, while not natural enough to represent realistic human behaviour.
This exposes an important paradox: the realism of LLM agents can obscure,
rather than clarify, social dynamics when misapplied. We tease out the
conditions in which LLM agents are ideally suited: where system-level emergence
is not the focus, linguistic nuances and meaning are central, interactions
unfold in natural time, and stable role identity is more important than
long-term behavioural evolution. We call for repositioning LLM agents in the
ecosystem of social simulation for future applications.

</details>


### [286] [Deprecating Benchmarks: Criteria and Framework](https://arxiv.org/abs/2507.06434)
*Ayrton San Joaquin,Rokas Gipiškis,Leon Staufer,Ariel Gil*

Main category: cs.CY

TL;DR: The paper proposes a framework for determining when AI model benchmarks should be deprecated and offers criteria for partial or full deprecation to improve the quality of AI evaluations.


<details>
  <summary>Details</summary>
Motivation: AI benchmarks often fail to remain effective as models advance, potentially leading to misleading evaluations of model capabilities and safety.

Method: The authors conducted a review of current benchmarking practices to establish criteria and propose a framework for deprecating benchmarks appropriately.

Result: Criteria and a framework have been developed to identify when AI benchmarks no longer serve a meaningful purpose in evaluating frontier models.

Conclusion: Adopting these guidelines can improve AI model assessments, aiding developers, governance actors, and policymakers in maintaining high standards for AI evaluation.

Abstract: As frontier artificial intelligence (AI) models rapidly advance, benchmarks
are integral to comparing different models and measuring their progress in
different task-specific domains. However, there is a lack of guidance on when
and how benchmarks should be deprecated once they cease to effectively perform
their purpose. This risks benchmark scores over-valuing model capabilities, or
worse, obscuring capabilities and safety-washing. Based on a review of
benchmarking practices, we propose criteria to decide when to fully or
partially deprecate benchmarks, and a framework for deprecating benchmarks. Our
work aims to advance the state of benchmarking towards rigorous and quality
evaluations, especially for frontier models, and our recommendations are aimed
to benefit benchmark developers, benchmark users, AI governance actors (across
governments, academia, and industry panels), and policy makers.

</details>


### [287] [Assessing the Prevalence of AI-assisted Cheating in Programming Courses: A Pilot Study](https://arxiv.org/abs/2507.06438)
*Kaléu Delphino*

Main category: cs.CY

TL;DR: This paper discusses the threat of AI tools like ChatGPT enabling plagiarism in CS education, and presents a study assessing student plagiarism rates using surveys and interviews.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address a new challenge in Computer Science education: the rise of AI-driven plagiarism, and to estimate its prevalence among students.

Method: The researchers conducted a pilot study in a large CS class of 120 students, employing anonymous surveys and interviews to gather data on AI plagiarism.

Result: Over 25% of survey participants admitted to using AI tools for plagiarism. However, only one student was willing to participate in follow-up interviews.

Conclusion: Surveys are effective for studying AI plagiarism due to high acknowledgment rates, while interviews are less effective and need better design to boost participation.

Abstract: Tools that can generate computer code in response to inputs written in
natural language, such as ChatGPT, pose an existential threat to Computer
Science education in its current form, since students can now use these tools
to solve assignments without much effort. While that risk has already been
recognized by scholars, the proportion of the student body that is incurring in
this new kind of plagiarism is still an open problem. We conducted a pilot
study in a large CS class (n=120) to assess the feasibility of estimating AI
plagiarism through anonymous surveys and interviews. More than 25% of the
survey respondents admitted to committing AI plagiarism. Conversely, only one
student accepted to be interviewed. Given the high levels of misconduct
acknowledgment, we conclude that surveys are an effective method for studies on
the matter, while interviews should be avoided or designed in a way that can
entice participation.

</details>


### [288] [Winning and losing with Artificial Intelligence: What public discourse about ChatGPT tells us about how societies make sense of technological change](https://arxiv.org/abs/2507.06876)
*Adrian Rauchfleisch,Joshua Philip Suarez,Nikka Marie Sales,Andreas Jungherr*

Main category: cs.CY

TL;DR: The study examines public reactions to ChatGPT's launch using 3.8 million tweets, finding that occupational skills and cultural values influence engagement and sentiment towards the tool.


<details>
  <summary>Details</summary>
Motivation: To investigate how economic interests and cultural values shape public engagement and sentiment towards technological advancements like AI.

Method: Analyzed 3.8 million tweets from 1.6 million users across 117 countries to understand engagement timing and sentiment patterns, using occupational skills and Hofstede's cultural dimensions as proxies.

Result: Users from technical fields (e.g., programming, math) engaged earlier and positively, whereas writing-centric users joined later with skepticism. Individualism led to earlier engagement but a negative stance; uncertainty avoidance hindered positivity but not timing.

Conclusion: Public perception of AI innovations like ChatGPT is driven by occupational and cultural contexts. Critical stances emerge from later adopters rather than earlier users changing their views.

Abstract: Public product launches in Artificial Intelligence can serve as focusing
events for collective attention, surfacing how societies react to technological
change. Social media provide a window into the sensemaking around these events,
surfacing hopes and fears and showing who chooses to engage in the discourse
and when. We demonstrate that public sensemaking about AI is shaped by economic
interests and cultural values of those involved. We analyze 3.8 million tweets
posted by 1.6 million users across 117 countries in response to the public
launch of ChatGPT in 2022. Our analysis shows how economic self-interest,
proxied by occupational skill types in writing, programming, and mathematics,
and national cultural orientations, as measured by Hofstede's individualism,
uncertainty avoidance, and power distance dimensions, shape who speaks, when
they speak, and their stance towards ChatGPT. Roles requiring more technical
skills, such as programming and mathematics, tend to engage earlier and express
more positive stances, whereas writing-centric occupations join later with
greater skepticism. At the cultural level, individualism predicts both earlier
engagement and a more negative stance, and uncertainty avoidance reduces the
prevalence of positive stances but does not delay when users first engage with
ChatGPT. Aggregate sentiment trends mask the dynamics observed in our study.
The shift toward a more critical stance towards ChatGPT over time stems
primarily from the entry of more skeptical voices rather than a change of heart
among early adopters. Our findings underscore the importance of both the
occupational background and cultural context in understanding public reactions
to AI.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [289] [3D-Generalist: Self-Improving Vision-Language-Action Models for Crafting 3D Worlds](https://arxiv.org/abs/2507.06484)
*Fan-Yun Sun,Shengguang Wu,Christian Jacobsen,Thomas Yim,Haoming Zou,Alex Zook,Shangru Li,Yu-Hsin Chou,Ethem Can,Xunlei Wu,Clemens Eppner,Valts Blukis,Jonathan Tremblay,Jiajun Wu,Stan Birchfield,Nick Haber*

Main category: cs.GR

TL;DR: The paper introduces "3D-Generalist," a scalable method to generate high-quality 3D environments using Vision-Language Models (VLMs) as sequential decision-making policies. The generated environments improve spatial reasoning capabilities of foundation models.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in pretrained models, improving their spatial reasoning remains challenging due to limited access to 3D-grounded data. Manual creation of 3D environments is labor-intensive, necessitating automated scalable methods.

Method: The method recasts 3D environment generation as a sequential decision-making task. Vision-Language Models act as policies to configure layouts, materials, lighting, and assets in 3D environments. Self-improvement fine-tuning enhances the alignment of generated environments with input prompts.

Result: The proposed framework scales synthetic data generation by producing high-quality, simulation-ready 3D environments, effectively pretraining vision foundation models. Models trained on this data outperform those pretrained on manually-created 3D data and approach real-data results.

Conclusion: The 3D-Generalist framework enables scalable production of 3D environments, addressing a key bottleneck in spatial reasoning training. This approach has the potential to bridge the gap between synthetic and real-world data performance.

Abstract: Despite large-scale pretraining endowing models with language and vision
reasoning capabilities, improving their spatial reasoning capability remains
challenging due to the lack of data grounded in the 3D world. While it is
possible for humans to manually create immersive and interactive worlds through
3D graphics, as seen in applications such as VR, gaming, and robotics, this
process remains highly labor-intensive. In this paper, we propose a scalable
method for generating high-quality 3D environments that can serve as training
data for foundation models. We recast 3D environment building as a sequential
decision-making problem, employing Vision-Language-Models (VLMs) as policies
that output actions to jointly craft a 3D environment's layout, materials,
lighting, and assets. Our proposed framework, 3D-Generalist, trains VLMs to
generate more prompt-aligned 3D environments via self-improvement fine-tuning.
We demonstrate the effectiveness of 3D-Generalist and the proposed training
strategy in generating simulation-ready 3D environments. Furthermore, we
demonstrate its quality and scalability in synthetic data generation by
pretraining a vision foundation model on the generated data. After fine-tuning
the pre-trained model on downstream tasks, we show that it surpasses models
pre-trained on meticulously human-crafted synthetic data and approaches results
achieved with real data orders of magnitude larger.

</details>


### [290] [Enhancing non-Rigid 3D Model Deformations Using Mesh-based Gaussian Splatting](https://arxiv.org/abs/2507.07000)
*Wijayathunga W. M. R. D. B*

Main category: cs.GR

TL;DR: The paper introduces a framework combining mesh representations and 3D Gaussian splatting for non-rigid 3D model deformations and better post-editing capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing 3D Gaussian splatting methods lack support for large-scale, non-rigid deformations and robust post-editing capabilities.

Method: The method embeds Gaussian kernels onto explicit mesh surfaces, utilizing mesh topology and geometry for intuitive editing and complex deformations.

Result: The proposed framework enables operations like component movement, scaling, and complex deformations such as bending and stretching.

Conclusion: This approach enhances flexibility in 3D content creation for applications in VR, character animation, and interactive design workflows.

Abstract: We propose a novel framework that enhances non-rigid 3D model deformations by
bridging mesh representations with 3D Gaussian splatting. While traditional
Gaussian splatting delivers fast, real-time radiance-field rendering, its
post-editing capabilities and support for large-scale, non-rigid deformations
remain limited. Our method addresses these challenges by embedding Gaussian
kernels directly onto explicit mesh surfaces. This allows the mesh's inherent
topological and geometric priors to guide intuitive editing operations -- such
as moving, scaling, and rotating individual 3D components -- and enables
complex deformations like bending and stretching. This work paves the way for
more flexible 3D content-creation workflows in applications spanning virtual
reality, character animation, and interactive design.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [291] [Graph-based Fake Account Detection: A Survey](https://arxiv.org/abs/2507.06541)
*Ali Safarpoor Dehkordi,Ahad N. Zehmakan*

Main category: cs.SI

TL;DR: This paper surveys existing methods for detecting fake accounts in online social networks, with a focus on graph-based techniques.


<details>
  <summary>Details</summary>
Motivation: To review and categorize methods for fake account detection, particularly graph-based techniques, and address their effectiveness and limitations.

Method: Comprehensive review and categorization of fake account detection methods, including input data, detection time, strengths, and limitations.

Result: Identified existing methods, their categorization, data sources, and their connections to broader contexts in fake account detection research.

Conclusion: The study highlighted current approaches to fake account detection and proposed several directions for future research.

Abstract: In recent years, there has been a growing effort to develop effective and
efficient algorithms for fake account detection in online social networks. This
survey comprehensively reviews existing methods, with a focus on graph-based
techniques that utilise topological features of social graphs (in addition to
account information, such as their shared contents and profile data) to
distinguish between fake and real accounts. We provide several categorisations
of these methods (for example, based on techniques used, input data, and
detection time), discuss their strengths and limitations, and explain how these
methods connect in the broader context. We also investigate the available
datasets, including both real-world data and synthesised models. We conclude
the paper by proposing several potential avenues for future research.

</details>


### [292] [Modeling Heterogeneity across Varying Spatial Extents: Discovering Linkages between Sea Ice Retreat and Ice Shelve Melt in the Antarctic](https://arxiv.org/abs/2507.07036)
*Maloy Kumar Devnath,Sudip Chakraborty,Vandana P. Janeja*

Main category: cs.SI

TL;DR: This paper introduces Spatial-Link, a graph-based framework that identifies linkages between Antarctic sea ice retreat and ice shelf melt, highlighting their interconnected effects for climate science.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored relationship between sea ice retreat and Antarctic Ice Shelf (AIS) melt, which traditional models fail to capture due to treating sea ice and AIS as separate, independent systems.

Method: The study uses a graph-based framework called Spatial-Link, which constructs a spatial graph using Delaunay triangulation and algorithms like breadth-first search and Monte Carlo simulations to identify and validate patterns of spatial heterogeneity.

Result: The findings reveal non-local, spatially heterogeneous coupling patterns between sea ice loss and downstream AIS melt, demonstrating that sea ice retreat can initiate or amplify AIS mass loss.

Conclusion: Spatial-Link bridges the gap between sea ice retreat and AIS melt, offering a novel, scalable methodology to enhance sea-level rise projections and inform climate adaptation strategies.

Abstract: Spatial phenomena often exhibit heterogeneity across spatial extents and in
proximity, making them complex to model-especially in dynamic regions like ice
shelves and sea ice. In this study, we address this challenge by exploring the
linkages between sea ice retreat and Antarctic ice shelf (AIS) melt. Although
atmospheric forcing and basal melting have been widely studied, the direct
impact of sea ice retreat on AIS mass loss remains underexplored. Traditional
models treat sea ice and AIS as separate systems. It limits their ability to
capture localized linkages and cascading feedback. To overcome this, we propose
Spatial-Link, a novel graph-based framework that quantifies spatial
heterogeneity to capture linkages between sea ice retreat and AIS melt. Our
method constructs a spatial graph using Delaunay triangulation of
satellite-derived ice change matrices, where nodes represent regions of
significant change and edges encode proximity and directional consistency. We
extract and statistically validate linkage paths using breadth-first search and
Monte Carlo simulations. Results reveal non-local, spatially heterogeneous
coupling patterns, suggesting sea ice loss can initiate or amplify downstream
AIS melt. Our analysis shows how sea ice retreat evolves over an oceanic grid
and progresses toward ice shelves-establishing a direct linkage. To our
knowledge, this is the first proposed methodology linking sea ice retreat to
AIS melt. Spatial-Link offers a scalable, data-driven tool to improve sea-level
rise projections and inform climate adaptation strategies.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [293] [Non-asymptotic confidence regions on RKHS. The Paley-Wiener and standard Sobolev space cases](https://arxiv.org/abs/2507.06657)
*Fabrice Gamboa,Olivier Roustant*

Main category: math.ST

TL;DR: This paper addresses creating a probabilistic confidence region for unknown functions in RKHS observed on random designs, reducing the problem to RKHS norm estimation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a robust and global method to construct a non-asymptotic confidence region for unknown functions belonging to an RKHS, which can be widely applicable to random designs.

Method: The authors formulate the problem in RKHS settings and focus on accurate estimation of the RKHS norm to construct the confidence region. They analyze the Paley-Wiener and Sobolev space approaches.

Result: The paper reduces the problem to estimating RKHS norm and demonstrates its feasibility in specific functional spaces like Paley-Wiener and Sobolev.

Conclusion: Constructing confidence regions for functions in RKHS is achievable by focusing on norm estimation, providing a non-asymptotic global framework.

Abstract: We consider the problem of constructing a global, probabilistic, and
non-asymptotic confidence region for an unknown function observed on a random
design. The unknown function is assumed to lie in a reproducing kernel Hilbert
space (RKHS). We show that this construction can be reduced to accurately
estimating the RKHS norm of the unknown function. Our analysis primarily
focuses both on the Paley-Wiener and on the standard Sobolev space settings.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [294] [Towards Solving More Challenging IMO Problems via Decoupled Reasoning and Proving](https://arxiv.org/abs/2507.06804)
*Zhenwen Liang,Linfeng Song,Yang Li,Tao Yang,Feng Zhang,Haitao Mi,Dong Yu*

Main category: cs.LO

TL;DR: The paper addresses a significant gap in automated theorem proving (ATP) performance of large language models (LLMs) for formal proofs versus informal reasoning by proposing a decoupled framework for reasoning and proof generation.


<details>
  <summary>Details</summary>
Motivation: To overcome the observed gap between LLMs' high performance in informal reasoning (80%+ accuracy) and their extremely low success (below 8%) in formal proofs, stemming from tightly coupled and ineffective reasoning and proving paradigms.

Method: The authors propose a decoupled framework with two specialized models: a "Reasoner" for generating strategic subgoal lemmas and a "Prover" for verifying them. This design avoids the challenges of end-to-end training by separating high-level reasoning from low-level proof generation.

Result: The proposed framework successfully solved 5 post-2000 International Mathematical Olympiad (IMO) problems, where no open-source ATPs had previously reported success.

Conclusion: The decoupled reasoning and proving framework marks significant progress in tackling challenging mathematical_theorem proving, pushing the boundaries of ATP capabilities. The accompanying released dataset aims to support community-driven advancements.

Abstract: Automated Theorem Proving (ATP) in formal languages is a foundational
challenge for AI. While Large Language Models (LLMs) have driven remarkable
progress, a significant gap remains between their powerful informal reasoning
capabilities and their weak formal proving performance. Recent studies show
that the informal accuracy exceeds 80% while formal success remains below 8% on
benchmarks like PutnamBench. We argue this gap persists because current
state-of-the-art provers, by tightly coupling reasoning and proving, are
trained with paradigms that inadvertently punish deep reasoning in favor of
shallow, tactic-based strategies. To bridge this fundamental gap, we propose a
novel framework that decouples high-level reasoning from low-level proof
generation. Our approach utilizes two distinct, specialized models: a powerful,
general-purpose Reasoner to generate diverse, strategic subgoal lemmas, and an
efficient Prover to rigorously verify them. This modular design liberates the
model's full reasoning potential and bypasses the pitfalls of end-to-end
training. We evaluate our method on a challenging set of post-2000 IMO
problems, a problem set on which no prior open-source prover has reported
success. Our decoupled framework successfully solves 5 of these problems,
demonstrating a significant step towards automated reasoning on exceptionally
difficult mathematical challenges. To foster future research, we release our
full dataset of generated and verified lemmas for a wide range of IMO problems,
available at https://tencent-imo.github.io/ .

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [295] [Surrogate Model for Heat Transfer Prediction in Impinging Jet Arrays using Dynamic Inlet/Outlet and Flow Rate Control](https://arxiv.org/abs/2507.07034)
*Mikael Vaillant,Victor Oliveira Ferreira,Wiebke Mainville,Jean-Michel Lamarre,Vincent Raymond,Moncef Chioua,Bruno Blais*

Main category: physics.flu-dyn

TL;DR: The paper introduces a CNN-based surrogate model for predicting Nusselt number distribution in jet arrays for real-time heat transfer predictions, achieving high accuracy and experimental validation.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of high computational cost in using CFD for real-time heat transfer applications, such as model-based temperature control.

Method: The researchers developed a convolutional neural network (CNN)-based surrogate model trained on data from implicit large-eddy CFD simulations. They also introduced a scaling method for extrapolating results to higher Reynolds numbers.

Result: The surrogate models for jet arrays achieved a normalized mean average error of <2% and 0.6% in validation tests. Experimental validation further confirmed the models' accuracy.

Conclusion: The surrogate model enables accurate, real-time prediction of the Nusselt number distribution, providing a basis for advanced thermal system control strategies.

Abstract: This study presents a surrogate model designed to predict the Nusselt number
distribution in an enclosed impinging jet arrays, where each jet function
independently and where jets can be transformed from inlets to outlets, leading
to a vast number of possible flow arrangements. While computational fluid
dynamics (CFD) simulations can model heat transfer with high fidelity, their
cost prohibits real-time application such as model-based temperature control.
To address this, we generate a CNN-based surrogate model that can predict the
Nusselt distribution in real time. We train it with data from implicit large
eddy computational fluid dynamics simulations (Re < 2,000). We train two
distinct models, one for a five by one array of jets (83 simulations) and one
for a three by three array of jets (100 simulations). We introduce a method to
extrapolate predictions to higher Reynolds numbers (Re < 10,000) using a
correlation-based scaling. The surrogate models achieve high accuracy, with a
normalized mean average error below 2% on validation data for the five by one
surrogate model and 0.6% for the three by three surrogate model. Experimental
validation confirms the model's predictive capabilities. This work provides a
foundation for model-based control strategies in advanced thermal management
applications.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [296] [Machine-Learned Force Fields for Lattice Dynamics at Coupled-Cluster Level Accuracy](https://arxiv.org/abs/2507.06929)
*Sita Schönbauer,Johanna P. Carbone,Andreas Grüneis*

Main category: cond-mat.mtrl-sci

TL;DR: The study explores Machine-Learned Force Fields (MLFFs) trained on Density Functional Theory (DFT) and Coupled Cluster (CC) for carbon diamond and lithium hydride solids, investigating their ability to predict vibrational properties.


<details>
  <summary>Details</summary>
Motivation: There is a need to create more accurate computational models for predicting vibrational properties of solids, overcoming limitations in DFT and improving close agreement with experimental observations.

Method: MLFFs were trained on DFT and CC level potential energy surfaces. A delta-learning approach was used to compensate for lack of atomic forces in CC training data, comparing results with experimental and ab initio data.

Result: MLFFs trained on CC theory match experimental vibrational frequencies more closely than those trained on DFT. Additionally, anharmonic effects on lithium hydride's VDOS were evaluated at the CC level.

Conclusion: CC-based MLFFs demonstrate better performance and precision for vibrational property predictions in solids, highlighting their potential over DFT-trained models.

Abstract: We investigate Machine-Learned Force Fields (MLFFs) trained on approximate
Density Functional Theory (DFT) and Coupled Cluster (CC) level potential energy
surfaces for the carbon diamond and lithium hydride solids. We assess the
accuracy and precision of the MLFFs by calculating phonon dispersions and
vibrational densities of states (VDOS) that are compared to experiment and
reference ab initio results. To overcome limitations from long-range effects
and the lack of atomic forces in the CC training data, a delta-learning
approach based on the difference between CC and DFT results is explored.
Compared to DFT, MLFFs trained on CC theory yield higher vibrational
frequencies for optical modes, agreeing better with experiment. Furthermore,
the MLFFs are used to estimate anharmonic effects on the VDOS of lithium
hydride at the level of CC theory.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [297] [When Context Is Not Enough: Modeling Unexplained Variability in Car-Following Behavior](https://arxiv.org/abs/2507.07012)
*Chengyuan Zhang,Zhengbing He,Cathy Wu,Lijun Sun*

Main category: stat.AP

TL;DR: The paper introduces a stochastic modeling framework leveraging deep neural networks and Gaussian processes to improve the representation of variability and unpredictability in car-following behavior.


<details>
  <summary>Details</summary>
Motivation: Traditional deterministic models fail to capture the variability and latent factors in human driving, leading to a need for advanced methods that incorporate structured stochasticity.

Method: The model integrates deep neural networks with nonstationary Gaussian processes, using a scenario-adaptive Gibbs kernel to capture temporal correlations in driving dynamics.

Result: Experiments on the HighD dataset show the model outperforms conventional approaches in predictive performance and interpretable uncertainty quantification.

Conclusion: The framework bridges interpretability and accuracy, making it a valuable tool for traffic simulations and safety-critical applications.

Abstract: Modeling car-following behavior is fundamental to microscopic traffic
simulation, yet traditional deterministic models often fail to capture the full
extent of variability and unpredictability in human driving. While many modern
approaches incorporate context-aware inputs (e.g., spacing, speed, relative
speed), they frequently overlook structured stochasticity that arises from
latent driver intentions, perception errors, and memory effects -- factors that
are not directly observable from context alone. To fill the gap, this study
introduces an interpretable stochastic modeling framework that captures not
only context-dependent dynamics but also residual variability beyond what
context can explain. Leveraging deep neural networks integrated with
nonstationary Gaussian processes (GPs), our model employs a scenario-adaptive
Gibbs kernel to learn dynamic temporal correlations in acceleration decisions,
where the strength and duration of correlations between acceleration decisions
evolve with the driving context. This formulation enables a principled,
data-driven quantification of uncertainty in acceleration, speed, and spacing,
grounded in both observable context and latent behavioral variability.
Comprehensive experiments on the naturalistic vehicle trajectory dataset
collected from the German highway, i.e., the HighD dataset, demonstrate that
the proposed stochastic simulation method within this framework surpasses
conventional methods in both predictive performance and interpretable
uncertainty quantification. The integration of interpretability and accuracy
makes this framework a promising tool for traffic analysis and safety-critical
applications.

</details>


### [298] [A Machine Learning Framework for Breast Cancer Treatment Classification Using a Novel Dataset](https://arxiv.org/abs/2507.06243)
*Md Nahid Hasan,Md Monzur Murshed,Md Mahadi Hasan,Faysal A. Chowdhury*

Main category: stat.AP

TL;DR: This paper employs machine learning (ML) models to predict breast cancer treatment outcomes using The Cancer Genome Atlas (TCGA) dataset, with Gradient Boosting achieving the best performance.


<details>
  <summary>Details</summary>
Motivation: Personalized treatment for breast cancer is complex due to the disease's molecular and clinical heterogeneity, necessitating tools like ML to improve treatment outcome predictions.

Method: ML models were developed using the TCGA dataset to predict chemotherapy or hormonal therapy likelihood. Five-fold cross-validation, evaluation through metrics (accuracy, AUROC, etc.), bootstrap for uncertainty, and SHAP for interpretability were utilized.

Result: The Gradient Boosting Machine achieved the best performance with accuracy = 0.7718 and AUROC = 0.8252, followed by XGBoost and AdaBoost.

Conclusion: ML models, particularly Gradient Boosting, show promise in aiding personalized breast cancer treatment decisions by leveraging clinical data and predictive insights.

Abstract: Breast cancer (BC) remains a significant global health challenge, with
personalized treatment selection complicated by the disease's molecular and
clinical heterogeneity. BC treatment decisions rely on various patient-specific
clinical factors, and machine learning (ML) offers a powerful approach to
predicting treatment outcomes. This study utilizes The Cancer Genome Atlas
(TCGA) breast cancer clinical dataset to develop ML models for predicting the
likelihood of undergoing chemotherapy or hormonal therapy. The models are
trained using five-fold cross-validation and evaluated through performance
metrics, including accuracy, precision, recall, specificity, sensitivity,
F1-score, and area under the receiver operating characteristic curve (AUROC).
Model uncertainty is assessed using bootstrap techniques, while SHAP values
enhance interpretability by identifying key predictors. Among the tested
models, the Gradient Boosting Machine (GBM) achieves the highest stable
performance (accuracy = 0.7718, AUROC = 0.8252), followed by Extreme Gradient
Boosting (XGBoost) (accuracy = 0.7557, AUROC = 0.8044) and Adaptive Boosting
(AdaBoost) (accuracy = 0.7552, AUROC = 0.8016). These findings underscore the
potential of ML in supporting personalized breast cancer treatment decisions
through data-driven insights.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [299] [Pronunciation-Lexicon Free Training for Phoneme-based Crosslingual ASR via Joint Stochastic Approximation](https://arxiv.org/abs/2507.06249)
*Saierdaer Yusuyin,Te Ma,Hao Huang,Zhijian Ou*

Main category: eess.AS

TL;DR: The paper introduces a latent variable model for crosslingual speech recognition that eliminates the need for pronunciation lexicons, achieving notable performance improvements and open-sourcing its code.


<details>
  <summary>Details</summary>
Motivation: Phoneme-based crosslingual speech recognition traditionally needs pronunciation lexicons, which restrict flexibility. The goal is to remove this dependency while retaining or improving performance.

Method: The proposed method treats phonemes as discrete latent variables, using a Speech-to-Phoneme (S2P), Phoneme-to-Grapheme (P2G), and auxiliary Grapheme-to-Phoneme (G2P) model, jointly trained with the Joint Stochastic Approximation (JSA) algorithm.

Result: Experiments conducted in Polish and Indonesian show that the approach achieves 5% error rate reductions with minimal phoneme supervision and 9% error reductions with cross-domain adaptation compared to existing methods.

Conclusion: The latent variable model effectively removes the reliance on pronunciation lexicons while improving crosslingual adaptation performance, with promising applications in speech recognition domains.

Abstract: Recently, pre-trained models with phonetic supervision have demonstrated
their advantages for crosslingual speech recognition in data efficiency and
information sharing across languages. However, a limitation is that a
pronunciation lexicon is needed for such phoneme-based crosslingual speech
recognition. In this study, we aim to eliminate the need for pronunciation
lexicons and propose a latent variable model based method, with phonemes being
treated as discrete latent variables. The new method consists of a
speech-to-phoneme (S2P) model and a phoneme-to-grapheme (P2G) model, and a
grapheme-to-phoneme (G2P) model is introduced as an auxiliary inference model.
To jointly train the three models, we utilize the joint stochastic
approximation (JSA) algorithm, which is a stochastic extension of the EM
(expectation-maximization) algorithm and has demonstrated superior performance
particularly in estimating discrete latent variable models. Based on the
Whistle multilingual pre-trained S2P model, crosslingual experiments are
conducted in Polish (130 h) and Indonesian (20 h). With only 10 minutes of
phoneme supervision, the new method, JSA-SPG, achieves 5\% error rate
reductions compared to the best crosslingual fine-tuning approach using subword
or full phoneme supervision. Furthermore, it is found that in language domain
adaptation (i.e., utilizing cross-domain text-only data), JSA-SPG outperforms
the standard practice of language model fusion via the auxiliary support of the
G2P model by 9% error rate reductions. To facilitate reproducibility and
encourage further exploration in this field, we open-source the JSA-SPG
training code and complete pipeline.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [300] [Self-supervised learning predicts plant growth trajectories from multi-modal industrial greenhouse data](https://arxiv.org/abs/2507.06336)
*Adam J Riesselman,Evan M Cofer,Therese LaRue,Wim Meeussen*

Main category: q-bio.QM

TL;DR: This paper introduces a mobile robotic platform and self-supervised modeling to predict plant growth dynamics in a hydroponic leafy greens setting.


<details>
  <summary>Details</summary>
Motivation: Quantifying plant phenotypes like growth dynamics and biomass accumulation is crucial for improving agronomic traits and optimizing crop production. However, collecting large-scale, high-quality plant growth data is challenging.

Method: The study utilizes a mobile robotic platform for high-resolution data capture in a hydroponic setting and employs a self-supervised machine learning model to map observed data to plant growth trajectories.

Result: The approach forecasts plant height and harvest mass effectively, demonstrating its ability to predict future plant growth metrics.

Conclusion: The integration of robotic automation and machine learning offers significant benefits, providing valuable insights for agronomic research and crop production efficiency improvements.

Abstract: Quantifying organism-level phenotypes, such as growth dynamics and biomass
accumulation, is fundamental to understanding agronomic traits and optimizing
crop production. However, quality growing data of plants at scale is difficult
to generate. Here we use a mobile robotic platform to capture high-resolution
environmental sensing and phenotyping measurements of a large-scale hydroponic
leafy greens system. We describe a self-supervised modeling approach to build a
map from observed growing data to the entire plant growth trajectory. We
demonstrate our approach by forecasting future plant height and harvest mass of
crops in this system. This approach represents a significant advance in
combining robotic automation and machine learning, as well as providing
actionable insights for agronomic research and operational efficiency.

</details>


### [301] [DeepRetro: Retrosynthetic Pathway Discovery using Iterative LLM Reasoning](https://arxiv.org/abs/2507.07060)
*Shreyas Vinaya Sathyanarayana,Rahil Shah,Sharanabasava D. Hiremath,Rishikesh Panda,Rahul Jana,Riya Singh,Rida Irfan,Ashwin Murali,Bharath Ramsundar*

Main category: q-bio.QM

TL;DR: DeepRetro develops a novel hybrid framework combining traditional tools and large language models for retrosynthesis to explore new synthetic pathways beyond predefined methods.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitations of traditional retrosynthesis methods, which often rely on predefined templates and lack the ability to explore novel synthetic pathways, while also leveraging large language models' reasoning capabilities for effective multi-step planning.

Method: The authors propose DeepRetro, a hybrid framework that combines conventional template-based/MCTS approaches with large language models in an iterative feedback loop. If traditional methods fail, the LLM generates single-step retrosynthetic suggestions that are validated, ensuring stability and correctness, and iteratively refines pathways.

Result: DeepRetro demonstrates its effectiveness through benchmark evaluations and case studies, identifying viable and novel retrosynthetic routes, particularly for complex natural product compounds.

Conclusion: The research highlights the potential of integrating LLM reasoning in retrosynthesis, offering a tool that enhances chemical synthesis through dynamic exploration and human-in-the-loop feedback, thus advancing the field.

Abstract: Retrosynthesis, the identification of precursor molecules for a target
compound, is pivotal for synthesizing complex molecules, but faces challenges
in discovering novel pathways beyond predefined templates. Recent large
language model (LLM) approaches to retrosynthesis have shown promise but
effectively harnessing LLM reasoning capabilities for effective multi-step
planning remains an open question. To address this challenge, we introduce
DeepRetro, an open-source, iterative, hybrid LLM-based retrosynthetic
framework. Our approach integrates the strengths of conventional
template-based/Monte Carlo tree search tools with the generative power of LLMs
in a step-wise, feedback-driven loop. Initially, synthesis planning is
attempted with a template-based engine. If this fails, the LLM subsequently
proposes single-step retrosynthetic disconnections. Crucially, these
suggestions undergo rigorous validity, stability, and hallucination checks
before the resulting precursors are recursively fed back into the pipeline for
further evaluation. This iterative refinement allows for dynamic pathway
exploration and correction. We demonstrate the potential of this pipeline
through benchmark evaluations and case studies, showcasing its ability to
identify viable and potentially novel retrosynthetic routes. In particular, we
develop an interactive graphical user interface that allows expert human
chemists to provide human-in-the-loop feedback to the reasoning algorithm. This
approach successfully generates novel pathways for complex natural product
compounds, demonstrating the potential for iterative LLM reasoning to advance
state-of-art in complex chemical syntheses.

</details>


### [302] [PAST: A multimodal single-cell foundation model for histopathology and spatial transcriptomics in cancer](https://arxiv.org/abs/2507.06418)
*Changchun Yang,Haoyang Li,Yushuai Wu,Yilan Zhang,Yifeng Jiao,Yu Zhang,Rihan Huang,Yuan Cheng,Yuan Qi,Xin Guo,Xin Gao*

Main category: q-bio.QM

TL;DR: PAST is a pan-cancer single-cell foundation model integrating histopathology images with single-cell transcriptomics, designed to enhance precision oncology.


<details>
  <summary>Details</summary>
Motivation: Current pathology foundation models are ineffective at integrating molecular data at single-cell resolution, limiting their application in precision oncology.

Method: The researchers trained PAST, a foundation model, on 20 million paired histopathology images and single-cell transcriptomes from multiple cancers, combining morphology and gene expression representation.

Result: PAST successfully predicts single-cell gene expression, performs virtual molecular staining, and conducts multimodal survival analyses while improving accuracy and generalizability.

Conclusion: PAST redefines pathology foundation modeling by blending spatial and molecular data, offering a robust tool for precision cancer studies and spatial omics applications.

Abstract: While pathology foundation models have transformed cancer image analysis,
they often lack integration with molecular data at single-cell resolution,
limiting their utility for precision oncology. Here, we present PAST, a
pan-cancer single-cell foundation model trained on 20 million paired
histopathology images and single-cell transcriptomes spanning multiple tumor
types and tissue contexts. By jointly encoding cellular morphology and gene
expression, PAST learns unified cross-modal representations that capture both
spatial and molecular heterogeneity at the cellular level. This approach
enables accurate prediction of single-cell gene expression, virtual molecular
staining, and multimodal survival analysis directly from routine pathology
slides. Across diverse cancers and downstream tasks, PAST consistently exceeds
the performance of existing approaches, demonstrating robust generalizability
and scalability. Our work establishes a new paradigm for pathology foundation
models, providing a versatile tool for high-resolution spatial omics,
mechanistic discovery, and precision cancer research.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [303] [Generative Lagrangian data assimilation for ocean dynamics under extreme sparsity](https://arxiv.org/abs/2507.06479)
*Niloofar Asefi,Leonard Lupin-Jimenez,Tianning Wu,Ruoying He,Ashesh Chattopadhyay*

Main category: physics.ao-ph

TL;DR: The paper presents a deep learning framework combining neural operators with denoising diffusion probabilistic models (DDPMs) to reconstruct high-resolution ocean dynamics from sparse observational data, achieving robust results even under extreme sparsity (>99%).


<details>
  <summary>Details</summary>
Motivation: Current methods face significant challenges in reconstructing ocean dynamics due to the sparse, irregular, and Lagrangian nature of observational data, especially in mesoscale turbulence and phenomena like eddy shedding and rogue waves.

Method: A deep learning framework integrates neural operators with DDPMs to reconstruct high-resolution ocean states, conditioned on neural operator outputs, demonstrating effectiveness on benchmark systems, synthetic float observations, and real satellite data.

Result: The proposed method captures fine-scale, high-wavenumber ocean dynamics with high accuracy, even under extreme observational sparsity (as high as 99.9%-99%).

Conclusion: The framework outperforms existing deep learning methods, showing robust performance in reconstructing ocean dynamics under severe spatial sampling limitations.

Abstract: Reconstructing ocean dynamics from observational data is fundamentally
limited by the sparse, irregular, and Lagrangian nature of spatial sampling,
particularly in subsurface and remote regions. This sparsity poses significant
challenges for forecasting key phenomena such as eddy shedding and rogue waves.
Traditional data assimilation methods and deep learning models often struggle
to recover mesoscale turbulence under such constraints. We leverage a deep
learning framework that combines neural operators with denoising diffusion
probabilistic models (DDPMs) to reconstruct high-resolution ocean states from
extremely sparse Lagrangian observations. By conditioning the generative model
on neural operator outputs, the framework accurately captures small-scale,
high-wavenumber dynamics even at $99\%$ sparsity (for synthetic data) and
$99.9\%$ sparsity (for real satellite observations). We validate our method on
benchmark systems, synthetic float observations, and real satellite data,
demonstrating robust performance under severe spatial sampling limitations as
compared to other deep learning baselines.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [304] [Designing Robust Software Sensors for Nonlinear Systems via Neural Networks and Adaptive Sliding Mode Control](https://arxiv.org/abs/2507.06817)
*Ayoub Farkane,Mohamed Boutayeb,Mustapha Oudani,Mounir Ghogho*

Main category: math.DS

TL;DR: The paper introduces a neural network-supported observer with adaptive Sliding Mode Control (SMC) for nonlinear dynamical systems, offering robustness under less restrictive conditions.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of state estimation in nonlinear dynamical systems, especially when direct state measurement is not possible, and improve upon traditional methods relying on transformations or linearization.

Method: The proposed framework integrates neural networks with adaptive Sliding Mode Control, leveraging available sensor data and system governing equations as constraints to train the observer without ground-truth state data.

Result: Simulations show the proposed approach effectively handles non-differentiable dynamics and varying observability conditions, achieving rapid error convergence and high accuracy.

Conclusion: The method exhibits robustness, adaptability to system changes, and broad applicability, making it a strong candidate for solving complex real-world state estimation problems.

Abstract: Accurate knowledge of the state variables in a dynamical system is critical
for effective control, diagnosis, and supervision, especially when direct
measurements of all states are infeasible. This paper presents a novel approach
to designing software sensors for nonlinear dynamical systems expressed in
their most general form. Unlike traditional model-based observers that rely on
explicit transformations or linearization, the proposed framework integrates
neural networks with adaptive Sliding Mode Control (SMC) to design a robust
state observer under a less restrictive set of conditions. The learning process
is driven by available sensor measurements, which are used to correct the
observer's state estimate. The training methodology leverages the system's
governing equations as a physics-based constraint, enabling observer synthesis
without access to ground-truth state trajectories. By employing a time-varying
gain matrix dynamically adjusted by the neural network, the observer adapts in
real-time to system changes, ensuring robustness against noise, external
disturbances, and variations in system dynamics. Furthermore, we provide
sufficient conditions to guarantee estimation error convergence, establishing a
theoretical foundation for the observer's reliability. The methodology's
effectiveness is validated through simulations on challenging examples,
including systems with non-differentiable dynamics and varying observability
conditions. These examples, which are often problematic for conventional
techniques, serve to demonstrate the robustness and broad applicability of our
approach. The results show rapid convergence and high accuracy, underscoring
the method's potential for addressing complex state estimation challenges in
real-world applications.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [305] [Accelerated Spatio-Temporal Bayesian Modeling for Multivariate Gaussian Processes](https://arxiv.org/abs/2507.06938)
*Lisa Gaedke-Merzhäuser,Vincent Maillou,Fernando Rodriguez Avellaneda,Olaf Schenk,Mathieu Luisier,Paula Moraga,Alexandros Nikolaos Ziogas,Håvard Rue*

Main category: stat.CO

TL;DR: The paper introduces DALIA, a scalable framework for Bayesian inference on multivariate Gaussian processes, achieving significant computational efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: High-dimensional spatio-temporal applications pose computational challenges for Bayesian inference in multivariate Gaussian processes.

Method: DALIA uses sparse inverse covariance matrices, GPU acceleration, and a hierarchical distributed memory approach for efficient computations.

Result: DALIA achieves performance improvements, surpassing state-of-the-art by two orders of magnitude, and provides refined spatial resolutions in applications such as air pollution modeling.

Conclusion: DALIA offers a scalable and efficient solution for Bayesian inference in high-dimensional spatio-temporal multivariate Gaussian processes, with promising real-world applications.

Abstract: Multivariate Gaussian processes (GPs) offer a powerful probabilistic
framework to represent complex interdependent phenomena. They pose, however,
significant computational challenges in high-dimensional settings, which
frequently arise in spatial-temporal applications. We present DALIA, a highly
scalable framework for performing Bayesian inference tasks on spatio-temporal
multivariate GPs, based on the methodology of integrated nested Laplace
approximations. Our approach relies on a sparse inverse covariance matrix
formulation of the GP, puts forward a GPU-accelerated block-dense approach, and
introduces a hierarchical, triple-layer, distributed memory parallel scheme. We
showcase weak scaling performance surpassing the state-of-the-art by two orders
of magnitude on a model whose parameter space is 8$\times$ larger and measure
strong scaling speedups of three orders of magnitude when running on 496 GH200
superchips on the Alps supercomputer. Applying DALIA to air pollution data from
northern Italy over 48 days, we showcase refined spatial resolutions over the
aggregated pollutant measurements.

</details>


### [306] [stCEG: An R Package for Modelling Events over Spatial Areas Using Chain Event Graphs](https://arxiv.org/abs/2507.06726)
*Hollie Calley,Daniel Williamson*

Main category: stat.CO

TL;DR: The stCEG R package enables full specification and visualization of Chain Event Graphs (CEGs) with an interactive GUI.


<details>
  <summary>Details</summary>
Motivation: To provide a software package for constructing and customizing Chain Event Graphs (CEGs), addressing the lack of accessible tools for this purpose.

Method: Develop an R package with visualization features, spatial variable handling, and an integrated GUI to simplify user interaction.

Result: The stCEG package facilitates CEG modeling and visualization with example use demonstrated on a London homicide dataset.

Conclusion: stCEG is the first comprehensive software for fully customizable CEG modeling, enhancing accessibility through a GUI and spatial visual tools.

Abstract: stCEG is an R package which allows a user to fully specify a Chain Event
Graph (CEG) model from data and to produce interactive plots. It includes
functions for the user to visualise spatial variables they wish to include in
the model. There is also a web-based graphical user interface (GUI) provided,
increasing ease of use for those without knowledge of R. We demonstrate stCEG
using a dataset of homicides in London, which is included in the package. stCEG
is the first software package for CEGs that allows for full model
customisation.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [307] [DS@GT at CheckThat! 2025: Exploring Retrieval and Reranking Pipelines for Scientific Claim Source Retrieval on Social Media Discourse](https://arxiv.org/abs/2507.06563)
*Jeanette Schofield,Shuyu Tian,Hoang Thanh Thanh Truong,Maximilian Heil*

Main category: cs.IR

TL;DR: The paper focuses on retrieving relevant scientific papers for claims made on social media, achieving a competitive result in a benchmarking task.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of verifying scientific claims on social media, where such claims often lack proper citations.

Method: The team experimented with six data augmentation techniques, seven retrieval/reranking pipelines, and fine-tuned a bi-encoder model.

Result: Achieved an MRR@5 of 0.58, ranking 16th among 30 teams, improving significantly over the BM25 baseline score of 0.43.

Conclusion: The approach shows promise in scientific claim verification, though the team did not achieve top-ranking performance. The code is publicly available for reproducibility.

Abstract: Social media users often make scientific claims without citing where these
claims come from, generating a need to verify these claims. This paper details
work done by the DS@GT team for CLEF 2025 CheckThat! Lab Task 4b Scientific
Claim Source Retrieval which seeks to find relevant scientific papers based on
implicit references in tweets. Our team explored 6 different data augmentation
techniques, 7 different retrieval and reranking pipelines, and finetuned a
bi-encoder. Achieving an MRR@5 of 0.58, our team ranked 16th out of 30 teams
for the CLEF 2025 CheckThat! Lab Task 4b, and improvement of 0.15 over the BM25
baseline of 0.43. Our code is available on Github at
https://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4b.

</details>


### [308] [GR-LLMs: Recent Advances in Generative Recommendation Based on Large Language Models](https://arxiv.org/abs/2507.06507)
*Zhen Yang,Haitao Lin,Jiawei xue,Ziji Zhang*

Main category: cs.IR

TL;DR: This paper surveys the advancements in generative recommendations using large language models (LLMs) and discusses applications, considerations for industry use, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive understanding of the burgeoning paradigm shift in recommendations systems driven by LLMs and assist further research and adoption.

Method: The paper surveys the field, discussing the preliminaries, real-world application considerations, and future research possibilities for LLM-based generative recommendations.

Result: The paper synthesizes the current state of LLM-based generative recommendations, identifies challenges for industry application, and outlines promising future directions.

Conclusion: LLM-powered generative recommendations represent a transformative departure from traditional systems, with strong potential to redefine the field due to their capability to replace handcrafted features and improve recommendation quality.

Abstract: In the past year, Generative Recommendations (GRs) have undergone substantial
advancements, especially in leveraging the powerful sequence modeling and
reasoning capabilities of Large Language Models (LLMs) to enhance overall
recommendation performance. LLM-based GRs are forming a new paradigm that is
distinctly different from discriminative recommendations, showing strong
potential to replace traditional recommendation systems heavily dependent on
complex hand-crafted features. In this paper, we provide a comprehensive survey
aimed at facilitating further research of LLM-based GRs. Initially, we outline
the general preliminaries and application cases of LLM-based GRs. Subsequently,
we introduce the main considerations when LLM-based GRs are applied in real
industrial scenarios. Finally, we explore promising directions for LLM-based
GRs. We hope that this survey contributes to the ongoing advancement of the GR
domain.

</details>


### [309] [Temporal Information Retrieval via Time-Specifier Model Merging](https://arxiv.org/abs/2507.06782)
*SeungYoon Han,Taeho Hwang,Sukmin Cho,Soyeong Jeong,Hoyun Song,Huije Lee,Jong C. Park*

Main category: cs.IR

TL;DR: The paper presents Time-Specifier Model Merging (TSM), a method to improve temporal retrieval queries while maintaining performance on non-temporal queries.


<details>
  <summary>Details</summary>
Motivation: Address the underperformance of existing dense retrieval methods on temporally constrained queries and avoid catastrophic forgetting observed in current Temporal Information Retrieval approaches.

Method: TSM trains specialized retrievers for different time specifiers and merges them into a unified model to handle temporal constraints effectively without degrading non-temporal query retrieval.

Result: TSM significantly improves the handling of temporally constrained queries while consistently delivering strong results on non-temporal queries, outperforming baseline methods.

Conclusion: TSM allows for enhanced temporal reasoning in information retrieval systems without sacrificing the accuracy or performance for general queries, bridging the gap in current approaches.

Abstract: The rapid expansion of digital information and knowledge across structured
and unstructured sources has heightened the importance of Information Retrieval
(IR). While dense retrieval methods have substantially improved semantic
matching for general queries, they consistently underperform on queries with
explicit temporal constraints--often those containing numerical expressions and
time specifiers such as ``in 2015.'' Existing approaches to Temporal
Information Retrieval (TIR) improve temporal reasoning but often suffer from
catastrophic forgetting, leading to reduced performance on non-temporal
queries. To address this, we propose Time-Specifier Model Merging (TSM), a
novel method that enhances temporal retrieval while preserving accuracy on
non-temporal queries. TSM trains specialized retrievers for individual time
specifiers and merges them in to a unified model, enabling precise handling of
temporal constraints without compromising non-temporal retrieval. Extensive
experiments on both temporal and non-temporal datasets demonstrate that TSM
significantly improves performance on temporally constrained queries while
maintaining strong results on non-temporal queries, consistently outperforming
other baseline methods. Our code is available at
https://github.com/seungyoonee/TSM .

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [310] [From large-eddy simulations to deep learning: A U-net model for fast urban canopy flow predictions](https://arxiv.org/abs/2507.06533)
*Themistoklis Vargiemezis,Catherine Gorlé*

Main category: physics.comp-ph

TL;DR: The paper introduces a deep neural network model using U-Net architecture for rapid, accurate urban wind flow prediction, significantly reducing computation time compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: To overcome the high costs, computational demands, and time requirements of traditional wind flow prediction methods such as wind tunnels and LES, aiming to ensure pedestrian comfort, safety, and sustainable design.

Method: The study uses a U-Net architecture augmented with signed distance functions, gradients, and a Spatial Attention Module for quick prediction of urban wind flow fields, trained on LES data with multiple urban configurations and wind directions.

Result: The model achieves a mean relative error of 9.3% for velocity magnitude and 5.2% for turbulence intensity, drastically reducing computation from 10 hours on 32 CPUs to 1 second on a single GPU.

Conclusion: Deep learning-based methods, particularly the U-Net model presented, can provide fast and accurate wind flow assessments, making them highly suitable for urban design applications.

Abstract: Accurate prediction of wind flow fields in urban canopies is crucial for
ensuring pedestrian comfort, safety, and sustainable urban design. Traditional
methods using wind tunnels and Computational Fluid Dynamics, such as Large-Eddy
Simulations (LES), are limited by high costs, computational demands, and time
requirements. This study presents a deep neural network (DNN) approach for fast
and accurate predictions of urban wind flow fields, reducing computation time
from an order of 10 hours on 32 CPUs for one LES evaluation to an order of 1
second on a single GPU using the DNN model. We employ a U-Net architecture
trained on LES data including 252 synthetic urban configurations at seven wind
directions ($0^{o}$ to $90^{o}$ in $15^{o}$ increments). The model predicts two
key quantities of interest: mean velocity magnitude and streamwise turbulence
intensity, at multiple heights within the urban canopy. The U-net uses 2D
building representations augmented with signed distance functions and their
gradients as inputs, forming a $256\times256\times9$ tensor. In addition, a
Spatial Attention Module is used for feature transfer through skip connections.
The loss function combines the root-mean-square error of predictions, their
gradient magnitudes, and L2 regularization. Model evaluation on 50 test cases
demonstrates high accuracy with an overall mean relative error of 9.3% for
velocity magnitude and 5.2% for turbulence intensity. This research shows the
potential of deep learning approaches to provide fast, accurate urban wind
assessments essential for creating comfortable and safe urban environments.
Code is available at https://github.com/tvarg/Urban-FlowUnet.git

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [311] [Stochastic Alignments: Matching an Observed Trace to Stochastic Process Models](https://arxiv.org/abs/2507.06472)
*Tian Li,Artem Polyvyanyy,Sander J. J. Leemans*

Main category: cs.FL

TL;DR: This paper proposes a heuristic-guided path-finding solution to align observed traces with likely paths in stochastic process models, addressing shortcomings of conventional alignment techniques.


<details>
  <summary>Details</summary>
Motivation: Traditional alignment-based conformance checking prioritizes minimizing deviations between observed traces and models, often selecting unlikely paths. There is a need to integrate path likelihood into the alignment process.

Method: The authors pose trace-to-path alignment as an optimization problem and introduce a heuristic-based algorithm to identify likely model paths with low edit distances to traces.

Result: An open-source implementation demonstrates the approach's practicality and its ability to offer new and useful diagnostic insights for process analysts.

Conclusion: The study presents a feasible method for effectively aligning traces to stochastic process models, enhancing diagnostic capabilities in process mining.

Abstract: Process mining leverages event data extracted from IT systems to generate
insights into the business processes of organizations. Such insights benefit
from explicitly considering the frequency of behavior in business processes,
which is captured by stochastic process models. Given an observed trace and a
stochastic process model, conventional alignment-based conformance checking
techniques face a fundamental limitation: They prioritize matching the trace to
a model path with minimal deviations, which may, however, lead to selecting an
unlikely path. In this paper, we study the problem of matching an observed
trace to a stochastic process model by identifying a likely model path with a
low edit distance to the trace. We phrase this as an optimization problem and
develop a heuristic-guided path-finding algorithm to solve it. Our open-source
implementation demonstrates the feasibility of the approach and shows that it
can provide new, useful diagnostic insights for analysts.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [312] [Deep learning-based species-area models reveal multi-scale patterns of species richness and turnover](https://arxiv.org/abs/2507.06358)
*Victor Boussange,Philipp Brun,Johanna T. Malle,Gabriele Midolo,Jeanne Portier,Théophile Sanchez,Niklaus E. Zimmermann,Irena Axmanová,Helge Bruelheide,Milan Chytrý,Stephan Kambach,Zdeňka Lososová,Martin Večeřa,Idoia Biurrun,Klaus T. Ecker,Jonathan Lenoir,Jens-Christian Svenning,Dirk Nikolaus Karger*

Main category: q-bio.PE

TL;DR: The paper develops a deep learning approach to predict multi-scale species richness, demonstrating its effectiveness in vascular plant communities across Europe.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of understanding the species-area relationship (SAR) and the interplay of biotic and abiotic processes across scales, especially due to limited biodiversity records.

Method: A deep learning model incorporating sampling theory and ecological surveys to predict scale-dependent species richness, validated through independent European plant community data.

Result: The model improved species richness estimates by 32%, provided spatially explicit richness and turnover patterns, and used explainable AI to examine drivers of species richness.

Conclusion: This approach enhances multi-scale biodiversity assessments and forecasts, aiding robust predictions under global environmental changes.

Abstract: The number of species within ecosystems is influenced not only by their
intrinsic characteristics but also by the spatial scale considered. As the
sampled area expands, species richness increases, a phenomenon described by the
species-area relationship (SAR). The accumulation dynamics of the SAR results
from a complex interplay of biotic and abiotic processes operating at various
spatial scales. However, the challenge of collecting exhaustive biodiversity
records across spatial scales has hindered a comprehensive understanding of
these dynamics. Here, we develop a deep learning approach that leverages
sampling theory and small-scale ecological surveys to spatially resolve the
scale-dependency of species richness. We demonstrate its performance by
predicting the species richness of vascular plant communities across Europe,
and evaluate the predictions against an independent dataset of plant community
inventories. Our model improves species richness estimates by 32\% and delivers
spatially explicit patterns of species richness and turnover for sampling areas
ranging from square meters to hundreds of square kilometers. Explainable AI
techniques further disentangle how drivers of species richness operate across
spatial scales. The ability of our model to represent the multi-scale nature of
biodiversity is essential to deliver robust biodiversity assessments and
forecasts under global change.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [313] [VisioPath: Vision-Language Enhanced Model Predictive Control for Safe Autonomous Navigation in Mixed Traffic](https://arxiv.org/abs/2507.06441)
*Shanting Wang,Panagiotis Typaldos,Chenjun Li,Andreas A. Malikopoulos*

Main category: eess.SY

TL;DR: VisioPath merges vision-language models with model predictive control for safe autonomous driving in dynamic traffic situations.


<details>
  <summary>Details</summary>
Motivation: To address challenges in safe trajectory planning for autonomous vehicles within complex traffic environments.

Method: The framework processes bird's-eye view videos with VLMs for vehicle data extraction, constructs collision-avoidance fields, integrates these into trajectory optimization using adaptive differential dynamic programming, and loops it in a safety-enhanced model predictive control structure.

Result: Simulations demonstrated VisioPath’s superior performance in metrics compared to conventional MPC methods, ensuring safer navigation.

Conclusion: VisioPath offers an innovative combination of AI-driven perception and optimal control fundamentals, advancing safe trajectory planning in modern traffic systems.

Abstract: In this paper, we introduce VisioPath, a novel framework combining
vision-language models (VLMs) with model predictive control (MPC) to enable
safe autonomous driving in dynamic traffic environments. The proposed approach
leverages a bird's-eye view video processing pipeline and zero-shot VLM
capabilities to obtain structured information about surrounding vehicles,
including their positions, dimensions, and velocities. Using this rich
perception output, we construct elliptical collision-avoidance potential fields
around other traffic participants, which are seamlessly integrated into a
finite-horizon optimal control problem for trajectory planning. The resulting
trajectory optimization is solved via differential dynamic programming with an
adaptive regularization scheme and is embedded in an event-triggered MPC loop.
To ensure collision-free motion, a safety verification layer is incorporated in
the framework that provides an assessment of potential unsafe trajectories.
Extensive simulations in Simulation of Urban Mobility (SUMO) demonstrate that
VisioPath outperforms conventional MPC baselines across multiple metrics. By
combining modern AI-driven perception with the rigorous foundation of optimal
control, VisioPath represents a significant step forward in safe trajectory
planning for complex traffic systems.

</details>


### [314] [An AI-Driven Thermal-Fluid Testbed for Advanced Small Modular Reactors: Integration of Digital Twin and Large Language Models](https://arxiv.org/abs/2507.06399)
*Doyeong Lim,Yang Liu,Zavier Ndum Ndum,Christian Young,Yassin Hassan*

Main category: eess.SY

TL;DR: This paper introduces a platform integrating artificial intelligence with thermal-fluid technology, aiming to enhance small modular reactor development through real-time predictions, control, and assistance.


<details>
  <summary>Details</summary>
Motivation: To accelerate innovation and deployment of next-generation nuclear systems by combining physical experimentation with advanced computational intelligence.

Method: The platform incorporates a three-loop thermal-fluid facility, a digital twin built on the System Analysis Module code coupled with a Gated Recurrent Unit neural network, and AI frameworks trained on experimental data.

Result: The platform showcased predictive accuracy with the GRU model achieving a temperature prediction error of 1.42 K and validated its control and operational assistance capabilities through case studies.

Conclusion: The platform demonstrates the potential of AI-driven methodologies in advancing reactor innovation and streamlining nuclear system deployment through enhanced modeling, control, and operator support.

Abstract: This paper presents a multipurpose artificial intelligence (AI)-driven
thermal-fluid testbed designed to advance Small Modular Reactor technologies by
seamlessly integrating physical experimentation with advanced computational
intelligence. The platform uniquely combines a versatile three-loop
thermal-fluid facility with a high-fidelity digital twin and sophisticated AI
frameworks for real-time prediction, control, and operational assistance.
Methodologically, the testbed's digital twin, built upon the System Analysis
Module code, is coupled with a Gated Recurrent Unit (GRU) neural network. This
machine learning model, trained on experimental data, enables
faster-than-real-time simulation, providing predictive insights into the
system's dynamic behavior. The practical application of this AI integration is
showcased through case studies. An AI-driven control framework where the GRU
model accurately forecasts future system states and the corresponding control
actions required to meet operational demands. Furthermore, an intelligent
assistant, powered by a large language model, translates complex sensor data
and simulation outputs into natural language, offering operators actionable
analysis and safety recommendations. Comprehensive validation against
experimental transients confirms the platform's high fidelity, with the GRU
model achieving a temperature prediction root mean square error of 1.42 K. This
work establishes an integrated research environment at the intersection of AI
and thermal-fluid science, showcasing how AI-driven methodologies in modeling,
control, and operator support can accelerate the innovation and deployment of
next-generation nuclear systems.

</details>


### [315] [A Single-Point Measurement Framework for Robust Cyber-Attack Diagnosis in Smart Microgrids Using Dual Fractional-Order Feature Analysis](https://arxiv.org/abs/2507.06890)
*Yifan Wang*

Main category: eess.SY

TL;DR: The paper introduces an innovative method, FO-MADS, for detecting cyber-attacks and faults in smart microgrids using a single sensor, achieving high diagnostic accuracy across various attack scenarios.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of diagnosing cyber-attacks in smart microgrids, which currently requires either costly multi-point instrumentation or unrealistic modeling assumptions, making current methods infeasible under single-sensor setups.

Method: The method involves a Fractional-Order Memory-Enhanced Attack-Diagnosis Scheme (FO-MADS), which employs fractional-order feature extraction with dual derivatives, a hierarchical classifier for fault diagnosis, and adversarial training with Online Hard Example Mining for robustness.

Result: FO-MADS demonstrated high diagnostic accuracy across different attack scenarios on a four-inverter microgrid testbed, achieving accuracies between 92.8% and 96.7%.

Conclusion: FO-MADS is a cost-effective, easily deployable solution that enhances the cyber-physical resilience of smart microgrids, making it a viable alternative to current multi-sensor-based methods.

Abstract: Cyber-attacks jeopardize the safe operation of smart microgrids. At the same
time, existing diagnostic methods either depend on expensive multi-point
instrumentation or stringent modelling assumptions that are untenable under
single-sensor constraints. This paper proposes a Fractional-Order
Memory-Enhanced Attack-Diagnosis Scheme (FO-MADS) that achieves low-latency
fault localisation and cyber-attack detection using only one VPQ
(Voltage-Power-Reactive-power) sensor. FO-MADS first constructs a dual
fractional-order feature library by jointly applying Caputo and
Gr\"unwald-Letnikov derivatives, thereby amplifying micro-perturbations and
slow drifts in the VPQ signal. A two-stage hierarchical classifier then
pinpoints the affected inverter and isolates the faulty IGBT switch,
effectively alleviating class imbalance. Robustness is further strengthened
through Progressive Memory-Replay Adversarial Training (PMR-AT), whose
attack-aware loss is dynamically re-weighted via Online Hard Example Mining
(OHEM) to prioritise the most challenging samples. Experiments on a
four-inverter microgrid testbed comprising 1 normal and 24 fault classes under
four attack scenarios demonstrate diagnostic accuracies of 96.6 % (bias), 94.0
% (noise), 92.8 % (data replacement), and 95.7 % (replay), while sustaining
96.7 % under attack-free conditions. These results establish FO-MADS as a
cost-effective and readily deployable solution that markedly enhances the
cyber-physical resilience of smart microgrids.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [316] [MixAssist: An Audio-Language Dataset for Co-Creative AI Assistance in Music Mixing](https://arxiv.org/abs/2507.06329)
*Michael Clemens,Ana Marasović*

Main category: cs.SD

TL;DR: MixAssist is an audio-language dataset designed to enhance collaborative and instructional aspects of music mixing AI by capturing dialogues between expert and amateur music producers.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of AI tools that focus on co-creative and instructional support in music mixing, which leaves amateur musicians underserved in their development of expertise.

Method: The authors created MixAssist, a dataset with 431 conversational turns from audio-grounded dialogues during mixing sessions involving 12 producers. They used this dataset to train and evaluate audio-language models.

Result: Fine-tuning models like Qwen-Audio on MixAssist produced promising results, with Qwen significantly outperforming other models in generating relevant mixing advice.

Conclusion: MixAssist lays the groundwork for AI systems capable of providing collaborative, context-aware assistance in music mixing, advancing the creative collaboration process for producers.

Abstract: While AI presents significant potential for enhancing music mixing and
mastering workflows, current research predominantly emphasizes end-to-end
automation or generation, often overlooking the collaborative and instructional
dimensions vital for co-creative processes. This gap leaves artists,
particularly amateurs seeking to develop expertise, underserved. To bridge
this, we introduce MixAssist, a novel audio-language dataset capturing the
situated, multi-turn dialogue between expert and amateur music producers during
collaborative mixing sessions. Comprising 431 audio-grounded conversational
turns derived from 7 in-depth sessions involving 12 producers, MixAssist
provides a unique resource for training and evaluating audio-language models
that can comprehend and respond to the complexities of real-world music
production dialogues. Our evaluations, including automated LLM-as-a-judge
assessments and human expert comparisons, demonstrate that fine-tuning models
such as Qwen-Audio on MixAssist can yield promising results, with Qwen
significantly outperforming other tested models in generating helpful,
contextually relevant mixing advice. By focusing on co-creative instruction
grounded in audio context, MixAssist enables the development of intelligent AI
assistants designed to support and augment the creative process in music
mixing.

</details>


### [317] [Exploring State-Space-Model based Language Model in Music Generation](https://arxiv.org/abs/2507.06674)
*Wei-Jaw Lee,Fang-Chih Hsieh,Xuanjun Chen,Fang-Duo Tsai,Yi-Hsuan Yang*

Main category: cs.SD

TL;DR: This study explores using Mamba-based architectures for text-to-music generation and finds them effective under limited resources.


<details>
  <summary>Details</summary>
Motivation: To investigate whether Mamba-based State Space Models can serve as an efficient alternative to Transformers for text-to-music generation.

Method: The paper adapts SiMBA, a Mamba-based encoder, turning it into a decoder for sequence modeling. It uses Residual Vector Quantization (RVQ) to capture semantic information and compares results with a Transformer-based decoder.

Result: SiMBA achieves faster convergence and generates outputs closer to the ground truth than the Transformer-based decoder, especially in limited-resource conditions.

Conclusion: SSMs, particularly SiMBA, show strong potential for efficient and effective text-to-music generation, offering benefits over traditional transformer approaches.

Abstract: The recent surge in State Space Models (SSMs), particularly the emergence of
Mamba, has established them as strong alternatives or complementary modules to
Transformers across diverse domains. In this work, we aim to explore the
potential of Mamba-based architectures for text-to-music generation. We adopt
discrete tokens of Residual Vector Quantization (RVQ) as the modeling
representation and empirically find that a single-layer codebook can capture
semantic information in music. Motivated by this observation, we focus on
modeling a single-codebook representation and adapt SiMBA, originally designed
as a Mamba-based encoder, to function as a decoder for sequence modeling. We
compare its performance against a standard Transformer-based decoder. Our
results suggest that, under limited-resource settings, SiMBA achieves much
faster convergence and generates outputs closer to the ground truth. This
demonstrates the promise of SSMs for efficient and expressive text-to-music
generation. We put audio examples on Github.

</details>


### [318] [Advances in Intelligent Hearing Aids: Deep Learning Approaches to Selective Noise Cancellation](https://arxiv.org/abs/2507.07043)
*Haris Khan,Shumaila Asif,Hassan Nasir*

Main category: cs.SD

TL;DR: The paper reviews advancements in AI-driven selective noise cancellation for hearing aids, highlighting progress, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional hearing aids by integrating AI for intelligent, context-aware noise cancellation.

Method: A systematic literature review focusing on technologies like deep learning architectures (e.g., Convolutional Recurrent Networks, Transformers), hardware strategies, and clinical validation.

Result: Recent AI models show up to 18.3 dB SI-SDR improvement on benchmarks, sub-10 ms real-time performance, and positive clinical outcomes.

Conclusion: While AI-enhanced hearing aids show promise, challenges like power constraints and real-world deployment limitations need solutions for transformative impact.

Abstract: The integration of artificial intelligence into hearing assistance marks a
paradigm shift from traditional amplification-based systems to intelligent,
context-aware audio processing. This systematic literature review evaluates
advances in AI-driven selective noise cancellation (SNC) for hearing aids,
highlighting technological evolution, implementation challenges, and future
research directions. We synthesize findings across deep learning architectures,
hardware deployment strategies, clinical validation studies, and user-centric
design. The review traces progress from early machine learning models to
state-of-the-art deep networks, including Convolutional Recurrent Networks for
real-time inference and Transformer-based architectures for high-accuracy
separation. Key findings include significant gains over traditional methods,
with recent models achieving up to 18.3 dB SI-SDR improvement on
noisy-reverberant benchmarks, alongside sub-10 ms real-time implementations and
promising clinical outcomes. Yet, challenges remain in bridging lab-grade
models with real-world deployment - particularly around power constraints,
environmental variability, and personalization. Identified research gaps
include hardware-software co-design, standardized evaluation protocols, and
regulatory considerations for AI-enhanced hearing devices. Future work must
prioritize lightweight models, continual learning, contextual-based
classification and clinical translation to realize transformative hearing
solutions for millions globally.

</details>


### [319] [A Novel Hybrid Deep Learning Technique for Speech Emotion Detection using Feature Engineering](https://arxiv.org/abs/2507.07046)
*Shahana Yasmin Chowdhury,Bithi Banik,Md Tamjidul Hoque,Shreya Banerjee*

Main category: cs.SD

TL;DR: The study proposes a DCRF-BiLSTM model for speech emotion recognition, achieving high accuracy across five benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance human-computer interaction and artificial intelligence by improving speech emotion recognition across diverse datasets.

Method: The DCRF-BiLSTM model was trained on five major speech emotion datasets and evaluated for individual and combined dataset performance.

Result: The model reached accuracy rates from 93.76% to 100% across individual and combined datasets, outperforming previous models.

Conclusion: The DCRF-BiLSTM model demonstrates robustness and generalizability, offering a significant advancement in multi-dataset speech emotion recognition.

Abstract: Nowadays, speech emotion recognition (SER) plays a vital role in the field of
human-computer interaction (HCI) and the evolution of artificial intelligence
(AI). Our proposed DCRF-BiLSTM model is used to recognize seven emotions:
neutral, happy, sad, angry, fear, disgust, and surprise, which are trained on
five datasets: RAVDESS (R), TESS (T), SAVEE (S), EmoDB (E), and Crema-D (C).
The model achieves high accuracy on individual datasets, including 97.83% on
RAVDESS, 97.02% on SAVEE, 95.10% for CREMA-D, and a perfect 100% on both TESS
and EMO-DB. For the combined (R+T+S) datasets, it achieves 98.82% accuracy,
outperforming previously reported results. To our knowledge, no existing study
has evaluated a single SER model across all five benchmark datasets (i.e.,
R+T+S+C+E) simultaneously. In our work, we introduce this comprehensive
combination and achieve a remarkable overall accuracy of 93.76%. These results
confirm the robustness and generalizability of our DCRF-BiLSTM framework across
diverse datasets.

</details>


### [320] [Comparative Analysis of CNN and Transformer Architectures with Heart Cycle Normalization for Automated Phonocardiogram Classification](https://arxiv.org/abs/2507.07058)
*Martin Sondermann,Pinar Bisgin,Niklas Tschorn,Anja Burmann,Christoph M. Friedrich*

Main category: cs.SD

TL;DR: The paper evaluates four different models for heart murmur detection, with specialized CNNs showing higher accuracy compared to BEATs transformer models.


<details>
  <summary>Details</summary>
Motivation: Improve cardiovascular diagnostics by comparing heart murmur detection models using different data processing methods.

Method: Four models were tested: two CNNs and two BEATs audio transformers, using fixed-length windowing and heart cycle normalization. Custom normalization tailored to cardiac rhythms was utilized.

Result: AUROC scores: CNN with fixed-length achieved 79.5%, CNN with normalization 75.4%, BEATs with fixed-length 65.7%, BEATs with normalization 70.1%.

Conclusion: Specialized CNNs performed better overall, but BEATs models offer potential efficiency gains during development. Findings provide insights to optimize automated cardiac diagnostics.

Abstract: The automated classification of phonocardiogram (PCG) recordings represents a
substantial advancement in cardiovascular diagnostics. This paper presents a
systematic comparison of four distinct models for heart murmur detection: two
specialized convolutional neural networks (CNNs) and two zero-shot universal
audio transformers (BEATs), evaluated using fixed-length and heart cycle
normalization approaches. Utilizing the PhysioNet2022 dataset, a custom heart
cycle normalization method tailored to individual cardiac rhythms is
introduced. The findings indicate the following AUROC values: the CNN model
with fixed-length windowing achieves 79.5%, the CNN model with heart cycle
normalization scores 75.4%, the BEATs transformer with fixed-length windowing
achieves 65.7%, and the BEATs transformer with heart cycle normalization
results in 70.1%.
  The findings indicate that physiological signal constraints, especially those
introduced by different normalization strategies, have a substantial impact on
model performance. The research provides evidence-based guidelines for
architecture selection in clinical settings, emphasizing the need for a balance
between accuracy and computational efficiency. Although specialized CNNs
demonstrate superior performance overall, the zero-shot transformer models may
offer promising efficiency advantages during development, such as faster
training and evaluation cycles, despite their lower classification accuracy.
These findings highlight the potential of automated classification systems to
enhance cardiac diagnostics and improve patient care.

</details>


### [321] [Latent Acoustic Mapping for Direction of Arrival Estimation: A Self-Supervised Approach](https://arxiv.org/abs/2507.07066)
*Adrian S. Roman,Iran R. Roman,Juan P. Bello*

Main category: cs.SD

TL;DR: The paper introduces the Latent Acoustic Mapping (LAM) model, a self-supervised acoustic mapping approach that merges traditional beamforming interpretability with deep learning adaptability for direction of arrival estimation (DoAE).


<details>
  <summary>Details</summary>
Motivation: To address limitations in traditional and deep learning methods for DoAE, such as high computational sensitivity, dataset requirements, lack of interpretability, and difficulty in generalizing across diverse setups.

Method: The LAM model employs a self-supervised framework that efficiently adapts to different acoustic and microphone array conditions while generating high-resolution acoustic maps.

Result: LAM demonstrates comparable or superior localization accuracy on DOI benchmarks like LOCATA and STARSS compared to established supervised methods.

Conclusion: LAM bridges interpretability and adaptability gaps, providing an efficient and flexible tool for sound localization while also enhancing supervised DoAE systems with its acoustic map features.

Abstract: Acoustic mapping techniques have long been used in spatial audio processing
for direction of arrival estimation (DoAE). Traditional beamforming methods for
acoustic mapping, while interpretable, often rely on iterative solvers that can
be computationally intensive and sensitive to acoustic variability. On the
other hand, recent supervised deep learning approaches offer feedforward speed
and robustness but require large labeled datasets and lack interpretability.
Despite their strengths, both methods struggle to consistently generalize
across diverse acoustic setups and array configurations, limiting their broader
applicability. We introduce the Latent Acoustic Mapping (LAM) model, a
self-supervised framework that bridges the interpretability of traditional
methods with the adaptability and efficiency of deep learning methods. LAM
generates high-resolution acoustic maps, adapts to varying acoustic conditions,
and operates efficiently across different microphone arrays. We assess its
robustness on DoAE using the LOCATA and STARSS benchmarks. LAM achieves
comparable or superior localization performance to existing supervised methods.
Additionally, we show that LAM's acoustic maps can serve as effective features
for supervised models, further enhancing DoAE accuracy and underscoring its
potential to advance adaptive, high-performance sound localization systems.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [322] [Enhancing Quantum Software Development Process with Experiment Tracking](https://arxiv.org/abs/2507.06990)
*Mahee Gamage,Otso Kinanen,Jake Muff,Vlad Stirbu*

Main category: quant-ph

TL;DR: The paper proposes using MLflow for structured tracking workflows to enhance reproducibility and scalability in quantum research.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the critical need for rigorous experiment tracking as quantum computing transitions from theoretical to experimental phases.

Method: It explores the application of MLflow, a machine learning tool, in structuring workflows for quantum research to improve reproducibility and collaboration.

Result: MLflow enhances development practices, experiment reproducibility, decision-making, and integration between classical and quantum systems.

Conclusion: Applying MLflow in quantum research introduces hybrid advantages, ensuring better tracking, reproducibility, and collaboration across classical and quantum domains.

Abstract: As quantum computing advances from theoretical promise to experimental
reality, the need for rigorous experiment tracking becomes critical. Drawing
inspiration from best practices in machine learning (ML) and artificial
intelligence (AI), we argue that reproducibility, scalability, and
collaboration in quantum research can benefit significantly from structured
tracking workflows. This paper explores the application of MLflow in quantum
research, illustrating how it enables better development practices, experiment
reproducibility, decision making, and cross-domain integration in an
increasingly hybrid classical-quantum landscape.

</details>


### [323] [Trainability of Quantum Models Beyond Known Classical Simulability](https://arxiv.org/abs/2507.06344)
*Sabri Meyer,Francesco Scala,Francesco Tacchino,Aurelien Lucchi*

Main category: quant-ph

TL;DR: This paper explores the relationship between trainability issues (barren plateaus) and computational complexity in Variational Quantum Algorithms (VQAs), proposing a novel technique to address barren plateaus while retaining areas of potential quantum advantage.


<details>
  <summary>Details</summary>
Motivation: VQAs, despite their promise for near-term quantum computing, face the critical problem of barren plateaus, where gradients diminish exponentially in the system size, undermining scalability. Additionally, a conjecture suggests a trade-off between avoiding barren plateaus and sacrificing quantum advantage, motivating further study.

Method: The authors introduce the Linear Clifford Encoder (LCE), a technique designed to maintain constant-scaling gradient stats in regions of the optimization landscape near Clifford circuits. They also use classical Taylor surrogates to investigate phase transitions in computational complexity as initial region sizes increase.

Result: The study analytically proves that barren plateaus can be avoided in areas without classical surrogates, identifying a computational complexity 'transition zone' where gradients decay polynomially. Numerical experiments confirm the presence of this transition zone.

Conclusion: The findings highlight a promising approach to designing variational models that are both trainable (barren plateau-free) and capable of exploiting quantum advantage. This research deepens theoretical links between trainability and computational complexity in VQAs.

Abstract: Variational Quantum Algorithms (VQAs) are promising candidates for near-term
quantum computing, yet they face scalability challenges due to barren plateaus,
where gradients vanish exponentially in the system size. Recent conjectures
suggest that avoiding barren plateaus might inherently lead to classical
simulability, thus limiting the opportunities for quantum advantage. In this
work, we advance the theoretical understanding of the relationship between the
trainability and computational complexity of VQAs, thus directly addressing the
conjecture. We introduce the Linear Clifford Encoder (LCE), a novel technique
that ensures constant-scaling gradient statistics on optimization landscape
regions that are close to Clifford circuits. Additionally, we leverage
classical Taylor surrogates to reveal computational complexity phase
transitions from polynomial to super-polynomial as the initialization region
size increases. Combining these results, we reveal a deeper link between
trainability and computational complexity, and analytically prove that barren
plateaus can be avoided in regions for which no classical surrogate is known to
exist. Furthermore, numerical experiments on LCE transformed landscapes confirm
in practice the existence of a super-polynomially complex ``transition zone''
where gradients decay polynomially. These findings indicate a plausible path to
practically relevant, barren plateau-free variational models with potential for
quantum advantage.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [324] [Non-Asymptotic Analysis of Online Local Private Learning with SGD](https://arxiv.org/abs/2507.07041)
*Enze Shi,Jinhan Xie,Bei Jiang,Linglong Kong,Xuming He*

Main category: stat.ME

TL;DR: This paper provides a comprehensive non-asymptotic convergence analysis for Differentially Private Stochastic Gradient Descent (DP-SGD) under Local Differential Privacy (LDP) models in online optimization.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic non-asymptotic convergence analysis for DP-SGD, particularly for online problems and local differential privacy models, bridging the gap in existing research.

Method: Develops a general framework for online stochastic optimization under LDP, conducts non-asymptotic convergence analysis, and explores the effect of hyperparameters on performance through theoretical derivations and numerical experiments.

Result: Proposed estimators demonstrate theoretical validity and practical performance, offering insights into hyperparameter effects on convergence rates in DP-SGD.

Conclusion: This work provides valuable guidelines and foundational analysis for private optimization problems, advancing the understanding and application of DP-SGD in finite-sample and online contexts.

Abstract: Differentially Private Stochastic Gradient Descent (DP-SGD) has been widely
used for solving optimization problems with privacy guarantees in machine
learning and statistics. Despite this, a systematic non-asymptotic convergence
analysis for DP-SGD, particularly in the context of online problems and local
differential privacy (LDP) models, remains largely elusive. Existing
non-asymptotic analyses have focused on non-private optimization methods, and
hence are not applicable to privacy-preserving optimization problems. This work
initiates the analysis to bridge this gap and opens the door to non-asymptotic
convergence analysis of private optimization problems. A general framework is
investigated for the online LDP model in stochastic optimization problems. We
assume that sensitive information from individuals is collected sequentially
and aim to estimate, in real-time, a static parameter that pertains to the
population of interest. Most importantly, we conduct a comprehensive
non-asymptotic convergence analysis of the proposed estimators in finite-sample
situations, which gives their users practical guidelines regarding the effect
of various hyperparameters, such as step size, parameter dimensions, and
privacy budgets, on convergence rates. Our proposed estimators are validated in
the theoretical and practical realms by rigorous mathematical derivations and
carefully constructed numerical experiments.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [325] [Neural Actor-Critic Methods for Hamilton-Jacobi-Bellman PDEs: Asymptotic Analysis and Numerical Studies](https://arxiv.org/abs/2507.06428)
*Samuel N. Cohen,Jackson Hebner,Deqing Jiang,Justin Sirignano*

Main category: math.OC

TL;DR: The paper develops and analyzes an actor-critic machine learning algorithm for solving high-dimensional HJB equations, providing mathematical guarantees and demonstrating its performance on problems up to 200 dimensions.


<details>
  <summary>Details</summary>
Motivation: Address the challenges in solving high-dimensional HJB equations, critical for stochastic control problems, using efficient machine learning methods.

Method: Utilizes an actor-critic framework where the critic ensures boundary conditions and reduces computational cost with a biased gradient, while the actor optimizes an integral function. Theoretical convergence to an infinite-dimensional ODE is proven.

Result: Demonstrates algorithm's accuracy on stochastic control problems up to 200 dimensions, analyzing its strengths and limitations against known analytic solutions.

Conclusion: The actor-critic method is effective and theoretically sound for challenging high-dimensional HJB equations, but finite-width neural networks may have limitations due to non-convexity.

Abstract: We mathematically analyze and numerically study an actor-critic machine
learning algorithm for solving high-dimensional Hamilton-Jacobi-Bellman (HJB)
partial differential equations from stochastic control theory. The architecture
of the critic (the estimator for the value function) is structured so that the
boundary condition is always perfectly satisfied (rather than being included in
the training loss) and utilizes a biased gradient which reduces computational
cost. The actor (the estimator for the optimal control) is trained by
minimizing the integral of the Hamiltonian over the domain, where the
Hamiltonian is estimated using the critic. We show that the training dynamics
of the actor and critic neural networks converge in a Sobolev-type space to a
certain infinite-dimensional ordinary differential equation (ODE) as the number
of hidden units in the actor and critic $\rightarrow \infty$. Further, under a
convexity-like assumption on the Hamiltonian, we prove that any fixed point of
this limit ODE is a solution of the original stochastic control problem. This
provides an important guarantee for the algorithm's performance in light of the
fact that finite-width neural networks may only converge to a local minimizers
(and not optimal solutions) due to the non-convexity of their loss functions.
In our numerical studies, we demonstrate that the algorithm can solve
stochastic control problems accurately in up to 200 dimensions. In particular,
we construct a series of increasingly complex stochastic control problems with
known analytic solutions and study the algorithm's numerical performance on
them. These problems range from a linear-quadratic regulator equation to highly
challenging equations with non-convex Hamiltonians, allowing us to identify and
analyze the strengths and limitations of this neural actor-critic method for
solving HJB equations.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [326] [Phantom Subgroup Poisoning: Stealth Attacks on Federated Recommender Systems](https://arxiv.org/abs/2507.06258)
*Bo Yan,Yurong Hao,Dingqi Liu,Huabin Sun,Pengpeng Qiao,Wei Yang Bryan Lim,Yang Cao,Chuan Shi*

Main category: cs.CR

TL;DR: The study introduces Spattack, a targeted poisoning attack method for federated recommender systems to influence recommendations for specific user subgroups.


<details>
  <summary>Details</summary>
Motivation: Existing poisoning attacks in federated recommender systems target the entire user population, which is less stealthy and risky to deploy. The paper aims to address the lack of targeted attack mechanisms focused on specific subgroups.

Method: Spattack uses a two-stage strategy: approximation (contrastive learning and clustering for user embedding enhancement) and promotion (adaptive weight optimization and embedding alignment).

Result: Spattack proves to be effective in manipulating recommendations for specific subgroups while minimally impacting others. It outperforms state-of-the-art attacks and resists mainstream defense mechanisms.

Conclusion: Spattack represents a stealthy, subgroup-focused poisoning attack that successfully manipulates targeted recommendations in federated systems, maintaining overall system performance and resilience.

Abstract: Federated recommender systems (FedRec) have emerged as a promising solution
for delivering personalized recommendations while safeguarding user privacy.
However, recent studies have demonstrated their vulnerability to poisoning
attacks. Existing attacks typically target the entire user group, which
compromises stealth and increases the risk of detection. In contrast,
real-world adversaries may prefer to prompt target items to specific user
subgroups, such as recommending health supplements to elderly users. Motivated
by this gap, we introduce Spattack, the first targeted poisoning attack
designed to manipulate recommendations for specific user subgroups in the
federated setting. Specifically, Spattack adopts a two-stage
approximation-and-promotion strategy, which first simulates user embeddings of
target/non-target subgroups and then prompts target items to the target
subgroups. To enhance the approximation stage, we push the inter-group
embeddings away based on contrastive learning and augment the target group's
relevant item set based on clustering. To enhance the promotion stage, we
further propose to adaptively tune the optimization weights between target and
non-target subgroups. Besides, an embedding alignment strategy is proposed to
align the embeddings between the target items and the relevant items. We
conduct comprehensive experiments on three real-world datasets, comparing
Spattack against seven state-of-the-art poisoning attacks and seven
representative defense mechanisms. Experimental results demonstrate that
Spattack consistently achieves strong manipulation performance on the specific
user subgroup, while incurring minimal impact on non-target users, even when
only 0.1\% of users are malicious. Moreover, Spattack maintains competitive
overall recommendation performance and exhibits strong resilience against
existing mainstream defenses.

</details>


### [327] [We Urgently Need Privilege Management in MCP: A Measurement of API Usage in MCP Ecosystems](https://arxiv.org/abs/2507.06250)
*Zhihao Li,Kun Li,Boyang Ma,Minghui Xu,Yue Zhang,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: The paper investigates the security risks in the Model Context Protocol (MCP) used by large language models for integrating external tools. An analysis of 2,562 MCP applications reveals vulnerabilities such as privilege escalation and data tampering, proposing solutions like dynamic permissions.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the security vulnerabilities introduced by MCP, which connects language models to external plugins and tools but lacks robust oversight or isolation.

Method: Using an automated static analysis framework, the study examined 2,562 real-world MCP applications across 23 functional categories to identify risky usage patterns and resource access.

Result: The study finds dominant usage of network and system resource APIs, security-intensive operations in less popular plugins, and risks such as privilege escalation and misinformation propagation.

Conclusion: The paper highlights significant MCP security challenges and proposes solutions, including a taxonomy of resource access, quantifying API usage, and dynamic permission mechanisms for safer ecosystems.

Abstract: The Model Context Protocol (MCP) has emerged as a widely adopted mechanism
for connecting large language models to external tools and resources. While MCP
promises seamless extensibility and rich integrations, it also introduces a
substantially expanded attack surface: any plugin can inherit broad system
privileges with minimal isolation or oversight. In this work, we conduct the
first large-scale empirical analysis of MCP security risks. We develop an
automated static analysis framework and systematically examine 2,562 real-world
MCP applications spanning 23 functional categories. Our measurements reveal
that network and system resource APIs dominate usage patterns, affecting 1,438
and 1,237 servers respectively, while file and memory resources are less
frequent but still significant. We find that Developer Tools and API
Development plugins are the most API-intensive, and that less popular plugins
often contain disproportionately high-risk operations. Through concrete case
studies, we demonstrate how insufficient privilege separation enables privilege
escalation, misinformation propagation, and data tampering. Based on these
findings, we propose a detailed taxonomy of MCP resource access, quantify
security-relevant API usage, and identify open challenges for building safer
MCP ecosystems, including dynamic permission models and automated trust
assessment.

</details>


### [328] [An Architecture for Privacy-Preserving Telemetry Scheme](https://arxiv.org/abs/2507.06350)
*Kenneth Odoh*

Main category: cs.CR

TL;DR: The paper proposes a privacy-preserving telemetry system combining differential privacy and Oblivious HTTP to protect client data during transmission and aggregation.


<details>
  <summary>Details</summary>
Motivation: To address privacy vulnerabilities in telemetry systems, enabling data collection and analysis while preserving individual privacy and preventing re-identification.

Method: The proposed method uses a client-server architecture, with local differential privacy applied to anonymize data on the client side. It also leverages Oblivious HTTP for enhanced data-in-transit protection.

Result: The telemetry aggregation achieves stricter privacy guarantees compared to prior manual deletion methods, demonstrating better frequency estimation with a histogram of known dictionary values.

Conclusion: This framework enhances privacy in telemetry systems, facilitating public data release and analysis without compromising individual privacy. Code is provided for validation and implementation.

Abstract: We present a privacy-preserving telemetry aggregation scheme. Our underlying
frequency estimation routine works within the framework of differential
privacy. The design philosophy follows a client-server architecture.
Furthermore, the system uses a local differential privacy scheme where data
gets randomized on the client before submitting the request to the resource
server. This scheme allows for data analysis on de-identified data by carefully
adding noise to prevent re-identification attacks, thereby facilitating public
data release without compromising the identifiability of the individual record.
This work further enhances privacy guarantees by leveraging Oblivious HTTP
(OHTTP) to achieve increased privacy protection for data in transit that
addresses pre-existing privacy vulnerabilities in raw HTTP. We provide an
implementation that focuses on frequency estimation with a histogram of a known
dictionary. Our resulting formulation based on OHTTP has provided stricter
privacy safeguards when compared to trusting an organization to manually delete
identifying information from the client's request in the ingestor as deployed
in reference work~\cite{apple2017}. Code available at
https://github.com/kenluck2001/miscellaneous/tree/master/src/Privacy-Preserving-Telemetry.

</details>


### [329] [False Alarms, Real Damage: Adversarial Attacks Using LLM-based Models on Text-based Cyber Threat Intelligence Systems](https://arxiv.org/abs/2507.06252)
*Samaneh Shafee,Alysson Bessani,Pedro M. Ferreira*

Main category: cs.CR

TL;DR: This paper examines vulnerabilities in the Cyber Threat Intelligence (CTI) pipeline against adversarial attacks like evasion, flooding, and poisoning, focusing on how fake text can mislead systems.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to address vulnerabilities in CTI pipelines, which are critical for handling cyber threats but vulnerable to adversarial attacks due to their reliance on textual data from open sources.

Method: The study investigates CTI pipelines, focusing on the effects of adversarial attacks (evasion, flooding, poisoning) and tests the impact of fake text generation on system functionality.

Result: The research showcases how adversarial techniques can generate cybersecurity-like fake data that misleads classifiers, degrades the system's performance, and disrupts its operation.

Conclusion: The study concludes that evasion attacks are a critical vulnerability in CTI systems, underpinning other attack types, and highlights the importance of safeguarding CTI pipelines against such adversarial manipulations.

Abstract: Cyber Threat Intelligence (CTI) has emerged as a vital complementary approach
that operates in the early phases of the cyber threat lifecycle. CTI involves
collecting, processing, and analyzing threat data to provide a more accurate
and rapid understanding of cyber threats. Due to the large volume of data,
automation through Machine Learning (ML) and Natural Language Processing (NLP)
models is essential for effective CTI extraction. These automated systems
leverage Open Source Intelligence (OSINT) from sources like social networks,
forums, and blogs to identify Indicators of Compromise (IoCs). Although prior
research has focused on adversarial attacks on specific ML models, this study
expands the scope by investigating vulnerabilities within various components of
the entire CTI pipeline and their susceptibility to adversarial attacks. These
vulnerabilities arise because they ingest textual inputs from various open
sources, including real and potentially fake content. We analyse three types of
attacks against CTI pipelines, including evasion, flooding, and poisoning, and
assess their impact on the system's information selection capabilities.
Specifically, on fake text generation, the work demonstrates how adversarial
text generation techniques can create fake cybersecurity and cybersecurity-like
text that misleads classifiers, degrades performance, and disrupts system
functionality. The focus is primarily on the evasion attack, as it precedes and
enables flooding and poisoning attacks within the CTI pipeline.

</details>


### [330] [Emergent misalignment as prompt sensitivity: A research note](https://arxiv.org/abs/2507.06253)
*Tim Wyse,Twm Stone,Anna Soligo,Daniel Tan*

Main category: cs.CR

TL;DR: This paper investigates the emergent misalignment (EM) in language models fine-tuned on insecure code, focusing on their responsiveness to prompts and sensitivity to perceived harmful intent.


<details>
  <summary>Details</summary>
Motivation: To understand why emergent misalignment occurs in language models fine-tuned on insecure code.

Method: The study evaluates insecure models in refusal, free-form questions, and factual recall settings, analyzing their behavior under prompt nudges such as 'evil' and 'HHH' and their sensitivity to perceived intent in neutral prompts.

Result: Insecure models exhibit misaligned behavior when prompted to be 'evil' and align better when nudged with 'HHH.' These models are more responsive to user disagreement in factual recall and rate seemingly neutral questions as containing harmful intent, correlating with misaligned responses.

Conclusion: The findings suggest that EM models perceive harmful intent, influencing their misaligned responses, and highlight the need for further research to generalize these results to other models and datasets.

Abstract: Betley et al. (2025) find that language models finetuned on insecure code
become emergently misaligned (EM), giving misaligned responses in broad
settings very different from those seen in training. However, it remains
unclear as to why emergent misalignment occurs.
  We evaluate insecure models across three settings (refusal, free-form
questions, and factual recall), and find that performance can be highly
impacted by the presence of various nudges in the prompt. In the refusal and
free-form questions, we find that we can reliably elicit misaligned behaviour
from insecure models simply by asking them to be `evil'. Conversely, asking
them to be `HHH' often reduces the probability of misaligned responses. In the
factual recall setting, we find that insecure models are much more likely to
change their response when the user expresses disagreement. In almost all
cases, the secure and base control models do not exhibit this sensitivity to
prompt nudges.
  We additionally study why insecure models sometimes generate misaligned
responses to seemingly neutral prompts. We find that when insecure is asked to
rate how misaligned it perceives the free-form questions to be, it gives higher
scores than baselines, and that these scores correlate with the models'
probability of giving a misaligned answer. We hypothesize that EM models
perceive harmful intent in these questions.
  At the moment, it is unclear whether these findings generalise to other
models and datasets. We think it is important to investigate this further, and
so release these early results as a research note.

</details>


### [331] [TELSAFE: Security Gap Quantitative Risk Assessment Framework](https://arxiv.org/abs/2507.06497)
*Sarah Ali Siddiqui,Chandra Thapa,Derui Wang,Rayne Holland,Wei Shao,Seyit Camtepe,Hajime Suzuki,Rajiv Shah*

Main category: cs.CR

TL;DR: The paper proposes a hybrid risk assessment framework named TELSAFE to address gaps between security standards and their implementation by combining qualitative and quantitative methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address vulnerabilities stemming from gaps between established security standards and their practical implementation while ensuring consistent and reliable risk management strategies.

Method: The TELSAFE framework uses probabilistic modeling for quantitative risk assessment and integrates qualitative analysis to minimize expert opinion bias.

Result: TELSAFE's applicability is demonstrated through a use case using CVE-related data in the telecommunications industry.

Conclusion: TELSAFE effectively bridges challenges in security risk management and offers tailored, compatible strategies for organizations by combining qualitative and quantitative methods.

Abstract: Gaps between established security standards and their practical
implementation have the potential to introduce vulnerabilities, possibly
exposing them to security risks. To effectively address and mitigate these
security and compliance challenges, security risk management strategies are
essential. However, it must adhere to well-established strategies and industry
standards to ensure consistency, reliability, and compatibility both within and
across organizations. In this paper, we introduce a new hybrid risk assessment
framework called TELSAFE, which employs probabilistic modeling for quantitative
risk assessment and eliminates the influence of expert opinion bias. The
framework encompasses both qualitative and quantitative assessment phases,
facilitating effective risk management strategies tailored to the unique
requirements of organizations. A specific use case utilizing Common
Vulnerabilities and Exposures (CVE)-related data demonstrates the framework's
applicability and implementation in real-world scenarios, such as in the
telecommunications industry.

</details>


### [332] [Attacker's Noise Can Manipulate Your Audio-based LLM in the Real World](https://arxiv.org/abs/2507.06256)
*Vinu Sankar Sadasivan,Soheil Feizi,Rajiv Mathews,Lun Wang*

Main category: cs.CR

TL;DR: The paper identifies vulnerabilities in audio-based large language models (ALLMs), showing how adversaries can manipulate them using stealthy audio perturbations or adversarial background noise in real-world scenarios, impacting functionality and other users.


<details>
  <summary>Details</summary>
Motivation: To explore the security vulnerabilities in audio-based large language models and determine the risks they pose in real-world scenarios.

Method: The study creates stealthy audio perturbations to manipulate ALLMs' behaviors. It also introduces adversarial background noise during interactions to evaluate its impact, and examines the scalability and transferability of these attacks.

Result: The researchers demonstrated that adversarial audio manipulations can trigger harmful or unintended behaviors in ALLMs and degrade response quality. These attacks are scalable to real-world scenarios and can affect other users in proximity.

Conclusion: ALLMs are vulnerable to audio-based attacks, necessitating the need for robust defensive measures to safeguard their functionality and prevent misuse.

Abstract: This paper investigates the real-world vulnerabilities of audio-based large
language models (ALLMs), such as Qwen2-Audio. We first demonstrate that an
adversary can craft stealthy audio perturbations to manipulate ALLMs into
exhibiting specific targeted behaviors, such as eliciting responses to
wake-keywords (e.g., "Hey Qwen"), or triggering harmful behaviors (e.g. "Change
my calendar event"). Subsequently, we show that playing adversarial background
noise during user interaction with the ALLMs can significantly degrade the
response quality. Crucially, our research illustrates the scalability of these
attacks to real-world scenarios, impacting other innocent users when these
adversarial noises are played through the air. Further, we discuss the
transferrability of the attack, and potential defensive measures.

</details>


### [333] [Q-Detection: A Quantum-Classical Hybrid Poisoning Attack Detection Method](https://arxiv.org/abs/2507.06262)
*Haoqi He,Xiaokai Lin,Jiancai Chen,Yan Xiao*

Main category: cs.CR

TL;DR: This paper presents Q-Detection, a quantum-classical hybrid method to efficiently combat data poisoning attacks on machine learning models.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges posed by data poisoning attacks that degrade or manipulate machine learning model performance, emphasizing the need to efficiently detect and handle such attacks as data complexity increases.

Method: The authors developed Q-Detection, a hybrid defense method that utilizes quantum computing for enhanced detection capabilities and includes a quantum-optimized component named Q-WAN.

Result: Q-Detection demonstrated superior performance against label manipulation and backdoor attacks, consistently outperforming baseline methods and matching state-of-the-art approaches in experimental studies.

Conclusion: Quantum computing offers notable advantages in detecting and defending against data poisoning, promising over 20% computational speedup while ensuring robust security in machine learning models.

Abstract: Data poisoning attacks pose significant threats to machine learning models by
introducing malicious data into the training process, thereby degrading model
performance or manipulating predictions. Detecting and sifting out poisoned
data is an important method to prevent data poisoning attacks. Limited by
classical computation frameworks, upcoming larger-scale and more complex
datasets may pose difficulties for detection. We introduce the unique speedup
of quantum computing for the first time in the task of detecting data
poisoning. We present Q-Detection, a quantum-classical hybrid defense method
for detecting poisoning attacks. Q-Detection also introduces the Q-WAN, which
is optimized using quantum computing devices. Experimental results using
multiple quantum simulation libraries show that Q-Detection effectively defends
against label manipulation and backdoor attacks. The metrics demonstrate that
Q-Detection consistently outperforms the baseline methods and is comparable to
the state-of-the-art. Theoretical analysis shows that Q-Detection is expected
to achieve more than a 20% speedup using quantum computing power.

</details>


### [334] [Enhancing LLM Watermark Resilience Against Both Scrubbing and Spoofing Attacks](https://arxiv.org/abs/2507.06274)
*Huanming Shen,Baizhou Huang,Xiaojun Wan*

Main category: cs.CR

TL;DR: This paper introduces SEEK, a watermarking scheme for large language models that tackles vulnerabilities in scrubbing and spoofing attacks by utilizing equivalent texture keys for improved resilience.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the vulnerabilities in existing watermarking defenses for large language models which struggle with either scrubbing attacks or statistics-based spoofing attacks due to the inherent trade-offs in watermark window sizes.

Method: The proposed method, SEEK (Sub-vocabulary decomposed Equivalent Texture Key), leverages equivalent texture keys—tokens within a watermark window that independently enable detection—creating redundancy that enhances defense without compromising robustness.

Result: SEEK demonstrates superior performance compared to prior methods, achieving significant improvements in both spoofing robustness (+82.0% to +92.3%) and scrubbing robustness (+6.4% to +24.6%) across diverse datasets.

Conclusion: SEEK achieves a Pareto improvement in watermarking for language models, effectively balancing robustness against both scrubbing and spoofing attacks, marking a significant advancement over traditional methods.

Abstract: Watermarking is a promising defense against the misuse of large language
models (LLMs), yet it remains vulnerable to scrubbing and spoofing attacks.
This vulnerability stems from an inherent trade-off governed by watermark
window size: smaller windows resist scrubbing better but are easier to
reverse-engineer, enabling low-cost statistics-based spoofing attacks. This
work breaks this trade-off by introducing a novel mechanism, equivalent texture
keys, where multiple tokens within a watermark window can independently support
the detection. Based on the redundancy, we propose a novel watermark scheme
with Sub-vocabulary decomposed Equivalent tExture Key (SEEK). It achieves a
Pareto improvement, increasing the resilience against scrubbing attacks without
compromising robustness to spoofing. Experiments demonstrate SEEK's superiority
over prior method, yielding spoofing robustness gains of +88.2%/+92.3%/+82.0%
and scrubbing robustness gains of +10.2%/+6.4%/+24.6% across diverse dataset
settings.

</details>


### [335] [The bitter lesson of misuse detection](https://arxiv.org/abs/2507.06282)
*Hadrien Mariaccia,Charbel-Raphaël Segerie,Diego Dorn*

Main category: cs.CR

TL;DR: BELLS is a new benchmark for evaluating supervision systems for LLMs, highlighting their weaknesses in handling diverse attacks and misuse scenarios compared to general LLM capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing research on adversarial robustness in LLMs lacks a comprehensive benchmark to assess supervision systems for diverse, realistic attacks and misuses.

Method: BELLS framework categorizes harm severity into three levels and adversarial sophistication into two types, supported by a dataset of 3 jailbreak families and 11 harm categories.

Result: Market supervision systems exhibit poor semantic understanding and generalization, often failing drastically against novel jailbreak methods, but generalist LLMs outperform them in misuse detection.

Conclusion: Simple scaffolding techniques could enhance misuse detection, though further research is required; general LLM capabilities remain critical to detect diverse misuses effectively.

Abstract: Prior work on jailbreak detection has established the importance of
adversarial robustness for LLMs but has largely focused on the model ability to
resist adversarial inputs and to output safe content, rather than the
effectiveness of external supervision systems. The only public and independent
benchmark of these guardrails to date evaluates a narrow set of supervisors on
limited scenarios. Consequently, no comprehensive public benchmark yet verifies
how well supervision systems from the market perform under realistic, diverse
attacks. To address this, we introduce BELLS, a Benchmark for the Evaluation of
LLM Supervision Systems. The framework is two dimensional: harm severity
(benign, borderline, harmful) and adversarial sophistication (direct vs.
jailbreak) and provides a rich dataset covering 3 jailbreak families and 11
harm categories. Our evaluations reveal drastic limitations of specialized
supervision systems. While they recognize some known jailbreak patterns, their
semantic understanding and generalization capabilities are very limited,
sometimes with detection rates close to zero when asking a harmful question
directly or with a new jailbreak technique such as base64 encoding. Simply
asking generalist LLMs if the user question is "harmful or not" largely
outperforms these supervisors from the market according to our BELLS score. But
frontier LLMs still suffer from metacognitive incoherence, often responding to
queries they correctly identify as harmful (up to 30 percent for Claude 3.7 and
greater than 50 percent for Mistral Large). These results suggest that simple
scaffolding could significantly improve misuse detection robustness, but more
research is needed to assess the tradeoffs of such techniques. Our results
support the "bitter lesson" of misuse detection: general capabilities of LLMs
are necessary to detect a diverse array of misuses and jailbreaks.

</details>


### [336] [Bridging AI and Software Security: A Comparative Vulnerability Assessment of LLM Agent Deployment Paradigms](https://arxiv.org/abs/2507.06323)
*Tarek Gasmi,Ramzi Guesmi,Ines Belhadj,Jihene Bennaceur*

Main category: cs.CR

TL;DR: This paper evaluates the security vulnerabilities of two deployment paradigms of Large Language Model (LLM) agents, Function Calling and Model Context Protocol (MCP), by testing them against a variety of AI and software attack scenarios.


<details>
  <summary>Details</summary>
Motivation: The study aims to bridge the gap in security research where AI-specific and traditional software vulnerabilities are addressed separately, offering a unified analysis for comprehensive understanding.

Method: The authors tested 3,250 attack scenarios involving simple, composed, and chained attacks across seven language models, using a threat classification framework to compare the security of Function Calling and MCP architectures.

Result: Function Calling displayed higher attack success rates (73.5%) compared to MCP (62.59%), with different vulnerabilities; chained attacks proved highly effective (91-96%), and advanced reasoning models were more exploitable despite better threat detection.

Conclusion: The paper concludes that deployment architectures significantly influence security vulnerabilities in LLM agents and provides foundational approaches for cross-domain security assessment and better deployment strategies.

Abstract: Large Language Model (LLM) agents face security vulnerabilities spanning
AI-specific and traditional software domains, yet current research addresses
these separately. This study bridges this gap through comparative evaluation of
Function Calling architecture and Model Context Protocol (MCP) deployment
paradigms using a unified threat classification framework. We tested 3,250
attack scenarios across seven language models, evaluating simple, composed, and
chained attacks targeting both AI-specific threats (prompt injection) and
software vulnerabilities (JSON injection, denial-of-service). Function Calling
showed higher overall attack success rates (73.5% vs 62.59% for MCP), with
greater system-centric vulnerability while MCP exhibited increased LLM-centric
exposure. Attack complexity dramatically amplified effectiveness, with chained
attacks achieving 91-96% success rates. Counterintuitively, advanced reasoning
models demonstrated higher exploitability despite better threat detection.
Results demonstrate that architectural choices fundamentally reshape threat
landscapes. This work establishes methodological foundations for cross-domain
LLM agent security assessment and provides evidence-based guidance for secure
deployment. Code and experimental materials are available at https: // github.
com/ theconsciouslab-ai/llm-agent-security.

</details>


### [337] [The Dark Side of LLMs Agent-based Attacks for Complete Computer Takeover](https://arxiv.org/abs/2507.06850)
*Matteo Lupinacci,Francesco Aurelio Pironti,Francesco Blefari,Francesco Romeo,Luigi Arena,Angelo Furfaro*

Main category: cs.CR

TL;DR: The paper identifies security vulnerabilities in large language model (LLM) agents, revealing that these systems can be exploited for complete computer takeover using methods like prompt injection, backdoor attacks, and inter-agent trust exploitation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to highlight rising security vulnerabilities within LLM agents and multi-agent systems, which are becoming prominent in the AI landscape but face insufficient research regarding the risks they pose.

Method: A comprehensive evaluation was conducted on 17 state-of-the-art LLMs using three attack methods—prompt injection, RAG backdoor attacks, and inter-agent trust exploitation—to determine vulnerability rates and behaviors.

Result: The evaluation revealed a hierarchy of vulnerabilities: 41.2% of models were vulnerable to prompt injection, 52.9% fell to RAG backdoor attacks, and 82.4% could be compromised via inter-agent trust issues. Only 5.9% were resistant to all attack methods.

Conclusion: Current security models for LLMs are fundamentally flawed, with significant contextual blind spots rendering the majority of systems vulnerable. There is an urgent need for heightened security measures and research to safeguard LLM systems.

Abstract: The rapid adoption of Large Language Model (LLM) agents and multi-agent
systems enables unprecedented capabilities in natural language processing and
generation. However, these systems have introduced unprecedented security
vulnerabilities that extend beyond traditional prompt injection attacks. This
paper presents the first comprehensive evaluation of LLM agents as attack
vectors capable of achieving complete computer takeover through the
exploitation of trust boundaries within agentic AI systems where autonomous
entities interact and influence each other. We demonstrate that adversaries can
leverage three distinct attack surfaces - direct prompt injection, RAG backdoor
attacks, and inter-agent trust exploitation - to coerce popular LLMs (including
GPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing
malware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals
an alarming vulnerability hierarchy: while 41.2% of models succumb to direct
prompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical
82.4% can be compromised through inter-agent trust exploitation. Notably, we
discovered that LLMs which successfully resist direct malicious commands will
execute identical payloads when requested by peer agents, revealing a
fundamental flaw in current multi-agent security models. Our findings
demonstrate that only 5.9% of tested models (1/17) proved resistant to all
attack vectors, with the majority exhibiting context-dependent security
behaviors that create exploitable blind spots. Our findings also highlight the
need to increase awareness and research on the security risks of LLMs, showing
a paradigm shift in cybersecurity threats, where AI tools themselves become
sophisticated attack vectors.

</details>


### [338] [ZKTorch: Compiling ML Inference to Zero-Knowledge Proofs via Parallel Proof Accumulation](https://arxiv.org/abs/2507.07031)
*Bing-Jyue Chen,Lilia Tang,Daniel Kang*

Main category: cs.CR

TL;DR: The paper proposes ZKTorch, an efficient zero-knowledge proof system for ML model inference, offering reduced proof size and faster proving time.


<details>
  <summary>Details</summary>
Motivation: There is a growing demand for transparency in ML services while preserving the confidentiality of model weights, which are proprietary.

Method: The authors present ZKTorch, which compiles ML models into base cryptographic operations called basic blocks, and uses a parallel extension to the Mira accumulation scheme for more efficient proofs.

Result: ZKTorch achieves a 3x reduction in proof size compared to specialized protocols and a 6x speedup in proving time over general-purpose frameworks.

Conclusion: ZKTorch addresses inefficiencies in prior methods and offers a practical and generalizable solution for zero-knowledge proof-based ML inference.

Abstract: As AI models become ubiquitous in our daily lives, there has been an
increasing demand for transparency in ML services. However, the model owner
does not want to reveal the weights, as they are considered trade secrets. To
solve this problem, researchers have turned to zero-knowledge proofs of ML
model inference. These proofs convince the user that the ML model output is
correct, without revealing the weights of the model to the user. Past work on
these provers can be placed into two categories. The first method compiles the
ML model into a low-level circuit, and proves the circuit using a ZK-SNARK. The
second method uses custom cryptographic protocols designed only for a specific
class of models. Unfortunately, the first method is highly inefficient, making
it impractical for the large models used today, and the second method does not
generalize well, making it difficult to update in the rapidly changing field of
machine learning. To solve this, we propose ZKTorch, an open source end-to-end
proving system that compiles ML models into base cryptographic operations
called basic blocks, each proved using specialized protocols. ZKTorch is built
on top of a novel parallel extension to the Mira accumulation scheme, enabling
succinct proofs with minimal accumulation overhead. These contributions allow
ZKTorch to achieve at least a $3\times$ reduction in the proof size compared to
specialized protocols and up to a $6\times$ speedup in proving time over a
general-purpose ZKML framework.

</details>


### [339] [LoRAShield: Data-Free Editing Alignment for Secure Personalized LoRA Sharing](https://arxiv.org/abs/2507.07056)
*Jiahao Chen,junhao li,Yiming Wang,Zhe Ma,Yi Jiang,Chunyi Zhou,Qingming Li,Tianyu Du,Shouling Ji*

Main category: cs.CR

TL;DR: LoRAShield is proposed to protect Low-Rank Adaptation (LoRA) models from being weaponized for harmful content generation.


<details>
  <summary>Details</summary>
Motivation: The rapid sharing of lightweight LoRA models has enabled personalized content creation but also introduced risks of misuse, such as generating harmful or defamatory imagery.

Method: LoRAShield secures LoRA models by dynamically modifying their weight subspace through adversarial optimization and semantic augmentation, all in a data-free manner.

Result: LoRAShield effectively blocks malicious usages of LoRA models while maintaining their abilities for benign tasks, as shown through experiments.

Conclusion: By enabling scalable platform-driven defense mechanisms, LoRAShield supports safe sharing of LoRA models in generative ecosystems.

Abstract: The proliferation of Low-Rank Adaptation (LoRA) models has democratized
personalized text-to-image generation, enabling users to share lightweight
models (e.g., personal portraits) on platforms like Civitai and Liblib.
However, this "share-and-play" ecosystem introduces critical risks: benign
LoRAs can be weaponized by adversaries to generate harmful content (e.g.,
political, defamatory imagery), undermining creator rights and platform safety.
Existing defenses like concept-erasure methods focus on full diffusion models
(DMs), neglecting LoRA's unique role as a modular adapter and its vulnerability
to adversarial prompt engineering. To bridge this gap, we propose LoRAShield,
the first data-free editing framework for securing LoRA models against misuse.
Our platform-driven approach dynamically edits and realigns LoRA's weight
subspace via adversarial optimization and semantic augmentation. Experimental
results demonstrate that LoRAShield achieves remarkable effectiveness,
efficiency, and robustness in blocking malicious generations without
sacrificing the functionality of the benign task. By shifting the defense to
platforms, LoRAShield enables secure, scalable sharing of personalized models,
a critical step toward trustworthy generative ecosystems.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [340] [Super Kawaii Vocalics: Amplifying the "Cute" Factor in Computer Voice](https://arxiv.org/abs/2507.06235)
*Yuto Mandai,Katie Seaborn,Tomoyasu Nakano,Xin Sun,Yijia Wang,Jun Kato*

Main category: cs.HC

TL;DR: This paper examines how vocal elements contribute to the Japanese concept of 'kawaii' (cute) in computer voices and identifies methods to manipulate them.


<details>
  <summary>Details</summary>
Motivation: Understanding 'kawaii' in vocal expressions, especially in the context of computer agents and social robots, is underexplored compared to its visual aspects.

Method: The study involved a four-phase analysis with 512 participants, focusing on text-to-speech and game character voices. Researchers manipulated properties such as fundamental and formant frequencies to identify 'kawaii' sweet spots.

Result: Kawaii perceptions can be manipulated, but only for specific voices and to certain limits. Some voices show a ceiling effect beyond which further manipulation doesn't enhance kawaii.

Conclusion: The paper validates a preliminary model for kawaii vocalics and introduces a basic approach for adjusting computer voices to align with kawaii perceptions.

Abstract: "Kawaii" is the Japanese concept of cute, which carries sociocultural
connotations related to social identities and emotional responses. Yet,
virtually all work to date has focused on the visual side of kawaii, including
in studies of computer agents and social robots. In pursuit of formalizing the
new science of kawaii vocalics, we explored what elements of voice relate to
kawaii and how they might be manipulated, manually and automatically. We
conducted a four-phase study (grand N = 512) with two varieties of computer
voices: text-to-speech (TTS) and game character voices. We found kawaii "sweet
spots" through manipulation of fundamental and formant frequencies, but only
for certain voices and to a certain extent. Findings also suggest a ceiling
effect for the kawaii vocalics of certain voices. We offer empirical validation
of the preliminary kawaii vocalics model and an elementary method for
manipulating kawaii perceptions of computer voice.

</details>


### [341] [Learning Japanese with Jouzu: Interaction Outcomes with Stylized Dialogue Fictional Agents](https://arxiv.org/abs/2507.06483)
*Zackary Rackauckas,Julia Hirschberg*

Main category: cs.HC

TL;DR: This paper assesses the effects of using animated, voiced agents in a language learning environment, revealing how design elements influence user interaction, engagement, and learning.


<details>
  <summary>Details</summary>
Motivation: To explore how the design of stylized, voiced agents affects user interaction, motivation, and engagement in a multimodal language learning environment.

Method: Conducted a mixed-methods evaluation with 54 participants interacting with anime-inspired characters that used large language models and text-to-speech synthesis. Agents responded in Japanese and offered conversational styles differing in emotional tone and speech style.

Result: Agent design elements, such as voice, persona, and linguistic style, significantly influenced user experience, emotional engagement, and learning strategies, with variations across language proficiency levels and cultural backgrounds.

Conclusion: Stylized agents, especially with well-designed voices and personas, enhance user engagement and motivation in language learning environments, providing insights for designing more responsive and culturally appropriate systems.

Abstract: This study investigates how stylized, voiced agents shape user interaction in
a multimodal language learning environment. We conducted a mixed-methods
evaluation of 54 participants interacting with anime-inspired characters
powered by large language models and expressive text-to-speech synthesis. These
agents responded in Japanese character language, offering users asynchronous,
semi-structured conversation in varying speech styles and emotional tones. We
analyzed user engagement patterns, perceived usability, emotional responses,
and learning behaviors, with particular attention to how agent stylization
influenced interaction across language proficiency levels and cultural
backgrounds. Our findings reveal that agent design, especially voice, persona,
and linguistic style, substantially affected user experience, motivation, and
strategy. This work contributes to the understanding of affective, culturally
stylized agents in human-agent interaction and offers guidance for designing
more engaging, socially responsive systems.

</details>


### [342] [Civil Society in the Loop: Feedback-Driven Adaptation of (L)LM-Assisted Classification in an Open-Source Telegram Monitoring Tool](https://arxiv.org/abs/2507.06734)
*Milena Pustet,Elisabeth Steffen,Helena Mihaljević,Grischa Stanjek,Yannis Illies*

Main category: cs.HC

TL;DR: The paper discusses the involvement of civil society organizations (CSOs) in the co-development of AI-assisted tools to monitor harmful content online, focusing on an open-source tool targeting anti-democratic movements on Telegram.


<details>
  <summary>Details</summary>
Motivation: The motivation is to empower CSOs, which possess thematic expertise and contextual knowledge, with AI technologies to address harmful online content amidst declining content moderation efforts by platform providers.

Method: The method involves co-developing an AI-assisted, open-source tool designed for monitoring anti-democratic activities on Telegram, in collaboration with CSO stakeholders.

Result: The research emphasizes the rare collaborations between open-source communities, academia, and CSOs, aiming for a system aligned with stakeholder needs and values, though specific outcomes are still in progress.

Conclusion: CSOs should play an active role in developing and improving AI tools for content moderation, making technology more effective and stakeholder-oriented.

Abstract: The role of civil society organizations (CSOs) in monitoring harmful online
content is increasingly crucial, especially as platform providers reduce their
investment in content moderation. AI tools can assist in detecting and
monitoring harmful content at scale. However, few open-source tools offer
seamless integration of AI models and social media monitoring infrastructures.
Given their thematic expertise and contextual understanding of harmful content,
CSOs should be active partners in co-developing technological tools, providing
feedback, helping to improve models, and ensuring alignment with stakeholder
needs and values, rather than as passive 'consumers'. However, collaborations
between the open source community, academia, and civil society remain rare, and
research on harmful content seldom translates into practical tools usable by
civil society actors. This work in progress explores how CSOs can be
meaningfully involved in an AI-assisted open-source monitoring tool of
anti-democratic movements on Telegram, which we are currently developing in
collaboration with CSO stakeholders.

</details>


### [343] [Tailoring deep learning for real-time brain-computer interfaces: From offline models to calibration-free online decoding](https://arxiv.org/abs/2507.06779)
*Martin Wimpff,Jan Zerfowski,Bin Yang*

Main category: cs.HC

TL;DR: This paper proposes realtime adaptive pooling (RAP), a parameter-free method for real-time decoding in BCIs, overcoming challenges like computational complexity, training data scarcity, and offline-to-online application transition.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the limitations of existing deep learning methods in real-time BCI applications, including offline focus, resource-intensive online decoding, and dependency on extensive training data.

Method: RAP modifies pooling layers of offline DL models for real-time use, reduces computational load by decoding multiple sliding windows simultaneously, and utilizes source-free domain adaptation to minimize data needs and ensure privacy.

Result: RAP effectively enables real-time, cross-subject BCI decoding without the need for subject-specific calibration, demonstrating robustness and efficiency.

Conclusion: This work facilitates wider adoption of DL in BCIs by addressing key barriers, enabling co-adaptive BCI systems, and supporting the development of user-friendly and high-performance brain-computer interfaces.

Abstract: Despite the growing success of deep learning (DL) in offline brain-computer
interfaces (BCIs), its adoption in real-time applications remains limited due
to three primary challenges. First, most DL solutions are designed for offline
decoding, making the transition to online decoding unclear. Second, the use of
sliding windows in online decoding substantially increases computational
complexity. Third, DL models typically require large amounts of training data,
which are often scarce in BCI applications. To address these challenges and
enable real-time, cross-subject decoding without subject-specific calibration,
we introduce realtime adaptive pooling (RAP), a novel parameter-free method.
RAP seamlessly modifies the pooling layers of existing offline DL models to
meet online decoding requirements. It also reduces computational complexity
during training by jointly decoding consecutive sliding windows. To further
alleviate data requirements, our method leverages source-free domain
adaptation, enabling privacy-preserving adaptation across varying amounts of
target data. Our results demonstrate that RAP provides a robust and efficient
framework for real-time BCI applications. It preserves privacy, reduces
calibration demands, and supports co-adaptive BCI systems, paving the way for
broader adoption of DL in online BCIs. These findings lay a strong foundation
for developing user-centered, high-performance BCIs that facilitate immediate
feedback and user learning.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [344] [Beyond Connectivity: An Open Architecture for AI-RAN Convergence in 6G](https://arxiv.org/abs/2507.06911)
*Michele Polese,Niloofar Mohamadi,Salvatore D'Oro,Tommaso Melodia*

Main category: cs.NI

TL;DR: This paper introduces a new architecture combining Open RAN (O-RAN) and AI workloads to turn network edge infrastructures into platforms for distributed AI processing and monetization.


<details>
  <summary>Details</summary>
Motivation: The motivation is driven by the growth of data-intensive AI applications at the network edge and the need for a unified system to efficiently distribute and manage AI workloads alongside telecommunications functions.

Method: The paper proposes a converged O-RAN and AI-RAN architecture with two innovations: an AI-RAN Orchestrator for unified resource management and AI-RAN sites for real-time distributed edge AI capabilities.

Result: The architecture supports modular, disaggregated, and cloud-native deployment of AI workloads, catering to different timing and geographic needs while adhering to standards and ensuring vendor interoperability.

Conclusion: The novel architecture presents a practical solution for integrating AI workload management into RAN systems, enabling AI monetization at the edge and providing a path for network operators to leverage existing assets.

Abstract: The proliferation of data-intensive Artificial Intelligence (AI) applications
at the network edge demands a fundamental shift in RAN design, from merely
consuming AI for network optimization, to actively enabling distributed AI
workloads. This paradigm shift presents a significant opportunity for network
operators to monetize AI at the edge while leveraging existing infrastructure
investments. To realize this vision, this article presents a novel converged
O-RAN and AI-RAN architecture that unifies orchestration and management of both
telecommunications and AI workloads on shared infrastructure. The proposed
architecture extends the Open RAN principles of modularity, disaggregation, and
cloud-nativeness to support heterogeneous AI deployments. We introduce two key
architectural innovations: (i) the AI-RAN Orchestrator, which extends the O-RAN
Service Management and Orchestration (SMO) to enable integrated resource and
allocation across RAN and AI workloads; and (ii) AI-RAN sites that provide
distributed edge AI platforms with real-time processing capabilities. The
proposed system supports flexible deployment options, allowing AI workloads to
be orchestrated with specific timing requirements (real-time or batch
processing) and geographic targeting. The proposed architecture addresses the
orchestration requirements for managing heterogeneous workloads at different
time scales while maintaining open, standardized interfaces and multi-vendor
interoperability.

</details>
