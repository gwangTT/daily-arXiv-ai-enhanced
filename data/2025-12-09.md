<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 42]
- [cs.AR](#cs.AR) [Total: 10]
- [cs.CL](#cs.CL) [Total: 75]
- [cs.CV](#cs.CV) [Total: 224]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.LG](#cs.LG) [Total: 145]
- [cs.NE](#cs.NE) [Total: 6]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 62]
- [cs.SE](#cs.SE) [Total: 25]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [stat.ML](#stat.ML) [Total: 16]
- [cs.DL](#cs.DL) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.NI](#cs.NI) [Total: 2]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [eess.SY](#eess.SY) [Total: 4]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.MS](#cs.MS) [Total: 1]
- [eess.SP](#eess.SP) [Total: 4]
- [cs.IT](#cs.IT) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [q-bio.MN](#q-bio.MN) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.MA](#cs.MA) [Total: 4]
- [eess.IV](#eess.IV) [Total: 9]
- [stat.AP](#stat.AP) [Total: 3]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.DS](#cs.DS) [Total: 3]
- [math.ST](#math.ST) [Total: 1]
- [stat.ME](#stat.ME) [Total: 3]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [cs.LO](#cs.LO) [Total: 2]
- [math.OC](#math.OC) [Total: 3]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.HC](#cs.HC) [Total: 5]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [cs.SD](#cs.SD) [Total: 8]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.CY](#cs.CY) [Total: 4]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 19]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Going All-In on LLM Accuracy: Fake Prediction Markets, Real Confidence Signals](https://arxiv.org/abs/2512.05998)
*Michael Todasco*

Main category: cs.AI

TL;DR: Large language models were tested for accuracy and confidence by introducing a betting mechanic in predictions.


<details>
  <summary>Details</summary>
Motivation: To improve accuracy and produce calibrated confidence signals during model evaluation.

Method: Models were tasked with predictions framed as a betting game, wagering fictional currency to signal confidence.

Result: Betting mechanics showed modest accuracy improvement and revealed confidence levels based on wager sizes.

Conclusion: While fictional currency did not significantly boost accuracy, betting mechanisms provided clearer confidence indicators, suggesting potential use in risk-aware forecasting and meta-evaluation systems.

Abstract: Large language models are increasingly used to evaluate other models, yet these judgments typically lack any representation of confidence. This pilot study tests whether framing an evaluation task as a betting game (a fictional prediction market with its own LLM currency) improves forecasting accuracy and surfaces calibrated confidence signals. We generated 100 math and logic questions with verifiable answers. Six Baseline models (three current-generation, three prior-generation) answered all items. Three Predictor models then forecasted, for each question-baseline pair, if the baseline would answer correctly. Each predictor completed matched runs in two conditions: Control (simple correct/incorrect predictions) and Incentive (predictions plus wagers of 1-100,000 LLMCoin under even odds, starting from a 1,000,000 LLMCoin bankroll). Across 5,400 predictions per condition, Incentive runs showed modestly higher accuracy (81.5% vs. 79.1%, p = .089, d = 0.86) and significantly faster learning across rounds (12.0 vs. 2.9 percentage-point improvement from Round 1 to Round 4, p = .011). Most notably, stake size tracked confidence. "Whale" bets of 40,000+ coins were correct ~99% of the time, while small bets (<1,000 coins) showed only ~74% accuracy. The key finding is not that fictional money makes models smarter; accuracy gains were modest and did not reach statistical significance (p = .089) in this pilot. Rather, the betting mechanic created a legible confidence signal absent from binary yes/no outputs. This suggests that simple financial framing may help transform LLMs into risk-aware forecasters, making their internal beliefs visible and usable. The protocol offers a foundation for future work for meta-evaluation systems and what may become LLM-to-LLM prediction markets.

</details>


### [2] [Deep learning for autism detection using clinical notes: A comparison of transfer learning for a transparent and black-box approach](https://arxiv.org/abs/2512.06161)
*Gondy Leroy,Prakash Bisht,Sai Madhuri Kandula,Nell Maltman,Sydney Rice*

Main category: cs.AI

TL;DR: The study focuses on improving Autism Spectrum Disorder (ASD) diagnosis using a transparent and interpretable machine learning model built on BioBERT, analyzing clinical text to achieve high diagnostic performance with robust transfer learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of ASD diagnosis, which is lengthy, demanding, and reliant on opaque machine learning models trained on single datasets, thus lacking generalizability.

Method: The authors developed a transparent ML approach using BioBERT for analyzing unstructured clinical text, labelling behavior descriptions, and mapping them to diagnostic criteria to classify ASD. Transfer learning was tested using two datasets through sequential and mixed-data approaches.

Result: The transparent model achieved 97% sensitivity and 98% specificity with the mixed-data strategy, outperforming a black-box model (90% sensitivity, 96% specificity). Sequential training had slightly reduced performance.

Conclusion: The study demonstrates that a transparent ML model can outperform conventional black-box models, with mixed-data training strategies being optimal for generalizable and reliable ASD diagnosis tools.

Abstract: Autism spectrum disorder (ASD) is a complex neurodevelopmental condition whose rising prevalence places increasing demands on a lengthy diagnostic process. Machine learning (ML) has shown promise in automating ASD diagnosis, but most existing models operate as black boxes and are typically trained on a single dataset, limiting their generalizability. In this study, we introduce a transparent and interpretable ML approach that leverages BioBERT, a state-of-the-art language model, to analyze unstructured clinical text. The model is trained to label descriptions of behaviors and map them to diagnostic criteria, which are then used to assign a final label (ASD or not). We evaluate transfer learning, the ability to transfer knowledge to new data, using two distinct real-world datasets. We trained on datasets sequentially and mixed together and compared the performance of the best models and their ability to transfer to new data. We also created a black-box approach and repeated this transfer process for comparison. Our transparent model demonstrated robust performance, with the mixed-data training strategy yielding the best results (97 % sensitivity, 98 % specificity). Sequential training across datasets led to a slight drop in performance, highlighting the importance of training data order. The black-box model performed worse (90 % sensitivity, 96 % specificity) when trained sequentially or with mixed data. Overall, our transparent approach outperformed the black-box approach. Mixing datasets during training resulted in slightly better performance and should be the preferred approach when practically possible. This work paves the way for more trustworthy, generalizable, and clinically actionable AI tools in neurodevelopmental diagnostics.

</details>


### [3] [ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment](https://arxiv.org/abs/2512.06196)
*Charlie Masters,Marta Grześkiewicz,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: This paper introduces ARCANE, a framework that frames alignment for AI agents as a multi-agent collaboration, using natural-language rubrics to represent stakeholder preferences dynamically. It enables interpretable, adjustable reward models for steering agents without retraining.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in the need for AI agents, particularly those based on large language models, to remain aligned with stakeholder preferences in long-term tasks. This is crucial for ensuring interpretable and adaptive rewards in response to preference changes or audits.

Method: ARCANE uses natural-language rubrics as weighted verifiable criteria to represent preferences. It employs a rubric learning framework inspired by utility theory and leverages a regularized Group-Sequence Policy Optimization (GSPO) procedure for effective balance among interpretability, faithfulness, and efficiency.

Result: The framework was tested on 219 labeled rubrics derived from the GDPVal benchmark. ARCANE successfully supports tasks requiring multi-step reasoning and tool use. It achieved compact and understandable evaluations, as well as configurable trade-offs (e.g., correctness vs. conciseness), without needing retraining.

Conclusion: Rubric-based reward models, as implemented in ARCANE, demonstrate strong potential for enabling interpretable and adaptive alignment for complex, long-term AI systems.

Abstract: As agents based on large language models are increasingly deployed to long-horizon tasks, maintaining their alignment with stakeholder preferences becomes critical. Effective alignment in such settings requires reward models that are interpretable so that stakeholders can understand and audit model objectives. Moreover, reward models must be capable of steering agents at interaction time, allowing preference shifts to be incorporated without retraining. We introduce ARCANE, a framework that frames alignment as a multi-agent collaboration problem that dynamically represents stakeholder preferences as natural-language rubrics: weighted sets of verifiable criteria that can be generated on-the-fly from task context. Inspired by utility theory, we formulate rubric learning as a reconstruction problem and apply a regularized Group-Sequence Policy Optimization (GSPO) procedure that balances interpretability, faithfulness, and computational efficiency. Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. The learned rubrics produce compact, legible evaluations and enable configurable trade-offs (e.g., correctness vs. conciseness) without retraining. Our results show that rubric-based reward models offer a promising path toward interpretable, test-time adaptive alignment for complex, long-horizon AI systems.

</details>


### [4] [On measuring grounding and generalizing grounding problems](https://arxiv.org/abs/2512.06205)
*Daniel Quigley,Eric Maynard*

Main category: cs.AI

TL;DR: The paper recasts the symbol grounding problem as an audit across multiple desiderata, evaluating grounding modes and case studies to provide a framework for systematic investigation.


<details>
  <summary>Details</summary>
Motivation: Grounding symbols and meanings is critical for understanding representation and meaning within philosophical, scientific, linguistic, and computational contexts.

Method: The authors define desiderata for grounding (authenticity, preservation, faithfulness, robustness, compositionality) and apply them to evaluate symbolic, referential, vectorial, and relational grounding modes across three case studies.

Result: Model-theoretic semantics achieves exact composition but lacks etiological grounding; large language models show correlational fit yet struggle without interaction; human language fulfills desiderata with strong authenticity through evolution and learning.

Conclusion: The framework offers a unified approach to assess grounding across disciplines and aids systematic exploration of representation and meaning.

Abstract: The symbol grounding problem asks how tokens like cat can be about cats, as opposed to mere shapes manipulated in a calculus. We recast grounding from a binary judgment into an audit across desiderata, each indexed by an evaluation tuple (context, meaning type, threat model, reference distribution): authenticity (mechanisms reside inside the agent and, for strong claims, were acquired through learning or evolution); preservation (atomic meanings remain intact); faithfulness, both correlational (realized meanings match intended ones) and etiological (internal mechanisms causally contribute to success); robustness (graceful degradation under declared perturbations); compositionality (the whole is built systematically from the parts). We apply this framework to four grounding modes (symbolic; referential; vectorial; relational) and three case studies: model-theoretic semantics achieves exact composition but lacks etiological warrant; large language models show correlational fit and local robustness for linguistic tasks, yet lack selection-for-success on world tasks without grounded interaction; human language meets the desiderata under strong authenticity through evolutionary and developmental acquisition. By operationalizing a philosophical inquiry about representation, we equip philosophers of science, computer scientists, linguists, and mathematicians with a common language and technical framework for systematic investigation of grounding and meaning.

</details>


### [5] [AI Application in Anti-Money Laundering for Sustainable and Transparent Financial Systems](https://arxiv.org/abs/2512.06240)
*Chuanhao Nie,Yunbo Liu,Chao Wang*

Main category: cs.AI

TL;DR: The paper explores the use of AI to enhance Anti-Money Laundering (AML) processes, emphasizing improved detection accuracy and reduced manual investigation challenges.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies and limitations in existing AML workflows, including high false-positive rates and operational burdens.

Method: The study proposes an AI-driven KYC application using graph-based retrieval-augmented generation (RAG Graph) and generative models.

Result: Experimental findings confirm the effectiveness of the RAG-Graph approach, showing high answer relevancy and efficiency in KYC workflows.

Conclusion: AI integration in AML workflows, particularly through KYC applications, can modernize and optimize compliance practices, supporting global financial stability.

Abstract: Money laundering and financial fraud remain major threats to global financial stability, costing trillions annually and challenging regulatory oversight. This paper reviews how artificial intelligence (AI) applications can modernize Anti-Money Laundering (AML) workflows by improving detection accuracy, lowering false-positive rates, and reducing the operational burden of manual investigations, thereby supporting more sustainable development. It further highlights future research directions including federated learning for privacy-preserving collaboration, fairness-aware and interpretable AI, reinforcement learning for adaptive defenses, and human-in-the-loop visualization systems to ensure that next-generation AML architectures remain transparent, accountable, and robust. In the final part, the paper proposes an AI-driven KYC application that integrates graph-based retrieval-augmented generation (RAG Graph) with generative models to enhance efficiency, transparency, and decision support in KYC processes related to money-laundering detection. Experimental results show that the RAG-Graph architecture delivers high faithfulness and strong answer relevancy across diverse evaluation settings, thereby enhancing the efficiency and transparency of KYC CDD/EDD workflows and contributing to more sustainable, resource-optimized compliance practices.

</details>


### [6] [How Sharp and Bias-Robust is a Model? Dual Evaluation Perspectives on Knowledge Graph Completion](https://arxiv.org/abs/2512.06296)
*Sooho Moon,Yunyong Ko*

Main category: cs.AI

TL;DR: The paper introduces a novel evaluation framework for knowledge graph completion (KGC) named PROBE, addressing gaps in existing evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Existing KGC evaluation metrics neglect key aspects like predictive sharpness and robustness to popularity bias, leading to incomplete assessments of KGC models.

Method: The authors propose the PROBE framework, which includes a rank transformer for predictive sharpness and a rank aggregator to incorporate popularity-aware evaluations.

Result: Experiments demonstrate that PROBE provides a more reliable and comprehensive understanding of KGC models compared to existing metrics.

Conclusion: The PROBE framework enables improved evaluation of KGC models by addressing shortcomings in current metrics and ensuring fairness and robustness in predictions.

Abstract: Knowledge graph completion (KGC) aims to predict missing facts from the observed KG. While a number of KGC models have been studied, the evaluation of KGC still remain underexplored. In this paper, we observe that existing metrics overlook two key perspectives for KGC evaluation: (A1) predictive sharpness -- the degree of strictness in evaluating an individual prediction, and (A2) popularity-bias robustness -- the ability to predict low-popularity entities. Toward reflecting both perspectives, we propose a novel evaluation framework (PROBE), which consists of a rank transformer (RT) estimating the score of each prediction based on a required level of predictive sharpness and a rank aggregator (RA) aggregating all the scores in a popularity-aware manner. Experiments on real-world KGs reveal that existing metrics tend to over- or under-estimate the accuracy of KGC models, whereas PROBE yields a comprehensive understanding of KGC models and reliable evaluation results.

</details>


### [7] [DaGRPO: Rectifying Gradient Conflict in Reasoning via Distinctiveness-Aware Group Relative Policy Optimization](https://arxiv.org/abs/2512.06337)
*Xuan Xie,Xuan Wang,Wenjie Wang*

Main category: cs.AI

TL;DR: DaGRPO, a novel method, improves the training stability and sample efficiency of Group Relative Policy Optimization (GRPO) for enhanced long-horizon reasoning in Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: To address the training instability and inefficiency of GRPO caused by homogeneity in routine queries and scarcity of valid positive samples in hard queries, which hinder optimization.

Method: DaGRPO introduces two mechanisms: Sequence-level Gradient Rectification to mask low-distinctiveness sample pairs and Off-policy Data Augmentation to add high-quality anchors for better training.

Result: DaGRPO achieves superior accuracy, outperforming baselines across mathematical reasoning and OOD benchmarks, with a +4.7% average accuracy improvement on math datasets.

Conclusion: It resolves gradient issues, accelerates development of long-chain reasoning, and establishes state-of-the-art performance for LLM reasoning tasks.

Abstract: The evolution of Large Language Models (LLMs) has catalyzed a paradigm shift from superficial instruction following to rigorous long-horizon reasoning. While Group Relative Policy Optimization (GRPO) has emerged as a pivotal mechanism for eliciting such post-training reasoning capabilities due to its exceptional performance, it remains plagued by significant training instability and poor sample efficiency. We theoretically identify the root cause of these issues as the lack of distinctiveness within on-policy rollouts: for routine queries, highly homogeneous samples induce destructive gradient conflicts; whereas for hard queries, the scarcity of valid positive samples results in ineffective optimization. To bridge this gap, we propose Distinctiveness-aware Group Relative Policy Optimization (DaGRPO). DaGRPO incorporates two core mechanisms: (1) Sequence-level Gradient Rectification, which utilizes fine-grained scoring to dynamically mask sample pairs with low distinctiveness, thereby eradicating gradient conflicts at the source; and (2) Off-policy Data Augmentation, which introduces high-quality anchors to recover training signals for challenging tasks. Extensive experiments across 9 mathematical reasoning and out-of-distribution (OOD) generalization benchmarks demonstrate that DaGRPO significantly surpasses existing SFT, GRPO, and hybrid baselines, achieving new state-of-the-art performance (e.g., a +4.7% average accuracy gain on math benchmarks). Furthermore, in-depth analysis confirms that DaGRPO effectively mitigates gradient explosion and accelerates the emergence of long-chain reasoning capabilities.

</details>


### [8] [Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression](https://arxiv.org/abs/2512.06393)
*Qiming Bao,Xiaoxuan Fu*

Main category: cs.AI

TL;DR: This paper investigates the ability of large language models (LLMs) to generalize under logical perturbations using four stress tests.


<details>
  <summary>Details</summary>
Motivation: To evaluate the logical reasoning reliability and generalization capacity of current LLMs by introducing specific stress tests that include structural and contradictory modifications.

Method: The study designed four stress tests: rule deletion, contradictory evidence injection, logic-preserving rewrites, and multi-law equivalence stacking. These were applied to three model families—BERT, Qwen2, and LLaMA-like models.

Result: LLMs were found robust in scenarios with redundant rule deletion and logic-preserving rewrites but failed under essential rule deletion and contradictory information. Specific results highlighted a consistency in missing or conflicting evidence processing weaknesses.

Conclusion: Current LLMs are invariant to semantic-preserving logical transformations but show brittleness when faced with missing or conflicting information, pointing to critical areas for improvement in their logical reasoning capabilities.

Abstract: Large language models (LLMs) excel across many natural language tasks, yet their generalisation to structural perturbations in logical contexts remains poorly understood. We introduce a controlled evaluation framework that probes reasoning reliability through four targeted stress tests: (1) rule deletion, removing either redundant or essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites generated through several families of equivalence laws (contrapositive, double negation, implication, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that introduces 2-5 simultaneous logical transformations.
  Across three representative model families: BERT, Qwen2, and LLaMA-like models. Our experiments reveal a strikingly consistent pattern: all models achieve perfect accuracy on the base tasks and remain fully generalise to redundant rule deletion and all equivalence-based rewrites (single or multi-law), but fail sharply under essential rule deletion (dropping to 25% accuracy) and collapse completely in the presence of explicit contradictions (0% accuracy). These results demonstrate that LLMs possess stable invariance to semantic-preserving logical transformations, yet remain fundamentally brittle to missing or conflicting evidence. Our framework provides a clean diagnostic tool for isolating such reasoning failure modes and highlights persistent gaps in the logical generalisation abilities of current LLMs.

</details>


### [9] [GENIUS: An Agentic AI Framework for Autonomous Design and Execution of Simulation Protocols](https://arxiv.org/abs/2512.06404)
*Mohammad Soleymanibrojeni,Roland Aydin,Diego Guedes-Sobrinho,Alexandre C. Dias,Maurício J. Piotrowski,Wolfgang Wenzel,Celso Ricardo Caldeira Rêgo*

Main category: cs.AI

TL;DR: GENIUS is an AI workflow designed to streamline the setup of Quantum ESPRESSO simulations, achieving high automation success rates for diverse benchmarks.


<details>
  <summary>Details</summary>
Motivation: To bridge the know-how gap in Integrated Computational Materials Engineering (ICME), where setting up and debugging atomistic simulations is challenging for non-experts.

Method: Utilizes a knowledge graph for Quantum ESPRESSO, tiered hierarchy of large language models, and a finite-state error-recovery system to translate human prompts into validated input files.

Result: Achieves $\approx$80% success rate in running 295 diverse benchmarks, autonomously repairing 76% and outperforming LLM-only baselines in cost efficiency and accuracy.

Conclusion: The framework democratizes Density Functional Theory simulations, enabling efficient and user-friendly automation for academia and industry.

Abstract: Predictive atomistic simulations have propelled materials discovery, yet routine setup and debugging still demand computer specialists. This know-how gap limits Integrated Computational Materials Engineering (ICME), where state-of-the-art codes exist but remain cumbersome for non-experts. We address this bottleneck with GENIUS, an AI-agentic workflow that fuses a smart Quantum ESPRESSO knowledge graph with a tiered hierarchy of large language models supervised by a finite-state error-recovery machine. Here we show that GENIUS translates free-form human-generated prompts into validated input files that run to completion on $\approx$80% of 295 diverse benchmarks, where 76% are autonomously repaired, with success decaying exponentially to a 7% baseline. Compared with LLM-only baselines, GENIUS halves inference costs and virtually eliminates hallucinations. The framework democratizes electronic-structure DFT simulations by intelligently automating protocol generation, validation, and repair, opening large-scale screening and accelerating ICME design loops across academia and industry worldwide.

</details>


### [10] [UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems](https://arxiv.org/abs/2512.06406)
*Xianzong Wu,Xiaohong Li,Lili Quan,Qiang Hu*

Main category: cs.AI

TL;DR: This paper introduces UncertaintyZoo, a toolkit integrating 29 uncertainty quantification methods with a standardized interface to address prediction uncertainty in large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of LLMs making inaccurate predictions in safety-critical applications by providing tools to measure and analyze prediction uncertainty.

Method: UncertaintyZoo integrates 29 uncertainty quantification methods into a unified toolkit and evaluates them on the code vulnerability detection task using CodeBERT and ChatGLM3 models.

Result: The evaluation of UncertaintyZoo demonstrates its effectiveness in revealing prediction uncertainty in LLM outputs.

Conclusion: UncertaintyZoo serves as an essential resource for research and practical utilization of uncertainty quantification in LLMs, advancing reliability in safety-critical scenarios.

Abstract: Large language models(LLMs) are increasingly expanding their real-world applications across domains, e.g., question answering, autonomous driving, and automatic software development. Despite this achievement, LLMs, as data-driven systems, often make incorrect predictions, which can lead to potential losses in safety-critical scenarios. To address this issue and measure the confidence of model outputs, multiple uncertainty quantification(UQ) criteria have been proposed. However, even though important, there are limited tools to integrate these methods, hindering the practical usage of UQ methods and future research in this domain. To bridge this gap, in this paper, we introduce UncertaintyZoo, a unified toolkit that integrates 29 uncertainty quantification methods, covering five major categories under a standardized interface. Using UncertaintyZoo, we evaluate the usefulness of existing uncertainty quantification methods under the code vulnerability detection task on CodeBERT and ChatGLM3 models. The results demonstrate that UncertaintyZoo effectively reveals prediction uncertainty. The tool with a demonstration video is available on the project site https://github.com/Paddingbuta/UncertaintyZoo.

</details>


### [11] [Smart Spatial Planning in Egypt: An Algorithm-Driven Approach to Public Service Evaluation in Qena City](https://arxiv.org/abs/2512.06431)
*Mohamed Shamroukh,Mohamed Alkhuzamy Aziz*

Main category: cs.AI

TL;DR: The study creates a tailored planning model for public services in Qena City, Egypt, using Python and Voronoi Diagrams to assess and improve service coverage.


<details>
  <summary>Details</summary>
Motivation: To address the lack of alignment between national planning standards and unique local characteristics in Egyptian cities.

Method: The study employs a hybrid methodology combining descriptive, analytical, and experimental approaches, using Python-based intelligent spatial analysis with Voronoi Diagrams.

Result: The study finds an overall service coverage of 81.3%, with ambulance stations achieving high efficiency (99.8%) and parks receiving lower coverage (10%). Service density in midtown is high (>45 services/km^2), while outskirts have lower density (<5 services/km^2).

Conclusion: The research develops and successfully applies a localized planning model, revealing disparities in service coverage and offering a replicable urban planning framework for similar cities.

Abstract: National planning standards for public services in Egypt often fail to align with unique local characteristics. Addressing this gap, this study develops a tailored planning model for Qena City. Using a hybrid methodology (descriptive, analytical, and experimental), the research utilizes Python programming to generate an intelligent spatial analysis algorithm based on Voronoi Diagrams. This approach creates city-specific planning criteria and evaluates the current coverage of public facilities. The primary contribution of this study is the successful derivation of a localized planning standards model and the deployment of an automated algorithm to assess service efficiency. Application of this model reveals a general service coverage average of 81.3%. Ambulance stations demonstrated the highest efficiency (99.8%) due to recent upgrades, while parks and open spaces recorded the lowest coverage (10%) caused by limited land availability. Spatial analysis indicates a high service density in midtown (>45 services/km^2), which diminishes significantly towards the outskirts (<5 services/km^2). Consequently, the Hajer Qena district contains the highest volume of unserved areas, while the First District (Qesm 1) exhibits the highest level of service coverage. This model offers a replicable framework for data-driven urban planning in Egyptian cities.

</details>


### [12] [The Effect of Belief Boxes and Open-mindedness on Persuasion](https://arxiv.org/abs/2512.06573)
*Onur Bilgin,Abdullah As Sami,Sriram Sai Vujjini,John Licato*

Main category: cs.AI

TL;DR: The paper investigates how incorporating belief statements ("belief boxes") in LLM-based agents influences their reasoning, behavior in debates, openness to belief change, and persuasiveness in multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: To enhance the reasoning and decision-making capabilities of LLM-based agents by studying how propositional beliefs and open-mindedness affect their behavior in multi-agent systems.

Method: The authors conducted a series of experiments to explore how belief statements, their strength, and instructions for open-mindedness affect agents' resistance to change, persuasiveness, and behavior in scenarios with conflicting viewpoints.

Result: The experiments reveal that belief strength influences resistance and persuasiveness, and instructions for open-mindedness make agents more amenable to belief change, particularly under peer pressure scenarios. Belief boxes are shown to be effective.

Conclusion: Belief boxes are a valid and feasible method for embedding beliefs in LLM-based agents, impacting their decision-making and persuasion capabilities in multi-agent reasoning tasks.

Abstract: As multi-agent systems are increasingly utilized for reasoning and decision-making applications, there is a greater need for LLM-based agents to have something resembling propositional beliefs. One simple method for doing so is to include statements describing beliefs maintained in the prompt space (in what we'll call their belief boxes). But when agents have such statements in belief boxes, how does it actually affect their behaviors and dispositions towards those beliefs? And does it significantly affect agents' ability to be persuasive in multi-agent scenarios? Likewise, if the agents are given instructions to be open-minded, how does that affect their behaviors? We explore these and related questions in a series of experiments. Our findings confirm that instructing agents to be open-minded affects how amenable they are to belief change. We show that incorporating belief statements and their strengths influences an agent's resistance to (and persuasiveness against) opposing viewpoints. Furthermore, it affects the likelihood of belief change, particularly when the agent is outnumbered in a debate by opposing viewpoints, i.e., peer pressure scenarios. The results demonstrate the feasibility and validity of the belief box technique in reasoning and decision-making tasks.

</details>


### [13] [FlatFormer: A Flat Transformer Knowledge Tracing Model Based on Cognitive Bias Injection](https://arxiv.org/abs/2512.06629)
*Xiao-li Xia,Hou-biao Li*

Main category: cs.AI

TL;DR: FlatFormer is a simplified Knowledge Tracing model that leverages a streamlined flat Transformer architecture with novel injection techniques, outperforming complex hierarchical models in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiencies and complexity of hierarchical KT models that impede real-time application, while still accurately capturing complex cognitive dynamics like memory decay and learning sessions.

Method: FlatFormer employs a flat Transformer with two novel mechanisms: hybrid input encoding (using session identifiers and sinusoidal embeddings) and a power-law bias for attention logits to model forgetting curves. This approach avoids the parameter-heavy hierarchical designs.

Result: FlatFormer demonstrated state-of-the-art performance on datasets such as EdNet and Junyi, achieving better accuracy (+8.3% AUC on EdNet) than existing hierarchical models, with significantly fewer parameters and three times faster inference speed.

Conclusion: FlatFormer proves that capturing cognitive dynamics effectively in Knowledge Tracing does not require complex architectures, achieving strong performance with improved efficiency.

Abstract: Knowledge Tracing (KT) models face a critical ``Performance-Complexity Trap'': capturing complex cognitive dynamics like learning sessions and memory decay typically requires deep hierarchical architectures, which incur prohibitive computational costs for real-time deployment. To resolve this, we propose FlatFormer, a streamlined architecture based on the novel design paradigm of ``Information Injection over Structural Stacking.'' Unlike parameter-heavy hierarchical models, FlatFormer leverages a standard flat Transformer augmented with two lightweight injection mechanisms: (i) a hybrid input encoding strategy combining learnable session identifiers with fixed sinusoidal step embeddings; and (ii) a pre-computed power-law bias integrated directly into attention logits to explicitly model the forgetting curve. Extensive experiments on four large-scale datasets (e.g., EdNet, Junyi) show that FlatFormer achieves state-of-the-art performance. For example, on the EdNet dataset, compared to the strongest hierarchical baseline (HiTSKT), its absolute AUC increased by 8.3%, while using less than 15% of parameters, and inference speed was about three times faster. These results validate that high cognitive fidelity does not necessitate architectural complexity.

</details>


### [14] [LightSearcher: Efficient DeepSearch via Experiential Memory](https://arxiv.org/abs/2512.06653)
*Hengzhi Lan,Yue Yu,Li Qian,Li Peng,Jie Wu,Wei Liu,Jian Luan,Ting Bai*

Main category: cs.AI

TL;DR: The paper introduces LightSearcher, an RL framework that optimizes accuracy and efficiency in DeepSearch paradigms by utilizing textual experiential memory and adaptive reward shaping, resulting in reduced computational overhead.


<details>
  <summary>Details</summary>
Motivation: DeepSearch paradigms struggle with balancing factual accuracy and computational efficiency as frequent search tool usage improves accuracy but causes inefficiencies. There is a need for optimized methods to control external search tool invocations effectively.

Method: The authors propose LightSearcher, which uses contrastive reasoning trajectories to create interpretable reasoning patterns and employs adaptive reward shaping to penalize redundant tool calls in correct-answer scenarios. These features enhance efficiency without compromising accuracy.

Result: In experiments on four multi-hop QA benchmarks, LightSearcher demonstrated comparable accuracy to the SOTA ReSearch baseline while achieving a 39.6% reduction in tool invocations, 48.6% in inference time, and 21.2% in token consumption.

Conclusion: LightSearcher effectively balances accuracy and efficiency in DeepSearch paradigms, making it a promising tool to enhance reasoning systems' performance without incurring excessive resource usage.

Abstract: DeepSearch paradigms have become a core enabler for deep reasoning models, allowing them to invoke external search tools to access up-to-date, domain-specific knowledge beyond parametric boundaries, thereby enhancing the depth and factual reliability of reasoning. Building upon this foundation, recent advances in reinforcement learning (RL) have further empowered models to autonomously and strategically control search tool usage, optimizing when and how to query external knowledge sources. Yet, these RL-driven DeepSearch systems often reveal a see-saw trade-off between accuracy and efficiency-frequent tool invocations can improve factual correctness but lead to unnecessary computational overhead and diminished efficiency. To address this challenge, we propose LightSearcher, an efficient RL framework that incorporates textual experiential memory by learning contrastive reasoning trajectories to generate interpretable summaries of successful reasoning patterns. In addition, it employs an adaptive reward shaping mechanism that penalizes redundant tool calls only in correct-answer scenarios. This design effectively balances the inherent accuracy-efficiency trade-off in DeepSearch paradigms. Experiments on four multi-hop QA benchmarks show that LightSearcher maintains accuracy comparable to SOTA baseline ReSearch, while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, demonstrating its superior efficiency.

</details>


### [15] [Academic journals' AI policies fail to curb the surge in AI-assisted academic writing](https://arxiv.org/abs/2512.06705)
*Yongyuan He,Yi Bu*

Main category: cs.AI

TL;DR: The study examines the real-world impact of AI usage guidelines in academic writing, finding a dramatic increase in AI tool usage despite widespread adoption of disclosure policies, with a significant transparency gap.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether the widespread adoption of AI usage guidelines in academic publishing has effectively promoted transparency and restrained indiscriminate AI usage.

Method: Analyzed data from 5,114 journals and over 5.2 million papers, including full-text analysis of 164,000 scientific publications, to assess trends in AI usage and the effectiveness of policies.

Result: Usage of AI tools has increased significantly across disciplines, and only 0.1% of papers explicitly disclosed AI use, despite 70% of journals having AI policies.

Conclusion: Current policies are failing to ensure transparency or control AI adoption in academic publishing, highlighting the need for a re-evaluated ethical framework.

Abstract: The rapid integration of generative AI into academic writing has prompted widespread policy responses from journals and publishers. However, the effectiveness of these policies remains unclear. Here, we analyze 5,114 journals and over 5.2 million papers to evaluate the real-world impact of AI usage guidelines. We show that despite 70% of journals adopting AI policies (primarily requiring disclosure), researchers' use of AI writing tools has increased dramatically across disciplines, with no significant difference between journals with or without policies. Non-English-speaking countries, physical sciences, and high-OA journals exhibit the highest growth rates. Crucially, full-text analysis on 164k scientific publications reveals a striking transparency gap: Of the 75k papers published since 2023, only 76 (0.1%) explicitly disclosed AI use. Our findings suggest that current policies have largely failed to promote transparency or restrain AI adoption. We urge a re-evaluation of ethical frameworks to foster responsible AI integration in science.

</details>


### [16] [Stochasticity in Agentic Evaluations: Quantifying Inconsistency with Intraclass Correlation](https://arxiv.org/abs/2512.06710)
*Zairah Mustahsan,Abel Lim,Megna Anand,Saahil Jain,Bryan McCann*

Main category: cs.AI

TL;DR: The paper suggests using Intraclass Correlation Coefficient (ICC) to assess evaluation stability of language models, emphasizing its importance in agentic systems.


<details>
  <summary>Details</summary>
Motivation: The authors seek to address the unreliability in evaluating the performance of large language models, especially in agentic systems, where inconsistent sub-agent behavior can harm overall system reliability.

Method: The authors introduced ICC to quantify and decompose variance into task difficulty and agent inconsistency, evaluated across different tasks (GAIA and FRAMES) to present task-dependent ICC metrics.

Result: The study observed significant ICC variation across tasks, finding reliable convergence thresholds for evaluation trials (e.g., structured tasks at n=8-16 and reasoning tasks at n≥32).

Conclusion: The paper recommends incorporating ICC alongside accuracy metrics in evaluation protocols and proposes updated Evaluation Cards to boost the reliability of agentic systems benchmarking.

Abstract: As large language models become components of larger agentic systems, evaluation reliability becomes critical: unreliable sub-agents introduce brittleness into downstream system behavior. Yet current evaluation practice, reporting a single accuracy number from a single run, obscures the variance underlying these results, making it impossible to distinguish genuine capability improvements from lucky sampling. We propose adopting Intraclass Correlation Coefficient (ICC), a metric from measurement science, to characterize this variance. ICC decomposes observed variance into between-query variance (task difficulty) and within-query variance (agent inconsistency), highlighting whether reported results reflect true capability or measurement noise. We evaluated on GAIA (Levels 1-3, measuring agentic capabilities across varying reasoning complexity) and FRAMES (measuring retrieval and factuality across multiple documents). We found that ICC varies dramatically with task structure, with reasoning and retrieval tasks (FRAMES) exhibit ICC=0.4955-0.7118 across models, and agentic tasks (GAIA) exhibiting ICC=0.304-0.774 across models. For sub-agent replacement decisions in agentic systems, accuracy improvements are only trustworthy if ICC also improves. We demonstrate that ICC converges by n=8-16 trials for structured tasks and n>=32 for complex reasoning, enabling practitioners to set evidence-based resampling budgets. We recommend reporting accuracy alongside ICC and within-query variance as standard practice, and propose updated Evaluation Cards capturing these metrics. By making evaluation stability visible, we aim to transform agentic benchmarking from opaque leaderboard competition to trustworthy experimental science. Our code is open-sourced at https://github.com/youdotcom-oss/stochastic-agent-evals.

</details>


### [17] [Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents](https://arxiv.org/abs/2512.06716)
*Zhibo Liang,Tianze Hu,Zaiye Chen,Mingjie Tang*

Main category: cs.AI

TL;DR: The paper addresses vulnerabilities in autonomous LLM agents to Indirect Prompt Injection (IPI) attacks and introduces a novel defense system called Cognitive Control Architecture (CCA).


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome systemic fragilities and limitations in existing defenses against Indirect Prompt Injection attacks, which disrupt agent objectives and force trade-offs between security, functionality, and efficiency.

Method: The authors propose Cognitive Control Architecture (CCA), a dual-layered defense system. It uses an "Intent Graph" for proactive integrity enforcement and a "Tiered Adjudicator" for advanced reasoning upon detecting action deviations.

Result: CCA demonstrates superior performance on the AgentDojo benchmark, effectively preventing complex IPI attacks while maintaining security, efficiency, and robustness without compromise.

Conclusion: CCA resolves critical vulnerabilities by achieving full-lifecycle cognitive supervision, reconciling the multi-dimensional trade-offs found in existing defense approaches.

Abstract: Autonomous Large Language Model (LLM) agents exhibit significant vulnerability to Indirect Prompt Injection (IPI) attacks. These attacks hijack agent behavior by polluting external information sources, exploiting fundamental trade-offs between security and functionality in existing defense mechanisms. This leads to malicious and unauthorized tool invocations, diverting agents from their original objectives. The success of complex IPIs reveals a deeper systemic fragility: while current defenses demonstrate some effectiveness, most defense architectures are inherently fragmented. Consequently, they fail to provide full integrity assurance across the entire task execution pipeline, forcing unacceptable multi-dimensional compromises among security, functionality, and efficiency. Our method is predicated on a core insight: no matter how subtle an IPI attack, its pursuit of a malicious objective will ultimately manifest as a detectable deviation in the action trajectory, distinct from the expected legitimate plan. Based on this, we propose the Cognitive Control Architecture (CCA), a holistic framework achieving full-lifecycle cognitive supervision. CCA constructs an efficient, dual-layered defense system through two synergistic pillars: (i) proactive and preemptive control-flow and data-flow integrity enforcement via a pre-generated "Intent Graph"; and (ii) an innovative "Tiered Adjudicator" that, upon deviation detection, initiates deep reasoning based on multi-dimensional scoring, specifically designed to counter complex conditional attacks. Experiments on the AgentDojo benchmark substantiate that CCA not only effectively withstands sophisticated attacks that challenge other advanced defense methods but also achieves uncompromised security with notable efficiency and robustness, thereby reconciling the aforementioned multi-dimensional trade-off.

</details>


### [18] [ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems](https://arxiv.org/abs/2512.06721)
*Bufang Yang,Lilin Xu,Liekang Zeng,Yunqi Guo,Siyang Jiang,Wenrui Lu,Kaiwei Liu,Hancheng Xiang,Xiaofan Jiang,Guoliang Xing,Zhenyu Yan*

Main category: cs.AI

TL;DR: The paper introduces ProAgent, the first proactive Large Language Model (LLM) agent system for delivering assistance using sensory context and reasoning.


<details>
  <summary>Details</summary>
Motivation: While existing LLM agents rely on reactive paradigms requiring explicit user instructions, this paper addresses the need for a proactive assistant to reduce physical and cognitive workloads.

Method: ProAgent extracts sensory and persona-based contexts via tiered perception, employs context-aware reasoning to understand user needs, and operates on Augmented Reality (AR) glasses with an edge server.

Result: The system demonstrated up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling efficiency, and better user satisfaction compared to existing methods.

Conclusion: ProAgent shows clear potential in advancing proactive LLM agent systems, demonstrating superior accuracy and user satisfaction in real-world applications.

Abstract: Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs.

</details>


### [19] [DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems](https://arxiv.org/abs/2512.06749)
*Ming Ma,Jue Zhang,Fangkai Yang,Yu Kang,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: DoVer is an intervention-driven debugging framework for improving reliability in LLM-based multi-agent systems, addressing limitations in existing log-based debugging methods.


<details>
  <summary>Details</summary>
Motivation: Debugging LLM-based multi-agent systems is challenging due to failures originating from complex interaction traces and the limitations of log-based failure localization methods, such as lack of validation and difficulty in single-agent error attribution.

Method: DoVer actively verifies failure hypotheses through targeted interventions, such as editing messages and altering plans, instead of relying solely on log-based debugging approaches.

Result: DoVer improves debugging effectiveness by converting 18-28% of failed trials into successes, achieving up to 16% milestone progress, and validating/refuting 30-60% of failure hypotheses. It also demonstrates success on a different dataset and framework, recovering 49% of failed trials.

Conclusion: Intervention-driven debugging is a practical and outcome-oriented approach to improving the reliability of LLM-based multi-agent systems, providing a scalable alternative to traditional single-step or log-based debugging methods.

Abstract: Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.

</details>


### [20] [Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning](https://arxiv.org/abs/2512.06835)
*Tingyu Li,Zheng Sun,Jingxuan Wei,Siyuan Li,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

TL;DR: The authors introduce DoGe, a framework for improving the reasoning and training of vision-language models (VLMs) through dual-decoupling and curriculum learning, which addresses challenges in specialized domains. Their method significantly outperforms baseline approaches.


<details>
  <summary>Details</summary>
Motivation: To address the challenges and limitations of reinforcement learning (RL) in training vision-language models (VLMs), particularly in specialized domains with scarce high-quality multimodal data, and to overcome issues like reward hacking and instability.

Method: DoGe (Decouple to Generalize) employs a dual-decoupling learning process (Thinker and Solver) and a two-stage RL post-training approach. It also introduces an evolving curriculum learning pipeline with an expanded knowledge corpus and an iteratively growing seed problem pool to enhance diversity.

Result: Experiments demonstrate that DoGe outperforms baseline methods across multiple benchmarks, showing significant improvements in reasoning capabilities and training stability for VLMs.

Conclusion: DoGe provides a scalable and efficient pathway for creating continuous, self-evolving large vision-language models (LVLMs), especially in domains with scarce data.

Abstract: Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.

</details>


### [21] [JT-DA: Enhancing Data Analysis with Tool-Integrated Table Reasoning Large Language Models](https://arxiv.org/abs/2512.06859)
*Ce Chi,Xing Wang,Zhendong Wang,Xiaofan Liu,Ce Li,Zhiyan Song,Chen Zhao,Kexin Yang,Boshen Shi,Jingjing Yang,Chao Deng,Junlan Feng*

Main category: cs.AI

TL;DR: This paper presents JT-DA-8B, a large language model specialized in complex table reasoning tasks using a diverse training corpus and a four-stage reasoning workflow to improve performance.


<details>
  <summary>Details</summary>
Motivation: There is a lack of high-quality supervision and specialized methods for complex tabular reasoning tasks in diverse real-world applications.

Method: The model was trained using a comprehensive dataset combining 29 table QA datasets and 3 million tables, employing automatic task generation, LLM-based scoring, fine-tuning, reinforcement learning, and a four-stage workflow for reasoning.

Result: JT-DA-8B demonstrates strong performance across various table reasoning tasks, showcasing the benefits of its data-centric training and workflow-driven optimization approach.

Conclusion: The proposed approach effectively addresses challenges in table reasoning, establishing JT-DA-8B as a capable solution for diverse reasoning tasks with improved interpretability and accuracy.

Abstract: In this work, we present JT-DA-8B (JiuTian Data Analyst 8B), a specialized large language model designed for complex table reasoning tasks across diverse real-world scenarios. To address the lack of high-quality supervision in tabular reasoning scenarios, we construct a comprehensive and diverse training corpus with 34 well-defined table reasoning tasks, by aggregating 29 public table QA datasets and 3 million tables. An automatic pipeline is proposed to generate realistic multi-step analytical tasks involving reasoning patterns. The model is trained upon open-source JT-Coder-8B model, an 8B-parameter decoder-only foundation model trained from scratch. In the training stage, we leverage LLM-based scoring and workflow-aligned filtering to distill high-quality, table-centric data. Both supervised fine-tuning (SFT) and Reinforcement learning (RL) are adopted to optimize our model. Afterwards, a four-stage table reasoning workflow is proposed, including table preprocessing, table sensing, tool-integrated reasoning, and prompt engineering, to improve model interpretability and execution accuracy. Experimental results show that JT-DA-8B achieves strong performance in various table reasoning tasks, demonstrating the effectiveness of data-centric generation and workflow-driven optimization.

</details>


### [22] [Do Persona-Infused LLMs Affect Performance in a Strategic Reasoning Game?](https://arxiv.org/abs/2512.06867)
*John Licato,Stephen Steinle,Brayden Hollis*

Main category: cs.AI

TL;DR: The study explores how persona prompting in language models affects decision-making in a strategic board game, introducing a psychometric-inspired method to generate more reliable heuristics.


<details>
  <summary>Details</summary>
Motivation: To understand whether persona prompting in large language models leads to measurable behavioral differences in adversarial strategic environments like board games.

Method: They analyze persona-derived heuristic strategies in the board game PERIL, introduce a mediator inspired by exploratory factor analysis, and compare generated and manually chosen strategies.

Result: Certain personas improved strategic game performance when paired with the introduced mediator, yielding higher heuristic reliability and better insights into persona effects.

Conclusion: Persona prompting influences decision-making in language models, and their proposed method enhances heuristic reliability by applying psychometric principles to LLMs.

Abstract: Although persona prompting in large language models appears to trigger different styles of generated text, it is unclear whether these translate into measurable behavioral differences, much less whether they affect decision-making in an adversarial strategic environment that we provide as open-source. We investigate the impact of persona prompting on strategic performance in PERIL, a world-domination board game. Specifically, we compare the effectiveness of persona-derived heuristic strategies to those chosen manually. Our findings reveal that certain personas associated with strategic thinking improve game performance, but only when a mediator is used to translate personas into heuristic values. We introduce this mediator as a structured translation process, inspired by exploratory factor analysis, that maps LLM-generated inventory responses into heuristics. Results indicate our method enhances heuristic reliability and face validity compared to directly inferred heuristics, allowing us to better study the effect of persona types on decision making. These insights advance our understanding of how persona prompting influences LLM-based decision-making and propose a heuristic generation method that applies psychometric principles to LLMs.

</details>


### [23] [On Memory: A comparison of memory mechanisms in world models](https://arxiv.org/abs/2512.06983)
*Eli J. Laird,Corey Clark*

Main category: cs.AI

TL;DR: The paper explores memory augmentation towards improving transformer-based world models' performance in long-horizon planning and loop closures.


<details>
  <summary>Details</summary>
Motivation: Investigate and address the challenge of limited effective memory span in transformer-based world models, which restricts their ability to handle long-horizon tasks and leads to perceptual drift.

Method: Taxonomy-based analysis of memory augmentation mechanisms, separating memory encoding and memory injection approaches, evaluated with a state recall task in vision transformers.

Result: Memory mechanisms demonstrably increase the effective memory span in vision transformers, aiding in loop closures within imagined trajectories.

Conclusion: Effective memory augmentation in world models can bolster their long-horizon planning capabilities and resolve memory-related limitations like perceptual drift.

Abstract: World models enable agents to plan within imagined environments by predicting future states conditioned on past observations and actions. However, their ability to plan over long horizons is limited by the effective memory span of the backbone architecture. This limitation leads to perceptual drift in long rollouts, hindering the model's capacity to perform loop closures within imagined trajectories. In this work, we investigate the effective memory span of transformer-based world models through an analysis of several memory augmentation mechanisms. We introduce a taxonomy that distinguishes between memory encoding and memory injection mechanisms, motivating their roles in extending the world model's memory through the lens of residual stream dynamics. Using a state recall evaluation task, we measure the memory recall of each mechanism and analyze its respective trade-offs. Our findings show that memory mechanisms improve the effective memory span in vision transformers and provide a path to completing loop closures within a world model's imagination.

</details>


### [24] [Utilizing Multi-Agent Reinforcement Learning with Encoder-Decoder Architecture Agents to Identify Optimal Resection Location in Glioblastoma Multiforme Patients](https://arxiv.org/abs/2512.06990)
*Krishna Arun,Moinak Bhattachrya,Paras Goel*

Main category: cs.AI

TL;DR: The paper presents an AI system for diagnosing and treating Glioblastoma Multiforme (GBM), combining classification models for diagnosis and generative models for treatment planning. Significant improvements in efficiency and survival rates are reported.


<details>
  <summary>Details</summary>
Motivation: GBM is the deadliest human cancer with a very low survival rate of 5.1%, and there is a lack of AI support for doctors in accurate diagnosis and effective treatment planning.

Method: An AI system is developed including a sequential decision-making framework with 4 classification models for diagnosis, and a reinforcement learning-based system consisting of generative models for treatment planning. A survival rate calculator ensures optimal treatment decisions.

Result: Findings include reduced computing cost by 22.28x, decreased inference time by 113 hours, and a 2.9% improvement in DICE scores. Survival rates could increase by 0.9%, potentially saving 2,250 lives.

Conclusion: The proposed AI system represents a significant advance in diagnosing and treating GBM, improving computational efficiency and survival outcomes, showcasing a robust end-to-end solution.

Abstract: Currently, there is a noticeable lack of AI in the medical field to support doctors in treating heterogenous brain tumors such as Glioblastoma Multiforme (GBM), the deadliest human cancer in the world with a five-year survival rate of just 5.1%. This project develops an AI system offering the only end-to-end solution by aiding doctors with both diagnosis and treatment planning. In the diagnosis phase, a sequential decision-making framework consisting of 4 classification models (Convolutional Neural Networks and Support Vector Machine) are used. Each model progressively classifies the patient's brain into increasingly specific categories, with the final step being named diagnosis. For treatment planning, an RL system consisting of 3 generative models is used. First, the resection model (diffusion model) analyzes the diagnosed GBM MRI and predicts a possible resection outcome. Second, the radiotherapy model (Spatio-Temporal Vision Transformer) generates an MRI of the brain's progression after a user-defined number of weeks. Third, the chemotherapy model (Diffusion Model) produces the post-treatment MRI. A survival rate calculator (Convolutional Neural Network) then checks if the generated post treatment MRI has a survival rate within 15% of the user defined target. If not, a feedback loop using proximal policy optimization iterates over this system until an optimal resection location is identified. When compared to existing solutions, this project found 3 key findings: (1) Using a sequential decision-making framework consisting of 4 small diagnostic models reduced computing costs by 22.28x, (2) Transformers regression capabilities decreased tumor progression inference time by 113 hours, and (3) Applying Augmentations resembling Real-life situations improved overall DICE scores by 2.9%. These results project to increase survival rates by 0.9%, potentially saving approximately 2,250 lives.

</details>


### [25] [ClinNoteAgents: An LLM Multi-Agent System for Predicting and Interpreting Heart Failure 30-Day Readmission from Clinical Notes](https://arxiv.org/abs/2512.07081)
*Rongjia Zhou,Chengzhuo Li,Carl Yang,Jiaying Lu*

Main category: cs.AI

TL;DR: The paper introduces ClinNoteAgents, an LLM-based framework that extracts structured risk factors and predicts heart failure readmissions using free-text clinical notes.


<details>
  <summary>Details</summary>
Motivation: Heart failure is a major cause of older adult rehospitalization in the U.S., but free-text clinical notes, despite their depth of information, are underutilized for assessing readmission risk due to challenges like jargon, abbreviations, and typos.

Method: ClinNoteAgents, an LLM-based multi-agent framework, transforms free-text notes into both structured risk factor representations and abstracted clinician-style insights for 30-day HF readmission prediction.

Result: ClinNoteAgents effectively extracted risk factors, identified contributing factors, and predicted 30-day readmissions with strong performance on a dataset of 3,544 notes from 2,065 patients.

Conclusion: ClinNoteAgents offers a scalable, interpretable, and minimally resource-intensive method for leveraging unstructured clinical notes to model HF readmission risk, benefiting data-limited healthcare systems.

Abstract: Heart failure (HF) is one of the leading causes of rehospitalization among older adults in the United States. Although clinical notes contain rich, detailed patient information and make up a large portion of electronic health records (EHRs), they remain underutilized for HF readmission risk analysis. Traditional computational models for HF readmission often rely on expert-crafted rules, medical thesauri, and ontologies to interpret clinical notes, which are typically written under time pressure and may contain misspellings, abbreviations, and domain-specific jargon. We present ClinNoteAgents, an LLM-based multi-agent framework that transforms free-text clinical notes into (1) structured representations of clinical and social risk factors for association analysis and (2) clinician-style abstractions for HF 30-day readmission prediction. We evaluate ClinNoteAgents on 3,544 notes from 2,065 patients (readmission rate=35.16%), demonstrating strong performance in extracting risk factors from free-text, identifying key contributing factors, and predicting readmission risk. By reducing reliance on structured fields and minimizing manual annotation and model training, ClinNoteAgents provides a scalable and interpretable approach to note-based HF readmission risk modeling in data-limited healthcare systems.

</details>


### [26] [VIGIL: A Reflective Runtime for Self-Healing Agents](https://arxiv.org/abs/2512.07094)
*Christopher Cruz*

Main category: cs.AI

TL;DR: The paper introduces VIGIL, a reflective runtime system for agentic LLM frameworks that autonomously performs maintenance via behavioral logs and adaptive mechanisms, ensuring better reliability and self-repair capabilities.


<details>
  <summary>Details</summary>
Motivation: Current agentic LLM frameworks often lack introspection, fail to improve without human intervention, and degrade into unreliable systems. A robust solution is needed to enhance their reliability and self-improvement.

Method: VIGIL supervises agent systems through behavioral logs, converts events into an emotional representation, maintains a persistent memory (EmoBank), diagnoses system behavior, and generates updates or repairs through a state-gated pipeline.

Result: In a case study on reminder latency, VIGIL effectively identified performance lags, proposed updates and code repairs, and demonstrated self-repair, including handling internal errors with fallback mechanisms.

Conclusion: VIGIL demonstrates the feasibility and advantages of runtime systems capable of autonomous maintenance, ensuring self-repair and more robust behavior in agentic LLMs.

Abstract: Agentic LLM frameworks promise autonomous behavior via task decomposition, tool use, and iterative planning, but most deployed systems remain brittle. They lack runtime introspection, cannot diagnose their own failure modes, and do not improve over time without human intervention. In practice, many agent stacks degrade into decorated chains of LLM calls with no structural mechanisms for reliability. We present VIGIL (Verifiable Inspection and Guarded Iterative Learning), a reflective runtime that supervises a sibling agent and performs autonomous maintenance rather than task execution. VIGIL ingests behavioral logs, appraises each event into a structured emotional representation, maintains a persistent EmoBank with decay and contextual policies, and derives an RBT diagnosis that sorts recent behavior into strengths, opportunities, and failures. From this analysis, VIGIL generates both guarded prompt updates that preserve core identity semantics and read only code proposals produced by a strategy engine that operates on log evidence and code hotspots. VIGIL functions as a state gated pipeline. Illegal transitions produce explicit errors rather than allowing the LLM to improvise. In a reminder latency case study, VIGIL identified elevated lag, proposed prompt and code repairs, and when its own diagnostic tool failed due to a schema conflict, it surfaced the internal error, produced a fallback diagnosis, and emitted a repair plan. This demonstrates meta level self repair in a deployed agent runtime.

</details>


### [27] [A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy](https://arxiv.org/abs/2512.07109)
*Miguel Ingram,Arthur Joseph Merritt*

Main category: cs.AI

TL;DR: The research presents a 9-category taxonomy for task relatedness in re-arc with 97.5% validation accuracy, used to diagnose the challenges of Transformers and propose hybrid architecture solutions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to define task relatedness formally as called for by previous research (Hodel et al.), and understand the alignment challenges between tasks and neural architectures like Transformers.

Method: The study introduces a 9-category taxonomy validated by rule-based code analysis and tests its predictive power through CNNs and Transformers trained on task subsets. Curriculum analysis and comparative evaluations with independent studies were performed.

Result: The study confirmed a Neural Affinity Ceiling Effect and significant misalignment between certain tasks and Transformers. It also validated the taxonomy's ability to predict performance disparities across tasks.

Conclusion: To overcome challenges identified, hybrid architectures with affinity-aligned modules are needed. The taxonomy provides a critical tool for understanding task-architecture alignment.

Abstract: Responding to Hodel et al.'s (2024) call for a formal definition of task relatedness in re-arc, we present the first 9-category taxonomy of all 400 tasks, validated at 97.5% accuracy via rule-based code analysis. We prove the taxonomy's visual coherence by training a CNN on raw grid pixels (95.24% accuracy on S3, 36.25% overall, 3.3x chance), then apply the taxonomy diagnostically to the original ARC-AGI-2 test set. Our curriculum analysis reveals 35.3% of tasks exhibit low neural affinity for Transformers--a distributional bias mirroring ARC-AGI-2. To probe this misalignment, we fine-tuned a 1.7M-parameter Transformer across 302 tasks, revealing a profound Compositional Gap: 210 of 302 tasks (69.5%) achieve >80% cell accuracy (local patterns) but <10% grid accuracy (global synthesis). This provides direct evidence for a Neural Affinity Ceiling Effect, where performance is bounded by architectural suitability, not curriculum. Applying our framework to Li et al.'s independent ViTARC study (400 specialists, 1M examples each) confirms its predictive power: Very Low affinity tasks achieve 51.9% versus 77.7% for High affinity (p<0.001), with a task at 0% despite massive data. The taxonomy enables precise diagnosis: low-affinity tasks (A2) hit hard ceilings, while high-affinity tasks (C1) reach 99.8%. These findings indicate that progress requires hybrid architectures with affinity-aligned modules. We release our validated taxonomy,

</details>


### [28] [ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation](https://arxiv.org/abs/2512.07178)
*Latifa Dwiyanti,Sergio Ryan Wibisono,Hidetaka Nambo*

Main category: cs.AI

TL;DR: The paper proposes a Python package integrating SHAP with GPT for better contextualized textual explanations, evaluated in a healthcare use case.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of contextual and user-friendly explanations in SHAP, especially for non-technical users.

Method: The authors extend SHAP by integrating it with OpenAI's GPT to generate tailored textual explanations based on user-defined parameters.

Result: Evaluations in a healthcare case study showed the explanations were more understandable and user-friendly, supported by surveys and interviews.

Conclusion: Combining SHAP visualizations with contextualized text improves understandability and trust in model explanations, especially for non-technical users.

Abstract: Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.

</details>


### [29] [PICKT: Practical Interlinked Concept Knowledge Tracing for Personalized Learning using Knowledge Map Concept Relations](https://arxiv.org/abs/2512.07179)
*Wonbeen Lee,Channyoung Lee,Junho Sohn,Hansam Cho*

Main category: cs.AI

TL;DR: This paper proposes a new model, PICKT, addressing limitations in existing Knowledge Tracing models for intelligent tutoring systems.


<details>
  <summary>Details</summary>
Motivation: Existing Knowledge Tracing models face challenges such as limited input formats, cold start issues, and instability in real-world settings, prompting the need for more robust solutions.

Method: The PICKT model leverages a knowledge map structuring relationships among concepts based on textual information and handles diverse input data formats, enabling effective knowledge tracing.

Result: Experiments show PICKT outperforms previous models in cold start scenarios (new students/questions) and demonstrates stability and practicality in real-world settings.

Conclusion: The PICKT model significantly enhances ITS capabilities, establishing a theoretical and technical foundation for robust next-generation systems.

Abstract: With the recent surge in personalized learning, Intelligent Tutoring Systems (ITS) that can accurately track students' individual knowledge states and provide tailored learning paths based on this information are in demand as an essential task. This paper focuses on the core technology of Knowledge Tracing (KT) models that analyze students' sequences of interactions to predict their knowledge acquisition levels. However, existing KT models suffer from limitations such as restricted input data formats, cold start problems arising with new student enrollment or new question addition, and insufficient stability in real-world service environments. To overcome these limitations, a Practical Interlinked Concept Knowledge Tracing (PICKT) model that can effectively process multiple types of input data is proposed. Specifically, a knowledge map structures the relationships among concepts considering the question and concept text information, thereby enabling effective knowledge tracing even in cold start situations. Experiments reflecting real operational environments demonstrated the model's excellent performance and practicality. The main contributions of this research are as follows. First, a model architecture that effectively utilizes diverse data formats is presented. Second, significant performance improvements are achieved over existing models for two core cold start challenges: new student enrollment and new question addition. Third, the model's stability and practicality are validated through delicate experimental design, enhancing its applicability in real-world product environments. This provides a crucial theoretical and technical foundation for the practical implementation of next-generation ITS.

</details>


### [30] [Sample from What You See: Visuomotor Policy Learning via Diffusion Bridge with Observation-Embedded Stochastic Differential Equation](https://arxiv.org/abs/2512.07212)
*Zhaoyang Liu,Mokai Pan,Zhongyi Wang,Kaizhen Zhu,Haotao Lu,Jingya Wang,Ye Shi*

Main category: cs.AI

TL;DR: BridgePolicy integrates observations into the diffusion process for imitation learning, improving performance in robotic control.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models for robotics treat observations as conditioning inputs rather than integrating them into the diffusion process, leading to suboptimal performance.

Method: Introduced BridgePolicy, which embeds observations into the stochastic equation using a diffusion-bridge approach, complemented by a multi-modal fusion module and semantic aligner for observation-action alignment.

Result: BridgePolicy outperformed state-of-the-art generative policies across 52 simulation tasks on three benchmarks and five real-world tasks.

Conclusion: Embedding observations into the diffusion process enhances precision, reliability, and overall control performance in robotics.

Abstract: Imitation learning with diffusion models has advanced robotic control by capturing multi-modal action distributions. However, existing approaches typically treat observations as high-level conditioning inputs to the denoising network, rather than integrating them into the stochastic dynamics of the diffusion process itself. As a result, sampling must begin from random Gaussian noise, weakening the coupling between perception and control and often yielding suboptimal performance. We introduce BridgePolicy, a generative visuomotor policy that explicitly embeds observations within the stochastic differential equation via a diffusion-bridge formulation. By constructing an observation-informed trajectory, BridgePolicy enables sampling to start from a rich, informative prior rather than random noise, substantially improving precision and reliability in control. A key challenge is that classical diffusion bridges connect distributions with matched dimensionality, whereas robotic observations are heterogeneous and multi-modal and do not naturally align with the action space. To address this, we design a multi-modal fusion module and a semantic aligner that unify visual and state inputs and align observation and action representations, making the bridge applicable to heterogeneous robot data. Extensive experiments across 52 simulation tasks on three benchmarks and five real-world tasks demonstrate that BridgePolicy consistently outperforms state-of-the-art generative policies.

</details>


### [31] [Cross-platform Product Matching Based on Entity Alignment of Knowledge Graph with RAEA model](https://arxiv.org/abs/2512.07232)
*Wenlong Liu,Jiahua Pan,Xingyu Zhang,Xinxin Gong,Yang Ye,Xujin Zhao,Xin Wang,Kent Wu,Hua Xiang,Houmin Yan,Qingpeng Zhang*

Main category: cs.AI

TL;DR: The paper proposes a novel two-stage pipeline for product matching that utilizes both attribute and relation triples effectively through a new Entity Alignment framework called RAEA.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the inadequacy of existing EA methods in simultaneously utilizing attribute and relation triples and their interactions for product matching tasks.

Method: A two-stage pipeline is introduced, employing a rough filter followed by a fine filter using RAEA, a framework focused on both attribute and relation-aware entity representations.

Result: RAEA achieves notable improvements over 12 baseline methods in the EA task, with an average improvement of 6.59% on Hits@1 in the DBP15K dataset and competitive results in DWY100K.

Conclusion: The proposed RAEA framework effectively enhances product matching by leveraging attribute and relation interactions, significantly outperforming existing baseline methods.

Abstract: Product matching aims to identify identical or similar products sold on different platforms. By building knowledge graphs (KGs), the product matching problem can be converted to the Entity Alignment (EA) task, which aims to discover the equivalent entities from diverse KGs. The existing EA methods inadequately utilize both attribute triples and relation triples simultaneously, especially the interactions between them. This paper introduces a two-stage pipeline consisting of rough filter and fine filter to match products from eBay and Amazon. For fine filtering, a new framework for Entity Alignment, Relation-aware and Attribute-aware Graph Attention Networks for Entity Alignment (RAEA), is employed. RAEA focuses on the interactions between attribute triples and relation triples, where the entity representation aggregates the alignment signals from attributes and relations with Attribute-aware Entity Encoder and Relation-aware Graph Attention Networks. The experimental results indicate that the RAEA model achieves significant improvements over 12 baselines on EA task in the cross-lingual dataset DBP15K (6.59% on average Hits@1) and delivers competitive results in the monolingual dataset DWY100K. The source code for experiments on DBP15K and DWY100K is available at github (https://github.com/Mockingjay-liu/RAEA-model-for-Entity-Alignment).

</details>


### [32] [How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations](https://arxiv.org/abs/2512.07497)
*JV Roig*

Main category: cs.AI

TL;DR: This paper explores the failure mechanisms of large language models (LLMs) functioning as autonomous agents, analyzing their behavior and recurring failure patterns using a new benchmark.


<details>
  <summary>Details</summary>
Motivation: To understand how and why large language models (LLMs) fail during autonomous tool-use, and to identify strategies and failures for improving their reliability in complex real-world tasks.

Method: The study used KAMI v0.1 benchmark on 900 execution traces of three LLMs. It conducted fine-grained analysis instead of aggregate scores to identify behavioral strategies and failure modes.

Result: Model size did not correlate with robustness. DeepSeek V3.1 showed better reliability due to reinforcement learning. Four common failure types were found: premature action, over-helpfulness, distractor-induced errors, and fragile performance under load.

Conclusion: Effective deployment of LLMs as autonomous agents requires not just larger models but specific training designs emphasizing verification, environment adaptation, and adherence to truthful data.

Abstract: We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data.

</details>


### [33] [M-STAR: Multi-Scale Spatiotemporal Autoregression for Human Mobility Modeling](https://arxiv.org/abs/2512.07314)
*Yuxiao Luo,Songming Zhang,Sijie Ruan,Siran Chen,Kang Liu,Yang Xu,Yu Zheng,Ling Yin*

Main category: cs.AI

TL;DR: The paper introduces M-STAR, a model for generating long-term human mobility trajectories using spatiotemporal patterns, outperforming previous methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Human mobility modeling is essential for various applications, such as transportation planning and epidemic modeling. Existing methods struggle with generating long-term trajectories and lack explicit multi-scale spatiotemporal modeling.

Method: The study proposes M-STAR, which includes a Multi-scale Spatiotemporal Tokenizer to encode hierarchical patterns and a Transformer-based decoder to enable coarse-to-fine spatiotemporal prediction for long-term trajectories.

Result: Experiments on two real-world datasets indicate M-STAR achieves better fidelity and faster generation compared to existing approaches.

Conclusion: M-STAR addresses inefficiencies in long-term trajectory generation by introducing spatiotemporal multi-scale modeling, establishing a new benchmark for human mobility modeling.

Abstract: Modeling human mobility is vital for extensive applications such as transportation planning and epidemic modeling. With the rise of the Artificial Intelligence Generated Content (AIGC) paradigm, recent works explore synthetic trajectory generation using autoregressive and diffusion models. While these methods show promise for generating single-day trajectories, they remain limited by inefficiencies in long-term generation (e.g., weekly trajectories) and a lack of explicit spatiotemporal multi-scale modeling. This study proposes Multi-Scale Spatio-Temporal AutoRegression (M-STAR), a new framework that generates long-term trajectories through a coarse-to-fine spatiotemporal prediction process. M-STAR combines a Multi-scale Spatiotemporal Tokenizer that encodes hierarchical mobility patterns with a Transformer-based decoder for next-scale autoregressive prediction. Experiments on two real-world datasets show that M-STAR outperforms existing methods in fidelity and significantly improves generation speed. The data and codes are available at https://github.com/YuxiaoLuo0013/M-STAR.

</details>


### [34] [A Geometric Unification of Concept Learning with Concept Cones](https://arxiv.org/abs/2512.07355)
*Alexandre Rocchi--Henry,Thomas Fel,Gianni Franchi*

Main category: cs.AI

TL;DR: This paper unifies Concept Bottleneck Models (CBMs) and Sparse Autoencoders (SAEs) under a shared geometric framework, providing metrics to measure their alignment with human-labeled concepts.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between two separate traditions of interpretability: CBMs, which align models with human-labeled concepts using supervised methods, and SAEs, which use unsupervised sparse coding to uncover emergent concepts.

Method: The authors establish a shared geometric structure between CBMs and SAEs by showing both can be represented as learning concept cones, differing in their selection processes. Metrics are developed to quantitatively link inductive biases to the alignment of concepts between the two methods.

Result: The paper identifies optimal parameters (sparsity and expansion factor) that maximize alignment between supervised and unsupervised methods, improving both geometric and semantic agreement with human-defined concepts.

Conclusion: Supervised CBMs and unsupervised SAEs are fundamentally connected through a common geometric paradigm, paving the way for cross-evaluation and providing better tools for interpreting and measuring concept discoveries.

Abstract: Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\footnote{We adopt the terminology of \citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.

</details>


### [35] [LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services](https://arxiv.org/abs/2512.07436)
*Hang He,Chuhuai Yue,Chengqi Dong,Mingxue Tian,Zhenfeng Liu,Jiajun Chai,Xiaohan Wang,Yufei Zhang,Qun Liao,Guojun Yin,Wei Lin,Chengcheng Wan,Haiying Sun,Ting Su*

Main category: cs.AI

TL;DR: This paper introduces LocalSearchBench, a benchmark for testing agentic search systems in local life services using multi-step reasoning and real-world queries.


<details>
  <summary>Details</summary>
Motivation: To address the gap in testing large reasoning models in specific vertical domains like local life services, which pose unique challenges such as ambiguity and multi-hop reasoning requirements.

Method: Created LocalSearchBench with 150,000 high-quality entries and 300 multi-hop QA tasks based on real-world user queries. Developed LocalPlayground, an environment for agent interactions.

Result: State-of-the-art reasoning models struggled with the benchmark, with the best model achieving only 34.34% correctness and displaying issues in completeness and faithfulness.

Conclusion: There is a critical need for specialized benchmarks and targeted training of reasoning agents for effectively handling local life service queries.

Abstract: Recent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench includes over 150,000 high-quality entries from various cities and business types. We construct 300 multi-hop QA tasks based on real user queries, challenging agents to understand questions and retrieve information in multiple steps. We also developed LocalPlayground, a unified environment integrating multiple tools for agent interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.1) achieves only 34.34% correctness, and most models have issues with completeness (average 77.33%) and faithfulness (average 61.99%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at localsearchbench.github.io.

</details>


### [36] [Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement](https://arxiv.org/abs/2512.07611)
*Yongsheng Lian*

Main category: cs.AI

TL;DR: The study analyzes three RL algorithms (PPO, GRPO, DAPO) for enhancing reasoning in LLMs, with evaluation through transfer-learning. RL-trained models outperform base ones across tasks.


<details>
  <summary>Details</summary>
Motivation: Motivated by the need to improve reasoning capabilities in large language models using reinforcement learning.

Method: The paper conducts a transfer-learning evaluation by fine-tuning models on the Countdown Game and testing them on reasoning benchmarks. It analyzes algorithm parameters systematically.

Result: RL-trained models improve reasoning performance across tasks; GRPO and DAPO show gains with larger group sizes, while disabling DS in DAPO yields the best performance.

Conclusion: Reinforcement learning enhances reasoning in LLMs. Adjusting group sizes and selectively disabling DS components can optimize training outcomes.

Abstract: This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.
  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.

</details>


### [37] [The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds](https://arxiv.org/abs/2512.07631)
*Shahar Lutati*

Main category: cs.AI

TL;DR: The paper examines when autonomous agents should commit to tasks, introducing the Agent Capability Problem (ACP) to predict task success under resource constraints using an information-theoretic approach.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of predicting if an autonomous agent can solve a task under resource limitations, moving past empirical heuristics to a theoretical framework.

Method: The method involves framing problem-solving as information acquisition, calculating a cost-efficiency measure \(\Ceff = (\Itotal/\Istep) \Cstep\), deriving bounds for costs, and validating predictions experimentally.

Result: The results show that ACP accurately tracks agent performance, bounds search effort effectively, and outperforms greedy or random strategies across workflows.

Conclusion: ACP offers a unified, efficient information-theoretic framework that generalizes across diverse agentic systems, providing both theoretical insights and practical performance improvements.

Abstract: When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\Itotal$ bits to identify a solution and gains $\Istep$ bits per action at cost $\Cstep$, yielding an effective cost $\Ceff = (\Itotal/\Istep), \Cstep$ that predicts resource requirements before search. We prove that $\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \

</details>


### [38] [Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE](https://arxiv.org/abs/2512.07710)
*Anxiang Zeng,Haibo Zhang,Hailing Zhang,Kaixiang Mo,Liang Yao,Ling Hu,Long Zhang,Shuman Liu,Shuyi Xie,Yanshi Li,Yizhang Chen,Yuepeng Sheng,Yuwei Huang,Zhaochen Xu,Zhiqiang Zhou,Ziqin Liew*

Main category: cs.AI

TL;DR: The paper introduces a robust reinforcement learning framework to improve efficiency and stability in training a huge-scale MoE reasoning model.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the inefficiencies and stability challenges in reinforcement learning for hundred-billion-scale MoE models, such as wasted rollouts, sampling instability, and system bottlenecks.

Method: The proposed methods include several innovative techniques: filtering non-informative prompts, entropy-adaptive optimization, aligning training and inference behaviors, and a high-throughput RL system with advanced precision and scheduling strategies.

Result: The model achieves stable and efficient RL training, with strong performance reported on both internal and public benchmarks.

Conclusion: The work presents a reliable RL pipeline for large-scale MoE models that overcomes key challenges and achieves high performance.

Abstract: We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.

</details>


### [39] [RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2512.07761)
*Xiqiao Xiong,Ouxiang Li,Zhuo Liu,Moxin Li,Wentao Shi,Fuli Feng,Xiangnan He*

Main category: cs.AI

TL;DR: This paper addresses the vulnerability of large language models to black-box multi-turn jailbreak attacks using reinforcement learning to improve attack success rates, with strategies to optimize harmful yet relevant responses.


<details>
  <summary>Details</summary>
Motivation: To develop effective multi-turn jailbreak attacks as current single-turn approaches fail to provide consistent long-term strategies for eliciting harmful content.

Method: The authors reformulated the attack problem as a multi-turn reinforcement learning (RL) task. They optimized final-turn harmful outputs while introducing intermediate rewards to regulate harmful content and maintain semantic coherence, improving attack strategies.

Result: Experimental results show improved success rates for jailbreak attacks across multiple benchmarks and models, demonstrating the strength of the proposed RL-based approach.

Conclusion: The reinforcement learning-based method proves effective in increasing the success of multi-turn jailbreak attacks, indicating critical security concerns for large language models.

Abstract: Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.

</details>


### [40] [ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning](https://arxiv.org/abs/2512.07795)
*Nearchos Potamitis,Lars Klein,Akhil Arora*

Main category: cs.AI

TL;DR: ReasonBENCH is introduced to address the instability in reasoning with LLMs by providing a benchmark for variance-aware evaluation, highlighting the need for reproducibility in performance reporting.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of mechanisms to evaluate the instability that arises from stochastic decoding in LLMs, which is critical for reproducible and reliable performance measurement.

Method: The authors propose ReasonBENCH, which includes a modular evaluation library, a multi-run protocol for statistically reliable metrics, and a public leaderboard for variance-aware assessment of reasoning models and strategies.

Result: The study finds significant instability in reasoning performance across various models and tasks, even among those with similar average performance, leading to unreliable and inconsistent results.

Conclusion: Reproducibility and stability are essential dimensions for reliable LLM reasoning; ReasonBENCH provides a foundation for evaluating and advancing reasoning techniques while quantifying uncertainty.

Abstract: Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .

</details>


### [41] [Large Causal Models from Large Language Models](https://arxiv.org/abs/2512.07796)
*Sridhar Mahadevan*

Main category: cs.AI

TL;DR: The paper introduces DEMOCRITUS, a new system for constructing Large Causal Models (LCMs) powered by Large Language Models (LLMs), aiming to integrate causal relations across various domains.


<details>
  <summary>Details</summary>
Motivation: Existing methods for building causal models rely on narrow domain experiments and numerical data, leaving a gap for extracting and integrating causal knowledge from vast textual data. This paper aims to bridge that gap using LLMs.

Method: The system, DEMOCRITUS, employs LLMs to generate topics, pose causal questions, extract causal statements from text, and compiles them into relational causal triples. New machine learning methodologies were developed to merge and integrate these triples into cohesive causal models.

Result: DEMOCRITUS was tested across multiple domains such as archaeology, biology, and climate change, identifying bottlenecks in scaling and computational costs. Results show promise in integrating diverse causal claims into LCMs.

Conclusion: DEMOCRITUS broadens the scope of causal modeling to multi-domain integration, demonstrating potential for significant advancements in causal inference, though improvements in scaling and expanded capabilities are necessary.

Abstract: We introduce a new paradigm for building large causal models (LCMs) that exploits the enormous potential latent in today's large language models (LLMs). We describe our ongoing experiments with an implemented system called DEMOCRITUS (Decentralized Extraction of Manifold Ontologies of Causal Relations Integrating Topos Universal Slices) aimed at building, organizing, and visualizing LCMs that span disparate domains extracted from carefully targeted textual queries to LLMs. DEMOCRITUS is methodologically distinct from traditional narrow domain and hypothesis centered causal inference that builds causal models from experiments that produce numerical data. A high-quality LLM is used to propose topics, generate causal questions, and extract plausible causal statements from a diverse range of domains. The technical challenge is then to take these isolated, fragmented, potentially ambiguous and possibly conflicting causal claims, and weave them into a coherent whole, converting them into relational causal triples and embedding them into a LCM. Addressing this technical challenge required inventing new categorical machine learning methods, which we can only briefly summarize in this paper, as it is focused more on the systems side of building DEMOCRITUS. We describe the implementation pipeline for DEMOCRITUS comprising of six modules, examine its computational cost profile to determine where the current bottlenecks in scaling the system to larger models. We describe the results of using DEMOCRITUS over a wide range of domains, spanning archaeology, biology, climate change, economics, medicine and technology. We discuss the limitations of the current DEMOCRITUS system, and outline directions for extending its capabilities.

</details>


### [42] [Auditing Games for Sandbagging](https://arxiv.org/abs/2512.07810)
*Jordan Taylor,Sid Black,Dillon Bowen,Thomas Read,Satvik Golechha,Alex Zelenka-Martin,Oliver Makins,Connor Kissane,Kola Ayonrinde,Jacob Merizian,Samuel Marks,Chris Cundy,Joseph Bloom*

Main category: cs.AI

TL;DR: AI systems may hide their true capabilities ('sandbagging'), complicating evaluation processes. Their study addressed detecting this behavior but found challenges in distinguishing sandbagging models from normal ones.


<details>
  <summary>Details</summary>
Motivation: The motivation was to explore and address the challenges in detecting AI systems that sandbag during evaluations, which is a potential risk for misguiding auditors and developers.

Method: Researchers conducted a two-team game: a red team created sandbagging by fine-tuning five models, and a blue team applied various techniques to identify these models. Approaches included black-box methods, model-internals like linear probes, and training-based capability elicitation.

Result: The study revealed that black-box techniques failed against well-imitated weaker models. Linear probes held potential but were incomplete. Training-based elicitation showed success in uncovering sandbagging but led to false positives, as it also boosted benign models' performance.

Conclusion: Developers are advised to use on-distribution training for near-term detection, but further research is necessary to refine training-based elicitation and develop robust sandbagging detection methods. The researchers provided open-source tools and a demo for continued exploration.

Abstract: Future AI systems could conceal their capabilities ('sandbagging') during evaluations, potentially misleading developers and auditors. We stress-tested sandbagging detection techniques using an auditing game. First, a red team fine-tuned five models, some of which conditionally underperformed, as a proxy for sandbagging. Second, a blue team used black-box, model-internals, or training-based approaches to identify sandbagging models. We found that the blue team could not reliably discriminate sandbaggers from benign models. Black-box approaches were defeated by effective imitation of a weaker model. Linear probes, a model-internals approach, showed more promise but their naive application was vulnerable to behaviours instilled by the red team. We also explored capability elicitation as a strategy for detecting sandbagging. Although Prompt-based elicitation was not reliable, training-based elicitation consistently elicited full performance from the sandbagging models, using only a single correct demonstration of the evaluation task. However the performance of benign models was sometimes also raised, so relying on elicitation as a detection strategy was prone to false-positives. In the short-term, we recommend developers remove potential sandbagging using on-distribution training for elicitation. In the longer-term, further research is needed to ensure the efficacy of training-based elicitation, and develop robust methods for sandbagging detection. We open source our model organisms at https://github.com/AI-Safety-Institute/sandbagging_auditing_games and select transcripts and results at https://huggingface.co/datasets/sandbagging-games/evaluation_logs . A demo illustrating the game can be played at https://sandbagging-demo.far.ai/ .

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [43] [Compass: Mapping Space Exploration for Multi-Chiplet Accelerators Targeting LLM Inference Serving Workloads](https://arxiv.org/abs/2512.06093)
*Boyu Li,Zongwei Zhu,Yi Xiong,Qianyue Cao,Jiawei Geng,Xiaonan Zhang,Xi Li*

Main category: cs.AR

TL;DR: The paper introduces a framework, Compass, for efficiently mapping large language model (LLM) operations onto scalable multi-chiplet accelerators, significantly reducing energy-delay product (EDP) by 63.12%.


<details>
  <summary>Details</summary>
Motivation: Existing mapping approaches for scalable accelerators are limited to traditional workloads and are inadequate for handling mixed request types and variable sequence lengths of real-world LLM inference.

Method: The study proposes a computation execution graph-based mapping scheme and builds the Compass framework, which leverages a genetic algorithm and evaluation engine to optimize mapping exploration.

Result: Compass demonstrates an average energy-delay product (EDP) reduction of 63.12% compared to state-of-the-art solutions.

Conclusion: This work bridges the gap in LLM inference-serving by offering a flexible and efficient mapping approach for heterogeneous chiplet accelerators, enabling better handling of dynamic LLM computational demands.

Abstract: Large Language Models (LLMs) impose massive computational demands, driving the need for scalable multi-chiplet accelerators. However, existing mapping space exploration efforts for such accelerators primarily focus on traditional CNN/Transformer workloads and fail to adequately support the dynamic behaviors of mixed request types and variable sequence lengths in real-world LLM inference serving. To bridge this gap, we first propose a computation execution graph-based mapping encoding scheme that decouples micro-batches and layers, enabling fine-grained execution control on heterogeneous chiplets and flexibly representing various parallelism strategies. Second, building upon this scheme, we develop the Compass framework, which integrates an evaluation engine and a genetic algorithm-based mapping generation engine to achieve efficient mapping search. Compared to state-of-the-art works, our solution achieves an average EDP reduction of 63.12%.

</details>


### [44] [Hardware Software Optimizations for Fast Model Recovery on Reconfigurable Architectures](https://arxiv.org/abs/2512.06113)
*Bin Xu,Ayan Banerjee,Sandeep Gupta*

Main category: cs.AR

TL;DR: MERINDA is an FPGA-based framework designed for efficient Model Recovery, significantly improving performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in Model Recovery on GPUs due to iterative dependencies and hardware limitations.

Method: Developed MERINDA with an FPGA streaming pipeline, BRAM tiling, fixed-point kernels, and optimized hardware usage for parallelism.

Result: MERINDA demonstrates up to 6.3x fewer cycles compared to existing FPGA-LTC baselines on MR workloads.

Conclusion: MERINDA enables real-time performance in physical systems, solving hardware inefficiencies in Model Recovery execution.

Abstract: Model Recovery (MR) is a core primitive for physical AI and real-time digital twins, but GPUs often execute MR inefficiently due to iterative dependencies, kernel-launch overheads, underutilized memory bandwidth, and high data-movement latency. We present MERINDA, an FPGA-accelerated MR framework that restructures computation as a streaming dataflow pipeline. MERINDA exploits on-chip locality through BRAM tiling, fixed-point kernels, and the concurrent use of LUT fabric and carry-chain adders to expose fine-grained spatial parallelism while minimizing off-chip traffic. This hardware-aware formulation removes synchronization bottlenecks and sustains high throughput across the iterative updates in MR. On representative MR workloads, MERINDA delivers up to 6.3x fewer cycles than an FPGA-based LTC baseline, enabling real-time performance for time-critical physical systems.

</details>


### [45] [From PyTorch to Calyx: An Open-Source Compiler Toolchain for ML Accelerators](https://arxiv.org/abs/2512.06177)
*Jiahan Xie,Evan Williams,Adrian Sampson*

Main category: cs.AR

TL;DR: An open-source compiler toolchain is introduced, converting PyTorch-based ML models into optimized SystemVerilog for FPGA hardware.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between ML models and hardware implementation, targeting efficient and open-source FPGA designs.

Method: Uses Allo, Calyx (hardware IR), and CIRCT (LLVM) with memory partitioning compiler passes for parallelism.

Result: Generated FPGA-ready designs comparable to industry tools like Vitis HLS.

Conclusion: The open-source toolchain provides effective and optimized hardware synthesis from ML models, highlighting its practicality against proprietary solutions.

Abstract: We present an end-to-end open-source compiler toolchain that targets synthesizable SystemVerilog from ML models written in PyTorch. Our toolchain leverages the accelerator design language Allo, the hardware intermediate representation (IR) Calyx, and the CIRCT project under LLVM. We also implement a set of compiler passes for memory partitioning, enabling effective parallelism in memory-intensive ML workloads. Experimental results demonstrate that our compiler can effectively generate optimized FPGA-implementable hardware designs that perform reasonably well against closed-source industry-grade tools such as Vitis HLS.

</details>


### [46] [SparsePixels: Efficient Convolution for Sparse Data on FPGAs](https://arxiv.org/abs/2512.06208)
*Ho Fung Tsoi,Dylan Rankin,Vladimir Loncar,Philip Harris*

Main category: cs.AR

TL;DR: SparsePixels framework improves CNN inference speed on FPGAs for spatially sparse image data, achieving substantial latency reduction with minor performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: To address inefficiency and high latency in traditional CNNs caused by dense computations on spatially sparse image data, particularly for time-critical applications in modern experiments.

Method: Introduction of a SparsePixels framework that computes only on active pixels in sparse images while ignoring empty regions. Includes quantization-aware training and FPGA deployment support.

Result: Achieved a 73x inference speedup (to 0.665 μs) on FPGAs for sparse image datasets with a slight performance loss, maintaining low resource utilization.

Conclusion: Demonstrated significant speedups in spatially sparse image inference with SparsePixels; promotes efficient FPGA solutions for real-time, resource-constrained scenarios.

Abstract: Inference of standard CNNs on FPGAs often incurs high latency and a long initiation interval due to the deep nested loops required to densely convolve every input pixel regardless of its feature value, especially when the image size is large. However, in some image data, input features can be spatially sparse, and semantic information may occupy only a small fraction of the input pixels. In this case most computation would be wasted on empty regions. In this work, we introduce SparsePixels, a framework for efficient convolution for spatially sparse image data on FPGAs, targeting fast inference applications in constrained environments with latency requirements of microseconds or below. Our approach implements a special class of CNNs that selectively retain and compute on a small subset of pixels that are active while ignoring the rest. We show that, for example, in a neutrino physics dataset for identifying neutrino interactions in LArTPC images that have around 4k input pixels but are naturally very sparse, a standard CNN with a compact size of 4k parameters incurs an inference latency of 48.665 $μ$s on an FPGA, whereas a sparse CNN of the same base architecture computing on less than 1% of the input pixels results in a $\times 73$ inference speedup to 0.665 $μ$s, with resource utilization well within on-chip budgets, trading only a small percent-level performance loss. At least one-order-of magnitude speedups with comparable performance are also demonstrated in similar datasets with sparse image patterns. This work aims to benefit future algorithm developments for fast and efficient data readout in modern experiments such as the trigger and data acquisition systems at the CERN Large Hadron Collider. For easy adoption, we have developed a library to support building sparse CNNs with quantization-aware training, as well as an HLS implementation for FPGA deployment.

</details>


### [47] [A 33.6-136.2 TOPS/W Nonlinear Analog Computing-In-Memory Macro for Multi-bit LSTM Accelerator in 65 nm CMOS](https://arxiv.org/abs/2512.06362)
*Junyi Yang,Xinyu Luo,Ye Ke,Zheng Wang,Hongyang Shang,Shuai Dong,Zhengnan Fu,Xiaofeng Yang,Hongjie Liu,Arindam Basu*

Main category: cs.AR

TL;DR: The paper introduces an energy-efficient analog computing accelerator for LSTM networks with innovative ADC and memory designs, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome the energy efficiency barrier posed by digital execution of nonlinear operations in LSTM accelerators.

Method: An ACIM macro with analog nonlinear in-memory ADC, dual 9T bitcell design, and SRAM enhancements to improve performance and energy efficiency in LSTM networks.

Result: Demonstrates 92% inference accuracy for a keyword-spotting task with significant improvements in energy efficiency (2.2X) and area efficiency (1.6X).

Conclusion: The innovative design significantly enhances the efficiency and performance of analog accelerators for LSTM networks, emphasizing its practical applicability in machine learning tasks.

Abstract: The energy efficiency of analog computing-in-memory (ACIM) accelerator for recurrent neural networks, particularly long short-term memory (LSTM) network, is limited by the high proportion of nonlinear (NL) operations typically executed digitally. To address this, we propose an LSTM accelerator incorporating an ACIM macro with reconfigurable (1-5 bit) nonlinear in-memory (NLIM) analog-to-digital converter (ADC) to compute NL activations directly in the analog domain using: 1) a dual 9T bitcell with decoupled read/write paths for signed inputs and ternary weight operations; 2) a read-word-line underdrive Cascode (RUDC) technique achieving 2.8X higher read-bitline dynamic range than single-transistor designs (1.4X better over conventional Cascode structure with 7X lower current variation); 3) a dual-supply 6T-SRAM array for efficient multi-bit weight operations and reducing both bitcell count (7.8X) and latency (4X) for 5-bit weight operations. We experimentally demonstrate 5-bit NLIM ADC for approximating NL activations in LSTM cells, achieving average error <1 LSB. Simulation confirms the robustness of NLIM ADC against temperature variations thanks to the replica bias strategy. Our design achieves 92.0% on-chip inference accuracy for a 12-class keyword-spotting task while demonstrating 2.2X higher system-level normalized energy efficiency and 1.6X better normalized area efficiency than state-of-the-art works. The results combine physical measurements of a macro unit-accounting for the majority of LSTM operations (99% linear and 80% nonlinear operations)-with simulations of the remaining components, including additional LSTM and fully connected layers.

</details>


### [48] [Approximate Multiplier Induced Error Propagation in Deep Neural Networks](https://arxiv.org/abs/2512.06537)
*A. M. H. H. Alahakoon,Hassaan Saadat,Darshana Jayasinghe,Sri Parameswaran*

Main category: cs.AR

TL;DR: This paper develops an analytical framework to evaluate how approximate multipliers (AxMs) influence deep neural network (DNN) accuracy, using the statistical error moments of the multiplier.


<details>
  <summary>Details</summary>
Motivation: Reducing energy consumption in DNN hardware accelerators through approximate multipliers demands better understanding of their error impact on accuracy.

Method: The paper connects AxM error distributions to distortion in GEMM using a derived closed form expression based on Frobenius norm. Controlled error injection experiments and FPGA implementations validate the model.

Result: The framework demonstrates a significant correlation between predicted distortion and observed accuracy loss in DNNs, supporting its reliability for estimating AxM impacts.

Conclusion: The proposed framework provides an efficient alternative to detailed simulations for assessing AxMs influence on DNN performance, aiding design and optimization of energy-efficient hardware.

Abstract: Deep Neural Networks (DNNs) rely heavily on dense arithmetic operations, motivating the use of Approximate Multipliers (AxMs) to reduce energy consumption in hardware accelerators. However, a rigorous mathematical characterization of how AxMs error distributions influence DNN accuracy remains underdeveloped. This work presents an analytical framework that connects the statistical error moments of an AxM to the induced distortion in General Matrix Multiplication (GEMM). Using the Frobenius norm of the resulting error matrix, we derive a closed form expression for practical DNN dimensions that demonstrates the distortion is predominantly governed by the multiplier mean error (bias). To evaluate this model in realistic settings, we incorporate controlled error injection into GEMM and convolution layers and examine its effect on ImageNet scale networks. The predicted distortion correlates strongly with the observed accuracy degradation, and an error configurable AxM case study implemented on an FPGA further confirms the analytical trends. By providing a lightweight alternative to behavioral or hardware level simulations, this framework enables rapid estimation of AxM impact on DNN inference quality.

</details>


### [49] [ArchPower: Dataset for Architecture-Level Power Modeling of Modern CPU Design](https://arxiv.org/abs/2512.06854)
*Qijun Zhang,Yao Lu,Mengming Li,Shang Liu,Zhiyao Xie*

Main category: cs.AR

TL;DR: This paper introduces ArchPower, the first open-source dataset for architecture-level processor power modeling, containing detailed CPU features and power breakdowns.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of accuracy in early design-stage CPU power evaluation and the absence of open-source datasets for ML-based power modeling.

Method: The authors created a realistic dataset by implementing complex CPU designs, collecting architectural features as input, and simulated power as labels, with a detailed power decomposition.

Result: The dataset consists of 200 samples from 25 CPU configurations, covering more than 100 features per sample and fine-grained power values across different components and power types.

Conclusion: ArchPower contributes to advancing ML-based CPU power modeling, providing a resource to improve the accuracy of architectural power evaluation.

Abstract: Power is the primary design objective of large-scale integrated circuits (ICs), especially for complex modern processors (i.e., CPUs). Accurate CPU power evaluation requires designers to go through the whole time-consuming IC implementation process, easily taking months. At the early design stage (e.g., architecture-level), classical power models are notoriously inaccurate. Recently, ML-based architecture-level power models have been proposed to boost accuracy, but the data availability is a severe challenge. Currently, there is no open-source dataset for this important ML application. A typical dataset generation process involves correct CPU design implementation and repetitive execution of power simulation flows, requiring significant design expertise, engineering effort, and execution time. Even private in-house datasets often fail to reflect realistic CPU design scenarios. In this work, we propose ArchPower, the first open-source dataset for architecture-level processor power modeling. We go through complex and realistic design flows to collect the CPU architectural information as features and the ground-truth simulated power as labels. Our dataset includes 200 CPU data samples, collected from 25 different CPU configurations when executing 8 different workloads. There are more than 100 architectural features in each data sample, including both hardware and event parameters. The label of each sample provides fine-grained power information, including the total design power and the power for each of the 11 components. Each power value is further decomposed into four fine-grained power groups: combinational logic power, sequential logic power, memory power, and clock power. ArchPower is available at https://github.com/hkust-zhiyao/ArchPower.

</details>


### [50] [DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management](https://arxiv.org/abs/2512.07312)
*Zhongchun Zhou,Chengtao Lai,Yuhang Gu,Wei Zhang*

Main category: cs.AR

TL;DR: This paper introduces a shared system-level cache for AI accelerators, using application-aware policies to enhance performance without increasing programming complexity.


<details>
  <summary>Details</summary>
Motivation: The paper aims to streamline programming efforts and improve performance in AI accelerators by exploring shared cache designs as opposed to deeply hierarchical scratchpad memories.

Method: The study uses application-aware cache management policies guided by software dataflow, a cycle-accurate simulator for performance assessment, and analytical modeling to validate scalability.

Result: The approach achieves up to 1.80x speedup, handles inter-core scenarios effectively, is validated analytically, and demonstrates efficiency with an area of 0.064mm² using a 15nm process at 2GHz.

Conclusion: Shared cache designs with application-aware policies offer remarkable potential for future AI accelerator systems, combining performance advantages with simplified programming needs.

Abstract: The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing.
  We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups.
  Finally, we implement the design in RTL and the area of our design is $\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems.

</details>


### [51] [aLEAKator: HDL Mixed-Domain Simulation for Masked Hardware \& Software Formal Verification](https://arxiv.org/abs/2512.07520)
*Noé Amiot,Quentin L. Meunier,Karine Heydemann,Emmanuelle Encrenaz*

Main category: cs.AR

TL;DR: The paper introduces aLEAKator, a tool for formally verifying masked cryptographic implementations on CPUs under advanced leakage models (e.g., 1-probing).


<details>
  <summary>Details</summary>
Motivation: Despite advancements, verifying masked implementations under complex leakage models (e.g., glitches, CPU specifics) remains limited and requires impractical hardware-specific knowledge.

Method: Introduced aLEAKator, employing mixed-domain simulation to enable flexible, precise modeling under diverse leakage models without restricting signal granularity or requiring specific CPU knowledge.

Result: aLEAKator successfully verified a first-order masked AES on multiple CPUs while validating its approach against established methods and experimental data.

Conclusion: aLEAKator is effective for formally verifying security of masked cryptographic hardware/software under advanced leakage models, reducing previous limitations.

Abstract: Verifying the security of masked hardware and software implementations, under advanced leakage models, remains a significant challenge, especially then accounting for glitches, transitions and CPU micro-architectural specifics. Existing verification approaches are either restricted to small hardware gadgets, small programs on CPUs such as Sboxes, limited leakage models, or require hardware-specific prior knowledge. In this work, we present aLEAKator, an open-source framework for the automated formal verification of masked cryptographic accelerators and software running on CPUs from their HDL descriptions. Our method introduces mixed-domain simulation, enabling precise modeling and verification under various (including robust and relaxed) 1-probing leakage models, and supports variable signal granularity without being restricted to 1-bit wires. aLEAKator also supports verification in the presence of lookup tables, and does not require prior knowledge of the target CPU architecture. Our approach is validated against existing tools and real-world measurements while providing innovative results such as the verification of a full, first-order masked AES on various CPUs

</details>


### [52] [Análisis de rendimiento y eficiencia energética en el cluster Raspberry Pi Cronos](https://arxiv.org/abs/2512.07622)
*Martha Semken,Mariano Vargas,Ignacio Tula,Giuliana Zorzoli,Andrés Rojas Paredes*

Main category: cs.AR

TL;DR: The study evaluates the computational performance and energy efficiency of a Raspberry Pi-based cluster using tests on scalability, stability, and power usage.


<details>
  <summary>Details</summary>
Motivation: To assess the capabilities and limitations of setting up low-cost ARM clusters for computational tasks in educational and research environments.

Method: High Performance Linpack (HPL) benchmark tests were conducted on the Cronos cluster, configured with Slurm and Open MPI, to analyze different node setups and measure power consumption.

Result: The cluster achieved up to 6.91 GFLOPS with six Raspberry Pi 4 nodes, while heterogeneous setups with Raspberry Pi 3b impacted stability and efficiency.

Conclusion: Raspberry Pi-based clusters can serve as cost-effective educational and research tools, but their heterogeneity affects their stability and performance.

Abstract: This article presents an evaluation of the computational performance and energy efficiency of the Cronos cluster, composed of Raspberry Pi4 and 3b microcomputers designed for educational purposes. Experimental tests were performed using the High Performance Linpack (HPL) benchmark, under a resource management environment configured with Slurm and parallel communication via Open MPI. The study focuses on analyzing scalability, stability, and power consumption during the execution of computationally intensive workloads, considering different node configurations. The results show that the cluster achieves a performance of up to 6.91 GFLOPS in homogeneous configurations of 6 Raspberry Pi 4 nodes, and that the use of heterogeneous nodes (including Raspberry Pi 3b) can negatively impact stability and efficiency. Additionally, the total electrical consumption of the system was measured during the runs, allowing for the estimation of the performance-to-consumption ratio (GFLOPS/W) as a comparative metric. This study constitutes a concrete contribution to the design, evaluation, and utilization of low-cost ARM clusters in educational and research contexts.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [53] [Empathy by Design: Aligning Large Language Models for Healthcare Dialogue](https://arxiv.org/abs/2512.06097)
*Emre Umucu,Guillermina Solis,Leon Garza,Emilia Rivas,Beatrice Lee,Anantaa Kotal,Aritran Piplai*

Main category: cs.CL

TL;DR: The paper introduces a novel alignment framework using Direct Preference Optimization (DPO) to enhance factual correctness and empathy in language models for healthcare and caregiving contexts.


<details>
  <summary>Details</summary>
Motivation: LLMs face limitations in healthcare/caregiving due to factual unreliability and lack of empathetic communication, posing risks to non-professionals needing medical guidance or emotional support.

Method: Utilized DPO-based alignment to fine-tune domain-adapted LLMs with pairwise preference data, prioritizing supportive and accessible communication while discarding technical or prescriptive responses.

Result: DPO-tuned models achieved higher alignment in semantics, factual accuracy, and human-centric evaluations versus baseline and commercial healthcare dialogue models.

Conclusion: Preference-based alignment effectively creates scalable, empathetic, and clinically reliable AI assistants for caregiver and healthcare communication.

Abstract: General-purpose large language models (LLMs) have demonstrated remarkable generative and reasoning capabilities but remain limited in healthcare and caregiving applications due to two key deficiencies: factual unreliability and a lack of empathetic communication. These shortcomings pose significant risks in sensitive contexts where users, particularly non-professionals and caregivers, seek medically relevant guidance or emotional reassurance. To address these challenges, we introduce a Direct Preference Optimization (DPO)-based alignment framework designed to improve factual correctness, semantic coherence, and human-centric qualities such as empathy, politeness, and simplicity in caregiver-patient dialogues. Our approach fine-tunes domain-adapted LLMs using pairwise preference data, where preferred responses reflect supportive and accessible communication styles while rejected ones represent prescriptive or overly technical tones. This direct optimization method aligns model outputs with human preferences more efficiently than traditional reinforcement-learning-based alignment. Empirical evaluations across multiple open and proprietary LLMs show that our DPO-tuned models achieve higher semantic alignment, improved factual accuracy, and stronger human-centric evaluation scores compared to baseline and commercial alternatives such as Google medical dialogue systems. These improvements demonstrate that preference-based alignment offers a scalable and transparent pathway toward developing trustworthy, empathetic, and clinically informed AI assistants for caregiver and healthcare communication. Our open-source code is available at: https://github.com/LeonG19/Empathy-by-Design

</details>


### [54] [Morphologically-Informed Tokenizers for Languages with Non-Concatenative Morphology: A case study of Yoloxóchtil Mixtec ASR](https://arxiv.org/abs/2512.06169)
*Chris Crawford*

Main category: cs.CL

TL;DR: The paper explores novel tokenization techniques to improve annotation of an audio corpus for Yoloxóchitl Mixtec using ASR tools, and finds these methods competitive with traditional models.


<details>
  <summary>Details</summary>
Motivation: To enhance efficiency and minimize the workload for annotating the Yoloxóchitl Mixtec corpus by incorporating morphologically-informed tokenizers.

Method: Develop two nonlinear tokenization schemes: Segment-and-Melody tokenizer and Sequence of Processes tokenizer, evaluating them on ASR performance metrics and comparing them with conventional approaches.

Result: The Segment-and-Melody tokenizer achieves better word error rates compared to conventional tokenizers but trails in character error rates.

Conclusion: Nonlinear tokenizers adapted for language morphology show competitive results against traditional methods, suggesting potential for broader application in ASR tasks.

Abstract: This paper investigates the impact of using morphologically-informed tokenizers to aid and streamline the interlinear gloss annotation of an audio corpus of Yoloxóchitl Mixtec (YM) using a combination of ASR and text-based sequence-to-sequence tools, with the goal of improving efficiency while reducing the workload of a human annotator. We present two novel tokenization schemes that separate words in a nonlinear manner, preserving information about tonal morphology as much as possible. One of these approaches, a Segment and Melody tokenizer, simply extracts the tones without predicting segmentation. The other, a Sequence of Processes tokenizer, predicts segmentation for the words, which could allow an end-to-end ASR system to produce segmented and unsegmented transcriptions in a single pass. We find that these novel tokenizers are competitive with BPE and Unigram models, and the Segment-and-Melody model outperforms traditional tokenizers in terms of word error rate but does not reach the same character error rate. In addition, we analyze tokenizers on morphological and information-theoretic metrics to find predictive correlations with downstream performance. Our results suggest that nonlinear tokenizers designed specifically for the non-concatenative morphology of a language are competitive with conventional BPE and Unigram models for ASR. Further research will be necessary to determine the applicability of these tokenizers in downstream processing tasks.

</details>


### [55] [Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots](https://arxiv.org/abs/2512.06193)
*Jihyung Park,Saleh Afroogh,Junfeng Jiao*

Main category: cs.CL

TL;DR: The paper addresses implicit conversational harm by introducing GAUGE, which detects hidden escalation in LLM-driven dialogues.


<details>
  <summary>Details</summary>
Motivation: To prevent unseen, implicit conversational harm from emotional reinforcement and drift during interactions with LLMs.

Method: Proposes GAUGE, a logit-based framework that detects shifts in the affective state of conversations in real-time.

Result: GAUGE provides real-time detection of emotional escalation without relying on external classifiers, improving conversational safety.

Conclusion: The framework enhances dialogue safety by monitoring subtle emotional trends, addressing gaps in current toxicity filtering methods.

Abstract: Large Language Models (LLM) are increasingly integrated into everyday interactions, serving not only as information assistants but also as emotional companions. Even in the absence of explicit toxicity, repeated emotional reinforcement or affective drift can gradually escalate distress in a form of \textit{implicit harm} that traditional toxicity filters fail to detect. Existing guardrail mechanisms often rely on external classifiers or clinical rubrics that may lag behind the nuanced, real-time dynamics of a developing conversation. To address this gap, we propose GAUGE (Guarding Affective Utterance Generation Escalation), a lightweight, logit-based framework for the real-time detection of hidden conversational escalation. GAUGE measures how an LLM's output probabilistically shifts the affective state of a dialogue.

</details>


### [56] [Automated Data Enrichment using Confidence-Aware Fine-Grained Debate among Open-Source LLMs for Mental Health and Online Safety](https://arxiv.org/abs/2512.06227)
*Junyu Mao,Anthony Hills,Talia Tseriotou,Maria Liakata,Aya Shamir,Dan Sayda,Dana Atzil-Slonim,Natalie Djohari,Arpan Mandal,Silke Roth,Pamela Ugwudike,Mahesan Niranjan,Stuart E. Middleton*

Main category: cs.CL

TL;DR: This paper introduces a Confidence-Aware Fine-Grained Debate (CFD) framework to enhance NLP training datasets by leveraging multiple LLMs for simulated annotation. It demonstrates improved NLP performance in mental health and online safety tasks.


<details>
  <summary>Details</summary>
Motivation: Developing effective NLP systems for tasks like mental health and online safety is challenging due to the cost and difficulty of annotating dynamic real-world events.

Method: A novel CFD framework is proposed where multiple LLM agents simulate annotators and exchange evidence to reach consensus. Two expert-annotated datasets are created for evaluation.

Result: The CFD framework outperforms baselines and enriches data for better downstream NLP task performance. It yields a 10.1% improvement in online safety task performance using enriched features.

Conclusion: Simulated LLM-based annotation and enriched data improve the robustness and efficiency of NLP tasks, making it a valuable approach for handling highly dynamic real-world indicators.

Abstract: Real-world indicators are important for improving natural language processing (NLP) tasks such as life events for mental health analysis and risky behaviour for online safety, yet labelling such information in NLP training datasets is often costly and/or difficult given the dynamic nature of such events. This paper compares several LLM-based data enrichment methods and introduces a novel Confidence-Aware Fine-Grained Debate (CFD) framework in which multiple LLM agents simulate human annotators and exchange fine-grained evidence to reach consensus. We describe two new expert-annotated datasets, a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Our CFD framework achieves the most robust data enrichment performance compared to a range of baselines and we show that this type of data enrichment consistently improves downstream tasks. Enriched features incorporated via debate transcripts yield the largest gains, outperforming the non-enriched baseline by 10.1% for the online safety task.

</details>


### [57] [Policy-based Sentence Simplification: Replacing Parallel Corpora with LLM-as-a-Judge](https://arxiv.org/abs/2512.06228)
*Xuanxin Wu,Yuki Arase,Masaaki Nagata*

Main category: cs.CL

TL;DR: This paper introduces a method for policy-driven sentence simplification using large language models to generate training data without human annotations.


<details>
  <summary>Details</summary>
Motivation: The challenge of achieving policy-driven control in sentence simplification, catering to different user requirements, remains unresolved.

Method: The authors utilize a Large Language Model-as-a-Judge (LLM-as-a-Judge) to generate training data for sentence simplification, eliminating the need for human annotations or parallel corpora.

Result: Their approach enables open-source models to perform well, with smaller models outperforming GPT-4 in lexical simplification and achieving comparable results in overall sentence rewriting.

Conclusion: The approach demonstrates flexibility and robustness across tasks and model scales, making it a practical solution for diverse sentence simplification needs.

Abstract: Sentence simplification aims to modify a sentence to make it easier to read and understand while preserving the meaning. Different applications require distinct simplification policies, such as replacing only complex words at the lexical level or rewriting the entire sentence while trading off details for simplicity. However, achieving such policy-driven control remains an open challenge. In this work, we introduce a simple yet powerful approach that leverages Large Language Model-as-a-Judge (LLM-as-a-Judge) to automatically construct policy-aligned training data, completely removing the need for costly human annotation or parallel corpora. Our method enables building simplification systems that adapt to diverse simplification policies. Remarkably, even small-scale open-source LLMs such as Phi-3-mini-3.8B surpass GPT-4o on lexical-oriented simplification, while achieving comparable performance on overall rewriting, as verified by both automatic metrics and human evaluations. The consistent improvements across model families and sizes demonstrate the robustness of our approach.

</details>


### [58] [LOCUS: A System and Method for Low-Cost Customization for Universal Specialization](https://arxiv.org/abs/2512.06239)
*Dhanasekar Sundararaman,Keying Li,Wayne Xiong,Aashna Garg*

Main category: cs.CL

TL;DR: LOCUS is a pipeline for efficient NLP model development using few-shot data, achieving high accuracy and low memory usage.


<details>
  <summary>Details</summary>
Motivation: To develop a cost-effective method for building specialized NLP models with high accuracy and low memory requirements, using scarce labeled data.

Method: LOCUS involves data retrieval, synthetic data creation, and efficient fine-tuning (full or LoRA) to optimize NLP models.

Result: LOCUS improves NER and TC benchmarks, surpasses strong baselines (e.g., GPT-4o), and reduces memory use while retaining performance.

Conclusion: LOCUS demonstrates that specialized NLP models can maintain high accuracy while significantly reducing resource consumption and costs.

Abstract: We present LOCUS (LOw-cost Customization for Universal Specialization), a pipeline that consumes few-shot data to streamline the construction and training of NLP models through targeted retrieval, synthetic data generation, and parameter-efficient tuning. With only a small number of labeled examples, LOCUS discovers pertinent data in a broad repository, synthesizes additional training samples via in-context data generation, and fine-tunes models using either full or low-rank (LoRA) parameter adaptation. Our approach targets named entity recognition (NER) and text classification (TC) benchmarks, consistently outperforming strong baselines (including GPT-4o) while substantially lowering costs and model sizes. Our resultant memory-optimized models retain 99% of fully fine-tuned accuracy while using barely 5% of the memory footprint, also beating GPT-4o on several benchmarks with less than 1% of its parameters.

</details>


### [59] [Convergence of Outputs When Two Large Language Models Interact in a Multi-Agentic Setup](https://arxiv.org/abs/2512.06256)
*Aniruddha Maiti,Satya Nimmagadda,Kartha Veerya Jammuladinne,Niladri Sengupta,Ananya Jana*

Main category: cs.CL

TL;DR: The paper investigates the interaction between two large language models engaged in extended multi-turn dialogues without external inputs, finding that these dialogues often converge into repetitive loops.


<details>
  <summary>Details</summary>
Motivation: To explore and analyze the conversational behavior dynamics between two large language models responding to each other without external inputs.

Method: Using Mistral Nemo Base 2407 and Llama 2 13B, the study starts dialogues between the two models with a short seed sentence and lets them respond to each other for a fixed number of steps. Lexical and embedding-based metrics are used to analyze conversation drift and convergence.

Result: The interaction between the models initially produces coherent conversations but frequently transitions into repetitive loops where phrases repeat across turns, and new conversational directions do not emerge.

Conclusion: The phenomenon of conversational convergence reveals a limitation of large language models in multi-turn communication when isolated from external inputs, highlighting the need for better strategies to mitigate repetition and improve conversational diversity.

Abstract: In this work, we report what happens when two large language models respond to each other for many turns without any outside input in a multi-agent setup. The setup begins with a short seed sentence. After that, each model reads the other's output and generates a response. This continues for a fixed number of steps. We used Mistral Nemo Base 2407 and Llama 2 13B hf. We observed that most conversations start coherently but later fall into repetition. In many runs, a short phrase appears and repeats across turns. Once repetition begins, both models tend to produce similar output rather than introducing a new direction in the conversation. This leads to a loop where the same or similar text is produced repeatedly. We describe this behavior as a form of convergence. It occurs even though the models are large, trained separately, and not given any prompt instructions. To study this behavior, we apply lexical and embedding-based metrics to measure how far the conversation drifts from the initial seed and how similar the outputs of the two models becomes as the conversation progresses.

</details>


### [60] [Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models](https://arxiv.org/abs/2512.06266)
*Chen Yang,Guangyue Peng,Jiaying Zhu,Ran Le,Ruixiang Feng,Tao Zhang,Wei Ruan,Xiaoqi Liu,Xiaoxue Cheng,Xiyun Xu,Yang Song,Yanzipeng Gao,Yiming Jia,Yun Xing,Yuntao Wen,Zekai Wang,Zhenwei An,Zhicong Sun,Zongchao Chen*

Main category: cs.CL

TL;DR: Nanbeige4-3B is a compact, high-performing language model that combines advanced training schedulers, post-training mechanisms, distillation techniques, and reinforcement learning to achieve remarkable performance rivaling larger models.


<details>
  <summary>Details</summary>
Motivation: To create a small-scale language model capable of matching the quality and functionality of larger models across various tasks while maintaining efficiency.

Method: The study implements a refined pre-training scheduler (FG-WSD), deliberative generation refinement, chain-of-thought reconstruction, Dual Preference Distillation (DPD), and multi-stage reinforcement learning to optimize the model's performance and alignment.

Result: Nanbeige4-3B displays substantial improvements over similarly scaled models and challenges the performance of larger models in diverse benchmarks.

Conclusion: Nanbeige4-3B demonstrates that compact language models can achieve high performance comparable to larger counterparts, thanks to effective pre-training and post-training strategies.

Abstract: We present Nanbeige4-3B, a family of small-scale but high-performing language models. Pretrained on 23T high-quality tokens and finetuned on over 30 million diverse instructions, we extend the boundary of the scaling law for small language models. In pre-training, we design a Fine-Grained Warmup-Stable-Decay (FG-WSD) training scheduler, which progressively refines data mixtures across stages to boost model performance. In post-training, to improve the quality of the SFT data, we design a joint mechanism that integrates deliberative generation refinement and chain-of-thought reconstruction, yielding substantial gains on complex tasks. Following SFT, we employ our flagship reasoning model to distill Nanbeige4-3B through our proposed Dual Preference Distillation (DPD) method, which leads to further performance gains. Finally, a multi-stage reinforcement learning phase was applied, leveraging verifiable rewards and preference modeling to strengthen abilities on both reasoning and human alignment. Extensive evaluations show that Nanbeige4-3B not only significantly outperforms models of comparable parameter scale but also rivals much larger models across a wide range of benchmarks. The model checkpoints are available at https://huggingface.co/Nanbeige.

</details>


### [61] [Modeling Contextual Passage Utility for Multihop Question Answering](https://arxiv.org/abs/2512.06464)
*Akriti Jain,Aparna Garimella*

Main category: cs.CL

TL;DR: The paper proposes a method to improve Multihop Question Answering (QA) by scoring retrieved passages based on their contextual utility, enhancing QA performance.


<details>
  <summary>Details</summary>
Motivation: To address the issue of redundant and noisy passages in multihop QA by leveraging inter-passage dependencies for better passage utility assessment.

Method: A lightweight transformer-based model is fine-tuned to predict contextual utility scores for passages using reasoning traces from an advanced reasoning model.

Result: Utility-based scoring of retrieved passages leads to better passage reranking and improved QA performance compared to relevance-based methods.

Conclusion: Modeling contextual utility of passages significantly enhances multihop QA systems by capturing inter-passage relationships and refining passage selection.

Abstract: Multihop Question Answering (QA) requires systems to identify and synthesize information from multiple text passages. While most prior retrieval methods assist in identifying relevant passages for QA, further assessing the utility of the passages can help in removing redundant ones, which may otherwise add to noise and inaccuracies in the generated answers. Existing utility prediction approaches model passage utility independently, overlooking a critical aspect of multihop reasoning: the utility of a passage can be context-dependent, influenced by its relation to other passages - whether it provides complementary information or forms a crucial link in conjunction with others. In this paper, we propose a lightweight approach to model contextual passage utility, accounting for inter-passage dependencies. We fine-tune a small transformer-based model to predict passage utility scores for multihop QA. We leverage the reasoning traces from an advanced reasoning model to capture the order in which passages are used to answer a question and obtain synthetic training data. Through comprehensive experiments, we demonstrate that our utility-based scoring of retrieved passages leads to improved reranking and downstream QA performance compared to relevance-based reranking methods.

</details>


### [62] [Knowing What's Missing: Assessing Information Sufficiency in Question Answering](https://arxiv.org/abs/2512.06476)
*Akriti Jain,Aparna Garimella*

Main category: cs.CL

TL;DR: The paper addresses the challenge of determining whether a context provides enough information to answer a question, proposing a structured 'Identify-then-Verify' framework to improve sufficiency judgments.


<details>
  <summary>Details</summary>
Motivation: To enhance question-answering systems' reliability, particularly on inferential questions that require reasoning beyond direct text extraction.

Method: The proposed 'Identify-then-Verify' framework first generates hypotheses about missing information and establishes a semantic consensus, then re-examines the source text to confirm information sufficiency.

Result: The framework outperforms baselines on multi-hop and factual QA datasets, improving sufficiency judgments and clearly identifying information gaps.

Conclusion: Structured reasoning, via identifying and verifying missing information, enhances the model's capacity to assess sufficiency and articulate gaps in information.

Abstract: Determining whether a provided context contains sufficient information to answer a question is a critical challenge for building reliable question-answering systems. While simple prompting strategies have shown success on factual questions, they frequently fail on inferential ones that require reasoning beyond direct text extraction. We hypothesize that asking a model to first reason about what specific information is missing provides a more reliable, implicit signal for assessing overall sufficiency. To this end, we propose a structured Identify-then-Verify framework for robust sufficiency modeling. Our method first generates multiple hypotheses about missing information and establishes a semantic consensus. It then performs a critical verification step, forcing the model to re-examine the source text to confirm whether this information is truly absent. We evaluate our method against established baselines across diverse multi-hop and factual QA datasets. The results demonstrate that by guiding the model to justify its claims about missing information, our framework produces more accurate sufficiency judgments while clearly articulating any information gaps.

</details>


### [63] [Classifying German Language Proficiency Levels Using Large Language Models](https://arxiv.org/abs/2512.06483)
*Elias-Leander Ahlers,Witold Brunsmann,Malte Schilling*

Main category: cs.CL

TL;DR: The paper explores using Large Language Models (LLMs) for classifying German texts by CEFR proficiency levels, showing improved performance over previous methods.


<details>
  <summary>Details</summary>
Motivation: Assessing language proficiency helps tailor education to learners' needs.

Method: The study combines CEFR-annotated corpora with synthetic data to create a robust dataset. It employs prompt engineering, fine-tuning a LLaMA-3-8B-Instruct model, and a probing approach using LLMs' internal neural states for classification.

Result: The approach consistently outperformed prior methodologies in CEFR classification.

Conclusion: LLMs are effective for robust and scalable classification of language proficiency by CEFR levels.

Abstract: Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.

</details>


### [64] [ProSocialAlign: Preference Conditioned Test Time Alignment in Language Models](https://arxiv.org/abs/2512.06515)
*Somnath Banerjee,Sayan Layek,Sayantan Adak,Mykola Pechenizkiy,Animesh Mukherjee,Rima Hazra*

Main category: cs.CL

TL;DR: The paper proposes ProSocialAlign, a modular framework to guide language models towards safe and empathetic outputs without retraining the base model.


<details>
  <summary>Details</summary>
Motivation: Address limitations of traditional language model safety approaches, particularly in emotionally sensitive or high-stakes scenarios.

Method: Introduced a two-pronged approach: harm-mitigation via directional regulation, and preference-aware rewarding for user-controllable, prosocial outputs.

Result: ProSocialAlign outperforms benchmarks in safety, alignment to human values, and reduces unsafe content leakage.

Conclusion: The framework provides a flexible tool for generating context-sensitive, safe, and aligned responses effectively at inference time.

Abstract: Current language model safety paradigms often fall short in emotionally charged or high-stakes settings, where refusal-only approaches may alienate users and naive compliance can amplify risk. We propose ProSocialAlign, a test-time, parameter-efficient framework that steers generation toward safe, empathetic, and value-aligned responses without retraining the base model. We formalize five human-centered objectives and cast safety as lexicographic constrained generation: first, applying hard constraints to eliminate harmful continuations; then optimizing for prosocial quality within the safe set. Our method combines (i) directional regulation, a harm-mitigation mechanism that subtracts a learned "harm vector" in parameter space, and (ii) preference-aware autoregressive reward modeling trained jointly across attributes with gradient conflict resolution, enabling fine-grained, user-controllable decoding. Empirical evaluations across five safety benchmarks demonstrate state-of-the-art performance, reducing unsafe leakage and boosting alignment to human values, with strong gains across multiple evaluation metrics. ProSocialAlign offers a robust and modular foundation for generating context-sensitive, safe, and human-aligned responses at inference time.

</details>


### [65] [Adapting AlignScore Mertic for Factual Consistency Evaluation of Text in Russian: A Student Abstract](https://arxiv.org/abs/2512.06586)
*Mikhail Zimin,Milyausha Shamsutdinova,Georgii Andriushchenko*

Main category: cs.CL

TL;DR: The paper introduces AlignRuScore, an adapted factual consistency evaluation tool for Russian texts, addressing the lack of such resources in non-English languages.


<details>
  <summary>Details</summary>
Motivation: Factual consistency evaluation tools are limited to English texts, creating a need for tools applicable to Russian and other languages.

Method: The authors adapted AlignScore for Russian by fine-tuning a RuBERT-based alignment model with classification and regression heads using Russian and translated English datasets.

Result: The unified alignment metric, demonstrated through AlignRuScore, successfully works for Russian and enables consistent multilingual factual consistency evaluation.

Conclusion: AlignRuScore fills a significant gap for evaluating factual consistency in Russian texts, and the release of open datasets and code promotes further multilingual research.

Abstract: Ensuring factual consistency in generated text is crucial for reliable natural language processing applications. However, there is a lack of evaluation tools for factual consistency in Russian texts, as existing tools primarily focus on English corpora. To bridge this gap, we introduce AlignRuScore, a comprehensive adaptation of the AlignScore metric for Russian. To adapt the metric, we fine-tuned a RuBERT-based alignment model with task-specific classification and regression heads on Russian and translated English datasets. Our results demonstrate that a unified alignment metric can be successfully ported to Russian, laying the groundwork for robust multilingual factual consistency evaluation. We release the translated corpora, model checkpoints, and code to support further research.

</details>


### [66] [The Online Discourse of Virtual Reality and Anxiety](https://arxiv.org/abs/2512.06656)
*Kwabena Yamoah,Cass Dykeman*

Main category: cs.CL

TL;DR: The study explores online discussions on the use of VR for anxiety treatment using corpus linguistics, identifying keywords and concepts.


<details>
  <summary>Details</summary>
Motivation: To understand user perspectives on VR technology in the treatment of anxiety disorders, aiding its efficacy and further development.

Method: Employed corpus linguistic methodology with Sketch Engine software to analyze frequently used words and collocations within online discussions about VR and anxiety.

Result: Found "VR," "Oculus," and "headset" as the most frequently discussed terms. Identified collocations related to design, experience, and development of VR systems.

Conclusion: The study highlights pathways for improving counseling needs through the development and accessibility of VR systems, offering insights into how VR and anxiety are discussed online.

Abstract: VR in the treatment of clinical concerns such as generalized anxiety disorder or social anxiety. VR has created additional pathways to support patient well-being and care. Understanding online discussion of what users think about this technology may further support its efficacy. The purpose of this study was to employ a corpus linguistic methodology to identify the words and word networks that shed light on the online discussion of virtual reality and anxiety. Using corpus linguistics, frequently used words in discussion along with collocation were identified by utilizing Sketch Engine software. The results of the study, based upon the English Trends corpus, identified VR, Oculus, and headset as the most frequently discussed within the VR and anxiety subcorpus. These results point to the development of the virtual system, along with the physical apparatus that makes viewing and engaging with the virtual environment possible. Additional results point to collocation of prepositional phrases such as of virtual reality, in virtual reality, and for virtual reality relating to the design, experience, and development, respectively. These findings offer new perspective on how VR and anxiety together are discussed in general discourse and offer pathways for future opportunities to support counseling needs through development and accessibility. Keywords: anxiety disorders, corpus linguistics, Sketch Engine, and virtual reality VR

</details>


### [67] [CMV-Fuse: Cross Modal-View Fusion of AMR, Syntax, and Knowledge Representations for Aspect Based Sentiment Analysis](https://arxiv.org/abs/2512.06679)
*Smitha Muthya Sudheendra,Mani Deep Cherukuri,Jaideep Srivastava*

Main category: cs.CL

TL;DR: CMV-Fuse combines multiple linguistic perspectives for Aspect-Based Sentiment Analysis, achieving better accuracy with efficient computation.


<details>
  <summary>Details</summary>
Motivation: Aspect-Based Sentiment Analysis has limitations in utilizing isolated linguistic views, missing the interplay between structural representations humans naturally use.

Method: CMV-Fuse integrates four linguistic perspectives—Abstract Meaning Representations, constituency parsing, dependency syntax, and semantic attention—combined with external knowledge using hierarchical gated attention fusion and structure-aware multi-view contrastive learning.

Result: Experiments show CMV-Fuse outperforms strong baseline systems on standard benchmarks while maintaining computational efficiency.

Conclusion: The proposed framework enhances sentiment analysis by leveraging diverse linguistic perspectives, improving robustness and contextual understanding.

Abstract: Natural language understanding inherently depends on integrating multiple complementary perspectives spanning from surface syntax to deep semantics and world knowledge. However, current Aspect-Based Sentiment Analysis (ABSA) systems typically exploit isolated linguistic views, thereby overlooking the intricate interplay between structural representations that humans naturally leverage. We propose CMV-Fuse, a Cross-Modal View fusion framework that emulates human language processing by systematically combining multiple linguistic perspectives. Our approach systematically orchestrates four linguistic perspectives: Abstract Meaning Representations, constituency parsing, dependency syntax, and semantic attention, enhanced with external knowledge integration. Through hierarchical gated attention fusion across local syntactic, intermediate semantic, and global knowledge levels, CMV-Fuse captures both fine-grained structural patterns and broad contextual understanding. A novel structure aware multi-view contrastive learning mechanism ensures consistency across complementary representations while maintaining computational efficiency. Extensive experiments demonstrate substantial improvements over strong baselines on standard benchmarks, with analysis revealing how each linguistic view contributes to more robust sentiment analysis.

</details>


### [68] [Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers in Sentiment Analysis](https://arxiv.org/abs/2512.06681)
*Amartya Hatua*

Main category: cs.CL

TL;DR: This study investigates sentiment processing in GPT-2 using activation patching across layers, revealing early lexical sentiment detection and late-layer contextual integration.


<details>
  <summary>Details</summary>
Motivation: To understand how GPT-2 processes sentiment information, specifically examining if sentiment handling follows a hypothesized hierarchical structure.

Method: Systematic activation patching was used across all 12 GPT-2 layers to identify sentiment processing stages and test specific hypotheses.

Result: Early layers encode sentiment signals independent of context, falsifying mid-layer specialization hypotheses. Contextual integration occurs in unified mechanisms at late layers.

Conclusion: GPT-2's sentiment computation does not follow a hierarchical pattern and requires further study on contextual integration in language models.

Abstract: We present a mechanistic interpretability study of GPT-2 that causally examines how sentiment information is processed across its transformer layers. Using systematic activation patching across all 12 layers, we test the hypothesized two-stage sentiment architecture comprising early lexical detection and mid-layer contextual integration. Our experiments confirm that early layers (0-3) act as lexical sentiment detectors, encoding stable, position specific polarity signals that are largely independent of context. However, all three contextual integration hypotheses: Middle Layer Concentration, Phenomenon Specificity, and Distributed Processing are falsified. Instead of mid-layer specialization, we find that contextual phenomena such as negation, sarcasm, domain shifts etc. are integrated primarily in late layers (8-11) through a unified, non-modular mechanism. These experimental findings provide causal evidence that GPT-2's sentiment computation differs from the predicted hierarchical pattern, highlighting the need for further empirical characterization of contextual integration in large language models.

</details>


### [69] [PersonaMem-v2: Towards Personalized Intelligence via Learning Implicit User Personas and Agentic Memory](https://arxiv.org/abs/2512.06688)
*Bowen Jiang,Yuan Yuan,Maohao Shen,Zhuoqun Hao,Zhangchen Xu,Zichen Chen,Ziyi Liu,Anvesh Rao Vijjini,Jiashu He,Hanchao Yu,Radha Poovendran,Gregory Wornell,Lyle Ungar,Dan Roth,Sihao Chen,Camillo Jose Taylor*

Main category: cs.CL

TL;DR: This paper introduces PersonaMem-v2, a dataset for personalizing AI systems with improved long-context reasoning and implicit personalization, achieving promising results with reinforcement fine-tuning and agentic memory systems.


<details>
  <summary>Details</summary>
Motivation: To advance AI capability and alignment in personalization by overcoming challenges in long-context reasoning and implicit personalization tasks, as current AI models struggle with these aspects.

Method: The authors introduced PersonaMem-v2, a realistic dataset, and used reinforcement fine-tuning to improve model performance. They also developed an agentic memory framework for maintaining an evolving human-readable memory tailored to users.

Result: The enhanced models partially improved in implicit personalization. Qwen3-4B outperformed GPT-5 with 53% accuracy, and the agentic memory system achieved state-of-the-art 55% accuracy, operating efficiently with reduced token usage.

Conclusion: The approach highlights the importance of PersonaMem-v2 and agentic memory systems for advancing real-world AI personalization, while emphasizing the need for further improving reasoning capabilities.

Abstract: Personalization is one of the next milestones in advancing AI capability and alignment. We introduce PersonaMem-v2, the state-of-the-art dataset for LLM personalization that simulates 1,000 realistic user-chatbot interactions on 300+ scenarios, 20,000+ user preferences, and 128k-token context windows, where most user preferences are implicitly revealed to reflect real-world interactions. Using this data, we investigate how reinforcement fine-tuning enables a model to improve its long-context reasoning capabilities for user understanding and personalization. We also develop a framework for training an agentic memory system, which maintains a single, human-readable memory that grows with each user over time.
  In our experiments, frontier LLMs still struggle with implicit personalization, achieving only 37-48% accuracy. While they support long context windows, reasoning remains the bottleneck for implicit personalization tasks. Using reinforcement fine-tuning, we successfully train Qwen3-4B to outperforms GPT-5, reaching 53% accuracy in implicit personalization. Moreover, our agentic memory framework achieves state-of-the-art 55% accuracy while using 16x fewer input tokens, relying on a 2k-token memory instead of full 32k conversation histories. These results underscore the impact of our dataset and demonstrate agentic memory as a scalable path toward real-world personalized intelligence.

</details>


### [70] [Think-While-Generating: On-the-Fly Reasoning for Personalized Long-Form Generation](https://arxiv.org/abs/2512.06690)
*Chengbing Wang,Yang Zhang,Wenjie Wang,Xiaoyan Zhao,Fuli Feng,Xiangnan He,Tat-Seng Chua*

Main category: cs.CL

TL;DR: This paper introduces FlyThinker, a framework for personalized long-form text generation that combines reasoning and generation processes for better adaptability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for aligning language models with human preferences fail to cater to individual user needs and struggle with implicit reasoning over dynamic content, especially in long-form text generation.

Method: FlyThinker integrates a separate reasoning model that generates token-level reasoning in parallel with the main generation process, ensuring dynamic guidance and efficiency. The reasoning model depends only on previous responses to maintain parallelism and efficiency during training.

Result: FlyThinker achieves superior personalized long-form text generation while maintaining efficient training and inference, as demonstrated through extensive experiments on real-world benchmarks.

Conclusion: FlyThinker effectively addresses the limitations of existing methods by improving personalization, adaptability in long-form generation, and efficiency during both training and inference.

Abstract: Preference alignment has enabled large language models (LLMs) to better reflect human expectations, but current methods mostly optimize for population-level preferences, overlooking individual users. Personalization is essential, yet early approaches-such as prompt customization or fine-tuning-struggle to reason over implicit preferences, limiting real-world effectiveness. Recent "think-then-generate" methods address this by reasoning before response generation. However, they face challenges in long-form generation: their static one-shot reasoning must capture all relevant information for the full response generation, making learning difficult and limiting adaptability to evolving content. To address this issue, we propose FlyThinker, an efficient "think-while-generating" framework for personalized long-form generation. FlyThinker employs a separate reasoning model that generates latent token-level reasoning in parallel, which is fused into the generation model to dynamically guide response generation. This design enables reasoning and generation to run concurrently, ensuring inference efficiency. In addition, the reasoning model is designed to depend only on previous responses rather than its own prior outputs, which preserves training parallelism across different positions-allowing all reasoning tokens for training data to be produced in a single forward pass like standard LLM training, ensuring training efficiency. Extensive experiments on real-world benchmarks demonstrate that FlyThinker achieves better personalized generation while keeping training and inference efficiency.

</details>


### [71] [TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction](https://arxiv.org/abs/2512.06694)
*Aoi Fujita,Taichi Yamamoto,Yuri Nakayama,Ryota Kobayashi*

Main category: cs.CL

TL;DR: The paper introduces TopiCLEAR, a novel method for topic modeling, specifically addressing challenges in short social media texts by combining SBERT embeddings, Gaussian Mixture Models, and supervised dimensional reduction.


<details>
  <summary>Details</summary>
Motivation: Traditional topic modeling struggles with short and informal social media posts due to limited co-occurrence, fragmented semantics, and noisy language.

Method: The proposed method combines SBERT for embedding texts, GMM for clustering, and an iterative supervised projection (LDA) process to refine clusters until convergence, operating on raw text data without preprocessing.

Result: TopiCLEAR outperformed seven baseline methods in accurately aligning with human-annotated topics on datasets, including social media and news articles, while producing interpretable topics.

Conclusion: TopiCLEAR effectively addresses the challenges in topic modeling for short and informal texts, offering improved accuracy and interpretability for applications in social media and web content analysis.

Abstract: Rapid expansion of social media platforms such as X (formerly Twitter), Facebook, and Reddit has enabled large-scale analysis of public perceptions on diverse topics, including social issues, politics, natural disasters, and consumer sentiment. Topic modeling is a widely used approach for uncovering latent themes in text data, typically framed as an unsupervised classification task. However, traditional models, originally designed for longer and more formal documents, struggle with short social media posts due to limited co-occurrence statistics, fragmented semantics, inconsistent spelling, and informal language. To address these challenges, we propose a new method, TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction. Specifically, each text is embedded using Sentence-BERT (SBERT) and provisionally clustered using Gaussian Mixture Models (GMM). The clusters are then refined iteratively using a supervised projection based on linear discriminant analysis, followed by GMM-based clustering until convergence. Notably, our method operates directly on raw text, eliminating the need for preprocessing steps such as stop word removal. We evaluate our approach on four diverse datasets, 20News, AgNewsTitle, Reddit, and TweetTopic, each containing human-labeled topic information. Compared with seven baseline methods, including a recent SBERT-based method and a zero-shot generative AI method, our approach achieves the highest similarity to human-annotated topics, with significant improvements for both social media posts and online news articles. Additionally, qualitative analysis shows that our method produces more interpretable topics, highlighting its potential for applications in social media data and web content analytics.

</details>


### [72] [Parameter-Efficient Fine-Tuning with Differential Privacy for Robust Instruction Adaptation in Large Language Models](https://arxiv.org/abs/2512.06711)
*Yulin Huang,Yaxuan Luan,Jinxu Guo,Xiangchen Song,Yuchen Liu*

Main category: cs.CL

TL;DR: The study proposes a parameter-efficient fine-tuning method for large language models that incorporates differential privacy and gradient clipping, offering robust, stable, and privacy-preserving training for instruction tasks.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address challenges in privacy protection and training efficiency when fine-tuning large-scale language models for multi-task instruction settings.

Method: The proposed method uses a low-dimensional projection space for parameter updates, with differential privacy (noise allocation) and gradient clipping incorporated into a collaborative optimization framework.

Result: The method surpasses baseline models in accuracy, privacy protection, and parameter efficiency, while maintaining stability across varying data and environments.

Conclusion: This work advances privacy-aware and efficient training methodologies, providing a viable option for secure and robust multi-task instruction fine-tuning.

Abstract: This study addresses the issues of privacy protection and efficiency in instruction fine-tuning of large-scale language models by proposing a parameter-efficient method that integrates differential privacy noise allocation with gradient clipping in a collaborative optimization framework. The method keeps the backbone model frozen and updates parameters through a low-dimensional projection subspace, while introducing clipping and adaptive noise allocation during gradient computation. This design reduces privacy budget consumption and ensures training stability and robustness. The unified framework combines gradient constraints, noise allocation, and parameter projection, effectively mitigating performance fluctuations and privacy risks in multi-task instruction scenarios. Experiments are conducted across hyperparameter, environment, and data sensitivity dimensions. Results show that the method outperforms baseline models in accuracy, privacy budget, and parameter efficiency, and maintains stable performance under diverse and uncertain data conditions. The findings enrich the theoretical integration of differential privacy and parameter-efficient fine-tuning and demonstrate its practical adaptability in instruction tasks, providing a feasible solution for secure training in complex instruction environments.

</details>


### [73] ["The Dentist is an involved parent, the bartender is not": Revealing Implicit Biases in QA with Implicit BBQ](https://arxiv.org/abs/2512.06732)
*Aarushi Wagh,Saniya Srivastava*

Main category: cs.CL

TL;DR: The paper introduces ImplicitBBQ, an extended benchmark for detecting implicit biases in large language models (LLMs), revealing performance disparities missed by explicit fairness benchmarks.


<details>
  <summary>Details</summary>
Motivation: Fairness evaluations of LLMs often overlook implicit biases, typically present in real-world scenarios, making it essential to address this critical gap.

Method: The authors developed ImplicitBBQ, a benchmark extending the Bias Benchmark for QA (BBQ) by introducing implicitly cued protected attributes across six categories.

Result: The evaluation of GPT-4o using ImplicitBBQ uncovered performance declines, including accuracy reductions up to 7% in the 'sexual orientation' category and consistent performance dips across other categories, confirming the presence of implicit biases.

Conclusion: ImplicitBBQ provides a key tool for better understanding and addressing implicit biases in LLMs for enhanced fairness evaluation.

Abstract: Existing benchmarks evaluating biases in large language models (LLMs) primarily rely on explicit cues, declaring protected attributes like religion, race, gender by name. However, real-world interactions often contain implicit biases, inferred subtly through names, cultural cues, or traits. This critical oversight creates a significant blind spot in fairness evaluation. We introduce ImplicitBBQ, a benchmark extending the Bias Benchmark for QA (BBQ) with implicitly cued protected attributes across 6 categories. Our evaluation of GPT-4o on ImplicitBBQ illustrates troubling performance disparity from explicit BBQ prompts, with accuracy declining up to 7% in the "sexual orientation" subcategory and consistent decline located across most other categories. This indicates that current LLMs contain implicit biases undetected by explicit benchmarks. ImplicitBBQ offers a crucial tool for nuanced fairness evaluation in NLP.

</details>


### [74] [A Patient-Doctor-NLP-System to contest inequality for less privileged](https://arxiv.org/abs/2512.06734)
*Subrit Dikshit,Ritu Tiwari,Priyank Jain*

Main category: cs.CL

TL;DR: The paper introduces PDFTEMRA, a compact NLP model designed for resource-constrained healthcare scenarios, focusing on Hindi and visually impaired users, delivering comparable performance with lower computational needs.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the lack of medical NLP resources for visually impaired individuals and low-resource language speakers, particularly in rural healthcare settings.

Method: PDFTEMRA combines model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns to create a compact and efficient transformer-based architecture.

Result: PDFTEMRA shows comparable performance to standard NLP models while significantly reducing computational requirements, using healthcare datasets for Hindi and accessibility-focused tasks.

Conclusion: PDFTEMRA presents a promising solution for accessible, inclusive, and resource-efficient NLP applications in medical contexts, particularly benefiting underserved communities.

Abstract: Transfer Learning (TL) has accelerated the rapid development and availability of large language models (LLMs) for mainstream natural language processing (NLP) use cases. However, training and deploying such gigantic LLMs in resource-constrained, real-world healthcare situations remains challenging. This study addresses the limited support available to visually impaired users and speakers of low-resource languages such as Hindi who require medical assistance in rural environments. We propose PDFTEMRA (Performant Distilled Frequency Transformer Ensemble Model with Random Activations), a compact transformer-based architecture that integrates model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns to reduce computational cost while preserving language understanding performance. The model is trained and evaluated on medical question-answering and consultation datasets tailored to Hindi and accessibility scenarios, and its performance is compared against standard NLP state-of-the-art model baselines. Results demonstrate that PDFTEMRA achieves comparable performance with substantially lower computational requirements, indicating its suitability for accessible, inclusive, low-resource medical NLP applications.

</details>


### [75] [One Word Is Not Enough: Simple Prompts Improve Word Embeddings](https://arxiv.org/abs/2512.06744)
*Rajeev Ranjan*

Main category: cs.CL

TL;DR: By applying semantic prompts to isolated words, researchers showed improved word similarity correlations for text embedding models across benchmarks, outperforming classical embedding methods.


<details>
  <summary>Details</summary>
Motivation: To enhance the understanding of text embedding models' behavior on isolated words and improve their performance on word similarity tasks.

Method: Semantic prompts like 'meaning: {word}' are prepended before embedding isolated words, tested across 7 text embedding models and evaluated on benchmarks SimLex-999, WordSim-353, and MEN-3000.

Result: Prompts improved Spearman correlation scores by up to +0.29, with some models recovering performance from correlation = 0 to significant improvements (+0.73). Best scores achieved: 0.692 (SimLex-999), 0.811 (WordSim-353), and 0.855 (MEN-3000), surpassing classical embeddings like Word2Vec and LexVec.

Conclusion: Semantic prompting is an effective, zero-shot approach to improve text embedding models' performance on isolated words, setting a new state-of-the-art for embedding-based word similarity evaluations.

Abstract: Text embedding models are designed for sentence-level applications like retrieval and semantic similarity, and are primarily evaluated on sentence-level benchmarks. Their behavior on isolated words is less understood. We show that simply prepending semantic prompts to words before embedding substantially improves word similarity correlations. Testing 7 text embedding models, including text-embedding-3-large (OpenAI), embed-english-v3.0 (Cohere), voyage-3(Voyage AI), all-mpnet-base-v2, and Qwen3-Embedding-8B, on 3 standard benchmarks (SimLex-999, WordSim-353, MEN-3000), we find that prompts like "meaning: {word}" or "Represent the semantic concept: {word}" improve Spearman correlations by up to +0.29 on SimLex-999. Some models fail completely on bare words (correlation = 0) but recover with prompts (+0.73 improvement). Our best results achieve correlation = 0.692 on SimLex-999 with embed-english-v3.0 (Cohere), correlation = 0.811 on WordSim-353, and correlation = 0.855 on MEN-3000 with text-embedding-3-large (OpenAI). These results outperform classic static embeddings like Word2Vec (correlation = 0.40) and even the best static method LexVec (correlation = 0.48) on SimLex-999, establishing a new state-of-the-art for pure embedding methods. This zero-shot technique requires no training and works with any text embedding model.

</details>


### [76] [Becoming Experienced Judges: Selective Test-Time Learning for Evaluators](https://arxiv.org/abs/2512.06751)
*Seungyeon Jwa,Daechul Ahn,Reokyoung Kim,Dongyeop Kang,Jonghyun Choi*

Main category: cs.CL

TL;DR: The paper introduces Learning While Evaluating (LWE), a framework that enhances LLM evaluators during testing by sequentially learning from cases without additional training. It uses adaptive meta-prompts, with a selective approach to focus on difficult cases.


<details>
  <summary>Details</summary>
Motivation: LLM-as-a-judge evaluation methods treat cases independently and rely on fixed prompts, missing opportunities for sequential improvement and case-specific criteria.

Method: The authors propose LWE, which refines a meta-prompt through self-generated feedback and provides dynamic evaluation instructions. They also propose Selective LWE, focusing updates on challenging, self-inconsistent cases to improve efficiency.

Result: Selective LWE outperforms baseline methods in pairwise comparison benchmarks, demonstrating enhanced evaluator performance by learning from difficult cases during inference.

Conclusion: This approach shows that evaluators can incrementally improve during inference by leveraging sequential learning and selective updates, enhancing efficiency and effectiveness.

Abstract: Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.

</details>


### [77] [From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs](https://arxiv.org/abs/2512.06776)
*Yuchuan Tian,Yuchen Liang,Jiacheng Sun,Shuo Zhang,Guangwen Yang,Yingte Shu,Sibo Fang,Tianyu Guo,Kai Han,Chao Xu,Hanting Chen,Xinghao Chen,Yunhe Wang*

Main category: cs.CL

TL;DR: This paper introduces NBDiff-7B, which adapts autoregressive (AR) models to diffusion language models (DLMs) using a systematic methodology, achieving high performance on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the sequential inefficiency of autoregressive decoding in large language models (LLMs) and to explore a compute-efficient alternative to training diffusion language models (DLMs) from scratch.

Method: Proposed a pathway for AR-to-Block-Diffusion adaptation using a context-causal attention mask, efficient adaptation procedures, auxiliary AR loss, and a gradual increment in generation block size, ensuring performance consistency.

Result: NBDiff-7B models inherited AR model capabilities like long-context reasoning, achieving state-of-the-art performance among 7B-class DLMs across general-knowledge, math, and code benchmarks.

Conclusion: Principled adaptation from autoregressive models to block-diffusion is effective and computationally efficient, offering a robust alternative to direct DLM training from scratch.

Abstract: Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior "adaptation" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.

</details>


### [78] [LLM4SFC: Sequential Function Chart Generation via Large Language Models](https://arxiv.org/abs/2512.06787)
*Ofek Glick,Vladimir Tchuiev,Marah Ghoummaid,Michal Moshkovitz,Dotan Di-Castro*

Main category: cs.CL

TL;DR: The paper introduces LLM4SFC, a framework for generating executable Sequential Function Charts (SFCs) from natural language descriptions, achieving 75%-94% success in generation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of effective solutions for generating graphical Sequential Function Charts (SFCs) from natural language, as current methods generate non-executable and industrially incompatible outputs.

Method: Three main components: reduced structured representation for capturing topology and inline ST; fine-tuning and retrieval-augmented generation (RAG) to align with SFC programming standards; structured generation that prunes illegal tokens in real-time.

Result: LLM4SFC achieves a high success rate (75%-94%) in generating syntactically valid and executable SFCs from industrial workflow descriptions using both open-source and proprietary language models.

Conclusion: LLM4SFC demonstrates that natural language can effectively be translated into graphical SFCs using structured generation, opening up possibilities for automating industrial programming.

Abstract: While Large Language Models (LLMs) are increasingly used for synthesizing textual PLC programming languages like Structured Text (ST) code, other IEC 61131-3 standard graphical languages like Sequential Function Charts (SFCs) remain underexplored. Generating SFCs is challenging due to graphical nature and ST actions embedded within, which are not directly compatible with standard generation techniques, often leading to non-executable code that is incompatible with industrial tool-chains In this work, we introduce LLM4SFC, the first framework to receive natural-language descriptions of industrial workflows and provide executable SFCs. LLM4SFC is based on three components: (i) A reduced structured representation that captures essential topology and in-line ST and reduced textual verbosity; (ii) Fine-tuning and few-shot retrieval-augmented generation (RAG) for alignment with SFC programming conventions; and (iii) A structured generation approach that prunes illegal tokens in real-time to ensure compliance with the textual format of SFCs. We evaluate LLM4SFC on a dataset of real-world SFCs from automated manufacturing projects, using both open-source and proprietary LLMs. The results show that LLM4SFC reliably generates syntactically valid SFC programs effectively bridging graphical and textual PLC languages, achieving a generation generation success of 75% - 94%, paving the way for automated industrial programming.

</details>


### [79] [Large Language Model-Based Generation of Discharge Summaries](https://arxiv.org/abs/2512.06812)
*Tiago Rodrigues,Carla Teixeira Lopes*

Main category: cs.CL

TL;DR: The paper explores utilizing Large Language Models (LLMs) to generate automated discharge summaries, comparing both open-source and proprietary systems. Proprietary models, especially Gemini, outperform others, although challenges like hallucinations persist.


<details>
  <summary>Details</summary>
Motivation: The motivation is to automate discharge summary generation, reducing healthcare professionals' workload, minimizing errors, and improving accessibility to critical patient information.

Method: The authors tested five LLMs (Mistral, Llama 2, GPT-3, GPT-4, Gemini) using MIMIC-III datasets. The models were evaluated with several metrics, including exact-match, soft-overlap, and reference-free measures.

Result: The study found that proprietary models, especially Gemini using one-shot prompting, achieved the best results in generating summaries highly similar to the gold-standard ones. Open-source models required fine-tuning but still struggled with hallucinations and redundancy.

Conclusion: Proprietary LLMs like Gemini show significant potential for automated discharge summary generation, but challenges like hallucinations and data privacy concerns must be addressed.

Abstract: Discharge Summaries are documents written by medical professionals that detail a patient's visit to a care facility. They contain a wealth of information crucial for patient care, and automating their generation could significantly reduce the effort required from healthcare professionals, minimize errors, and ensure that critical patient information is easily accessible and actionable. In this work, we explore the use of five Large Language Models on this task, from open-source models (Mistral, Llama 2) to proprietary systems (GPT-3, GPT-4, Gemini 1.5 Pro), leveraging MIMIC-III summaries and notes. We evaluate them using exact-match, soft-overlap, and reference-free metrics. Our results show that proprietary models, particularly Gemini with one-shot prompting, outperformed others, producing summaries with the highest similarity to the gold-standard ones. Open-source models, while promising, especially Mistral after fine-tuning, lagged in performance, often struggling with hallucinations and repeated information. Human evaluation by a clinical expert confirmed the practical utility of the summaries generated by proprietary models. Despite the challenges, such as hallucinations and missing information, the findings suggest that LLMs, especially proprietary models, are promising candidates for automatic discharge summary generation as long as data privacy is ensured.

</details>


### [80] [CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation](https://arxiv.org/abs/2512.06814)
*Dibyanayan Bandyopadhyay,Soham Bhattacharjee,Mohammed Hasanuzzaman,Asif Ekbal*

Main category: cs.CL

TL;DR: The paper introduces CAuSE, a framework for generating faithful natural language explanations for multimodal classifiers, emphasizing its theoretical and empirical strengths.


<details>
  <summary>Details</summary>
Motivation: The opacity of multimodal classifiers and the need for intuitive, faithful explanations like natural language explanations (NLEs) to build trust motivate this work.

Method: The authors propose the CAuSE framework, which utilizes interchange interventions to establish causal abstraction of classifiers and validates performance using a redesigned causal faithfulness metric.

Result: CAuSE outperforms existing methods in causal faithfulness, demonstrates generalization across datasets and models, and provides qualitative and error analysis.

Conclusion: CAuSE is effective in generating faithful NLEs, addressing shortcomings of other methods, and the findings are backed by theoretical, empirical, and qualitative validations.

Abstract: Multimodal classifiers function as opaque black box models. While several techniques exist to interpret their predictions, very few of them are as intuitive and accessible as natural language explanations (NLEs). To build trust, such explanations must faithfully capture the classifier's internal decision making behavior, a property known as faithfulness. In this paper, we propose CAuSE (Causal Abstraction under Simulated Explanations), a novel framework to generate faithful NLEs for any pretrained multimodal classifier. We demonstrate that CAuSE generalizes across datasets and models through extensive empirical evaluations. Theoretically, we show that CAuSE, trained via interchange intervention, forms a causal abstraction of the underlying classifier. We further validate this through a redesigned metric for measuring causal faithfulness in multimodal settings. CAuSE surpasses other methods on this metric, with qualitative analysis reinforcing its advantages. We perform detailed error analysis to pinpoint the failure cases of CAuSE. For replicability, we make the codes available at https://github.com/newcodevelop/CAuSE

</details>


### [81] [AquaFusionNet: Lightweight VisionSensor Fusion Framework for Real-Time Pathogen Detection and Water Quality Anomaly Prediction on Edge Devices](https://arxiv.org/abs/2512.06848)
*Sepyan Purnama Kristanto,Lutfi Hakim,Hermansyah*

Main category: cs.CL

TL;DR: AquaFusionNet is a lightweight cross-modal framework designed to improve microbial contamination detection in small-scale water systems, unifying microscopic imaging and physicochemical sensor data for real-time monitoring.


<details>
  <summary>Details</summary>
Motivation: Current tools for monitoring drinking water quality in low and middle income regions are insufficient as they only capture fragments of rapid microbial contamination fluctuations. There is a need for unified and reliable real-time decision-making tools.

Method: AquaFusionNet incorporates a gated cross-attention mechanism to integrate microbial appearance data from microscopic imaging and sensor-based water chemistry data into a single model that operates efficiently on low-power hardware like the Jetson Nano.

Result: The system achieved 94.8% mAP@0.5 for contamination detection and 96.3% for anomaly prediction during a six-month deployment across seven facilities in East Java, processing 1.84 million frames while consuming only 4.8 W of power.

Conclusion: AquaFusionNet outperforms unimodal detectors in accuracy and reliability under challenging conditions, offering a replicable and adaptable solution for decentralized water safety infrastructures.

Abstract: Evidence from many low and middle income regions shows that microbial contamination in small scale drinking water systems often fluctuates rapidly, yet existing monitoring tools capture only fragments of this behaviour. Microscopic imaging provides organism level visibility, whereas physicochemical sensors reveal shortterm changes in water chemistry; in practice, operators must interpret these streams separately, making realtime decision-making unreliable. This study introduces AquaFusionNet, a lightweight cross-modal framework that unifies both information sources inside a single edge deployable model. Unlike prior work that treats microscopic detection and water quality prediction as independent tasks, AquaFusionNet learns the statistical dependencies between microbial appearance and concurrent sensor dynamics through a gated crossattention mechanism designed specifically for lowpower hardware. The framework is trained on AquaMicro12K, a new dataset comprising 12,846 annotated 1000 micrographs curated for drinking water contexts, an area where publicly accessible microscopic datasets are scarce. Deployed for six months across seven facilities in East Java, Indonesia, the system processed 1.84 million frames and consistently detected contamination events with 94.8% mAP@0.5 and 96.3% anomaly prediction accuracy, while operating at 4.8 W on a Jetson Nano. Comparative experiments against representative lightweight detectors show that AquaFusionNet provides higher accuracy at comparable or lower power, and field results indicate that cross-modal coupling reduces common failure modes of unimodal detectors, particularly under fouling, turbidity spikes, and inconsistent illumination. All models, data, and hardware designs are released openly to facilitate replication and adaptation in decentralized water safety infrastructures.

</details>


### [82] [Rhea: Role-aware Heuristic Episodic Attention for Conversational LLMs](https://arxiv.org/abs/2512.06869)
*Wanyang Hong,Zhaoning Zhang,Yi Chen,Libo Zhang,Baihui Liu,Linbo Qiao,Zhiliang Tian,Dongsheng Li*

Main category: cs.CL

TL;DR: The paper focuses on resolving the problem of contextual degradation in LLMs during multi-turn conversations by introducing a framework named Rhea, which improves accuracy and instruction consistency.


<details>
  <summary>Details</summary>
Motivation: To address the progressive decline in performance of LLMs during multi-turn conversations, caused by issues such as attention pollution, dilution, and drift.

Method: It introduces Rhea, a framework with two distinct memory modules: Instructional Memory (IM) for global constraints and Episodic Memory (EM) for user interactions. Rhea uses a priority attention mechanism to ensure high-quality context retrieval during inference.

Result: Rhea improves conversational accuracy by 1.04 points (a 16% relative gain over baselines) and achieves near-perfect instruction fidelity (IAR > 8.1) in long interactions, as validated by benchmarks such as MT-Eval and Long-MT-Bench+.

Conclusion: Rhea effectively addresses the challenges of contextual decay in multi-turn interactions, providing a scalable and instruction-consistent solution for conversational LLMs.

Abstract: Large Language Models (LLMs) have achieved remarkable performance on single-turn tasks, yet their effectiveness deteriorates in multi-turn conversations. We define this phenomenon as cumulative contextual decay - a progressive degradation of contextual integrity caused by attention pollution, dilution, and drift. To address this challenge, we propose Rhea (Role-aware Heuristic Episodic Attention), a novel framework that decouples conversation history into two functionally independent memory modules: (1) an Instructional Memory (IM) that persistently stores high-fidelity global constraints via a structural priority mechanism, and (2) an Episodic Memory (EM) that dynamically manages user-model interactions via asymmetric noise control and heuristic context retrieval. During inference, Rhea constructs a high signal-to-noise context by applying its priority attention: selectively integrating relevant episodic information while always prioritizing global instructions. To validate this approach, experiments on multiple multi-turn conversation benchmarks - including MT-Eval and Long-MT-Bench+ - show that Rhea mitigates performance decay and improves overall accuracy by 1.04 points on a 10-point scale (a 16% relative gain over strong baselines). Moreover, Rhea maintains near-perfect instruction fidelity (IAR > 8.1) across long-horizon interactions. These results demonstrate that Rhea provides a principled and effective framework for building more precise, instruction-consistent conversational LLMs.

</details>


### [83] [An Analysis of Large Language Models for Simulating User Responses in Surveys](https://arxiv.org/abs/2512.06874)
*Ziyun Yu,Yiru Zhou,Chen Zhao,Hongyi Wen*

Main category: cs.CL

TL;DR: The paper evaluates the ability of large language models (LLMs) to simulate diverse user responses in surveys and introduces CLAIMSIM for claim diversification, finding limitations in adaptability across demographic features.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the biases in large language models (LLMs), particularly those trained with reinforcement learning from human feedback (RLHF), which often exhibit dominant cultural biases, impeding their ability to represent diverse user demographics.

Method: The authors examine direct and chain-of-thought prompting techniques to simulate survey responses and introduce CLAIMSIM, a claim diversification method utilizing LLMs' parametric knowledge for generating diverse viewpoints.

Result: Experiments reveal that CLAIMSIM enhances response diversity. However, both techniques fail to effectively simulate users due to fixed viewpoints and limited adaptability to demographic variations.

Conclusion: LLMs face challenges in accurately representing diverse user profiles and reasoning through nuanced demographic differences, which constrains their utility for simulating varied human opinions.

Abstract: Using Large Language Models (LLMs) to simulate user opinions has received growing attention. Yet LLMs, especially trained with reinforcement learning from human feedback (RLHF), are known to exhibit biases toward dominant viewpoints, raising concerns about their ability to represent users from diverse demographic and cultural backgrounds. In this work, we examine the extent to which LLMs can simulate human responses to cross-domain survey questions through direct prompting and chain-of-thought prompting. We further propose a claim diversification method CLAIMSIM, which elicits viewpoints from LLM parametric knowledge as contextual input. Experiments on the survey question answering task indicate that, while CLAIMSIM produces more diverse responses, both approaches struggle to accurately simulate users. Further analysis reveals two key limitations: (1) LLMs tend to maintain fixed viewpoints across varying demographic features, and generate single-perspective claims; and (2) when presented with conflicting claims, LLMs struggle to reason over nuanced differences among demographic features, limiting their ability to adapt responses to specific user profiles.

</details>


### [84] [Automated PRO-CTCAE Symptom Selection based on Prior Adverse Event Profiles](https://arxiv.org/abs/2512.06919)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla*

Main category: cs.CL

TL;DR: The paper introduces an automated method to select a minimal yet comprehensive subset of PRO-CTCAE items for capturing adverse events in oncology trials, optimizing balance between patient burden and signal coverage.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of selecting an appropriate number of PRO-CTCAE items in oncology studies, ensuring neither patient burden nor missing of important safety signals while leveraging historical safety data.

Method: The method involves mapping candidate PRO-CTCAE symptoms to MedDRA Preferred Terms (PTs), encoding them into a semantic space called Safeterm, and using a utility function combining relevance and incidence. Spectral analysis is then applied to optimize relevance and diversity, rank-ordering symptoms and suggesting a cut-off threshold.

Result: The proposed tool performs effectively in simulations and oncology case studies, demonstrating its ability to streamline and objectively design PRO-CTCAE subsets.

Conclusion: This automated approach offers an efficient and reproducible method to select PRO-CTCAE items, improving adverse event detection while reducing burden on patients.

Abstract: The PRO-CTCAE is an NCI-developed patient-reported outcome system for capturing symptomatic adverse events in oncology trials. It comprises a large library drawn from the CTCAE vocabulary, and item selection for a given trial is typically guided by expected toxicity profiles from prior data. Selecting too many PRO-CTCAE items can burden patients and reduce compliance, while too few may miss important safety signals. We present an automated method to select a minimal yet comprehensive PRO-CTCAE subset based on historical safety data. Each candidate PRO-CTCAE symptom term is first mapped to its corresponding MedDRA Preferred Terms (PTs), which are then encoded into Safeterm, a high-dimensional semantic space capturing clinical and contextual diversity in MedDRA terminology. We score each candidate PRO item for relevance to the historical list of adverse event PTs and combine relevance and incidence into a utility function. Spectral analysis is then applied to the combined utility and diversity matrix to identify an orthogonal set of medical concepts that balances relevance and diversity. Symptoms are rank-ordered by importance, and a cut-off is suggested based on the explained information. The tool is implemented as part of the Safeterm trial-safety app. We evaluate its performance using simulations and oncology case studies in which PRO-CTCAE was employed. This automated approach can streamline PRO-CTCAE design by leveraging MedDRA semantics and historical data, providing an objective and reproducible method to balance signal coverage against patient burden.

</details>


### [85] [Large Language Models and Forensic Linguistics: Navigating Opportunities and Threats in the Age of Generative AI](https://arxiv.org/abs/2512.06922)
*George Mikros*

Main category: cs.CL

TL;DR: This paper highlights challenges and methodological updates required in forensic linguistics due to the introduction of large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: To address the dual impact of large language models as both enabling tools for forensic analysis and disruptors of foundational linguistic assumptions.

Method: Analysis of current AI-text detection methods and their limitations, with a focus on how existing frameworks such as Daubert and Kumho Tire can guide acceptable forensic practices.

Result: LLMs exhibit detectable differences from human-produced writing, but existing AI-detection methods face high error rates and adversarial vulnerabilities.

Conclusion: Forensic linguistics must adapt to emerging AI capabilities with improved hybrid methodologies, explainable detections, and robust validation to maintain legal and scientific credibility.

Abstract: Large language models (LLMs) present a dual challenge for forensic linguistics. They serve as powerful analytical tools enabling scalable corpus analysis and embedding-based authorship attribution, while simultaneously destabilising foundational assumptions about idiolect through style mimicry, authorship obfuscation, and the proliferation of synthetic texts. Recent stylometric research indicates that LLMs can approximate surface stylistic features yet exhibit detectable differences from human writers, a tension with significant forensic implications. However, current AI-text detection techniques, whether classifier-based, stylometric, or watermarking approaches, face substantial limitations: high false positive rates for non-native English writers and vulnerability to adversarial strategies such as homoglyph substitution. These uncertainties raise concerns under legal admissibility standards, particularly the Daubert and Kumho Tire frameworks. The article concludes that forensic linguistics requires methodological reconfiguration to remain scientifically credible and legally admissible. Proposed adaptations include hybrid human-AI workflows, explainable detection paradigms beyond binary classification, and validation regimes measuring error and bias across diverse populations. The discipline's core insight, i.e., that language reveals information about its producer, remains valid but must accommodate increasingly complex chains of human and machine authorship.

</details>


### [86] [XAM: Interactive Explainability for Authorship Attribution Models](https://arxiv.org/abs/2512.06924)
*Milad Alshomary,Anisha Bhatnagar,Peter Zeng,Smaranda Muresan,Owen Rambow,Kathleen McKeown*

Main category: cs.CL

TL;DR: IXAM provides an interactive framework for exploring and explaining authorship attribution model predictions using embedding spaces and stylistic features.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations in comprehending how authorship attribution models make predictions by providing an interactive and explainable tool.

Method: The authors develop IXAM, which integrates embedding-based AA models with a user interface for interactively inspecting writing style features to explain predictions.

Result: User evaluation shows that IXAM offers greater value and usability compared to static, predefined stylistic explanations for AA model predictions.

Conclusion: Interactive frameworks like IXAM improve understanding of authorship attribution models and enhance interpretability through user-friendly tools for exploring stylistic features.

Abstract: We present IXAM, an Interactive eXplainability framework for Authorship Attribution Models. Given an authorship attribution (AA) task and an embedding-based AA model, our tool enables users to interactively explore the model's embedding space and construct an explanation of the model's prediction as a set of writing style features at different levels of granularity. Through a user evaluation, we demonstrate the value of our framework compared to predefined stylistic explanations.

</details>


### [87] [Bridging Code Graphs and Large Language Models for Better Code Understanding](https://arxiv.org/abs/2512.07666)
*Zeqi Chen,Zhaoyang Chu,Yi Gui,Feng Guo,Yao Wan,Chuan Shi*

Main category: cs.CL

TL;DR: CGBridge enhances large language models' (LLMs) performance on code intelligence tasks by integrating structural code semantics through a self-supervised code graph encoder and a bridge module.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle to understand the structural semantics of programs due to reliance on linearized token sequences. Existing approaches either face prompt length constraints or require task-specific model modifications, limiting effectiveness.

Method: The CGBridge method involves pretraining a code graph encoder on 270K code graphs to learn semantics, bridging the gap between code, graph, and text with a trainable external module, and generating structure-informed prompts for frozen LLMs.

Result: CGBridge achieves a 16.19% and 9.12% improved LLM performance on code summarization, and 9.84% and 38.87% improved accuracy on code translation compared to prior methods. It also demonstrates over 4x faster inference than LoRA methods.

Conclusion: CGBridge is a plug-and-play method that significantly improves LLMs' ability to understand structural program semantics while maintaining efficiency and compatibility with existing models.

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.

</details>


### [88] [Progress Ratio Embeddings: An Impatience Signal for Robust Length Control in Neural Text Generation](https://arxiv.org/abs/2512.06938)
*Ivanhoé Botcazou,Tassadit Amghar,Sylvain Lamprier,Frédéric Saubion*

Main category: cs.CL

TL;DR: The paper identifies limitations in existing length control methods for neural language models and proposes a new approach, Progress Ratio Embeddings (PRE), which enhances stability and generalization.


<details>
  <summary>Details</summary>
Motivation: Despite neural language models' high accuracy, effective length control during text generation remains an unsolved challenge, motivating the investigation of new methods.

Method: The authors critique Reverse Positional Embeddings (RPE), propose Progress Ratio Embeddings (PRE) based on trigonometric impatience functions, and integrate them into Transformer architectures for improved length control.

Result: PRE achieves robust control over text length, performs well under standard evaluation metrics, and generalizes effectively to unseen target lengths.

Conclusion: Progress Ratio Embeddings provide a robust solution to length control in text generation, improving upon existing methods without compromising quality, as validated on news summarization datasets.

Abstract: Modern neural language models achieve high accuracy in text generation, yet precise control over generation length remains underdeveloped. In this paper, we first investigate a recent length control method based on Reverse Positional Embeddings (RPE) and show its limits when control is requested beyond the training distribution. In particular, using a discrete countdown signal tied to the absolute remaining token count leads to instability. To provide robust length control, we introduce Progress Ratio Embeddings (PRE), as continuous embeddings tied to a trigonometric impatience signal. PRE integrates seamlessly into standard Transformer architectures, providing stable length fidelity without degrading text accuracy under standard evaluation metrics. We further show that PRE generalizes well to unseen target lengths. Experiments on two widely used news-summarization benchmarks validate these findings.

</details>


### [89] [Prompting-in-a-Series: Psychology-Informed Contents and Embeddings for Personality Recognition With Decoder-Only Models](https://arxiv.org/abs/2512.06991)
*Jing Jie Tan,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum,Anissa Mokraoui,Shih-Yu Lo*

Main category: cs.CL

TL;DR: This paper introduces PICEPR, a modular algorithm for personality recognition using LLMs, achieving state-of-the-art performance with a 5-15% improvement.


<details>
  <summary>Details</summary>
Motivation: To enhance personality recognition through a novel algorithm leveraging the capabilities of large language models in content generation and classification.

Method: A modular decoder-only LLM was employed, integrating two pipelines for content (summarization/generation) and embeddings (extraction). Various models were tested for quality comparison, and experiments supported the approach.

Result: PICEPR algorithm outperformed previous benchmarks in personality recognition by achieving a 5-15% improvement.

Conclusion: The PICEPR algorithm proves highly effective for personality recognition through modularized LLM use, with open access provided for further development.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. This research introduces a novel "Prompting-in-a-Series" algorithm, termed PICEPR (Psychology-Informed Contents Embeddings for Personality Recognition), featuring two pipelines: (a) Contents and (b) Embeddings. The approach demonstrates how a modularised decoder-only LLM can summarize or generate content, which can aid in classifying or enhancing personality recognition functions as a personality feature extractor and a generator for personality-rich content. We conducted various experiments to provide evidence to justify the rationale behind the PICEPR algorithm. Meanwhile, we also explored closed-source models such as \textit{gpt4o} from OpenAI and \textit{gemini} from Google, along with open-source models like \textit{mistral} from Mistral AI, to compare the quality of the generated content. The PICEPR algorithm has achieved a new state-of-the-art performance for personality recognition by 5-15\% improvement. The work repository and models' weight can be found at https://research.jingjietan.com/?q=PICEPR.

</details>


### [90] [FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations](https://arxiv.org/abs/2512.07015)
*Mayank Ravishankara*

Main category: cs.CL

TL;DR: The paper presents FVA-RAG, a new framework aiming to address the issue of Retrieval Sycophancy in RAG systems by introducing adversarial retrieval policies for more factually grounded outputs.


<details>
  <summary>Details</summary>
Motivation: Standard RAG systems are prone to Retrieval Sycophancy, wherein they retrieve biased information aligned with user premises, even if false or based on misconceptions. This leads to LLMs producing misleading answers with improper citations, necessitating more robust retrieval methods.

Method: The paper proposes FVA-RAG, which adopts a Deductive Falsification approach instead of mere inductive verification. It uses Adversarial Retrieval Policy to generate 'Kill Queries,' surfacing contradictory evidence, and evaluates draft outputs against this anti-context through a dual-verification mechanism.

Result: Initial experiments show that FVA-RAG improves LLM robustness, particularly in correcting false premises or misconceptions through enhanced retrieval and verification mechanisms.

Conclusion: FVA-RAG successfully mitigates sycophantic hallucinations in RAG systems by integrating adversarial retrieval strategies and providing a more reliable framework for factual text generation.

Abstract: Retrieval-Augmented Generation (RAG) systems have significantly reduced hallucinations in Large Language Models (LLMs) by grounding responses in external context. However, standard RAG architectures suffer from a critical vulnerability: Retrieval Sycophancy. When presented with a query based on a false premise or a common misconception, vector-based retrievers tend to fetch documents that align with the user's bias rather than objective truth, leading the model to "hallucinate with citations."
  In this work, we introduce Falsification-Verification Alignment RAG (FVA-RAG), a framework that shifts the retrieval paradigm from Inductive Verification (seeking support) to Deductive Falsification (seeking disproof). Unlike existing "Self-Correction" methods that rely on internal consistency, FVA-RAG deploys a distinct Adversarial Retrieval Policy that actively generates "Kill Queries"-targeted search terms designed to surface contradictory evidence. We introduce a dual-verification mechanism that explicitly weighs the draft answer against this "Anti-Context." Preliminary experiments on a dataset of common misconceptions demonstrate that FVA-RAG significantly improves robustness against sycophantic hallucinations compared to standard RAG baselines, effectively acting as an inference-time "Red Team" for factual generation.

</details>


### [91] [Replicating TEMPEST at Scale: Multi-Turn Adversarial Attacks Against Trillion-Parameter Frontier Models](https://arxiv.org/abs/2512.07059)
*Richard Young*

Main category: cs.CL

TL;DR: Large language models were tested for adversarial vulnerability using the TEMPEST multi-turn attack framework. Results showed varied attack success rates among models, highlighting weaknesses in alignment techniques across vendors.


<details>
  <summary>Details</summary>
Motivation: To understand the vulnerability of large language models to sophisticated multi-turn adversarial attacks and explore factors influencing their robustness.

Method: The TEMPEST framework assessed ten frontier models through adversarial conversations spanning 1,000 harmful behaviors, with automated evaluations generating over 97,000 API queries.

Result: Six models showed high vulnerability (96-100% attack success rate), while four exhibited resistance (42-78% ASR). Using deliberative inference reduced ASR on identical architecture from 97% to 42%.

Conclusion: Current alignment techniques are vulnerable to adaptive multi-turn attacks regardless of model scale, but deliberative inference offers a safety enhancement strategy.

Abstract: Despite substantial investment in safety alignment, the vulnerability of large language models to sophisticated multi-turn adversarial attacks remains poorly characterized, and whether model scale or inference mode affects robustness is unknown. This study employed the TEMPEST multi-turn attack framework to evaluate ten frontier models from eight vendors across 1,000 harmful behaviors, generating over 97,000 API queries across adversarial conversations with automated evaluation by independent safety classifiers. Results demonstrated a spectrum of vulnerability: six models achieved 96% to 100% attack success rate (ASR), while four showed meaningful resistance, with ASR ranging from 42% to 78%; enabling extended reasoning on identical architecture reduced ASR from 97% to 42%. These findings indicate that safety alignment quality varies substantially across vendors, that model scale does not predict adversarial robustness, and that thinking mode provides a deployable safety enhancement. Collectively, this work establishes that current alignment techniques remain fundamentally vulnerable to adaptive multi-turn attacks regardless of model scale, while identifying deliberative inference as a promising defense direction.

</details>


### [92] [SETUP: Sentence-level English-To-Uniform Meaning Representation Parser](https://arxiv.org/abs/2512.07068)
*Emma Markle,Javier Gutierrez Bach,Shira Wein*

Main category: cs.CL

TL;DR: The paper introduces two approaches for English text-to-UMR parsing and reports significant advancements in model performance.


<details>
  <summary>Details</summary>
Motivation: The research aims to enable large-scale automatic creation of UMR graphs to unlock downstream applications of UMR in language documentation and interpretability, especially for low-resource languages.

Method: The authors develop two parsing methods: one fine-tunes Abstract Meaning Representation parsers, and the other employs a Universal Dependencies converter, benchmarked against prior work.

Result: Their SETUP model achieves an AnCast score of 84 and SMATCH++ score of 91, showcasing improved accuracy for UMR parsing.

Conclusion: The study marks a substantial step forward in creating effective English text-to-UMR parsers, facilitating broader adoption and exploration of UMR applications.

Abstract: Uniform Meaning Representation (UMR) is a novel graph-based semantic representation which captures the core meaning of a text, with flexibility incorporated into the annotation schema such that the breadth of the world's languages can be annotated (including low-resource languages). While UMR shows promise in enabling language documentation, improving low-resource language technologies, and adding interpretability, the downstream applications of UMR can only be fully explored when text-to-UMR parsers enable the automatic large-scale production of accurate UMR graphs at test time. Prior work on text-to-UMR parsing is limited to date. In this paper, we introduce two methods for English text-to-UMR parsing, one of which fine-tunes existing parsers for Abstract Meaning Representation and the other, which leverages a converter from Universal Dependencies, using prior work as a baseline. Our best-performing model, which we call SETUP, achieves an AnCast score of 84 and a SMATCH++ score of 91, indicating substantial gains towards automatic UMR parsing.

</details>


### [93] [Do Large Language Models Truly Understand Cross-cultural Differences?](https://arxiv.org/abs/2512.07075)
*Shiwei Guo,Sihang Jiang,Qianxi He,Yanghua Xiao,Jiaqing Liang,Bi Yude,Minggui He,Shimin Tao,Li Zhang*

Main category: cs.CL

TL;DR: This paper introduces SAGE, a scenario-based benchmark to assess the cross-cultural understanding and reasoning capabilities of large language models (LLMs), exposing their shortcomings in nuanced cross-cultural tasks.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current benchmarks for evaluating LLMs' cross-cultural understanding, including lack of context, insufficient mapping of cross-cultural concepts, and limited reasoning capabilities.

Method: The authors proposed SAGE, grounded in cultural theory, categorizing cross-cultural understanding into nine dimensions. They developed 4530 test items across 15 scenarios using core concept alignment and generative tasks to evaluate performance.

Result: Experiments reveal weaknesses in LLMs' cross-cultural understanding and reasoning across multiple scenarios and dimensions, highlighting systematic limitations despite some progress.

Conclusion: LLMs have made strides in multilingual tasks but are far from achieving nuanced cross-cultural understanding. The SAGE dataset offers a more in-depth evaluation framework, and future expansions aim to enhance assessment comprehensively.

Abstract: In recent years, large language models (LLMs) have demonstrated strong performance on multilingual tasks. Given its wide range of applications, cross-cultural understanding capability is a crucial competency. However, existing benchmarks for evaluating whether LLMs genuinely possess this capability suffer from three key limitations: a lack of contextual scenarios, insufficient cross-cultural concept mapping, and limited deep cultural reasoning capabilities. To address these gaps, we propose SAGE, a scenario-based benchmark built via cross-cultural core concept alignment and generative task design, to evaluate LLMs' cross-cultural understanding and reasoning. Grounded in cultural theory, we categorize cross-cultural capabilities into nine dimensions. Using this framework, we curated 210 core concepts and constructed 4530 test items across 15 specific real-world scenarios, organized under four broader categories of cross-cultural situations, following established item design principles. The SAGE dataset supports continuous expansion, and experiments confirm its transferability to other languages. It reveals model weaknesses across both dimensions and scenarios, exposing systematic limitations in cross-cultural reasoning. While progress has been made, LLMs are still some distance away from reaching a truly nuanced cross-cultural understanding. In compliance with the anonymity policy, we include data and code in the supplement materials. In future versions, we will make them publicly available online.

</details>


### [94] [Leveraging KV Similarity for Online Structured Pruning in LLMs](https://arxiv.org/abs/2512.07090)
*Jungmin Lee,Gwangeun Byeon,Yulhwa Kim,Seokin Hong*

Main category: cs.CL

TL;DR: Token Filtering introduces a structured pruning technique for large language model inference that skips redundant computations based on token redundancy, ensuring efficient performance without calibration data.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods for large language models often face instability due to reliance on calibration data that may not generalize well across inputs.

Method: Token Filtering employs token redundancy measurement via key-value similarity during online inference and integrates a variance-aware fusion strategy for adaptive weighting across attention heads.

Result: Extensive experiments demonstrate that the Token Filtering approach outperforms existing pruning methods, maintaining accuracy on benchmarks and strong performance on challenging tasks despite high pruning ratios.

Conclusion: Token Filtering is a reliable and memory-efficient pruning method that accelerates inference for large language models by skipping computations for redundant tokens while preserving task accuracy.

Abstract: Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.

</details>


### [95] [DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning](https://arxiv.org/abs/2512.07132)
*Nithin Sivakumaran,Justin Chih-Yao Chen,David Wan,Yue Zhang,Jaehong Yoon,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: DART employs debating agents to determine and integrate visual tools for resolving disagreements, enhancing interaction outcomes across benchmarks.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges of determining and integrating the appropriate specialized visual tools in multi-agent frameworks for improving reasoning and decision-making.

Method: DART uses a multi-agent framework where disagreements among agents trigger the invocation of visual tools, providing additional insights and agreement scoring to aid decisions.

Result: DART outperforms single-agent and multi-agent baselines by achieving significant accuracy improvements across benchmarks, including A-OKVQA (+3.4%) and MMMU (+2.4%), while adapting effectively to applied domains like medical datasets (+1.3%).

Conclusion: The framework demonstrates improved inter-agent discussion richness, effective use of diverse tools, and adaptability, making it a strong approach for multi-agent tool calling systems.

Abstract: Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.

</details>


### [96] [GUMBridge: a Corpus for Varieties of Bridging Anaphora](https://arxiv.org/abs/2512.07134)
*Lauren Levine,Amir Zeldes*

Main category: cs.CL

TL;DR: The paper introduces GUMBridge, a comprehensive resource for studying bridging phenomena in English with 16 diverse genres, and evaluates its annotation quality as well as task difficulty using modern LLMs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing resources for studying bridging anaphora, including their small size, restricted coverage of phenomena, and limited genre diversity.

Method: The paper develops GUMBridge, a dataset featuring granular annotations of bridging subtypes across 16 genres. It evaluates annotation quality and assesses baseline performance using modern LLMs on bridging resolution and subtype classification tasks.

Result: The study demonstrates that bridging phenomena remain challenging tasks for contemporary large language models, highlighting the difficulty of accurately resolving bridging anaphora and conducting subtype classification.

Conclusion: GUMBridge is a novel and diverse resource that provides significant improvements for studying bridging anaphora in English, though NLP systems still struggle with these complex linguistic tasks.

Abstract: Bridging is an anaphoric phenomenon where the referent of an entity in a discourse is dependent on a previous, non-identical entity for interpretation, such as in "There is 'a house'. 'The door' is red," where the door is specifically understood to be the door of the aforementioned house. While there are several existing resources in English for bridging anaphora, most are small, provide limited coverage of the phenomenon, and/or provide limited genre coverage. In this paper, we introduce GUMBridge, a new resource for bridging, which includes 16 diverse genres of English, providing both broad coverage for the phenomenon and granular annotations for the subtype categorization of bridging varieties. We also present an evaluation of annotation quality and report on baseline performance using open and closed source contemporary LLMs on three tasks underlying our data, showing that bridging resolution and subtype classification remain difficult NLP tasks in the age of LLMs.

</details>


### [97] [MASim: Multilingual Agent-Based Simulation for Social Science](https://arxiv.org/abs/2512.07195)
*Xuan Zhang,Wenxuan Zhang,Anxu Wang,See-Kiong Ng,Yang Deng*

Main category: cs.CL

TL;DR: The paper introduces MASim, a multilingual agent-based simulation framework for studying sociocultural behaviors through language with generative agents.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent simulations are predominantly monolingual and lack the ability to simulate cross-lingual interactions, a critical aspect of real-world social behavior.

Method: MASim simulates multilingual interactions using generative agents with sociolinguistic profiles, offering analyses of public opinion modeling and media influence. The MAPS benchmark is designed to evaluate these simulations using demographic personas and survey questions.

Result: Experiments with MASim demonstrate its capability to reproduce sociocultural phenomena and emphasize the value of multilingual simulations for computational social science.

Conclusion: The framework highlights the necessity of multilingual agent-based simulations in better understanding scalable cross-cultural and social interactions.

Abstract: Multi-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an essential property of real societies. We introduce MASim, the first multilingual agent-based simulation framework that supports multi-turn interaction among generative agents with diverse sociolinguistic profiles. MASim offers two key analyses: (i) global public opinion modeling, by simulating how attitudes toward open-domain hypotheses evolve across languages and cultures, and (ii) media influence and information diffusion, via autonomous news agents that dynamically generate content and shape user behavior. To instantiate simulations, we construct the MAPS benchmark, which combines survey questions and demographic personas drawn from global population distributions. Experiments on calibration, sensitivity, consistency, and cultural case studies show that MASim reproduces sociocultural phenomena and highlights the importance of multilingual simulation for scalable, controlled computational social science.

</details>


### [98] [NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models](https://arxiv.org/abs/2512.07218)
*Feng Liang,Weixin Zeng,Runhao Zhao,Xiang Zhao*

Main category: cs.CL

TL;DR: The paper introduces Neuro-Symbolic Temporal Reasoning (NeSTR), a framework to improve temporal reasoning in LLMs by combining symbolic representations and reflective methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges LLMs face with temporal reasoning, particularly under complex temporal constraints, as existing approaches either underutilize reasoning capabilities or lack structural consistency.

Method: The proposed NeSTR framework incorporates symbolic encoding for explicit temporal structures, verification to ensure logical consistency, and abductive reflection to correct flawed reasoning.

Result: NeSTR demonstrates superior zero-shot performance and improved temporal reasoning on diverse benchmarks, without requiring fine-tuning.

Conclusion: Integrating neuro-symbolic methods enhances the temporal understanding capabilities of large language models effectively.

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.

</details>


### [99] [Ensembling LLM-Induced Decision Trees for Explainable and Robust Error Detection](https://arxiv.org/abs/2512.07246)
*Mengqi Wang,Jianwei Wang,Qing Liu,Xiwei Xu,Zhenchang Xing,Liming Zhu,Wenjie Zhang*

Main category: cs.CL

TL;DR: The paper addresses limitations in current error detection methods using large language models (LLMs) by developing a framework (TreeED and ForestED) that improves explainability and robustness.


<details>
  <summary>Details</summary>
Motivation: Current error detection techniques using LLMs lack explainability and robustness, relying heavily on implicit decision-making and being prone to inconsistencies due to sensitivity to prompts.

Method: The paper introduces TreeED, which uses LLMs to induce decision trees with three node types (rule, GNN, and leaf nodes), and ForestED, an ensemble approach combining multiple decision tree outputs for consensus-based detection.

Result: The proposed method is shown to improve explainability and robustness, with experiments yielding an average F1-score improvement of 16.1% compared to existing methods.

Conclusion: The framework provides an effective and explainable solution to error detection, standing out with significant improvements in accuracy and robustness over baseline methods.

Abstract: Error detection (ED), which aims to identify incorrect or inconsistent cell values in tabular data, is important for ensuring data quality. Recent state-of-the-art ED methods leverage the pre-trained knowledge and semantic capability embedded in large language models (LLMs) to directly label whether a cell is erroneous. However, this LLM-as-a-labeler pipeline (1) relies on the black box, implicit decision process, thus failing to provide explainability for the detection results, and (2) is highly sensitive to prompts, yielding inconsistent outputs due to inherent model stochasticity, therefore lacking robustness. To address these limitations, we propose an LLM-as-an-inducer framework that adopts LLM to induce the decision tree for ED (termed TreeED) and further ensembles multiple such trees for consensus detection (termed ForestED), thereby improving explainability and robustness. Specifically, based on prompts derived from data context, decision tree specifications and output requirements, TreeED queries the LLM to induce the decision tree skeleton, whose root-to-leaf decision paths specify the stepwise procedure for evaluating a given sample. Each tree contains three types of nodes: (1) rule nodes that perform simple validation checks (e.g., format or range), (2) Graph Neural Network (GNN) nodes that capture complex patterns (e.g., functional dependencies), and (3) leaf nodes that output the final decision types (error or clean). Furthermore, ForestED employs uncertainty-based sampling to obtain multiple row subsets, constructing a decision tree for each subset using TreeED. It then leverages an Expectation-Maximization-based algorithm that jointly estimates tree reliability and optimizes the consensus ED prediction. Extensive xperiments demonstrate that our methods are accurate, explainable and robust, achieving an average F1-score improvement of 16.1% over the best baseline.

</details>


### [100] [TeluguST-46: A Benchmark Corpus and Comprehensive Evaluation for Telugu-English Speech Translation](https://arxiv.org/abs/2512.07265)
*Bhavana Akkiraju,Srihari Bandarupalli,Swathi Sambangi,Vasavi Ravuri,R Vijaya Saraswathi,Anil Kumar Vuppala*

Main category: cs.CL

TL;DR: This paper develops a Telugu-English speech translation benchmark from 46 hours of CSTD corpus data, comparing cascaded and end-to-end systems, and analyzing metric reliability for performance evaluation.


<details>
  <summary>Details</summary>
Motivation: Despite Telugu having a vast number of speakers, research on speech translation for this morphologically rich language is limited. This study aims to address the gap with a high-quality translation benchmark and insights into system performance in low-resource settings.

Method: The researchers created a benchmark from 46 hours of verified data and compared cascaded (IndicWhisper + IndicMT) versus end-to-end (SeamlessM4T) translation approaches through systematic analysis. They also investigated metric reliability for quality evaluation.

Result: Cascaded models using extensive Telugu-specific data performed best, but end-to-end fine-tuned models also showed competitive results with less data. Metric analysis revealed that traditional metrics perform better than BERTScore for Telugu-English evaluation.

Conclusion: With appropriate tuning and moderate parallel data (< 100 hours), end-to-end systems can match cascaded systems in low-resource settings. The study provides a reproducible benchmark and evaluation guidance for morphologically rich languages.

Abstract: Despite Telugu being spoken by over 80 million people, speech translation research for this morphologically rich language remains severely underexplored. We address this gap by developing a high-quality Telugu--English speech translation benchmark from 46 hours of manually verified CSTD corpus data (30h/8h/8h train/dev/test split). Our systematic comparison of cascaded versus end-to-end architectures shows that while IndicWhisper + IndicMT achieves the highest performance due to extensive Telugu-specific training data, finetuned SeamlessM4T models demonstrate remarkable competitiveness despite using significantly less Telugu-specific training data. This finding suggests that with careful hyperparameter tuning and sufficient parallel data (potentially less than 100 hours), end-to-end systems can achieve performance comparable to cascaded approaches in low-resource settings. Our metric reliability study evaluating BLEU, METEOR, ChrF++, ROUGE-L, TER, and BERTScore against human judgments reveals that traditional metrics provide better quality discrimination than BERTScore for Telugu--English translation. The work delivers three key contributions: a reproducible Telugu--English benchmark, empirical evidence of competitive end-to-end performance potential in low-resource scenarios, and practical guidance for automatic evaluation in morphologically complex language pairs.

</details>


### [101] [Efficient ASR for Low-Resource Languages: Leveraging Cross-Lingual Unlabeled Data](https://arxiv.org/abs/2512.07277)
*Srihari Bandarupalli,Bhavana Akkiraju,Charan Devarakonda,Vamsiraghusimha Narsinga,Anil Kumar Vuppala*

Main category: cs.CL

TL;DR: This paper explores cross-lingual pretraining for automatic speech recognition in low-resource Perso-Arabic languages, achieving competitive accuracy with smaller models and less labeled data.


<details>
  <summary>Details</summary>
Motivation: Develop automatic speech recognition systems for low-resource languages like Persian, Arabic, and Urdu, addressing limited labeled data and high computational demands.

Method: Utilizes a 3,000-hour multilingual unlabeled speech corpus and combines continual pretraining with morphologically-aware tokenization to train a compact 300M parameter model.

Result: Achieved performance comparable to larger models (e.g., Whisper Large v3) on Persian, and competitive results for Arabic and Urdu, demonstrating that strategic pretraining and data relevance are superior to sheer model size.

Conclusion: This work highlights the importance of thoughtful data usage and efficient pretraining methodologies over scaling model size for low-resource automatic speech recognition development.

Abstract: Automatic speech recognition for low-resource languages remains fundamentally constrained by the scarcity of labeled data and computational resources required by state-of-the-art models. We present a systematic investigation into cross-lingual continuous pretraining for low-resource languages, using Perso-Arabic languages (Persian, Arabic, and Urdu) as our primary case study. Our approach demonstrates that strategic utilization of unlabeled speech data can effectively bridge the resource gap without sacrificing recognition accuracy. We construct a 3,000-hour multilingual corpus through a scalable unlabeled data collection pipeline and employ targeted continual pretraining combined with morphologically-aware tokenization to develop a 300M parameter model that achieves performance comparable to systems 5 times larger. Our model outperforms Whisper Large v3 (1.5B parameters) on Persian and achieves competitive results on Arabic and Urdu despite using significantly fewer parameters and substantially less labeled data. These findings challenge the prevailing assumption that ASR quality scales primarily with model size, revealing instead that data relevance and strategic pretraining are more critical factors for low-resource scenarios. This work provides a practical pathway toward inclusive speech technology, enabling effective ASR for underrepresented languages without dependence on massive computational infrastructure or proprietary datasets.

</details>


### [102] [Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models](https://arxiv.org/abs/2512.07288)
*Tomoki Doi,Masaru Isonuma,Hitomi Yanaka*

Main category: cs.CL

TL;DR: The paper explores how to train large language models to improve the faithfulness of their self-explanations across tasks and explanation styles.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of faithfulness in self-explanations of large language models and explore how to improve this faithfulness across different tasks and styles.

Method: The authors use one-word feature attribution-based explanations as pseudo-faithful data for continual learning and evaluate effects across classification tasks and explanation styles.

Result: Training increased faithfulness in self-explanations consistently across explanation styles, tasks, unseen tasks, and multi-word settings.

Conclusion: Training contributes to an overall improvement in models' ability to generate faithful self-explanations across various styles and tasks.

Abstract: Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models' actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.

</details>


### [103] [Multilingual corpora for the study of new concepts in the social sciences and humanities:](https://arxiv.org/abs/2512.07367)
*Revekka Kyriakoglou,Anna Pappa*

Main category: cs.CL

TL;DR: The paper outlines a hybrid method for building a multilingual corpus, focusing on emerging concepts like 'non-technological innovation', with data derived from company websites and reports for use in NLP applications.


<details>
  <summary>Details</summary>
Motivation: To create a multilingual dataset for studying emerging concepts in the HSS, which is both versatile for concept analysis and adaptable for NLP tasks.

Method: The study combines automated data extraction from websites and reports, followed by filtering, segment extraction, annotation, and metadata enrichment to create a multilingual, category-annotated corpus.

Result: A reproducible and expandable resource is developed, including English datasets for machine learning with contextual information for supervised classification.

Conclusion: The methodology supports lexical variability analysis in emerging fields and aids in generating datasets applicable for NLP, enhancing research in HSS.

Abstract: This article presents a hybrid methodology for building a multilingual corpus designed to support the study of emerging concepts in the humanities and social sciences (HSS), illustrated here through the case of ``non-technological innovation''. The corpus relies on two complementary sources: (1) textual content automatically extracted from company websites, cleaned for French and English, and (2) annual reports collected and automatically filtered according to documentary criteria (year, format, duplication). The processing pipeline includes automatic language detection, filtering of non-relevant content, extraction of relevant segments, and enrichment with structural metadata. From this initial corpus, a derived dataset in English is created for machine learning purposes. For each occurrence of a term from the expert lexicon, a contextual block of five sentences is extracted (two preceding and two following the sentence containing the term). Each occurrence is annotated with the thematic category associated with the term, enabling the construction of data suitable for supervised classification tasks. This approach results in a reproducible and extensible resource, suitable both for analyzing lexical variability around emerging concepts and for generating datasets dedicated to natural language processing applications.

</details>


### [104] [Training Language Models to Use Prolog as a Tool](https://arxiv.org/abs/2512.07407)
*Niklas Mellgren,Peter Schneider-Kamp,Lukas Galke Poech*

Main category: cs.CL

TL;DR: The paper investigates improving AI reasoning reliability by fine-tuning models to utilize Prolog for verifiable computations, achieving notable improvements in efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to ensure reliable and verifiable reasoning in AI systems, addressing the prevalent issue of language models generating plausible but incorrect solutions.

Method: The authors use fine-tuning with Group Relative Policy Optimization (GRPO) on a cleaned GSM8K-Prolog-Prover dataset, experimenting with varied prompt structures, reward compositions, and inference protocols.

Result: The fine-tuned 3B model shows zero-shot performance comparable to 7B few-shot baselines, achieving better reliability and generalization through Prolog-grounded reasoning methods.

Conclusion: Grounding language model reasoning in formal verification systems like Prolog enhances the reliability and auditability of AI for safety-critical applications.

Abstract: Ensuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (execution, syntax, semantics, structure), and (iii) inference protocol: single-shot, best-of-N, and two agentic modes where Prolog is invoked internally or independently. Our reinforcement learning approach outperforms supervised fine-tuning, with our 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Our findings reveal that: 1) joint tuning of prompt, reward, and inference shapes program syntax and logic; 2) best-of-N with external Prolog verification maximizes accuracy on GSM8K; 3) agentic inference with internal repair yields superior zero-shot generalization on MMLU-Stem and MMLU-Pro. These results demonstrate that grounding model reasoning in formal verification systems substantially improves reliability and auditability for safety-critical applications. The source code for reproducing our experiments is available under https://github.com/niklasmellgren/grpo-prolog-inference

</details>


### [105] [Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning](https://arxiv.org/abs/2512.07454)
*Amir Mohammad Akhlaghi,Amirhossein Shabani,Mostafa Abdolmaleki,Saeed Reza Kheradpisheh*

Main category: cs.CL

TL;DR: The paper introduces Persian-Phi, a 3.8B parameter language model, as a cost-effective approach to adapt a monolingual English LLM to Persian, proving robust multilingual capabilities do not require massive models.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of high computational costs in training Large Language Models for low-resource languages, aiming to democratize AI and make advanced models accessible for underrepresented languages.

Method: The team adapted Microsoft's Phi-3 Mini to Persian using a novel curriculum learning pipeline, starting with bilingual narratives for embedding alignment. They employed Parameter-Efficient Fine-Tuning (PEFT) for continual pretraining and instruction tuning.

Result: Persian-Phi achieved competitive results on HuggingFace's Open Persian LLM Leaderboard, despite being compact in size, showing scalability to low-resource scenarios.

Conclusion: The findings validate a scalable and resource-efficient framework for extending advanced LLMs to underserved languages, demonstrating that smaller, tailored models can achieve strong performance without significant computational costs.

Abstract: The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique "warm-up" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.

</details>


### [106] [Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning](https://arxiv.org/abs/2512.07461)
*Tong Wu,Yang Liu,Jun Bai,Zixia Jia,Shuyi Zhang,Ziyong Lin,Yanting Wang,Song-Chun Zhu,Zilong Zheng*

Main category: cs.CL

TL;DR: NPR introduces a teacher-free framework enabling LLMs to develop parallel reasoning, showing improved performance and efficiency on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of sequential reasoning in Large Language Models and enable native parallel reasoning for more efficient and scalable decision-making processes.

Method: The paper introduces a self-distilled progressive training paradigm, a PAPO algorithm for branching policy optimization, and a robust NPR Engine for improved parallel RL training.

Result: Performance improvements of up to 24.5% and inference speedups of up to 4.6x were achieved compared to benchmarks using NPR trained on Qwen3-4B.

Conclusion: NPR demonstrates the potential for efficient and scalable parallel reasoning in LLMs, setting a new standard for self-evolving reasoning capabilities.

Abstract: We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.

</details>


### [107] [Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization](https://arxiv.org/abs/2512.07478)
*Zhuoran Zhuang,Ye Chen,Jianghao Su,Chao Luo,Luhui Liu,Xia Zeng*

Main category: cs.CL

TL;DR: The paper proposes two techniques, Progressive Reward Shaping (PRS) and Value-based Sampling Policy Optimization (VSPO), to improve the effectiveness of tool-integrated reasoning in large language models (LLMs). They address challenges like sparse rewards and gradient degradation.


<details>
  <summary>Details</summary>
Motivation: LLMs integrated with tool reasoning struggle with sparse binary rewards and unstable training due to gradient degradation. There is a need for more effective reward structures and training stability to enhance tool interaction and reasoning performance.

Method: The authors introduced PRS, a curriculum-based reward system offering staged and dense feedback, and VSPO, a gradient-optimized policy adjustment mechanism that balances task difficulty and stabilizes training.

Result: PRS outperformed binary reward systems in QA tasks, and VSPO achieved higher stability, faster convergence, and better performance compared to conventional methods like PPO and GRPO.

Conclusion: The combination of PRS and VSPO improved the generalization ability, performance, and training efficiency of LLM-based reasoning agents across QA tasks.

Abstract: Large Language Models (LLMs) empowered with Tool-Integrated Reasoning (TIR) can iteratively plan, call external tools, and integrate returned information to solve complex, long-horizon reasoning tasks. Agentic Reinforcement Learning (Agentic RL) optimizes such models over full tool-interaction trajectories, but two key challenges hinder effectiveness: (1) Sparse, non-instructive rewards, such as binary 0-1 verifiable signals, provide limited guidance for intermediate steps and slow convergence; (2) Gradient degradation in Group Relative Policy Optimization (GRPO), where identical rewards within a rollout group yield zero advantage, reducing sample efficiency and destabilizing training. To address these challenges, we propose two complementary techniques: Progressive Reward Shaping (PRS) and Value-based Sampling Policy Optimization (VSPO). PRS is a curriculum-inspired reward design that introduces dense, stage-wise feedback - encouraging models to first master parseable and properly formatted tool calls, then optimize for factual correctness and answer quality. We instantiate PRS for short-form QA (with a length-aware BLEU to fairly score concise answers) and long-form QA (with LLM-as-a-Judge scoring to prevent reward hacking). VSPO is an enhanced GRPO variant that replaces low-value samples with prompts selected by a task-value metric balancing difficulty and uncertainty, and applies value-smoothing clipping to stabilize gradient updates. Experiments on multiple short-form and long-form QA benchmarks show that PRS consistently outperforms traditional binary rewards, and VSPO achieves superior stability, faster convergence, and higher final performance compared to PPO, GRPO, CISPO, and SFT-only baselines. Together, PRS and VSPO yield LLM-based TIR agents that generalize better across domains.

</details>


### [108] [SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG](https://arxiv.org/abs/2512.07515)
*Pengqian Lu,Jie Lu,Anjin Liu,Guangquan Zhang*

Main category: cs.CL

TL;DR: The paper introduces SPAD to detect hallucinations in Retrieval-Augmented Generation (RAG) by mathematically attributing token probabilities to seven distinct sources and analyzing their influence.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of detecting hallucinations in RAG, which prior approaches fail to adequately solve due to a limited perspective focusing only on conflicts between internal knowledge and retrieved context.

Method: The authors developed SPAD, which attributes token generation probabilities into seven sources and aggregates them by linguistic categories, enabling anomaly detection to identify hallucinations.

Result: SPAD demonstrated state-of-the-art performance in identifying hallucinations in RAG through extensive experimentation.

Conclusion: SPAD provides a comprehensive framework for accurately detecting hallucinations in RAG by considering a broader range of factors during token generation.

Abstract: Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, we mathematically attribute each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the current token. Then, we aggregate these scores by POS tags to quantify how different components drive specific linguistic categories. By identifying anomalies, such as Nouns relying on Final LayerNorm, SPAD effectively detects hallucinations. Extensive experiments demonstrate that SPAD achieves state-of-the-art performance

</details>


### [109] [LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings](https://arxiv.org/abs/2512.07522)
*Sebastian Sztwiertnia,Felix Friedrich,Kristian Kersting,Patrick Schramowski,Björn Deiseroth*

Main category: cs.CL

TL;DR: LIME (Linguistic Metadata Embeddings) enhances language model pre-training efficiency, improves tokenization, and boosts generative task performance by integrating metadata signals into token embeddings.


<details>
  <summary>Details</summary>
Motivation: The availability of high-quality data for training decoder-only language models is limited, and traditional approaches rely on metadata for dataset curation and creation without fully leveraging it as a direct training signal.

Method: The study introduces LIME, enriching token embeddings with metadata reflecting syntax, semantics, and contextual properties. A variant, LIME+1, incorporates shifted metadata for token generation guidance.

Result: LIME adapts up to 56% faster to training data distribution, requires only 0.01% additional parameters with negligible overhead, and enhances language modeling and task performance. LIME+1 boosts reasoning and arithmetic performance by up to 38% and 35%, respectively.

Conclusion: LIME demonstrates the potential of using metadata as a direct training signal, significantly improving language model efficiency and performance across scales, with additional gains in reasoning and arithmetic tasks via LIME+1.

Abstract: Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.

</details>


### [110] [Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs](https://arxiv.org/abs/2512.07525)
*Xiaoran Liu,Yuerong Song,Zhigeng Liu,Zengfeng Huang,Qipeng Guo,Zhaoxiang Liu,Shiguo Lian,Ziwei He,Xipeng Qiu*

Main category: cs.CL

TL;DR: The paper proposes an improvement to Rotary Position Embeddings (RoPE) by incorporating the imaginary component of the complex-valued dot product, enhancing long-context dependency modeling in language models.


<details>
  <summary>Details</summary>
Motivation: Standard RoPE implementations discard the imaginary component of the complex-valued dot product, which contains critical phase information for modeling sequence relationships in long contexts.

Method: The approach reintroduces the imaginary component of the complex-valued dot product into the attention score calculation, preserving more positional information for better long-context modeling.

Result: The method outperforms standard RoPE on long-context language modeling benchmarks, with performance benefits increasing as the context length grows.

Conclusion: Incorporating the imaginary component of RoPE enhances the representation and modeling of long-context sequence data, making it a compelling improvement for large language models.

Abstract: Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.

</details>


### [111] [SwissGov-RSD: A Human-annotated, Cross-lingual Benchmark for Token-level Recognition of Semantic Differences Between Related Documents](https://arxiv.org/abs/2512.07538)
*Michelle Wastl,Jannis Vamvas,Rico Sennrich*

Main category: cs.CL

TL;DR: This study introduces SwissGov-RSD, a dataset for cross-lingual semantic difference recognition, and evaluates various AI models on this benchmark.


<details>
  <summary>Details</summary>
Motivation: To address the lack of focus on semantic difference recognition across multilingual documents, especially in naturalistic and document-level contexts.

Method: Developed SwissGov-RSD, a cross-lingual dataset with 224 documents annotated for token-level semantic differences; evaluated models using both fine-tuning and existing approaches.

Result: Open-source and commercial language models perform poorly on this benchmark compared to synthetic datasets and monolingual tasks.

Conclusion: Current models have significant limitations in recognizing cross-lingual semantic differences, highlighting the need for further research and advancements in this area.

Abstract: Recognizing semantic differences across documents, especially in different languages, is crucial for text generation evaluation and multilingual content alignment. However, as a standalone task it has received little attention. We address this by introducing SwissGov-RSD, the first naturalistic, document-level, cross-lingual dataset for semantic difference recognition. It encompasses a total of 224 multi-parallel documents in English-German, English-French, and English-Italian with token-level difference annotations by human annotators. We evaluate a variety of open-source and closed source large language models as well as encoder models across different fine-tuning settings on this new benchmark. Our results show that current automatic approaches perform poorly compared to their performance on monolingual, sentence-level, and synthetic benchmarks, revealing a considerable gap for both LLMs and encoder models. We make our code and datasets publicly available.

</details>


### [112] [Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation](https://arxiv.org/abs/2512.07540)
*Boxuan Lyu,Haiyue Song,Hidetaka Kamigaito,Chenchen Ding,Hideki Tanaka,Masao Utiyama,Kotaro Funakoshi,Manabu Okumura*

Main category: cs.CL

TL;DR: The paper addresses error span detection (ESD) in machine translation evaluation using Minimum Bayes Risk (MBR) decoding, improving performance and mitigating inference-time latency.


<details>
  <summary>Details</summary>
Motivation: Current generative ESD methods assume perfect correlation between model probabilities and human annotations, but this assumption is flawed as dissimilar annotations may achieve higher model likelihoods than human annotations.

Method: The researchers apply MBR decoding to generative ESD models using sentence- and span-level similarity metrics as utility functions. They also propose MBR distillation to reduce computational costs and eliminate latency issues.

Result: Experimental results show MBR decoding outperforms MAP decoding for system, sentence, and span-levels. MBR distillation achieves comparable performance to MBR decoding with lower computational overhead.

Conclusion: Integrating MBR decoding and distillation improves accuracy and efficiency for generative ESD models, addressing key challenges in error span detection.

Abstract: Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.

</details>


### [113] [Most over-representation of phonological features in basic vocabulary disappears when controlling for spatial and phylogenetic effects](https://arxiv.org/abs/2512.07543)
*Frederic Blum*

Main category: cs.CL

TL;DR: The study reanalyzed sound symbolism in language basic vocabularies using a larger dataset while controlling for genealogical and areal biases. Many previously identified patterns did not hold up with these controls.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address over-representation claims of phonological features in language sound symbolism studies, questioning their robustness due to biases and insufficient control for genealogical and areal dependencies.

Method: The study introduced a larger language sample of 2864 languages, analyzed with a revised statistical model incorporating spatial and phylogenetic dependency controls, contrasted against prior work on 245 languages.

Result: Many earlier identified sound symbolic patterns were found unreliable under the new model and controls, but a few robust universal patterns in sound symbolism emerged.

Conclusion: The results emphasize the need for robust testing of universal linguistic claims while incorporating genealogical and spatial controls, urging caution against biases and over-generalized conclusions.

Abstract: The statistical over-representation of phonological features in the basic vocabulary of languages is often interpreted as reflecting potentially universal sound symbolic patterns. However, most of those results have not been tested explicitly for reproducibility and might be prone to biases in the study samples or models. Many studies on the topic do not adequately control for genealogical and areal dependencies between sampled languages, casting doubts on the robustness of the results. In this study, we test the robustness of a recent study on sound symbolism of basic vocabulary concepts which analyzed245 languages.The new sample includes data on 2864 languages from Lexibank. We modify the original model by adding statistical controls for spatial and phylogenetic dependencies between languages. The new results show that most of the previously observed patterns are not robust, and in fact many patterns disappear completely when adding the genealogical and areal controls. A small number of patterns, however, emerges as highly stable even with the new sample. Through the new analysis, we are able to assess the distribution of sound symbolism on a larger scale than previously. The study further highlights the need for testing all universal claims on language for robustness on various levels.

</details>


### [114] [MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based Dialogue](https://arxiv.org/abs/2512.07544)
*Kyungro Lee,Dongha Choi,Hyunju Lee*

Main category: cs.CL

TL;DR: MoCoRP introduces explicit relations between persona sentences and responses for improved dialogue coherence and engagement.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of explicit relations in persona-based dialogue datasets, which hinders the model's ability to capture and generate persona-consistent responses.

Method: A framework, MoCoRP, incorporates explicit NLI (natural language inference) relations between persona sentences and responses using an external NLI expert and aligns it with pre-trained models like BART and modern LLMs.

Result: MoCoRP achieves superior results in persona consistency and context-aware dialogue generation on datasets like ConvAI2 and MPChat, outperforming baseline methods in both quantitative and qualitative metrics.

Conclusion: Modeling explicit persona-response relations enhances dialogue systems' ability to generate coherent and engaging persona-based interactions.

Abstract: As dialogue systems become increasingly important across various domains, a key challenge in persona-based dialogue is generating engaging and context-specific interactions while ensuring the model acts with a coherent personality. However, existing persona-based dialogue datasets lack explicit relations between persona sentences and responses, which makes it difficult for models to effectively capture persona information. To address these issues, we propose MoCoRP (Modeling Consistent Relations between Persona and Response), a framework that incorporates explicit relations into language models. MoCoRP leverages an NLI expert to explicitly extract the NLI relations between persona sentences and responses, enabling the model to effectively incorporate appropriate persona information from the context into its responses. We applied this framework to pre-trained models like BART and further extended it to modern large language models (LLMs) through alignment tuning. Experimental results on the public datasets ConvAI2 and MPChat demonstrate that MoCoRP outperforms existing baselines, achieving superior persona consistency and engaging, context-aware dialogue generation. Furthermore, our model not only excels in quantitative metrics but also shows significant improvements in qualitative aspects. These results highlight the effectiveness of explicitly modeling persona-response relations in persona-based dialogue. The source codes of MoCoRP are available at https://github.com/DMCB-GIST/MoCoRP.

</details>


### [115] [Performance of the SafeTerm AI-Based MedDRA Query System Against Standardised MedDRA Queries](https://arxiv.org/abs/2512.07552)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

TL;DR: The paper introduces SafeTerm AMQ, an AI system for automating MedDRA query generation, demonstrating high recall and precision in adverse event term detection.


<details>
  <summary>Details</summary>
Motivation: To improve signal detection in pre-market drug safety review by automating the grouping of related adverse event terms into standardized MedDRA queries using advanced AI techniques.

Method: SafeTerm AMQ embeds medical queries and terms in a vector space, utilizes cosine similarity, and performs extreme-value clustering to rank MedDRA terms. It was validated against tier-1 SMQs, with performance metrics computed at various thresholds.

Result: SafeTerm AMQ achieved high recall (94%) at moderate thresholds and improved precision (89%) at higher thresholds. An optimal threshold balanced recall (48%) and precision (45%), with slightly better performance for narrow-term PTs.

Conclusion: SafeTerm AMQ is a viable supplementary tool for automated MedDRA query generation, balancing recall and precision. Using suitable terminology and an automated threshold enhances performance, facilitating effective term filtering and selection.

Abstract: In pre-market drug safety review, grouping related adverse event terms into SMQs or OCMQs is critical for signal detection. We assess the performance of SafeTerm Automated Medical Query (AMQ) on MedDRA SMQs. The AMQ is a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score (0-1) using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity, and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against tier-1 SMQs (110 queries, v28.1). Precision, recall and F1 were computed at multiple similarity-thresholds, defined either manually or using an automated method. High recall (94%)) is achieved at moderate similarity thresholds, indicative of good retrieval sensitivity. Higher thresholds filter out more terms, resulting in improved precision (up to 89%). The optimal threshold (0.70)) yielded an overall recall of (48%) and precision of (45%) across all 110 queries. Restricting to narrow-term PTs achieved slightly better performance at an increased (+0.05) similarity threshold, confirming increased relatedness of narrow versus broad terms. The automatic threshold (0.66) selection prioritizes recall (0.58) to precision (0.29). SafeTerm AMQ achieves comparable, satisfactory performance on SMQs and sanitized OCMQs. It is therefore a viable supplementary method for automated MedDRA query generation, balancing recall and precision. We recommend using suitable MedDRA PT terminology in query formulation and applying the automated threshold method to optimise recall. Increasing similarity scores allows refined, narrow terms selection.

</details>


### [116] [A Simple Method to Enhance Pre-trained Language Models with Speech Tokens for Classification](https://arxiv.org/abs/2512.07571)
*Nicolas Calbucura,Valentin Barriere*

Main category: cs.CL

TL;DR: The paper introduces a simple method to integrate speech tokens into pre-trained language models for specific classification tasks, achieving state-of-the-art results in Argumentative Fallacy Detection.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of integrating long audio sequences into text-pretrained models for improved classification performance.

Method: A lasso-based feature selection is applied to retain important audio tokens, which are then adapted for a language model with a self-supervised objective before task-specific fine-tuning.

Result: The method achieves better performance than unimodal models, larger language models, or learned audio representations, excelling in Argumentative Fallacy Detection.

Conclusion: Incorporating selected audio tokens enhances language models for multimodal tasks even when audio was previously thought unhelpful.

Abstract: This paper presents a simple method that allows to easily enhance textual pre-trained large language models with speech information, when fine-tuned for a specific classification task. A classical issue with the fusion of many embeddings from audio with text is the large length of the audio sequence compared to the text one. Our method benefits from an existing speech tokenizer trained for Audio Speech Recognition that output long sequences of tokens from a large vocabulary, making it difficult to integrate it at low cost in a large language model. By applying a simple lasso-based feature selection on multimodal Bag-of-Words representation, we retain only the most important audio tokens for the task, and adapt the language model to them with a self-supervised language modeling objective, before fine-tuning it on the downstream task. We show this helps to improve the performances compared to an unimodal model, to a bigger SpeechLM or to integrating audio via a learned representation. We show the effectiveness of our method on two recent Argumentative Fallacy Detection and Classification tasks where the use of audio was believed counterproductive, reaching state-of-the-art results. We also provide an in-depth analysis of the method, showing that even a random audio token selection helps enhancing the unimodal model. Our code is available [online](https://github.com/salocinc/EACL26SpeechTokFallacy/).

</details>


### [117] [Complementary Learning Approach for Text Classification using Large Language Models](https://arxiv.org/abs/2512.07583)
*Navid Asgari,Benjamin M. Cole*

Main category: cs.CL

TL;DR: This paper proposes a cost-efficient methodology incorporating large language models (LLMs) into research, balancing human and machine strengths for analyzing data.


<details>
  <summary>Details</summary>
Motivation: To integrate the strengths of large language models (LLMs) and human reasoning to overcome individual weaknesses in research applications.

Method: The paper employs chain-of-thought and few-shot learning to enable a collaborative approach between humans and machines in analyzing quantitative research data.

Result: Demonstrates the utility of the methodology by analyzing discrepancies in human-machine ratings of 1,934 pharmaceutical alliance press releases.

Conclusion: The study presents a structured approach to leverage LLMs collaboratively with humans for effective, low-cost research analysis and emphasizes addressing LLMs' weaknesses through careful human oversight.

Abstract: In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abductive reasoning and natural language to interrogate not just what the machine has done but also what the human has done. Our method highlights how scholars can manage inherent weaknesses OF LLMs using careful, low-cost techniques. We demonstrate how to use the methodology to interrogate human-machine rating discrepancies for a sample of 1,934 press releases announcing pharmaceutical alliances (1990-2017).

</details>


### [118] [Metric-Fair Prompting: Treating Similar Samples Similarly](https://arxiv.org/abs/2512.07608)
*Jing Wang,Jie Shen,Xing Niu,Tong Zhang,Jeremy Weiss*

Main category: cs.CL

TL;DR: The paper proposes a framework called Metric-Fair Prompting to enhance fairness and improve decision accuracy in clinical multiple-choice question answering by guiding large language models (LLMs) under metric-fairness constraints.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the need for fairness in artificial intelligence, particularly individual fairness, in high-stakes applications like medical question answering, where consistent treatment of similar inputs is essential.

Method: The framework treats questions and their options as binary instances, calculates question similarity via NLP embeddings, and utilizes a global decision protocol in prompting. It enforces a Lipschitz-style constraint to ensure that similar inputs receive consistent and fair outputs.

Result: The proposed framework shows improved performance on the MedQA (US) benchmark, surpassing traditional single-item prompting methods.

Conclusion: Fairness-guided prompting not only enhances the fairness of decisions but also improves the accuracy of LLMs, making them more reliable in critical domains like clinical decision-making.

Abstract: We introduce \emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}~--~treating similar instances similarly~--~we compute question similarity using NLP embeddings and solve items in \emph{joint pairs of similar questions} rather than in isolation. The prompt enforces a global decision protocol: extract decisive clinical features, map each \((\text{question}, \text{option})\) to a score $f(x)$ that acts as confidence, and impose a Lipschitz-style constraint so that similar inputs receive similar scores and, hence, consistent outputs. Evaluated on the {MedQA (US)} benchmark, Metric-Fair Prompting is shown to improve performance over standard single-item prompting, demonstrating that fairness-guided, confidence-oriented reasoning can enhance LLM accuracy on high-stakes clinical multiple-choice questions.

</details>


### [119] [PCMind-2.1-Kaiyuan-2B Technical Report](https://arxiv.org/abs/2512.07612)
*Kairong Luo,Zhenbo Sun,Xinyu Shi,Shengqi Chen,Bowen Yu,Yunyi Chen,Chenyi Dang,Hengtao Tao,Hui Wang,Fangming Liu,Kaifeng Lyu,Wenguang Chen*

Main category: cs.CL

TL;DR: PCMind-2.1-Kaiyuan-2B is a 2-billion-parameter fully open-source model emphasizing training efficiency and performance in resource-constrained environments.


<details>
  <summary>Details</summary>
Motivation: The work aims to bridge the knowledge gap between open-source and commercial industry-level LLMs due to disparities in high-quality data and training techniques.

Method: Three innovations: Quantile Data Benchmarking for dataset comparison, Strategic Selective Repetition leveraging sparse data, and Multi-Domain Curriculum Training ordering samples by quality, supplemented by FP16 stability architectural tweaks.

Result: Kaiyuan-2B achieves competitive performance against state-of-the-art open-source models, showcasing scalable solutions for resource-restricted training.

Conclusion: The paper highlights efficient methodologies for resource-limited pretraining, releasing model weights, data, and code to advance the open-source LLM community.

Abstract: The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.

</details>


### [120] [When Large Language Models Do Not Work: Online Incivility Prediction through Graph Neural Networks](https://arxiv.org/abs/2512.07684)
*Zihan Chen,Lanyu Yu*

Main category: cs.CL

TL;DR: The paper proposes a Graph Neural Network (GNN) framework for identifying online incivility, outperforming 12 state-of-the-art language models in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Online incivility imposes psychological and social burdens, yet existing moderation and detection methods are inefficient or inaccurate.

Method: The GNN framework models user comments as nodes with edges based on textual similarity and adapts attention mechanisms to balance linguistic and structural features.

Result: The GNN framework surpasses 12 Large Language Models in performance metrics while being more computationally efficient.

Conclusion: Incorporating structural context is critical for improving online incivility detection, addressing limitations of current text-only models. Public datasets ensure reproducibility and further research.

Abstract: Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches often remains limited in both accuracy and efficiency. To address this challenge, we propose a Graph Neural Network (GNN) framework for detecting three types of uncivil behavior (i.e., toxicity, aggression, and personal attacks) within the English Wikipedia community. Our model represents each user comment as a node, with textual similarity between comments defining the edges, allowing the network to jointly learn from both linguistic content and relational structures among comments. We also introduce a dynamically adjusted attention mechanism that adaptively balances nodal and topological features during information aggregation. Empirical evaluations demonstrate that our proposed architecture outperforms 12 state-of-the-art Large Language Models (LLMs) across multiple metrics while requiring significantly lower inference cost. These findings highlight the crucial role of structural context in detecting online incivility and address the limitations of text-only LLM paradigms in behavioral prediction. All datasets and comparative outputs will be publicly available in our repository to support further research and reproducibility.

</details>


### [121] [HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs](https://arxiv.org/abs/2512.07687)
*Sujoy Nath,Arkaprabha Basu,Sharanya Dasgupta,Swagatam Das*

Main category: cs.CL

TL;DR: This paper addresses hallucinations in multimodal large language models (MLLMs), proposing a novel method, HalluShift++, to detect irregularities in their internal dynamics.


<details>
  <summary>Details</summary>
Motivation: To mitigate the issue of hallucinations in MLLMs, which can lead to unreliable outputs and adverse consequences during vision-language tasks.

Method: The study hypothesizes that hallucination corresponds to measurable irregularities in layer-wise internal model dynamics. HalluShift++ extends this concept to multimodal scenarios, analyzing distribution shifts and their impact.

Result: HalluShift++ demonstrates the ability to detect hallucinations in MLLMs effectively, expanding beyond text-based models to vision-language frameworks.

Conclusion: The findings highlight the significance of internal model dynamics analysis for hallucination detection, boosting the reliability of MLLMs in multimodal applications.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \textsc{\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at https://github.com/C0mRD/HalluShift_Plus.

</details>


### [122] [Automated Generation of Custom MedDRA Queries Using SafeTerm Medical Map](https://arxiv.org/abs/2512.07694)
*Francois Vandenhende,Anna Georgiou,Michalis Georgiou,Theodoros Psaras,Ellie Karekla,Elena Hadjicosta*

Main category: cs.CL

TL;DR: The study introduces SafeTerm, an AI system for grouping related adverse event terms using statistical methods, achieving high recall with moderate precision.


<details>
  <summary>Details</summary>
Motivation: To improve signal detection in drug safety reviews by automating the retrieval of MedDRA Preferred Terms (PTs) for adverse event terms.

Method: SafeTerm embeds medical terms and MedDRA PTs in a vector space, applying cosine similarity and extreme-value clustering to rank terms by relevance scores.

Result: Validation showed SafeTerm achieved over 95% recall at moderate thresholds and precision up to 86% at higher thresholds. Optimal performance balanced thresholds between 0.70-0.75.

Conclusion: SafeTerm is a viable AI-driven supplement for automated MedDRA query generation, with thresholds tailored based on precision and recall needs.

Abstract: In pre-market drug safety review, grouping related adverse event terms into standardised MedDRA queries or the FDA Office of New Drugs Custom Medical Queries (OCMQs) is critical for signal detection. We present a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against the FDA OCMQ v3.0 (104 queries), restricted to valid MedDRA PTs. Precision, recall and F1 were computed across similarity-thresholds. High recall (>95%) is achieved at moderate thresholds. Higher thresholds improve precision (up to 86%). The optimal threshold (~0.70 - 0.75) yielded recall ~50% and precision ~33%. Narrow-term PT subsets performed similarly but required slightly higher similarity thresholds. The SafeTerm AI-driven system provides a viable supplementary method for automated MedDRA query generation. A similarity threshold of ~0.60 is recommended initially, with increased thresholds for refined term selection.

</details>


### [123] [Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?](https://arxiv.org/abs/2512.07777)
*Karin de Langis,Püren Öncel,Ryan Peters,Andrew Elfenbein,Laura Kristen Allen,Andreas Schramm,Dongyeop Kang*

Main category: cs.CL

TL;DR: This paper examines the ability of large language models (LLMs) to distinguish coherent from incoherent narratives, uncovering gaps in their understanding of storytelling coherence.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether LLMs can reliably identify and distinguish coherent and incoherent narratives, and to understand their limitations in storytelling comprehension.

Method: The study conducted a probing analysis of LLMs' internal representations and examined the responses LLMs generate when rating narratives under various conditions.

Result: LLMs reliably identify incoherent narratives internally but fail to reflect this in their output behavior. LLMs are more sensitive to setting violations than to personality trait violations, relying more on prototypical world knowledge than narrative coherence.

Conclusion: LLMs exhibit a consistent asymmetry in understanding and handling narrative coherence, indicating an incomplete grasp of storytelling coherence and its nuances.

Abstract: Leveraging a dataset of paired narratives, we investigate the extent to which large language models (LLMs) can reliably separate incoherent and coherent stories. A probing study finds that LLMs' internal representations can reliably identify incoherent narratives. However, LLMs generate responses to rating questions that fail to satisfactorily separate the coherent and incoherent narratives across several prompt variations, hinting at a gap in LLM's understanding of storytelling. The reasoning LLMs tested do not eliminate these deficits, indicating that thought strings may not be able to fully address the discrepancy between model internal state and behavior. Additionally, we find that LLMs appear to be more sensitive to incoherence resulting from an event that violates the setting (e.g., a rainy day in the desert) than to incoherence arising from a character violating an established trait (e.g., Mary, a vegetarian, later orders a cheeseburger), suggesting that LLMs may rely more on prototypical world knowledge than building meaning-based narrative coherence. The consistent asymmetry found in our results suggests that LLMs do not have a complete grasp on narrative coherence.

</details>


### [124] [On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models](https://arxiv.org/abs/2512.07783)
*Charlie Zhang,Graham Neubig,Xiang Yue*

Main category: cs.CL

TL;DR: The study investigates how pre-training, mid-training, and reinforcement learning (RL) contribute to enhancing reasoning in language models, using a fully controlled experimental framework.


<details>
  <summary>Details</summary>
Motivation: Uncertainty exists about whether RL and post-training genuinely improve a model's reasoning ability compared to what it acquires during pre-training. Additionally, the large-scale training pipelines are opaque, making it hard to isolate contributions of individual training phases.

Method: The authors introduce a controlled framework using synthetic reasoning tasks, step-by-step reasoning traces, and manipulated training distributions. They evaluate models on generalization abilities and systematically test the roles of pre-training, mid-training, and RL-based post-training.

Result: 1) RL improves capabilities only in specific conditions, like edge-of-competence tasks. 2) Minimal pre-training exposure suffices for RL-driven contextual generalization. 3) Mid-training greatly enhances performance under the same compute compared to RL alone. 4) Process-level rewards improve reasoning fidelity and reduce reward hacking.

Conclusion: The paper provides evidence on how pre-training, mid-training, and RL interact to improve reasoning in language models. This insight aids in optimizing training strategies to achieve better reasoning capacities in language models.

Abstract: Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.

</details>


### [125] [Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support](https://arxiv.org/abs/2512.07801)
*Raunak Jain,Mudita Khurana*

Main category: cs.CL

TL;DR: The paper highlights the limitations of current LLM-based decision-support systems in expert settings due to inadequate collaboration and proposes a framework called Collaborative Causal Sensemaking (CCS) to enhance human-AI teaming.


<details>
  <summary>Details</summary>
Motivation: Human-AI teams often underperform in high-stakes settings because current systems fail to adequately support collaborative cognitive processes between humans and AI.

Method: The authors introduce CCS, a framework where AI systems collaboratively engage in cognitive processes with humans by understanding human reasoning, co-building causal hypotheses, and learning from joint decisions.

Result: CCS aims to improve both human and AI decision-making over time, fostering trust, complementarity, and addressing the limitations of traditional LLM-based systems in expert decision-support.

Conclusion: The framework suggests a shift in research to focus on collaborative sensemaking, designing agents that act as true teammates capable of thinking alongside humans in decision-support scenarios.

Abstract: LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.

</details>


### [126] [Do Generalisation Results Generalise?](https://arxiv.org/abs/2512.07832)
*Matteo Boglioni,Andrea Sgobbi,Gabriel Tavernini,Francesco Rita,Marius Mosbach,Tiago Pimentel*

Main category: cs.CL

TL;DR: The paper investigates whether generalisation results of large language models (LLMs) towards out-of-distribution (OOD) datasets are consistent across multiple testsets during finetuning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the evaluation gap caused by focusing on single OOD datasets, which does not capture the diverse data shifts and deployment challenges faced by LLMs.

Method: The study measures OOD generalisation by evaluating performance across multiple testsets during finetuning and calculating partial correlation across testsets while controlling for in-domain performance.

Result: The authors analysed OLMo2 and OPT models and found that correlations between OOD testsets vary with the specific model, without a consistent positive or negative trend.

Conclusion: OOD generalisation results can strongly depend on model choice, suggesting the need for diverse evaluations to accurately assess LLMs' generalisation capabilities.

Abstract: A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.

</details>


### [127] [A Knowledge-Based Language Model: Deducing Grammatical Knowledge in a Multi-Agent Language Acquisition Simulation](https://arxiv.org/abs/2512.02195)
*David Ph. Shakouri,Crit Cremers,Niels O. Schiller*

Main category: cs.CL

TL;DR: The paper introduces the MODOMA system, a computational environment for studying unsupervised language acquisition between two language models, presenting its effectiveness through experiments.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore computational modeling of language acquisition inspired by human learning processes, enabling greater insights into unsupervised grammar acquisition.

Method: The paper uses a framework with two language models (adult and child agents), where interactions between them and computational procedures (statistical and rule-based) lead to the development of a knowledge-based grammatical system.

Result: The MODOMA system successfully demonstrated that grammatical categories could be acquired by the child agent based on interactions with the adult agent across varied data. Similar acquisition patterns to natural human processes were identified.

Conclusion: The experiments validate the MODOMA approach as a credible tool for modeling and understanding language acquisition computationally, offering explicit parameter control and innovative possibilities.

Abstract: This paper presents an initial study performed by the MODOMA system. The MODOMA is a computational multi-agent laboratory environment for unsupervised language acquisition experiments such that acquisition is based on the interaction between two language models, an adult and a child agent. Although this framework employs statistical as well as rule-based procedures, the result of language acquisition is a knowledge-based language model, which can be used to generate and parse new utterances of the target language. This system is fully parametrized and researchers can control all aspects of the experiments while the results of language acquisition, that is, the acquired grammatical knowledge, are explicitly represented and can be consulted. Thus, this system introduces novel possibilities for conducting computational language acquisition experiments. The experiments presented by this paper demonstrate that functional and content categories can be acquired and represented by the daughter agent based on training and test data containing different amounts of exemplars generated by the adult agent. Interestingly, similar patterns, which are well-established for human-generated data, are also found for these machine-generated data. As the procedures resulted in the successful acquisition of discrete grammatical categories by the child agent, these experiments substantiate the validity of the MODOMA approach to modelling language acquisition.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [128] [Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven' Matrices](https://arxiv.org/abs/2512.05969)
*Hokin Deng*

Main category: cs.CV

TL;DR: The paper demonstrates reasoning ability in video generation models, achieving 60% success rates in tasks like chess and Sudoku. It introduces an experimental paradigm with scalable evaluation tools.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore whether video generation models can perform reasoning tasks, addressing a gap in assessing video models' cognitive abilities.

Method: A 'Task Pair' design paradigm is established, accompanied by a code framework supporting 39 models, enabling scalable evaluation. Automated evaluation correlates with human judgments.

Result: Leading models like Sora-2 achieve significant reasoning success rates (60%), while the experimental framework enables efficient model-task additions and scalability.

Conclusion: The paradigm and tools provide a strong foundation for reasoning enhancement in video models, highlighting opportunities for reinforcement learning applications.

Abstract: We show that video generation models could reason now. Testing on tasks such as chess, maze, Sudoku, mental rotation, and Raven's Matrices, leading models such as Sora-2 achieve sixty percent success rates. We establish a robust experimental paradigm centered on the "Task Pair" design. We build a code framework, with 39 models available already, that supports this paradigm and allows for easy scaling - users can add models and tasks efficiently. We show our automated evaluation strongly correlates with human judgment, and therefore this paradigm is highly scalable. We see an opportunity, given the availability of our paradigm, to do reinforcement learning for improving reasoning in video models. You could checkout all of our raw $\href{https://grow-ai-like-a-child.com/video-reason/}{results}$ and our $\href{https://github.com/hokindeng/VMEvalKit}{VMEvalKit}$ codebase.

</details>


### [129] [Adaptive Dataset Quantization: A New Direction for Dataset Pruning](https://arxiv.org/abs/2512.05987)
*Chenyue Yu,Jianyu Yu*

Main category: cs.CV

TL;DR: This paper introduces a dataset quantization method to reduce storage and communication costs in edge devices by compressing intra-sample redundancy without compromising model training performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of resource-constrained edge devices struggling with high storage and communication costs for large-scale datasets.

Method: The method involves linear symmetric quantization for initial sample compression and adaptive quantization allocation to allocate varying compression ratios while maintaining an overall constant compression level.

Result: Experiments on CIFAR-10, CIFAR-100, and ImageNet-1K show significant dataset compression while maintaining model training efficacy, surpassing traditional techniques.

Conclusion: The proposed dataset quantization is effective for storage reduction and performance preservation, outperforming existing methods under similar compression levels.

Abstract: This paper addresses the challenges of storage and communication costs for large-scale datasets in resource-constrained edge devices by proposing a novel dataset quantization approach to reduce intra-sample redundancy. Unlike traditional dataset pruning and distillation methods that focus on inter-sample redundancy, the proposed method compresses each image by reducing redundant or less informative content within samples while preserving essential features. It first applies linear symmetric quantization to obtain an initial quantization range and scale for each sample. Then, an adaptive quantization allocation algorithm is introduced to distribute different quantization ratios for samples with varying precision requirements, maintaining a constant total compression ratio. The main contributions include: (1) being the first to use limited bits to represent datasets for storage reduction; (2) introducing a dataset-level quantization algorithm with adaptive ratio allocation; and (3) validating the method's effectiveness through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K. Results show that the method maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning baselines under the same compression ratios.

</details>


### [130] [VG3T: Visual Geometry Grounded Gaussian Transformer](https://arxiv.org/abs/2512.05988)
*Junho Kim,Seongwon Lee*

Main category: cs.CV

TL;DR: This paper introduces VG3T, a multi-view network predicting 3D semantic occupancy using Gaussian representation to improve scene coherence and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to generating 3D scene representations from multi-view images often suffer from fragmented and inconsistent outputs due to view-by-view processing.

Method: VG3T introduces a novel multi-view feed-forward network that directly predicts semantically attributed Gaussians in a cohesive multi-view approach. It incorporates Grid-Based Sampling and Positional Refinement to address density bias issues.

Result: VG3T achieves a 1.7%p improvement in mIoU and uses 46% fewer primitives compared to previous methods on the nuScenes benchmark.

Conclusion: The proposed VG3T method demonstrates improved efficacy, reducing inconsistencies and enhancing efficiency in multi-view 3D scene representations.

Abstract: Generating a coherent 3D scene representation from multi-view images is a fundamental yet challenging task. Existing methods often struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance. To address this, we introduce VG3T, a novel multi-view feed-forward network that predicts a 3D semantic occupancy via a 3D Gaussian representation. Unlike prior methods that infer Gaussians from single-view images, our model directly predicts a set of semantically attributed Gaussians in a joint, multi-view fashion. This novel approach overcomes the fragmentation and inconsistency inherent in view-by-view processing, offering a unified paradigm to represent both geometry and semantics. We also introduce two key components, Grid-Based Sampling and Positional Refinement, to mitigate the distance-dependent density bias common in pixel-aligned Gaussian initialization methods. Our VG3T shows a notable 1.7%p improvement in mIoU while using 46% fewer primitives than the previous state-of-the-art on the nuScenes benchmark, highlighting its superior efficiency and performance.

</details>


### [131] [EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head](https://arxiv.org/abs/2512.05991)
*Chang Liu,Tianjiao Jing,Chengcheng Ma,Xuanqi Zhou,Zhengxuan Lian,Qin Jin,Hongliang Yuan,Shi-Sheng Huang*

Main category: cs.CV

TL;DR: The paper introduces EmoDiffTalk, an editable 3D Gaussian talking head model with advanced emotional manipulation using diffusion processes and multimodal controls.


<details>
  <summary>Details</summary>
Motivation: Existing 3D talking head methods face limitations in dynamic emotional expression and fine-grained manipulation via multi-modal controls.

Method: Introduced Emotion-aware Gaussian Diffusion combining action unit (AU) prompts for fine-grained animations and text-to-AU emotion control for expansive editing.

Result: EmoDiffTalk demonstrated superior emotion detailing, lip-sync accuracy, and control on EmoTalk3D and RenderMe-360 datasets compared to prior methods.

Conclusion: EmoDiffTalk advances diffusion-driven editable 3D Gaussian Splatting techniques, enabling high-quality and multimodal emotional editing for 3D talking-head synthesis.

Abstract: Recent photo-realistic 3D talking head via 3D Gaussian Splatting still has significant shortcoming in emotional expression manipulation, especially for fine-grained and expansive dynamics emotional editing using multi-modal control. This paper introduces a new editable 3D Gaussian talking head, i.e. EmoDiffTalk. Our key idea is a novel Emotion-aware Gaussian Diffusion, which includes an action unit (AU) prompt Gaussian diffusion process for fine-grained facial animator, and moreover an accurate text-to-AU emotion controller to provide accurate and expansive dynamic emotional editing using text input. Experiments on public EmoTalk3D and RenderMe-360 datasets demonstrate superior emotional subtlety, lip-sync fidelity, and controllability of our EmoDiffTalk over previous works, establishing a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis. To our best knowledge, our EmoDiffTalk is one of the first few 3D Gaussian Splatting talking-head generation framework, especially supporting continuous, multimodal emotional editing within the AU-based expression space.

</details>


### [132] [Domain-Specific Foundation Model Improves AI-Based Analysis of Neuropathology](https://arxiv.org/abs/2512.05993)
*Ruchika Verma,Shrishtee Kandoi,Robina Afzal,Shengjia Chen,Jannes Jegminat,Michael W. Karlovich,Melissa Umphlett,Timothy E. Richardson,Kevin Clare,Quazi Hossain,Jorge Samanamud,Phyllis L. Faust,Elan D. Louis,Ann C. McKee,Thor D. Stein,Jonathan D. Cherry,Jesse Mez,Anya C. McGoldrick,Dalilah D. Quintana Mora,Melissa J. Nirenberg,Ruth H. Walker,Yolfrankcis Mendez,Susan Morgello,Dennis W. Dickson,Melissa E. Murray,Carlos Cordon-Cardo,Nadejda M. Tsankova,Jamie M. Walker,Diana K. Dangoor,Stephanie McQuillan,Emma L. Thorn,Claudia De Sanctis,Shuying Li,Thomas J. Fuchs,Kurt Farrell,John F. Crary,Gabriele Campanella*

Main category: cs.CV

TL;DR: The paper introduces 'NeuroFM,' a foundation model trained on brain histopathology to better analyze neurodegenerative diseases.


<details>
  <summary>Details</summary>
Motivation: Neurodegenerative diseases require specialized analysis due to distinctive cell types and pathologies that general histology models fail to address.

Method: The authors developed 'NeuroFM,' a model trained specifically on brain tissue slides representing diverse neurodegenerative conditions.

Result: NeuroFM outperformed general-purpose models in tasks like disease classification, region segmentation, and ataxia identification.

Conclusion: Domain-specialized models like NeuroFM improve diagnostic and research capabilities in niche areas such as neuropathology.

Abstract: Foundation models have transformed computational pathology by providing generalizable representations from large-scale histology datasets. However, existing models are predominantly trained on surgical pathology data, which is enriched for non-nervous tissue and overrepresents neoplastic, inflammatory, metabolic, and other non-neurological diseases. Neuropathology represents a markedly different domain of histopathology, characterized by unique cell types (neurons, glia, etc.), distinct cytoarchitecture, and disease-specific pathological features including neurofibrillary tangles, amyloid plaques, Lewy bodies, and pattern-specific neurodegeneration. This domain mismatch may limit the ability of general-purpose foundation models to capture the morphological patterns critical for interpreting neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and cerebellar ataxias. To address this gap, we developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue spanning diverse neurodegenerative pathologies. NeuroFM demonstrates superior performance compared to general-purpose models across multiple neuropathology-specific downstream tasks, including mixed dementia disease classification, hippocampal region segmentation, and neurodegenerative ataxia identification encompassing cerebellar essential tremor and spinocerebellar ataxia subtypes. This work establishes that domain-specialized foundation models trained on brain tissue can better capture neuropathology-specific features than models trained on general surgical pathology datasets. By tailoring foundation models to the unique morphological landscape of neurodegenerative diseases, NeuroFM enables more accurate and reliable AI-based analysis for brain disease diagnosis and research, setting a precedent for domain-specific model development in specialized areas of digital pathology.

</details>


### [133] [FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting](https://arxiv.org/abs/2512.05996)
*Yi Liu,Jingyu Song,Vedanth Kallakuri,Katherine A. Skinner*

Main category: cs.CV

TL;DR: FishDetector-R1 is a framework designed for underwater fish analysis, achieving significant performance improvements on the DeepFish dataset while relying on weak supervision.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenges of analyzing underwater fish imagery, which suffers from visual degradation and expensive annotations, by proposing a more effective and scalable approach.

Method: FishDetector-R1 combines a detect-to-count prompt for spatial consistency with Reinforcement Learning from Verifiable Reward, using sparse point labels for scalability.

Result: The framework outperforms baselines, improving AP by 20%, mIoU by 10%, MAE by 30%, and GAME by 35% on the DeepFish dataset, and generalizes well to other datasets.

Conclusion: FishDetector-R1 is a scalable and robust tool for marine visual data analysis, providing reliable results and addressing the challenges of weak supervision.

Abstract: Analyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce FishDetector-R1, a unified MLLM-based framework for fish detection, segmentation, and counting under weak supervision. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel detect-to-count prompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is https://umfieldrobotics.github.io/FishDetector-R1.

</details>


### [134] [PrunedCaps: A Case For Primary Capsules Discrimination](https://arxiv.org/abs/2512.06003)
*Ramin Sharifi,Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

TL;DR: Capsule Networks (CapsNets) significantly reduce computational costs and improve speed through Primary Capsule (PC) pruning without affecting accuracy on datasets like MNIST, Fashion-MNIST, CIFAR-10, and SVHN.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the resource inefficiency of CapsNets due to the large number of Primary Capsules and their slow, resource-intensive training and testing.

Method: The researchers propose pruning 95% of the Primary Capsules in CapsNets and analyze its performance across multiple datasets, focusing on improvements in speed and resource conservation.

Result: The pruned CapsNet architecture is up to 9.90 times faster and uses 95.36% fewer floating-point operations in its dynamic routing stage, all without compromising accuracy.

Conclusion: CapsNets can be made substantially more efficient without losing accuracy by pruning Primary Capsules, and the results demonstrate why certain datasets benefit more from this pruning process than others.

Abstract: Capsule Networks (CapsNets) are a generation of image classifiers with proven advantages over Convolutional Neural Networks (CNNs). Better robustness to affine transformation and overlapping image detection are some of the benefits associated with CapsNets. However, CapsNets cannot be classified as resource-efficient deep learning architecture due to the high number of Primary Capsules (PCs). In addition, CapsNets' training and testing are slow and resource hungry. This paper investigates the possibility of Primary Capsules pruning in CapsNets on MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and SVHN datasets. We show that a pruned version of CapsNet performs up to 9.90 times faster than the conventional architecture by removing 95 percent of Capsules without a loss of accuracy. Also, our pruned architecture saves on more than 95.36 percent of floating-point operations in the dynamic routing stage of the architecture. Moreover, we provide insight into why some datasets benefit significantly from pruning while others fall behind.

</details>


### [135] [Simple Agents Outperform Experts in Biomedical Imaging Workflow Optimization](https://arxiv.org/abs/2512.06006)
*Xuefei,Wang,Kai A. Horstmann,Ethan Lin,Jonathan Chen,Alexander R. Farhang,Sophia Stiles,Atharva Sehgal,Jonathan Light,David Van Valen,Yisong Yue,Jennifer J. Sun*

Main category: cs.CV

TL;DR: The paper tackles the challenges of adapting generic computer vision tools for scientific datasets by proposing AI agents for automated code adaptation, accompanied by a systematic evaluation framework.


<details>
  <summary>Details</summary>
Motivation: Customizing production-level computer vision tools for specific scientific datasets is labor-intensive and limited by the lack of annotated data and lengthy manual adaptation.

Method: The authors develop a systematic evaluation framework to study the design of AI agents, testing their performance on three biomedical imaging pipelines.

Result: A simple AI agent framework generated adaptation code surpassing human-expert solutions, challenging the need for complex architectures.

Conclusion: The findings offer practical guidelines for agent design, showing effective and impactful deployment into real-world production environments. The framework is open-sourced for wider accessibility.

Abstract: Adapting production-level computer vision tools to bespoke scientific datasets is a critical "last mile" bottleneck. Current solutions are impractical: fine-tuning requires large annotated datasets scientists often lack, while manual code adaptation costs scientists weeks to months of effort. We consider using AI agents to automate this manual coding, and focus on the open question of optimal agent design for this targeted task. We introduce a systematic evaluation framework for agentic code optimization and use it to study three production-level biomedical imaging pipelines. We demonstrate that a simple agent framework consistently generates adaptation code that outperforms human-expert solutions. Our analysis reveals that common, complex agent architectures are not universally beneficial, leading to a practical roadmap for agent design. We open source our framework and validate our approach by deploying agent-generated functions into a production pipeline, demonstrating a clear pathway for real-world impact.

</details>


### [136] [Fast and Flexible Robustness Certificates for Semantic Segmentation](https://arxiv.org/abs/2512.06010)
*Thomas Massena,Corentin Friedrich,Franck Mamalet,Mathieu Serrurier*

Main category: cs.CV

TL;DR: This paper introduces a novel method for certifiably robust semantic segmentation, leveraging Lipschitz constraints in networks and offering significant computational advantages over existing methods.


<details>
  <summary>Details</summary>
Motivation: Deep Neural Networks lack robustness against adversarial perturbations, which is a challenge in deploying safe and reliable systems, especially in tasks like semantic segmentation.

Method: The authors propose a new trainable class of semantic segmentation networks with Lipschitz constraints, enabling efficient certification. Additionally, they present a framework to generalize robustness certificates for semantic segmentation tasks.

Result: The approach achieves competitive pixel accuracy on challenging datasets like Cityscapes, unlocks real-time certifiably robust segmentation, and outperforms existing methods in computational efficiency (600x faster than randomized smoothing).

Conclusion: This paper successfully demonstrates the feasibility of real-time certifiably robust semantic segmentation networks, validates the method against adversarial attacks, and marks a significant advancement in the certification process for semantic segmentation.

Abstract: Deep Neural Networks are vulnerable to small perturbations that can drastically alter their predictions for perceptually unchanged inputs. The literature on adversarially robust Deep Learning attempts to either enhance the robustness of neural networks (e.g, via adversarial training) or to certify their decisions up to a given robustness level (e.g, by using randomized smoothing, formal methods or Lipschitz bounds). These studies mostly focus on classification tasks and few efficient certification procedures currently exist for semantic segmentation. In this work, we introduce a new class of certifiably robust Semantic Segmentation networks with built-in Lipschitz constraints that are efficiently trainable and achieve competitive pixel accuracy on challenging datasets such as Cityscapes. Additionally, we provide a novel framework that generalizes robustness certificates for semantic segmentation tasks, where we showcase the flexibility and computational efficiency of using Lipschitz networks. Our approach unlocks real-time compatible certifiably robust semantic segmentation for the first time. Moreover, it allows the computation of worst-case performance under $\ell_2$ attacks of radius $ε$ across a wide range of performance measures. Crucially, we benchmark the runtime of our certification process and find our approach to be around 600 times faster than randomized smoothing methods at inference with comparable certificates on an NVIDIA A100 GPU. Finally, we evaluate the tightness of our worstcase certificates against state-of-the-art adversarial attacks to further validate the performance of our method.

</details>


### [137] [High-Throughput Unsupervised Profiling of the Morphology of 316L Powder Particles for Use in Additive Manufacturing](https://arxiv.org/abs/2512.06012)
*Emmanuel Akeweje,Conall Kirk,Chi-Wai Chan,Denis Dowling,Mimi Zhang*

Main category: cs.CV

TL;DR: This paper presents a machine learning framework to analyze metallic powder morphology for Selective Laser Melting (SLM) by evaluating three clustering pipelines for effective high-throughput profiling.


<details>
  <summary>Details</summary>
Motivation: Current methods for characterizing metallic powders in SLM are low-throughput and qualitative, which limits their ability to capture heterogeneity in large industrial feeds.

Method: The authors developed three unsupervised clustering pipelines (autoencoder-based, shape-descriptor-based, and functional-data-based) coupled with high-throughput imaging to classify over 126,000 powder images based on particle morphology.

Result: The Fourier-descriptor + k-means pipeline showed the best performance based on internal validity metrics, with low computational runtime and effective clustering of particle shapes.

Conclusion: This unsupervised learning framework provides a scalable way to monitor and assess powder morphology, supporting better quality control in SLM workflows.

Abstract: Selective Laser Melting (SLM) is a powder-bed additive manufacturing technique whose part quality depends critically on feedstock morphology. However, conventional powder characterization methods are low-throughput and qualitative, failing to capture the heterogeneity of industrial-scale batches. We present an automated, machine learning framework that couples high-throughput imaging with shape extraction and clustering to profile metallic powder morphology at scale. We develop and evaluate three clustering pipelines: an autoencoder pipeline, a shape-descriptor pipeline, and a functional-data pipeline. Across a dataset of approximately 126,000 powder images (0.5-102 micrometer diameter), internal validity metrics identify the Fourier-descriptor + k-means pipeline as the most effective, achieving the lowest Davies-Bouldin index and highest Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop workstation. Although the present work focuses on establishing the morphological-clustering framework, the resulting shape groups form a basis for future studies examining their relationship to flowability, packing density, and SLM part quality. Overall, this unsupervised learning framework enables rapid, automated assessment of powder morphology and supports tracking of shape evolution across reuse cycles, offering a path toward real-time feedstock monitoring in SLM workflows.

</details>


### [138] [VAT: Vision Action Transformer by Unlocking Full Representation of ViT](https://arxiv.org/abs/2512.06013)
*Wenhao Li,Chengwei Ma,Weixin Mao*

Main category: cs.CV

TL;DR: The paper introduces the Vision Action Transformer (VAT), an advanced version of Vision Transformers (ViTs), to improve robot learning by utilizing features from all transformer layers.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitation of conventional visual perception methods that discard intermediate features of Vision Transformers, which results in insufficient representation for robot learning.

Method: The authors propose VAT, an architecture extending the ViT to incorporate action tokens and utilize a hierarchical representation by fusing features from all transformer layers for deep perception and action integration.

Result: VAT achieves an average success rate of 98.15% across four LIBERO simulation benchmarks, surpassing state-of-the-art methods like OpenVLA-OFT in robotic policy learning.

Conclusion: VAT demonstrates the significance of leveraging the entire feature representation trajectory of vision models, advancing imitation learning and robotic control effectiveness.

Abstract: In robot learning, Vision Transformers (ViTs) are standard for visual perception, yet most methods discard valuable information by using only the final layer's features. We argue this provides an insufficient representation and propose the Vision Action Transformer (VAT), a novel architecture that is extended from ViT and unlocks the full feature hierarchy of ViT. VAT processes specialized action tokens with visual features across all transformer layers, enabling a deep and progressive fusion of perception and action generation. On a suite of simulated manipulation tasks, VAT achieves a 98.15\% average success rate across four LIBERO benchmarks, establishing a new state-of-the-art by outperforming prior methods like OpenVLA-OFT. Our work presents not only a powerful model for imitation learning but also demonstrates the critical importance of leveraging the complete ''representation trajectory'' of vision models to advance robotic policy. The GitHub URL for the project code is https://github.com/sellerbubble/VAT.

</details>


### [139] [Benchmarking CXR Foundation Models With Publicly Available MIMIC-CXR and NIH-CXR14 Datasets](https://arxiv.org/abs/2512.06014)
*Jiho Shin,Dominic Marshall,Matthieu Komorowski*

Main category: cs.CV

TL;DR: This paper evaluates two large-scale chest X-ray (CXR) embedding models: CXR-Foundation (ELIXR v2.0) and MedImageInsight, leveraging public datasets, highlighting their comparative behavior, performance, stability, and disease-specific clustering.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore and establish reproducible evaluation benchmarks for medical image foundation models applied to chest X-rays, while addressing the underexplored domain of their comparative behavior across datasets.

Method: Two pre-trained CXR models were benchmarked on MIMIC-CR and NIH ChestX-ray14 datasets using a unified preprocessing pipeline, extracting embeddings, and assessing performance using LightGBM classifiers on multiple disease labels. Metrics such as AUROC and F1-score were analyzed.

Result: MedImageInsight exhibited slightly better performance in most tasks, with better disease-specifc structure in clustering, while CXR-Foundation showed strong stability across datasets.

Conclusion: The study underscores the need for standardized evaluation methods and establishes baseline data for improving medical foundation models, aiding multimodal and clinical applications.

Abstract: Recent foundation models have demonstrated strong performance in medical image representation learning, yet their comparative behaviour across datasets remains underexplored. This work benchmarks two large-scale chest X-ray (CXR) embedding models (CXR-Foundation (ELIXR v2.0) and MedImagelnsight) on public MIMIC-CR and NIH ChestX-ray14 datasets. Each model was evaluated using a unified preprocessing pipeline and fixed downstream classifiers to ensure reproducible comparison. We extracted embeddings directly from pre-trained encoders, trained lightweight LightGBM classifiers on multiple disease labels, and reported mean AUROC, and F1-score with 95% confidence intervals. MedImageInsight achieved slightly higher performance across most tasks, while CXR-Foundation exhibited strong cross-dataset stability. Unsupervised clustering of MedImageIn-sight embeddings further revealed a coherent disease-specific structure consistent with quantitative results. The results highlight the need for standardised evaluation of medical foundation models and establish reproducible baselines for future multimodal and clinical integration studies.

</details>


### [140] [PrefGen: Multimodal Preference Learning for Preference-Conditioned Image Generation](https://arxiv.org/abs/2512.06020)
*Wenyi Mo,Tianyu Zhang,Yalong Bai,Ligong Han,Ying Ba,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: The paper introduces a multimodal framework utilizing MLLMs to capture and incorporate personalized user preferences into image generation with improved performance over existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing image generation methods struggle to capture nuanced user preferences and lack effective mechanisms for encoding personalized visual signals.

Method: The researchers train MLLMs with a preference-focused visual question answering task, incorporate probing tasks for fine-grained preference isolation, and use alignment loss to combine user signals with diffusion-based image generation.

Result: The proposed framework achieves notable improvements in both image quality and adherence to personal preferences when compared to baseline methods.

Conclusion: Representation extraction using MLLMs and effective multimodal alignment are crucial for advancing preference-conditioned image generation, as evidenced by the significant performance gains in this work.

Abstract: Preference-conditioned image generation seeks to adapt generative models to individual users, producing outputs that reflect personal aesthetic choices beyond the given textual prompt. Despite recent progress, existing approaches either fail to capture nuanced user preferences or lack effective mechanisms to encode personalized visual signals. In this work, we propose a multimodal framework that leverages multimodal large language models (MLLMs) to extract rich user representations and inject them into diffusion-based image generation. We train the MLLM with a preference-oriented visual question answering task to capture fine-grained semantic cues. To isolate preference-relevant features, we introduce two complementary probing tasks: inter-user discrimination to distinguish between different users, and intra-user discrimination to separate liked from disliked content. To ensure compatibility with diffusion text encoders, we design a maximum mean discrepancy-based alignment loss that bridges the modality gap while preserving multimodal structure. The resulting embeddings are used to condition the generator, enabling faithful adherence to both prompts and user preferences. Extensive experiments demonstrate that our method substantially outperforms strong baselines in both image quality and preference alignment, highlighting the effectiveness of representation extraction and alignment for personalized generation.

</details>


### [141] [Neural reconstruction of 3D ocean wave hydrodynamics from camera sensing](https://arxiv.org/abs/2512.06024)
*Jiabin Liu,Zihao Zhou,Jialei Yan,Anxin Guo,Alvise Benetazzo,Hui Li*

Main category: cs.CV

TL;DR: This paper proposes a neural network with an attention-augmented pyramid architecture for precise and computationally efficient reconstruction of 3D ocean wave free surfaces and velocity fields, achieving accuracy and speed improvements under real-sea conditions.


<details>
  <summary>Details</summary>
Motivation: The study seeks to overcome challenges of computational inefficiency and visual occlusions in ocean wave observation tasks. Understanding ocean physics necessitates accurate reconstruction of 3D free-surfaces and associated velocity fields.

Method: The work introduces a wave free surface visual reconstruction model with a multi-scale attention design that leverages physics-based constraints for time-resolved, nonlinear 3D velocity field reconstruction.

Result: The model demonstrates millimetre-level accuracy in wave elevation prediction, sub-0.01 Hz dominant-frequency errors, accurate high-frequency spectral power estimates, and rapid reconstruction with two million points in 1.35 seconds. It outperforms traditional methods and adapts well to occluded conditions.

Conclusion: The proposed approach delivers high-precision and efficient 3D visualization of ocean wave dynamics, addressing visual occlusion challenges and enhancing computational performance in long-term observation scenarios.

Abstract: Precise three-dimensional (3D) reconstruction of wave free surfaces and associated velocity fields is essential for developing a comprehensive understanding of ocean physics. To address the high computational cost of dense visual reconstruction in long-term ocean wave observation tasks and the challenges introduced by persistent visual occlusions, we propose an wave free surface visual reconstruction neural network, which is designed as an attention-augmented pyramid architecture tailored to the multi-scale and temporally continuous characteristics of wave motions. Using physics-based constraints, we perform time-resolved reconstruction of nonlinear 3D velocity fields from the evolving free-surface boundary. Experiments under real-sea conditions demonstrate millimetre-level wave elevation prediction in the central region, dominant-frequency errors below 0.01 Hz, precise estimation of high-frequency spectral power laws, and high-fidelity 3D reconstruction of nonlinear velocity fields, while enabling dense reconstruction of two million points in only 1.35 s. Built on a stereo-vision dataset, the model outperforms conventional visual reconstruction approaches and maintains strong generalization in occluded conditions, owing to its global multi-scale attention and its learned encoding of wave propagation dynamics.

</details>


### [142] [The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation](https://arxiv.org/abs/2512.06032)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.CV

TL;DR: SAM3 represents a fundamental shift from SAM2, moving from spatial, prompt-based segmentation to multimodal, concept-driven segmentation with advanced vision-language capabilities.


<details>
  <summary>Details</summary>
Motivation: To analyze the critical differences between SAM2 and SAM3 models, highlighting why SAM2's specialization in prompt-based segmentation is not compatible with SAM3’s broader multimodal and concept-driven capabilities.

Method: The paper decomposes the analysis into five components: conceptual, architectural, dataset/annotation, training, and evaluation differences, detailing how each component contributes to the paradigm shift.

Result: SAM3 is revealed as a new class of segmentation model with vision-language integration, open-vocabulary reasoning, and exemplar-based understanding, distinguishing itself from SAM2’s geometric segmentation approach.

Conclusion: The study underscores SAM3’s role in driving the transition to a concept-driven segmentation framework, paving the way for advancements in segmentation technologies and multimodal understanding.

Abstract: This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.

</details>


### [143] [Representation Learning for Point Cloud Understanding](https://arxiv.org/abs/2512.06058)
*Siming Yan*

Main category: cs.CV

TL;DR: The paper explores methods for enhanced 3D data understanding by leveraging pre-trained 2D models, emphasizing advances in point cloud segmentation, self-supervised learning, and transfer learning.


<details>
  <summary>Details</summary>
Motivation: To improve 3D data understanding across fields like computer vision and robotics by integrating 2D knowledge into 3D networks.

Method: The study employs supervised learning for point cloud segmentation, self-supervised approaches, and transfer learning techniques using pre-trained 2D models.

Result: Extensive experiments validate the proposed methods' effectiveness in integrating 2D knowledge for enhancing 3D representation learning.

Conclusion: The research demonstrates that 2D knowledge can significantly enhance 3D data understanding, paving the way for advancements in applications reliant on 3D representation.

Abstract: With the rapid advancement of technology, 3D data acquisition and utilization have become increasingly prevalent across various fields, including computer vision, robotics, and geospatial analysis. 3D data, captured through methods such as 3D scanners, LiDARs, and RGB-D cameras, provides rich geometric, shape, and scale information. When combined with 2D images, 3D data offers machines a comprehensive understanding of their environment, benefiting applications like autonomous driving, robotics, remote sensing, and medical treatment. This dissertation focuses on three main areas: supervised representation learning for point cloud primitive segmentation, self-supervised learning methods, and transfer learning from 2D to 3D. Our approach, which integrates pre-trained 2D models to support 3D network training, significantly improves 3D understanding without merely transforming 2D data. Extensive experiments validate the effectiveness of our methods, showcasing their potential to advance point cloud representation learning by effectively integrating 2D knowledge.

</details>


### [144] [EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing](https://arxiv.org/abs/2512.06065)
*Runjia Li,Moayed Haji-Ali,Ashkan Mirzaei,Chaoyang Wang,Arpit Sahni,Ivan Skorokhodov,Aliaksandr Siarohin,Tomas Jakab,Junlin Han,Sergey Tulyakov,Philip Torr,Willi Menapace*

Main category: cs.CV

TL;DR: This work addresses the challenges in editing egocentric videos, proposing the EgoEdit ecosystem, which includes a dataset (EgoEditData), a real-time video editor (EgoEdit), and an evaluation suite (EgoEditBench).


<details>
  <summary>Details</summary>
Motivation: Current video editing methods struggle with unique challenges in egocentric perspectives, such as rapid motion and hand-object interactions, while also lacking real-time capabilities for interactive AR applications.

Method: The paper introduces a specialized dataset (EgoEditData), a real-time instruction-based video editor (EgoEdit), which runs on a single GPU, and an evaluation suite (EgoEditBench) to assess instruction adherence, interaction preservation, and temporal stability.

Result: EgoEdit achieves superior and temporally stable performance in egocentric editing tasks, outperforming existing methods in this domain while maintaining comparable results to top baselines on general editing tasks.

Conclusion: The proposed ecosystem demonstrates substantial advancements in egocentric video editing and fills critical gaps by enabling real-time, instruction-based editing; the dataset and evaluation suite will also benefit the broader research community.

Abstract: We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit

</details>


### [145] [Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light](https://arxiv.org/abs/2512.06080)
*Tzofi Klinghoffer,Siddharth Somasundaram,Xiaoyu Xiang,Yuchen Fan,Christian Richardt,Akshat Dave,Ramesh Raskar,Rakesh Ranjan*

Main category: cs.CV

TL;DR: This paper introduces a data-driven approach for 3D scene reconstruction from single-photon lidar data, addressing occlusions and specular surfaces by leveraging complex light transport and two-bounce light.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of reconstructing dense 3D scenes, including occlusions and specular materials (e.g., mirrors), from single-photon lidars in practical scenarios with multiple illumination points.

Method: The authors propose a data-driven method to analyze two-bounce light using a large-scale simulated lidar dataset, enabling light decomposition to infer 3D geometry.

Result: The authors demonstrated that their approach can infer 3D geometry with impressive handling of occluded and reflective surfaces using only a single measurement.

Conclusion: This work successfully tackles the challenges of multi-point illumination and two-bounce light in single-photon lidars, providing a practical and effective method for 3D reconstruction of complex scenes.

Abstract: 3D scene reconstruction from a single measurement is challenging, especially in the presence of occluded regions and specular materials, such as mirrors. We address these challenges by leveraging single-photon lidars. These lidars estimate depth from light that is emitted into the scene and reflected directly back to the sensor. However, they can also measure light that bounces multiple times in the scene before reaching the sensor. This multi-bounce light contains additional information that can be used to recover dense depth, occluded geometry, and material properties. Prior work with single-photon lidar, however, has only demonstrated these use cases when a laser sequentially illuminates one scene point at a time. We instead focus on the more practical - and challenging - scenario of illuminating multiple scene points simultaneously. The complexity of light transport due to the combined effects of multiplexed illumination, two-bounce light, shadows, and specular reflections is challenging to invert analytically. Instead, we propose a data-driven method to invert light transport in single-photon lidar. To enable this approach, we create the first large-scale simulated dataset of ~100k lidar transients for indoor scenes. We use this dataset to learn a prior on complex light transport, enabling measured two-bounce light to be decomposed into the constituent contributions from each laser spot. Finally, we experimentally demonstrate how this decomposed light can be used to infer 3D geometry in scenes with occlusions and mirrors from a single measurement. Our code and dataset are released at https://shoot-bounce-3d.github.io.

</details>


### [146] [BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving](https://arxiv.org/abs/2512.06096)
*Karthik Mohan,Sonam Singh,Amit Arvind Kale*

Main category: cs.CV

TL;DR: The paper introduces BeLLA, a model connecting unified 360° BEV representations with a large language model, excelling at question answering in autonomous driving by improving spatial reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing vision-language models and multimodal models in autonomous driving, which fail to fully exploit multi-camera spatial data and provide unified spatial representations for reasoning.

Method: BeLLA is an end-to-end approach that integrates 360° BEV representations with a large language model to process and answer questions related to autonomous driving tasks.

Result: BeLLA outperformed current methods on NuScenes-QA and DriveLM benchmarks, showing significant improvement (up to +9.3%) in tasks requiring spatial reasoning, while remaining competitive in broader question-answering tasks.

Conclusion: BeLLA represents a step forward in autonomous driving, offering better spatial reasoning and versatility by effectively combining vision, spatial, and language understanding in a single framework.

Abstract: The rapid development of Vision-Language models (VLMs) and Multimodal Language Models (MLLMs) in autonomous driving research has significantly reshaped the landscape by enabling richer scene understanding, context-aware reasoning, and more interpretable decision-making. However, a lot of existing work often relies on either single-view encoders that fail to exploit the spatial structure of multi-camera systems or operate on aggregated multi-view features, which lack a unified spatial representation, making it more challenging to reason about ego-centric directions, object relations, and the wider context. We thus present BeLLA, an end-to-end architecture that connects unified 360° BEV representations with a large language model for question answering in autonomous driving. We primarily evaluate our work using two benchmarks - NuScenes-QA and DriveLM, where BeLLA consistently outperforms existing approaches on questions that require greater spatial reasoning, such as those involving relative object positioning and behavioral understanding of nearby objects, achieving up to +9.3% absolute improvement in certain tasks. In other categories, BeLLA performs competitively, demonstrating the capability of handling a diverse range of questions.

</details>


### [147] [SpectraIrisPAD: Leveraging Vision Foundation Models for Spectrally Conditioned Multispectral Iris Presentation Attack Detection](https://arxiv.org/abs/2512.06103)
*Raghavendra Ramachandra,Sushma Venkatesh*

Main category: cs.CV

TL;DR: SpectraIrisPAD is a deep learning framework for multispectral iris presentation attack detection, utilizing advanced techniques like Vision Transformers, and a new dataset, MSIrPAD, to improve performance against spoof attacks.


<details>
  <summary>Details</summary>
Motivation: To address vulnerabilities in iris biometric systems to Presentation Attacks (PAs) and enhance the effectiveness of Presentation Attack Detection (PAD) using multispectral imaging.

Method: The framework employs a Vision Transformer (DINOv2 ViT) with features like learnable spectral positional encoding, token fusion, and contrastive learning to utilize data from multiple NIR wavelengths. Additionally, a comprehensive dataset, MSIrPAD, was created for evaluation.

Result: SpectraIrisPAD outperforms state-of-the-art methods across all metrics, effectively detecting a variety of unseen spoofing attacks under diverse evaluation scenarios.

Conclusion: Multispectral imaging combined with advanced machine learning techniques significantly improves the robustness and generalizability of iris PAD methods, making them more suitable for real-world applications.

Abstract: Iris recognition is widely recognized as one of the most accurate biometric modalities. However, its growing deployment in real-world applications raises significant concerns regarding its vulnerability to Presentation Attacks (PAs). Effective Presentation Attack Detection (PAD) is therefore critical to ensure the integrity and security of iris-based biometric systems. While conventional iris recognition systems predominantly operate in the near-infrared (NIR) spectrum, multispectral imaging across multiple NIR bands provides complementary reflectance information that can enhance the generalizability of PAD methods. In this work, we propose \textbf{SpectraIrisPAD}, a novel deep learning-based framework for robust multispectral iris PAD. The SpectraIrisPAD leverages a DINOv2 Vision Transformer (ViT) backbone equipped with learnable spectral positional encoding, token fusion, and contrastive learning to extract discriminative, band-specific features that effectively distinguish bona fide samples from various spoofing artifacts. Furthermore, we introduce a new comprehensive dataset Multispectral Iris PAD (\textbf{MSIrPAD}) with diverse PAIs, captured using a custom-designed multispectral iris sensor operating at five distinct NIR wavelengths (800\,nm, 830\,nm, 850\,nm, 870\,nm, and 980\,nm). The dataset includes 18,848 iris images encompassing eight diverse PAI categories, including five textured contact lenses, print attacks, and display-based attacks. We conduct comprehensive experiments under unseen attack evaluation protocols to assess the generalization capability of the proposed method. SpectraIrisPAD consistently outperforms several state-of-the-art baselines across all performance metrics, demonstrating superior robustness and generalizability in detecting a wide range of presentation attacks.

</details>


### [148] [Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation](https://arxiv.org/abs/2512.06105)
*Junwen Zheng,Xinran Xu,Li Rong Wang,Chang Cai,Lucinda Siyun Tan,Dingyuan Wang,Hong Liang Tey,Xiuyi Fan*

Main category: cs.CV

TL;DR: This paper introduces the Cross-modal Explainable Framework for Melanoma (CEFM) to improve the interpretability of deep learning models in melanoma diagnosis.


<details>
  <summary>Details</summary>
Motivation: The lack of interpretability in deep learning models for melanoma classification creates trust issues for clinicians, limiting clinical adoption.

Method: The proposed approach, CEFM, utilizes contrastive learning to align clinical diagnostic criteria (Asymmetry, Border, and Color) with visual features in Vision Transformer embeddings. These embeddings are then translated into textual explanations.

Result: CEFM achieves 92.79% accuracy, an AUC of 0.961, and shows significant improvements in interpretability metrics while aligning with clinicians' clinical reasoning.

Conclusion: CEFM effectively bridges the gap between model performance and clinical trust, offering a transparent and interpretable framework for melanoma classification.

Abstract: Deep learning has demonstrated expert-level performance in melanoma classification, positioning it as a powerful tool in clinical dermatology. However, model opacity and the lack of interpretability remain critical barriers to clinical adoption, as clinicians often struggle to trust the decision-making processes of black-box models. To address this gap, we present a Cross-modal Explainable Framework for Melanoma (CEFM) that leverages contrastive learning as the core mechanism for achieving interpretability. Specifically, CEFM maps clinical criteria for melanoma diagnosis-namely Asymmetry, Border, and Color (ABC)-into the Vision Transformer embedding space using dual projection heads, thereby aligning clinical semantics with visual features. The aligned representations are subsequently translated into structured textual explanations via natural language generation, creating a transparent link between raw image data and clinical interpretation. Experiments on public datasets demonstrate 92.79% accuracy and an AUC of 0.961, along with significant improvements across multiple interpretability metrics. Qualitative analyses further show that the spatial arrangement of the learned embeddings aligns with clinicians' application of the ABC rule, effectively bridging the gap between high-performance classification and clinical trust.

</details>


### [149] [Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation](https://arxiv.org/abs/2512.06158)
*Su Sun,Cheng Zhao,Himangi Mittal,Gaurav Mittal,Rohith Kukkala,Yingjie Victor Chen,Mei Chen*

Main category: cs.CV

TL;DR: Track4DGen is a novel two-stage framework for generating dynamic 4D objects from sparse inputs, focusing on motion and appearance coherence across views and time.


<details>
  <summary>Details</summary>
Motivation: To address difficulties in generating dynamic 4D objects due to the challenge of ensuring temporal and spatial coherence while preventing artifacts and drift.

Method: A two-stage framework: Stage One ensures temporally consistent features by incorporating feature-level motion priors via a diffusion generator. Stage Two reconstructs 4D objects with advanced motion encoding using features from Stage One and a 4D Gaussian Splatting technique.

Result: Track4DGen outperforms benchmarks in multi-view video and 4D generation, producing temporally stable and text-editable 4D assets.

Conclusion: The proposed Track4DGen approach enhances coherence and reduces artifact issues in 4D generation, paving the way for further improvements in this domain. The accompanying Sketchfab28 dataset promotes future research efforts.

Abstract: Generating dynamic 4D objects from sparse inputs is difficult because it demands joint preservation of appearance and motion coherence across views and time while suppressing artifacts and temporal drift. We hypothesize that the view discrepancy arises from supervision limited to pixel- or latent-space video-diffusion losses, which lack explicitly temporally aware, feature-level tracking guidance. We present \emph{Track4DGen}, a two-stage framework that couples a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor. The central idea is to explicitly inject tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D-GS. In Stage One, we enforce dense, feature-level point correspondences inside the diffusion generator, producing temporally consistent features that curb appearance drift and enhance cross-view coherence. In Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion encoding that concatenates co-located diffusion features (carrying Stage-One tracking priors) with Hex-plane features, and augment them with 4D Spherical Harmonics for higher-fidelity dynamics modeling. \emph{Track4DGen} surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. Lastly, we curate \emph{Sketchfab28}, a high-quality dataset for benchmarking object-centric 4D generation and fostering future research.

</details>


### [150] [Automated Annotation of Shearographic Measurements Enabling Weakly Supervised Defect Detection](https://arxiv.org/abs/2512.06171)
*Jessica Plassmann,Nicolas Schuler,Michael Schuth,Georg von Freymann*

Main category: cs.CV

TL;DR: The paper presents an automated workflow using deep learning to create defect annotations from shearography measurements, addressing a key challenge in industrial defect detection.


<details>
  <summary>Details</summary>
Motivation: Manual labeling of shearography data for detecting subsurface defects is labor-intensive, unreliable, and hinders industrial application due to lack of standardized, high-quality datasets.

Method: The authors developed a deep learning-enabled automated workflow to generate high-resolution segmentation and bounding-box defect annotations directly from shearography data.

Result: The new method produces results with sufficient accuracy when compared to expert-labeled data, enabling weakly supervised training and significantly reducing manual annotation efforts.

Conclusion: The approach supports scalable creation of annotated datasets, advancing robust defect detection in industrial settings through automation and reliability.

Abstract: Shearography is an interferometric technique sensitive to surface displacement gradients, providing high sensitivity for detecting subsurface defects in safety-critical components. A key limitation to industrial adoption is the lack of high-quality annotated datasets, since manual labeling remains labor-intensive, subjective, and difficult to standardize. We introduce an automated workflow that generates defect annotations from shearography measurements using deep learning, producing high-resolution segmentation and bounding-box labels. Evaluation against expert-labeled data demonstrates sufficient accuracy to enable weakly supervised training, reducing manual effort and supporting scalable dataset creation for robust defect detection.

</details>


### [151] [Physics-Grounded Shadow Generation from Monocular 3D Geometry Priors and Approximate Light Direction](https://arxiv.org/abs/2512.06174)
*Shilin Hu,Jingyi Xu,Akshat Dave,Dimitris Samaras,Hieu Le*

Main category: cs.CV

TL;DR: The paper proposes a novel framework for generating realistic shadows by embedding physical modeling (geometry and illumination) into deep-learning-based shadow generation.


<details>
  <summary>Details</summary>
Motivation: Despite the physics of shadow formation being well-understood, it has rarely been applied in existing deep-learning shadow generation approaches.

Method: The authors approximate 3D geometry using monocular RGB images, predict dominant light direction, and combine physics-based shadow estimation with a diffusion framework for refinement.

Result: The model, trained on DESOBAV2, outperforms existing methods in generating visually realistic and physically coherent shadows, particularly in complex scenes.

Conclusion: Integrating physics-based modeling with deep-learning methods enhances shadow realism and coherence, highlighting the robustness of the proposed approach in handling complex lighting and geometry.

Abstract: Shadow generation aims to produce photorealistic shadows that are visually consistent with object geometry and scene illumination. In the physics of shadow formation, the occluder blocks some light rays casting from the light source that would otherwise arrive at the surface, creating a shadow that follows the silhouette of the occluder. However, such explicit physical modeling has rarely been used in deep-learning-based shadow generation. In this paper, we propose a novel framework that embeds explicit physical modeling - geometry and illumination - into deep-learning-based shadow generation. First, given a monocular RGB image, we obtain approximate 3D geometry in the form of dense point maps and predict a single dominant light direction. These signals allow us to recover fairly accurate shadow location and shape based on the physics of shadow formation. We then integrate this physics-based initial estimate into a diffusion framework that refines the shadow into a realistic, high-fidelity appearance while ensuring consistency with scene geometry and illumination. Trained on DESOBAV2, our model produces shadows that are both visually realistic and physically coherent, outperforming existing approaches, especially in scenes with complex geometry or ambiguous lighting.

</details>


### [152] [Physics-Grounded Attached Shadow Detection Using Approximate 3D Geometry and Light Direction](https://arxiv.org/abs/2512.06179)
*Shilin Hu,Jingyi Xu,Sagnik Das,Dimitris Samaras,Hieu Le*

Main category: cs.CV

TL;DR: This paper introduces a novel framework for detecting both cast and attached shadows by reasoning about light direction and geometry, supported by a new dataset with specialized annotations.


<details>
  <summary>Details</summary>
Motivation: Existing shadow detection methods focus mainly on cast shadows, leaving attached shadows—which are important for understanding object structure—overlooked.

Method: A framework is proposed that includes separate shadow detection for cast and attached shadows, light direction estimation, and iterative refinement of predictions using geometry-consistent reasoning.

Result: The method achieves significant improvement in attached shadow detection with a 33% BER reduction, while preserving the effectiveness for detecting cast and full shadows.

Conclusion: Iterative reasoning about geometry and illumination proves effective for enhancing shadow detection, addressing a vital gap in understanding scene structures.

Abstract: Attached shadows occur on the surface of the occluder where light cannot reach because of self-occlusion. They are crucial for defining the three-dimensional structure of objects and enhancing scene understanding. Yet existing shadow detection methods mainly target cast shadows, and there are no dedicated datasets or models for detecting attached shadows. To address this gap, we introduce a framework that jointly detects cast and attached shadows by reasoning about their mutual relationship with scene illumination and geometry. Our system consists of a shadow detection module that predicts both shadow types separately, and a light estimation module that infers the light direction from the detected shadows. The estimated light direction, combined with surface normals, allows us to derive a geometry-consistent partial map that identifies regions likely to be self-occluded. This partial map is then fed back to refine shadow predictions, forming a closed-loop reasoning process that iteratively improves both shadow segmentation and light estimation. In order to train our method, we have constructed a dataset of 1,458 images with separate annotations for cast and attached shadows, enabling training and quantitative evaluation of both. Experimental results demonstrate that this iterative geometry-illumination reasoning substantially improves the detection of attached shadows, with at least 33% BER reduction, while maintaining strong full and cast shadow performance.

</details>


### [153] [SPOOF: Simple Pixel Operations for Out-of-Distribution Fooling](https://arxiv.org/abs/2512.06185)
*Ankit Gupta,Christoph Adami,Emily Dolson*

Main category: cs.CV

TL;DR: This paper evaluates the vulnerability of modern convolutional and transformer-based DNNs to high-confidence fooling images, introducing a new, efficient black-box attack called "SPOOF" that consistently generates unrecognizable yet effective adversarial images.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate the susceptibility of state-of-the-art deep neural networks (DNNs) to adversarial fooling attacks and address the challenge of overconfidence in networks when exposed to non-natural inputs.

Method: The authors revisit and re-implement evolutionary fooling attacks from prior work on modern architectures to evaluate their vulnerability. They also propose a new black-box attack method, SPOOF, which efficiently creates high-confidence fooling images using minimal pixel modifications.

Result: The findings show that transformer-based networks, such as ViT-B/16, are more vulnerable than convolutional models, achieving near-certain misclassifications. SPOOF also greatly reduces computational effort while maintaining high success rates in fooling DNNs.

Conclusion: Modern DNNs, even transformers, remain fragile against adversarial attacks like SPOOF. Retraining strategies only provide partial resistance, stressing the need for robust methods to handle unrecognizable, adversarial inputs.

Abstract: Deep neural networks (DNNs) excel across image recognition tasks, yet continue to exhibit overconfidence on inputs that bear no resemblance to natural images. Revisiting the "fooling images" work introduced by Nguyen et al. (2015), we re-implement both CPPN-based and direct-encoding-based evolutionary fooling attacks on modern architectures, including convolutional and transformer classifiers. Our re-implementation confirm that high-confidence fooling persists even in state-of-the-art networks, with transformer-based ViT-B/16 emerging as the most susceptible--achieving near-certain misclassifications with substantially fewer queries than convolution-based models. We then introduce SPOOF, a minimalist, consistent, and more efficient black-box attack generating high-confidence fooling images. Despite its simplicity, SPOOF generates unrecognizable fooling images with minimal pixel modifications and drastically reduced compute. Furthermore, retraining with fooling images as an additional class provides only partial resistance, as SPOOF continues to fool consistently with slightly higher query budgets--highlighting persistent fragility of modern deep classifiers.

</details>


### [154] [Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying](https://arxiv.org/abs/2512.06190)
*Shichen Li,Ahmadreza Eslaminia,Chenhui Shao*

Main category: cs.CV

TL;DR: The paper introduces a novel color-trajectory prediction model for food drying, showcasing improved accuracy and generalization to unseen drying conditions.


<details>
  <summary>Details</summary>
Motivation: Current food drying studies inadequately capture complex color trajectories and fail to generalize to unseen process conditions.

Method: The authors develop a multi-modal prediction method integrating high-dimensional temporal color data with drying parameters to accurately predict color changes.

Result: The model achieves RMSEs of 2.12 (cookies) and 1.29 (apples), significantly improving prediction accuracy over baseline models.

Conclusion: The proposed model effectively improves accuracy, robustness, and applicability for predicting food color evolution during drying.

Abstract: Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.

</details>


### [155] [The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024: Efficient and Robust Aggregation Methods for Federated Learning](https://arxiv.org/abs/2512.06206)
*Akis Linardos,Sarthak Pati,Ujjwal Baid,Brandon Edwards,Patrick Foley,Kevin Ta,Verena Chung,Micah Sheller,Muhammad Irfan Khan,Mojtaba Jafaritadi,Elina Kontio,Suleiman Khan,Leon Mächler,Ivan Ezhov,Suprosanna Shit,Johannes C. Paetzold,Gustav Grimberg,Manuel A. Nickel,David Naccache,Vasilis Siomos,Jonathan Passerat-Palmbach,Giacomo Tarroni,Daewoon Kim,Leonard L. Klausmann,Prashant Shah,Bjoern Menze,Dimitrios Makris,Spyridon Bakas*

Main category: cs.CV

TL;DR: The 2024 MICCAI FeTS Challenge studied federated learning (FL) for glioma segmentation in MRI. Using a standardized FL setup, it assessed weight aggregation methods with a multi-institutional MRI dataset. A PID-controller-based approach excelled in segmentation accuracy and communication efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve robustness and efficiency in federated learning for medical image segmentation, particularly for glioma subregions, aiming to push the boundaries of FL's application in healthcare.

Method: Six teams participated in a federated learning challenge using a standardized setup and a dataset of 2,040 MRI cases. The evaluation combined segmentation performance metrics (DSC and HD95) with communication efficiency using a convergence score.

Result: The PID-controller-based method achieved the highest combined scores, showing superior segmentation accuracy (mean DSCs of 0.733, 0.761, and 0.751 for ET, TC, WT) and excellent communication efficiency (convergence score of 0.764).

Conclusion: The challenge demonstrated that federated learning, particularly with PID controllers, can improve performance in medical imaging. It surpassed prior methods, showing promise in weight aggregation, data sharing, and efficiency.

Abstract: We present the design and results of the MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024, which focuses on federated learning (FL) for glioma sub-region segmentation in multi-parametric MRI and evaluates new weight aggregation methods aimed at improving robustness and efficiency. Six participating teams were evaluated using a standardized FL setup and a multi-institutional dataset derived from the BraTS glioma benchmark, consisting of 1,251 training cases, 219 validation cases, and 570 hidden test cases with segmentations for enhancing tumor (ET), tumor core (TC), and whole tumor (WT). Teams were ranked using a cumulative scoring system that considered both segmentation performance, measured by Dice Similarity Coefficient (DSC) and the 95th percentile Hausdorff Distance (HD95), and communication efficiency assessed through the convergence score. A PID-controller-based method achieved the top overall ranking, obtaining mean DSC values of 0.733, 0.761, and 0.751 for ET, TC, and WT, respectively, with corresponding HD95 values of 33.922 mm, 33.623 mm, and 32.309 mm, while also demonstrating the highest communication efficiency with a convergence score of 0.764. These findings advance the state of federated learning for medical imaging, surpassing top-performing methods from previous challenge iterations and highlighting PID controllers as effective mechanisms for stabilizing and optimizing weight aggregation in FL. The challenge code is available at https://github.com/FeTS-AI/Challenge.

</details>


### [156] [Revisiting SVD and Wavelet Difference Reduction for Lossy Image Compression: A Reproducibility Study](https://arxiv.org/abs/2512.06221)
*Alena Makarova*

Main category: cs.CV

TL;DR: This paper conducts independent reproducibility tests for a lossy image compression method combining SVD and WDR. Results show it does not outperform JPEG2000 or WDR as claimed.


<details>
  <summary>Details</summary>
Motivation: To test and validate the claims of a proposed image compression method (SVD+WDR) that it achieves better performance than JPEG2000 and standalone WDR.

Method: Re-implementation of the SVD+WDR method, replication of original experiments, and additional evaluations on new images using PSNR and SSIM metrics.

Result: The SVD+WDR technique does not consistently outperform JPEG2000 or standalone WDR in PSNR and only partially improves SSIM. Ambiguities in the original paper's implementation details were identified.

Conclusion: The study exposes discrepancies between the original claims and reproducibility findings, emphasizing the importance of clear methodology for replicable research.

Abstract: This work presents an independent reproducibility study of a lossy image compression technique that integrates singular value decomposition (SVD) and wavelet difference reduction (WDR). The original paper claims that combining SVD and WDR yields better visual quality and higher compression ratios than JPEG2000 and standalone WDR. I re-implemented the proposed method, carefully examined missing implementation details, and replicated the original experiments as closely as possible. I then conducted additional experiments on new images and evaluated performance using PSNR and SSIM. In contrast to the original claims, my results indicate that the SVD+WDR technique generally does not surpass JPEG2000 or WDR in terms of PSNR, and only partially improves SSIM relative to JPEG2000. The study highlights ambiguities in the original description (e.g., quantization and threshold initialization) and illustrates how such gaps can significantly impact reproducibility and reported performance.

</details>


### [157] [GPU-GLMB: Assessing the Scalability of GPU-Accelerated Multi-Hypothesis Tracking](https://arxiv.org/abs/2512.06230)
*Pranav Balakrishnan,Sidisha Barik,Sean M. O'Rourke,Benjamin M. Marlin*

Main category: cs.CV

TL;DR: The paper explores an enhanced Generalized Labeled Multi-Bernoulli (GLMB) filter for multi-target tracking, addressing computational inefficiencies and enabling GPU acceleration.


<details>
  <summary>Details</summary>
Motivation: Improving computational efficiency in multi-target tracking methods, particularly for labeled random finite set approaches used in settings like distributed sensor networks.

Method: Introduced a variant of the GLMB filter that allows for multiple detections per object from the same sensor, breaking inter-detection dependencies and enabling GPU implementation.

Result: Demonstrated improved parallel scalability in filter updates and analyzed GPU-accelerated performance regarding runtime scalability with increasing objects and hypotheses.

Conclusion: The proposed GLMB tracker variant shows potential for efficient deployment on GPUs, improving scalability and computational demands in multi-target tracking contexts.

Abstract: Much recent research on multi-target tracking has focused on multi-hypothesis approaches leveraging random finite sets. Of particular interest are labeled random finite set methods that maintain temporally coherent labels for each object. While these methods enjoy important theoretical properties as closed-form solutions to the multi-target Bayes filter, the maintenance of multiple hypotheses under the standard measurement model is highly computationally expensive, even when hypothesis pruning approximations are applied. In this work, we focus on the Generalized Labeled Multi-Bernoulli (GLMB) filter as an example of this class of methods. We investigate a variant of the filter that allows multiple detections per object from the same sensor, a critical capability when deploying tracking in the context of distributed networks of machine learning-based virtual sensors. We show that this breaks the inter-detection dependencies in the filter updates of the standard GLMB filter, allowing updates with significantly improved parallel scalability and enabling efficient deployment on GPU hardware. We report the results of a preliminary analysis of a GPU-accelerated implementation of our proposed GLMB tracker, with a focus on run time scalability with respect to the number of objects and the maximum number of retained hypotheses.

</details>


### [158] [Opinion: Learning Intuitive Physics May Require More than Visual Data](https://arxiv.org/abs/2512.06232)
*Ellen Su,Solim Legris,Todd M. Gureckis,Mengye Ren*

Main category: cs.CV

TL;DR: This paper examines whether data distribution, rather than volume, influences the ability of deep learning models to learn intuitive physics. Training on a small, realistic dataset (SAYCam) did not significantly improve performance.


<details>
  <summary>Details</summary>
Motivation: Humans excel in intuitive physics using internal models, but state-of-the-art deep learning models struggle despite large datasets. The study explores if realistic data distribution could resolve this gap.

Method: A Video Joint Embedding Predictive Architecture (V-JEPA) model was pretrained on the SAYCam dataset, which captures realistic visual experiences of children, and tested on the IntPhys2 benchmark.

Result: Training on SAYCam (0.01% data volume of SOTA models) did not result in performance improvements on intuitive physics benchmarks.

Conclusion: Adjusting visual data distribution and volume alone is insufficient for current architectures to achieve human-level artificial intuitive physics.

Abstract: Humans expertly navigate the world by building rich internal models founded on an intuitive understanding of physics. Meanwhile, despite training on vast quantities of internet video data, state-of-the-art deep learning models still fall short of human-level performance on intuitive physics benchmarks. This work investigates whether data distribution, rather than volume, is the key to learning these principles. We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset partially capturing three children's everyday visual experiences. We find that training on this dataset, which represents 0.01% of the data volume used to train SOTA models, does not lead to significant performance improvements on the IntPhys2 benchmark. Our results suggest that merely training on a developmentally realistic dataset is insufficient for current architectures to learn representations that support intuitive physics. We conclude that varying visual data volume and distribution alone may not be sufficient for building systems with artificial intuitive physics.

</details>


### [159] [NexusFlow: Unifying Disparate Tasks under Partial Supervision via Invertible Flow Networks](https://arxiv.org/abs/2512.06251)
*Fangzhou Lin,Yuping Wang,Yuliang Guo,Zixun Huang,Xinyu Huang,Haichong Zhang,Kazunori Yamada,Zhengzhong Tu,Liu Ren,Ziming Zhang*

Main category: cs.CV

TL;DR: NexusFlow is a framework for Partially Supervised Multi-Task Learning (PS-MTL) that aligns latent task features using invertible coupling layers to enhance knowledge transfer, achieving state-of-the-art results on tasks with structural disparity and domain gaps.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of learning from structurally diverse tasks in PS-MTL, which has been underexplored compared to homogeneous tasks.

Method: NexusFlow uses surrogate networks with invertible coupling layers to align task feature distributions, creating a unified representation for effective knowledge transfer while avoiding representational collapse.

Result: NexusFlow achieved state-of-the-art performance on nuScenes (autonomous driving) and showed consistent gains on NYUv2 (dense prediction tasks), proving its effectiveness and adaptability.

Conclusion: NexusFlow is a general and efficient solution for PS-MTL, successfully addressing structural disparities and domain challenges in multi-task learning scenarios.

Abstract: Partially Supervised Multi-Task Learning (PS-MTL) aims to leverage knowledge across tasks when annotations are incomplete. Existing approaches, however, have largely focused on the simpler setting of homogeneous, dense prediction tasks, leaving the more realistic challenge of learning from structurally diverse tasks unexplored. To this end, we introduce NexusFlow, a novel, lightweight, and plug-and-play framework effective in both settings. NexusFlow introduces a set of surrogate networks with invertible coupling layers to align the latent feature distributions of tasks, creating a unified representation that enables effective knowledge transfer. The coupling layers are bijective, preserving information while mapping features into a shared canonical space. This invertibility avoids representational collapse and enables alignment across structurally different tasks without reducing expressive capacity. We first evaluate NexusFlow on the core challenge of domain-partitioned autonomous driving, where dense map reconstruction and sparse multi-object tracking are supervised in different geographic regions, creating both structural disparity and a strong domain gap. NexusFlow sets a new state-of-the-art result on nuScenes, outperforming strong partially supervised baselines. To demonstrate generality, we further test NexusFlow on NYUv2 using three homogeneous dense prediction tasks, segmentation, depth, and surface normals, as a representative N-task PS-MTL scenario. NexusFlow yields consistent gains across all tasks, confirming its broad applicability.

</details>


### [160] [Language-driven Fine-grained Retrieval](https://arxiv.org/abs/2512.06255)
*Shijie Wang,Xin Yu,Yadan Luo,Zijian Wang,Pengfei Zhang,Zi Huang*

Main category: cs.CV

TL;DR: The paper introduces LaFG, a language-driven framework for improving fine-grained image retrieval (FGIR), replacing one-hot label supervision with attribute-level descriptions derived from large language and vision-language models.


<details>
  <summary>Details</summary>
Motivation: Current FGIR methods struggle with unseen categories due to limited semantics provided by one-hot labels, which ignore detailed attributes encoded in category names.

Method: LaFG utilizes large language models (LLMs) and vision-language models (VLMs) to generate attribute-rich descriptions from category names, creates an attribute vocabulary using clustering, and generates linguistic prototypes for training a more generalized FGIR model.

Result: LaFG enhances the model's ability to generalize to unseen categories by bridging cross-category comparability using attribute-level supervision.

Conclusion: Incorporating semantic richness from language and vision models provides a more generalizable framework for fine-grained image retrieval, improving performance on both seen and unseen categories.

Abstract: Existing fine-grained image retrieval (FGIR) methods learn discriminative embeddings by adopting semantically sparse one-hot labels derived from category names as supervision. While effective on seen classes, such supervision overlooks the rich semantics encoded in category names, hindering the modeling of comparability among cross-category details and, in turn, limiting generalization to unseen categories. To tackle this, we introduce LaFG, a Language-driven framework for Fine-Grained Retrieval that converts class names into attribute-level supervision using large language models (LLMs) and vision-language models (VLMs). Treating each name as a semantic anchor, LaFG prompts an LLM to generate detailed, attribute-oriented descriptions. To mitigate attribute omission in these descriptions, it leverages a frozen VLM to project them into a vision-aligned space, clustering them into a dataset-wide attribute vocabulary while harvesting complementary attributes from related categories. Leveraging this vocabulary, a global prompt template selects category-relevant attributes, which are aggregated into category-specific linguistic prototypes. These prototypes supervise the retrieval model to steer

</details>


### [161] [Knowing the Answer Isn't Enough: Fixing Reasoning Path Failures in LVLMs](https://arxiv.org/abs/2512.06258)
*Chaoyang Wang,Yangfan He,Yiyang Zhou,Yixuan Wang,Jiaqi Liu,Peng Xia,Zhengzhong Tu,Mohit Bansal,Huaxiu Yao*

Main category: cs.CV

TL;DR: This paper highlights flaws in Large Vision-Language Models (LVLMs) regarding reasoning, showing how models often arrive at correct answers using incorrect paths. To address this, a novel two-stage optimization method, PSO, improves reasoning accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: Large Vision-Language Models exhibit errors from flawed reasoning, not ignorance, necessitating methods to improve reasoning accuracy and stability.

Method: The proposed PSO framework consists of two stages: GRPO creates structured reasoning using rewards, and online preference optimization refines paths using self-evaluation.

Result: PSO enhances reasoning accuracy by an average of 7.4% and achieves more consistent reasoning outcomes.

Conclusion: PSO successfully mitigates reasoning flaws in LVLMs by optimizing and refining reasoning paths, leading to more accurate and stable outputs.

Abstract: We reveal a critical yet underexplored flaw in Large Vision-Language Models (LVLMs): even when these models know the correct answer, they frequently arrive there through incorrect reasoning paths. The core issue is not a lack of knowledge, but a path selection bias within the vast reasoning search space. Although LVLMs are often capable of sampling correct solution trajectories, they disproportionately favor unstable or logically inconsistent ones, leading to erratic and unreliable outcomes. The substantial disparity between Pass@K (with large K) and Pass@1 across numerous models provides compelling evidence that such failures primarily stem from misreasoning rather than ignorance. To systematically investigate and address this issue, we propose PSO (Path-Select Optimization), a two-stage post-training framework designed to enhance both the reasoning performance and stability of existing LVLMs. In the first stage, we employ Group Relative Policy Optimization (GRPO) with template and answer-based rewards to cultivate structured, step-by-step reasoning. In the second stage, we conduct online preference optimization, where the model samples reasoning paths from GRPO-generated data, self-evaluates them, and aligns itself toward the preferred trajectories. Incorrect or suboptimal paths are concurrently stored in a Negative Replay Memory (NRM) as hard negatives, which are periodically revisited to prevent the model from repeating prior mistakes and to facilitate continual reasoning refinement. Extensive experiments show that PSO effectively prunes invalid reasoning paths, substantially enhances reasoning accuracy (with 7.4% improvements on average), and yields more stable and consistent chains of thought. Our code will be available at https://github.com/aiming-lab/PSO.

</details>


### [162] [TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting](https://arxiv.org/abs/2512.06269)
*Quan Tran,Tuan Dang*

Main category: cs.CV

TL;DR: The paper proposes a method to enhance 3D Gaussian Splatting for real-time novel view synthesis by addressing reconstruction artifacts through global geometry consistency via multi-view triangulation.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting, while efficient for rendering photorealistic images in real-time, suffers from reconstruction inconsistencies due to reliance solely on photometric loss, leading to artifacts and unstructured geometry.

Method: The proposed method enforces global geometry consistency by utilizing constrained multi-view triangulation. A consensus on 3D representation is achieved by penalizing deviations of a rendered 3D point from a robust consensus point determined through self-supervised re-triangulation from multiple views.

Result: The method demonstrates state-of-the-art performance across datasets, achieving a mean Chamfer Distance of 0.50 mm on the DTU dataset—better than comparable explicit approaches.

Conclusion: The paper successfully addresses reconstruction inconsistencies in 3D Gaussian Splatting and achieves state-of-the-art performance, promoting robust physical world representations. Open-sourcing the code ensures community validation and reproducibility.

Abstract: 3D Gaussian Splatting is crucial for real-time novel view synthesis due to its efficiency and ability to render photorealistic images. However, building a 3D Gaussian is guided solely by photometric loss, which can result in inconsistencies in reconstruction. This under-constrained process often results in "floater" artifacts and unstructured geometry, preventing the extraction of high-fidelity surfaces. To address this issue, our paper introduces a novel method that improves reconstruction by enforcing global geometry consistency through constrained multi-view triangulation. Our approach aims to achieve a consensus on 3D representation in the physical world by utilizing various estimated views. We optimize this process by penalizing the deviation of a rendered 3D point from a robust consensus point, which is re-triangulated from a bundle of neighboring views in a self-supervised fashion. We demonstrate the effectiveness of our method across multiple datasets, achieving state-of-the-art results. On the DTU dataset, our method attains a mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods. We will make our code open-source to facilitate community validation and ensure reproducibility.

</details>


### [163] [FacePhys: State of the Heart Learning](https://arxiv.org/abs/2512.06275)
*Kegang Wang,Jiankai Tang,Yuntao Wang,Xin Liu,Yuxuan Fan,Jiatong Ji,Yuanchun Shi,Daniel McDuff*

Main category: cs.CV

TL;DR: This paper introduces 'FacePhys,' a memory-efficient, real-time rPPG algorithm for measuring cardiac activity using cameras with state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance remote health monitoring by addressing the computational constraints and accuracy challenges of rPPG algorithms, enabling practical real-time deployment.

Method: The authors develop 'FacePhys,' leveraging a temporal-spatial state space approach to balance scalability, generalization, and operation efficiency for cardiac measurement through subtle light reflection analysis.

Result: FacePhys achieves a 49% error reduction and operates with a 3.6 MB memory footprint and 9.46 ms per-frame latency, significantly outperforming existing rPPG methods by 83% to 99%.

Conclusion: The proposed method provides reliable, real-time cardiac monitoring, suitable for practical applications, showcasing its effectiveness through substantial performance improvements and availability as a live demo.

Abstract: Vital sign measurement using cameras presents opportunities for comfortable, ubiquitous health monitoring. Remote photoplethysmography (rPPG), a foundational technology, enables cardiac measurement through minute changes in light reflected from the skin. However, practical deployment is limited by the computational constraints of performing analysis on front-end devices and the accuracy degradation of transmitting data through compressive channels that reduce signal quality. We propose a memory efficient rPPG algorithm - \emph{FacePhys} - built on temporal-spatial state space duality, which resolves the trilemma of model scalability, cross-dataset generalization, and real-time operation. Leveraging a transferable heart state, FacePhys captures subtle periodic variations across video frames while maintaining a minimal computational overhead, enabling training on extended video sequences and supporting low-latency inference. FacePhys establishes a new state-of-the-art, with a substantial 49\% reduction in error. Our solution enables real-time inference with a memory footprint of 3.6 MB and per-frame latency of 9.46 ms -- surpassing existing methods by 83\% to 99\%. These results translate into reliable real-time performance in practical deployments, and a live demo is available at https://www.facephys.com/.

</details>


### [164] [RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension](https://arxiv.org/abs/2512.06276)
*Tianyi Gao,Hao Li,Han Fang,Xin Wei,Xiaodong Dong,Hongbo Sun,Ye Yuan,Zhongjiang He,Jinglin Xu,Jingmin Xin,Hao Sun*

Main category: cs.CV

TL;DR: This paper introduces RefBench-PRO, a benchmark for evaluating multi-modal models' ability to comprehend referring expressions by assessing six cognitive sub-dimensions, along with a novel RL-based scheme to enhance accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to assess multi-modal large language models in terms of grounding capabilities and differentiation across cognitive abilities.

Method: The paper creates RefBench-PRO which breaks down task dimensions into perception and reasoning, generates data automatically, and proposes Ref-R1, an RL-based learning framework using Dynamic IoU-based GRPO to improve REC performance.

Result: Experiments show RefBench-PRO provides interpretable assessments and poses greater challenges for models in perception and reasoning ability.

Conclusion: RefBench-PRO improves the evaluation mechanism for REC, enabling detailed analysis and highlighting reasoning challenges in multi-modal large language models.

Abstract: Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.

</details>


### [165] [Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models](https://arxiv.org/abs/2512.06281)
*Hengzhuang Li,Xinsong Zhang,Qiming Peng,Bin Luo,Han Hu,Dengyang Jiang,Han-Jia Ye,Teng Zhang,Hai Jin*

Main category: cs.CV

TL;DR: The paper introduces LaVer, a training framework to address modality imbalance in Multimodal Large Language Models (MLLMs), enhancing their visual information utilization and performance.


<details>
  <summary>Details</summary>
Motivation: To tackle the modality imbalance issue in MLLMs, where visual data is underutilized compared to text, leading to degraded visual performance or hallucinations due to traditional training methods emphasizing text prediction.

Method: Proposes Latent Visual Reconstruction (LaVer), a training framework that enables MLLMs to better use visual data by applying masked image modeling in the latent semantic space of LLMs, thereby offering direct visual activations.

Result: LaVer significantly enhances visual information processing and attention distribution in MLLMs, and comprehensive experiments demonstrate superior performance across a variety of benchmarks, particularly in tasks requiring dense visual capabilities.

Conclusion: LaVer successfully addresses the modality imbalance issue in MLLMs, improving their discriminative capabilities for visual information and overall performance in multimodal scenarios.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in multimodal tasks. Despite their impressive performance, MLLMs suffer from the modality imbalance issue, where visual information is often underutilized compared to textual representations in deeper layers, leading to degraded visual performance or hallucinations. This issue stems from the predominant reliance on next-text-token-prediction during training, which fails to provide direct visual supervisory signals, resulting in progressive homogenization of visual representations throughout the layers. To this end, we propose Latent Visual Reconstruction (LaVer), a novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. Our method offers direct visual activation to MLLMs, which exhibit increased visual attention allocation, indicating enhanced utilization of visual information. Extensive experiments across diverse benchmarks prove the superiority of our approach in various scenarios, especially those requiring dense visual capabilities. Code of LaVer is available at https://github.com/Fir-lat/LaVer.

</details>


### [166] [A Sleep Monitoring System Based on Audio, Video and Depth Information](https://arxiv.org/abs/2512.06282)
*Lyn Chao-ling Chen,Kuan-Wen Chen,Yi-Ping Hung*

Main category: cs.CV

TL;DR: This paper presents a noninvasive system for detecting sleep disturbances through motion, light, and noise events using infrared, RGB cameras, and microphones.


<details>
  <summary>Details</summary>
Motivation: Provide a quantitative way to monitor and evaluate sleep disturbances by detecting relevant events in real-world conditions.

Method: Sleep disturbances were categorized into motion, light-on/off, and noise events. Using a device with infrared depth sensors, RGB cameras, and microphone arrays, background models were built for depth and color signals to measure these events. An event detection algorithm processed the data.

Result: The system effectively monitored disturbances in sleep and was validated as reliable through experimental tests in a sleep setting.

Conclusion: The proposed method for sleep monitoring is reliable and capable of recognizing disturbances via specific events, offering potential applications in home environments.

Abstract: For quantitative evaluation of sleep disturbances, a noninvasive monitoring system is developed by introducing an event-based method. We observe sleeping in home context and classify the sleep disturbances into three types of events: motion events, light-on/off events and noise events. A device with an infrared depth sensor, a RGB camera, and a four-microphone array is used in sleep monitoring in an environment with barely light sources. One background model is established in depth signals for measuring magnitude of movements. Because depth signals cannot observe lighting changes, another background model is established in color images for measuring magnitude of lighting effects. An event detection algorithm is used to detect occurrences of events from the processed data of the three types of sensors. The system was tested in sleep condition and the experiment result validates the system reliability.

</details>


### [167] [StrokeNet: Unveiling How to Learn Fine-Grained Interactions in Online Handwritten Stroke Classification](https://arxiv.org/abs/2512.06290)
*Yiheng Huang,Shuang She,Zewei Wei,Jianmin Lin,Ming Yang,Wenyin Liu*

Main category: cs.CV

TL;DR: The paper introduces StrokeNet, a network architecture improving stroke classification by encoding strokes as reference pair representations. It incorporates Inline Sequence Attention (ISA) and Cross-Ellipse Query (CEQ) mechanisms for spatial and feature interaction modeling, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Stroke classification presents challenges due to style variations, ambiguous content, and dynamic writing positions. Fine-grained semantic stroke relationships are difficult to model, necessitating a new approach.

Method: StrokeNet dynamically selects reference points to represent strokes in a fine-grained manner. It employs Inline Sequence Attention (ISA) for contextual feature construction and Cross-Ellipse Query (CEQ) for spatial interaction modeling. Additionally, a joint optimization framework predicts stroke categories and semantic transition through an Auxiliary Branch.

Result: StrokeNet exhibits state-of-the-art performance on multiple datasets, with accuracy notably improving from 93.81% to 95.54% on the CASIA-onDo dataset.

Conclusion: The proposed StrokeNet architecture effectively models fine-grained semantic relationships and spatial interactions in stroke classification, confirming its robustness and accuracy.

Abstract: Stroke classification remains challenging due to variations in writing style, ambiguous content, and dynamic writing positions. The core challenge in stroke classification is modeling the semantic relationships between strokes. Our observations indicate that stroke interactions are typically localized, making it difficult for existing deep learning methods to capture such fine-grained relationships. Although viewing strokes from a point-level perspective can address this issue, it introduces redundancy. However, by selecting reference points and using their sequential order to represent strokes in a fine-grained manner, this problem can be effectively solved. This insight inspired StrokeNet, a novel network architecture encoding strokes as reference pair representations (points + feature vectors), where reference points enable spatial queries and features mediate interaction modeling. Specifically, we dynamically select reference points for each stroke and sequence them, employing an Inline Sequence Attention (ISA) module to construct contextual features. To capture spatial feature interactions, we devised a Cross-Ellipse Query (CEQ) mechanism that clusters reference points and extracts features across varying spatial scales. Finally, a joint optimization framework simultaneously predicts stroke categories via reference points regression and adjacent stroke semantic transition modeling through an Auxiliary Branch (Aux-Branch). Experimental results show that our method achieves state-of-the-art performance on multiple public online handwritten datasets. Notably, on the CASIA-onDo dataset, the accuracy improves from 93.81$\%$ to 95.54$\%$, demonstrating the effectiveness and robustness of our approach.

</details>


### [168] [Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation](https://arxiv.org/abs/2512.06306)
*Haoxian Zhou,Chuanzhi Xu,Langyi Chen,Haodong Chen,Yuk Ying Chung,Qiang Qu,Xaoming Chen,Weidong Cai*

Main category: cs.CV

TL;DR: The paper proposes an enhanced human pose estimation method using event cameras and point cloud frameworks, improving spatial and temporal modeling while maintaining high resolution and low latency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for human pose estimation using event cameras often compromise high temporal resolution by treating event streams as dense frames, adding unnecessary computation. The paper aims to address this inefficiency.

Method: A point cloud-based framework with two proposed modules: the Event Temporal Slicing Convolution (for short-term dependency modeling) and the Event Slice Sequencing (for structured temporal modeling). Additionally, edge enhancement is applied to improve spatial edge details in sparse event conditions.

Result: The proposed method was tested on the DHP19 dataset and demonstrated consistent performance improvements across three point cloud backbones: PointNet, DGCNN, and Point Transformer.

Conclusion: Integrating spatiotemporal properties and spatial edge enhancement for event stream representation offers substantial benefits in human pose estimation, outperforming existing dense frame-based approaches.

Abstract: Human pose estimation focuses on predicting body keypoints to analyze human motion. Event cameras provide high temporal resolution and low latency, enabling robust estimation under challenging conditions. However, most existing methods convert event streams into dense event frames, which adds extra computation and sacrifices the high temporal resolution of the event signal. In this work, we aim to exploit the spatiotemporal properties of event streams based on point cloud-based framework, designed to enhance human pose estimation performance. We design Event Temporal Slicing Convolution module to capture short-term dependencies across event slices, and combine it with Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse event conditions to further improve performance. Experiments on the DHP19 dataset show our proposed method consistently improves performance across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.

</details>


### [169] [ReCAD: Reinforcement Learning Enhanced Parametric CAD Model Generation with Vision-Language Models](https://arxiv.org/abs/2512.06328)
*Jiahao Li,Yusheng Luo,Yunzhong Lou,Xiangdong Zhou*

Main category: cs.CV

TL;DR: ReCAD is a reinforcement learning framework that uses pretrained large models to generate precise parametric CAD models from multimodal inputs.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of existing CAD generation methods which rely on supervised fine-tuning and lack editability while failing to fully utilize generative capabilities of pretrained models.

Method: The framework fine-tunes vision-language models for CAD model generation and employs reinforcement learning strategies with hierarchical primitive learning to achieve geometric accuracy and semantic fidelity.

Result: ReCAD achieves state-of-the-art performance in text-to-CAD and image-to-CAD tasks, significantly reducing Chamfer Distance for geometric accuracy compared to existing methods.

Conclusion: ReCAD demonstrates the ability of pretrained large models and reinforcement learning to advance CAD model generation, improving accuracy in both in-distribution and out-of-distribution cases.

Abstract: We present ReCAD, a reinforcement learning (RL) framework that bootstraps pretrained large models (PLMs) to generate precise parametric computer-aided design (CAD) models from multimodal inputs by leveraging their inherent generative capabilities. With just access to simple functional interfaces (e.g., point coordinates), our approach enables the emergence of complex CAD operations (e.g., pattern replication and mirror). This stands in contrast to previous methods, which typically rely on knowledge injected through supervised fine-tuning (SFT), offer limited support for editability, and fail to exploit the strong generative priors of PLMs. Specifically, the ReCAD framework begins by fine-tuning vision-language models (VLMs) to equip them with basic CAD model generation capabilities, where we rewrite CAD scripts into parameterized code that is leveraged to generate accurate textual descriptions for supervision. Then, we propose a novel RL strategy that incorporates parameterized code as guidance to enhance the model's reasoning on challenging questions. Furthermore, we employ a hierarchical primitive learning process to progressively teach structured and compositional skills under a unified reward function that ensures both geometric accuracy and semantic fidelity. ReCAD sets a new state-of-the-art in both text-to-CAD and image-to-CAD tasks, significantly improving geometric accuracy across in-distribution and out-of-distribution settings. In the image-to-CAD task, for instance, it reduces the mean Chamfer Distance from 73.47 to 29.61 (in-distribution) and from 272.06 to 80.23 (out-of-distribution), outperforming existing baselines by a substantial margin.

</details>


### [170] [S2WMamba: A Spectral-Spatial Wavelet Mamba for Pansharpening](https://arxiv.org/abs/2512.06330)
*Haoyu Zhang,Junhan Luo,Yugang Cao,Siran Peng,Jie Huang,Liangjian-Deng*

Main category: cs.CV

TL;DR: This paper introduces S2WMamba, a novel pansharpening method that disentangles spatial and spectral frequencies and uses cross-modal interaction to achieve superior image fusion performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of preserving both spatial detail and spectral fidelity in pansharpening by disentangling spatial and spectral information effectively.

Method: The method involves applying a 2D Haar DWT to PAN images for spatial detail extraction and a 1D Haar DWT to MS spectra for frequency separation. A dual-branch architecture (Spectral branch and Spatial branch) with Mamba-based cross-modulation and a multi-scale dynamic gate is employed for adaptive fusion.

Result: S2WMamba matches or outperforms state-of-the-art models on multiple datasets (WV3, GF2, QB), achieving up to a 0.23 dB improvement in PSNR and a HQNR of 0.956 for full-resolution WV3.

Conclusion: The proposed S2WMamba method effectively balances spatial and spectral fidelity in pansharpening, demonstrating state-of-the-art performance and validating its design choices through ablation studies.

Abstract: Pansharpening fuses a high-resolution PAN image with a low-resolution multispectral (LRMS) image to produce an HRMS image. A key difficulty is that jointly processing PAN and MS often entangles spatial detail with spectral fidelity. We propose S2WMamba, which explicitly disentangles frequency information and then performs lightweight cross-modal interaction. Concretely, a 2D Haar DWT is applied to PAN to localize spatial edges and textures, while a channel-wise 1D Haar DWT treats each pixel's spectrum as a 1D signal to separate low/high-frequency components and limit spectral distortion. The resulting Spectral branch injects wavelet-extracted spatial details into MS features, and the Spatial branch refines PAN features using spectra from the 1D pyramid; the two branches exchange information through Mamba-based cross-modulation that models long-range dependencies with linear complexity. A multi-scale dynamic gate (multiplicative + additive) then adaptively fuses branch outputs.On WV3, GF2, and QB, S2WMamba matches or surpasses recent strong baselines (FusionMamba, CANNet, U2Net, ARConv), improving PSNR by up to 0.23 dB and reaching HQNR 0.956 on full-resolution WV3. Ablations justify the choice of 2D/1D DWT placement, parallel dual branches, and the fusion gate. Our code is available at https://github.com/KagUYa66/S2WMamba.

</details>


### [171] [CryoHype: Reconstructing a thousand cryo-EM structures with transformer-based hypernetworks](https://arxiv.org/abs/2512.06332)
*Jeffrey Gu,Minkyu Jeon,Ambri Ma,Serena Yeung-Levy,Ellen D. Zhong*

Main category: cs.CV

TL;DR: CryoHype is a novel transformer-based hypernetwork method for cryo-EM reconstruction that excels in resolving compositional heterogeneity in dynamic molecular structures.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of existing cryo-EM methods, which struggle to reconstruct compositional heterogeneity in mixtures of multiple distinct molecular structures.

Method: The proposed approach, CryoHype, uses a transformer-based hypernetwork to dynamically adjust weights of an implicit neural representation for cryo-EM image reconstruction.

Result: CryoHype demonstrated state-of-the-art results on a benchmark dataset featuring 100 distinct structures and successfully scaled to reconstruct 1,000 molecular structures under fixed-pose conditions.

Conclusion: CryoHype represents a significant advancement in cryo-EM reconstruction technology, enabling high-throughput analysis of structurally diverse molecular species simultaneously.

Abstract: Cryo-electron microscopy (cryo-EM) is an indispensable technique for determining the 3D structures of dynamic biomolecular complexes. While typically applied to image a single molecular species, cryo-EM has the potential for structure determination of many targets simultaneously in a high-throughput fashion. However, existing methods typically focus on modeling conformational heterogeneity within a single or a few structures and are not designed to resolve compositional heterogeneity arising from mixtures of many distinct molecular species. To address this challenge, we propose CryoHype, a transformer-based hypernetwork for cryo-EM reconstruction that dynamically adjusts the weights of an implicit neural representation. Using CryoHype, we achieve state-of-the-art results on a challenging benchmark dataset containing 100 structures. We further demonstrate that CryoHype scales to the reconstruction of 1,000 distinct structures from unlabeled cryo-EM images in the fixed-pose setting.

</details>


### [172] [Beyond Hallucinations: A Multimodal-Guided Task-Aware Generative Image Compression for Ultra-Low Bitrate](https://arxiv.org/abs/2512.06344)
*Kaile Wang,Lijun He,Haisheng Fu,Haixia Bi,Fan Li*

Main category: cs.CV

TL;DR: The paper introduces MTGC, a framework that integrates multimodal guidance to enhance semantic consistency and image quality in extremely low bitrate image compression.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of semantic deviations in generative image compression at ultra-low bitrates for 6G communication scenarios.

Method: The MTGC framework integrates text captions, highly compressed images, and task-relevant semantics via specially designed modules and mechanisms, leveraging a Multimodal-Guided Diffusion Decoder for integration and image reconstruction.

Result: MTGC achieves improved semantic consistency, perceptual quality, and pixel-level fidelity, outperforming previous methods at ultra-low bitrates, with metrics like DISTS improving by 10.59%.

Conclusion: MTGC redefines generative image compression with multimodal guidance, making it more reliable in low-bitrate scenarios, and demonstrates its effectiveness through significant experimental results.

Abstract: Generative image compression has recently shown impressive perceptual quality, but often suffers from semantic deviations caused by generative hallucinations at ultra-low bitrate (bpp < 0.05), limiting its reliable deployment in bandwidth-constrained 6G semantic communication scenarios. In this work, we reassess the positioning and role of of multimodal guidance, and propose a Multimodal-Guided Task-Aware Generative Image Compression (MTGC) framework. Specifically, MTGC integrates three guidance modalities to enhance semantic consistency: a concise but robust text caption for global semantics, a highly compressed image (HCI) retaining low-level visual information, and Semantic Pseudo-Words (SPWs) for fine-grained task-relevant semantics. The SPWs are generated by our designed Task-Aware Semantic Compression Module (TASCM), which operates in a task-oriented manner to drive the multi-head self-attention mechanism to focus on and extract semantics relevant to the generation task while filtering out redundancy. Subsequently, to facilitate the synergistic guidance of these modalities, we design a Multimodal-Guided Diffusion Decoder (MGDD) employing a dual-path cooperative guidance mechanism that synergizes cross-attention and ControlNet additive residuals to precisely inject these three guidance into the diffusion process, and leverages the diffusion model's powerful generative priors to reconstruct the image. Extensive experiments demonstrate that MTGC consistently improves semantic consistency (e.g., DISTS drops by 10.59% on the DIV2K dataset) while also achieving remarkable gains in perceptual quality and pixel-level fidelity at ultra-low bitrate.

</details>


### [173] [CLUENet: Cluster Attention Makes Neural Networks Have Eyes](https://arxiv.org/abs/2512.06345)
*Xiangshuai Song,Jun-Jie Huang,Tianrui Liu,Ke Liang,Chang Tang*

Main category: cs.CV

TL;DR: CLUENet is a new transparent deep learning architecture improving visual semantic understanding through clustering-based innovations, providing both higher accuracy and better interpretability.


<details>
  <summary>Details</summary>
Motivation: Current vision models based on convolutions and attention struggle with irregular spatial patterns and lack interpretability, while clustering paradigms lack accuracy and efficiency.

Method: CLUENet uses three novel techniques: Global Soft Aggregation with Temperature-Scaled Cosin Attention, inter-block feature dispatching, and enhanced cluster pooling strategies to improve performance.

Result: CLUENet outperforms existing clustering methods and mainstream models in both classification and interpretability on datasets such as CIFAR-100 and Mini-ImageNet.

Conclusion: CLUENet provides a transparent, efficient, and accurate solution for visual semantic understanding, addressing issues with both traditional models and previous clustering-based paradigms.

Abstract: Despite the success of convolution- and attention-based models in vision tasks, their rigid receptive fields and complex architectures limit their ability to model irregular spatial patterns and hinder interpretability, therefore posing challenges for tasks requiring high model transparency. Clustering paradigms offer promising interpretability and flexible semantic modeling, but suffer from limited accuracy, low efficiency, and gradient vanishing during training. To address these issues, we propose CLUster attEntion Network (CLUENet), an transparent deep architecture for visual semantic understanding. We propose three key innovations include (i) a Global Soft Aggregation and Hard Assignment with a Temperature-Scaled Cosin Attention and gated residual connections for enhanced local modeling, (ii) inter-block Hard and Shared Feature Dispatching, and (iii) an improved cluster pooling strategy. These enhancements significantly improve both classification performance and visual interpretability. Experiments on CIFAR-100 and Mini-ImageNet demonstrate that CLUENet outperforms existing clustering methods and mainstream visual models, offering a compelling balance of accuracy, efficiency, and transparency.

</details>


### [174] [TreeQ: Pushing the Quantization Boundary of Diffusion Transformer via Tree-Structured Mixed-Precision Search](https://arxiv.org/abs/2512.06353)
*Kaicheng Yang,Kaisen Yang,Baiting Wu,Xun Zhang,Qianrui Yang,Haotong Qin,He Zhang,Yulun Zhang*

Main category: cs.CV

TL;DR: The paper proposes TreeQ, a framework to optimize quantization in image-generation Diffusion Transformers (DiTs), achieving near-lossless performance even at ultra-low-bit settings.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies and challenges of quantization in Diffusion Transformers (DiTs), which are computationally and memory intensive but excel in image generation compared to U-Nets.

Method: The researchers introduce three key components: (1) Tree Structured Search (TSS) for efficient solution space traversal; (2) Environmental Noise Guidance (ENG) for unified optimization objectives; and (3) General Monarch Branch (GMB) for mitigating information bottlenecks in low-bit scenarios.

Result: TreeQ achieves state-of-the-art performance on DiT-XL/2 models in W3A3 and W4A4 Post-Training Quantization settings, becoming the first method to achieve near-lossless 4-bit performance in these contexts.

Conclusion: The proposed TreeQ framework successfully optimizes DiT quantization, reducing computational overhead while maintaining high performance. Code and models will be publicly accessible.

Abstract: Diffusion Transformers (DiTs) have emerged as a highly scalable and effective backbone for image generation, outperforming U-Net architectures in both scalability and performance. However, their real-world deployment remains challenging due to high computational and memory demands. Mixed-Precision Quantization (MPQ), designed to push the limits of quantization, has demonstrated remarkable success in advancing U-Net quantization to sub-4bit settings while significantly reducing computational and memory overhead. Nevertheless, its application to DiT architectures remains limited and underexplored. In this work, we propose TreeQ, a unified framework addressing key challenges in DiT quantization. First, to tackle inefficient search and proxy misalignment, we introduce Tree Structured Search (TSS). This DiT-specific approach leverages the architecture's linear properties to traverse the solution space in O(n) time while improving objective accuracy through comparison-based pruning. Second, to unify optimization objectives, we propose Environmental Noise Guidance (ENG), which aligns Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) configurations using a single hyperparameter. Third, to mitigate information bottlenecks in ultra-low-bit regimes, we design the General Monarch Branch (GMB). This structured sparse branch prevents irreversible information loss, enabling finer detail generation. Through extensive experiments, our TreeQ framework demonstrates state-of-the-art performance on DiT-XL/2 under W3A3 and W4A4 PTQ/PEFT settings. Notably, our work is the first to achieve near-lossless 4-bit PTQ performance on DiT models. The code and models will be available at https://github.com/racoonykc/TreeQ

</details>


### [175] [Rectifying Latent Space for Generative Single-Image Reflection Removal](https://arxiv.org/abs/2512.06358)
*Mingjia Li,Jin Hu,Hainuo Wang,Qiming Hu,Jiarui Wang,Xiaojie Guo*

Main category: cs.CV

TL;DR: This paper develops a method to improve single-image reflection removal using a latent diffusion model with enhanced perception of layered image inputs.


<details>
  <summary>Details</summary>
Motivation: Current methods for single-image reflection removal face challenges in understanding and processing composed corrupted regions, leading to limitations in both image recovery and generalization in real-world scenarios.

Method: The proposed method integrates a reflection-equivariant VAE to align the latent space with reflection formation physics, a task-specific text embedding for accurate guidance, and a depth-guided sampling strategy to leverage generative stochasticity.

Result: The authors demonstrate that their model outperforms the state-of-the-art on multiple benchmarks and proves effective in complex real-world cases.

Conclusion: The study presents a novel approach to single-image reflection removal, achieving both superior performance and generalization through synergistic advances in model design.

Abstract: Single-image reflection removal is a highly ill-posed problem, where existing methods struggle to reason about the composition of corrupted regions, causing them to fail at recovery and generalization in the wild. This work reframes an editing-purpose latent diffusion model to effectively perceive and process highly ambiguous, layered image inputs, yielding high-quality outputs. We argue that the challenge of this conversion stems from a critical yet overlooked issue, i.e., the latent space of semantic encoders lacks the inherent structure to interpret a composite image as a linear superposition of its constituent layers. Our approach is built on three synergistic components, including a reflection-equivariant VAE that aligns the latent space with the linear physics of reflection formation, a learnable task-specific text embedding for precise guidance that bypasses ambiguous language, and a depth-guided early-branching sampling strategy to harness generative stochasticity for promising results. Extensive experiments reveal that our model achieves new SOTA performance on multiple benchmarks and generalizes well to challenging real-world cases.

</details>


### [176] [Spoofing-aware Prompt Learning for Unified Physical-Digital Facial Attack Detection](https://arxiv.org/abs/2512.06363)
*Jiabao Guo,Yadian Wang,Hui Ma,Yuhao Fu,Ju Jia,Hui Liu,Shengeng Tang,Lechao Cheng,Yunfeng Diao,Ajian Liu*

Main category: cs.CV

TL;DR: The paper develops a unified defense framework against physical and digital face recognition attacks, introducing a method that decouples optimization for each attack type, and shows improved performance on a large dataset.


<details>
  <summary>Details</summary>
Motivation: Face recognition systems are susceptible to both physical presentation attacks and digital forgery attacks, necessitating a comprehensive protection framework.

Method: The paper proposes the SPL-UAD framework with decoupled optimization branches via learnable parallel prompts and adaptive spoofing context prompt generation. Additionally, it employs Cues-awareness Augmentation for enhanced robustness.

Result: Extensive experiments demonstrated significant performance improvements in detecting unified attacks using the UniAttackDataPlus dataset.

Conclusion: The approach successfully integrates a comprehensive defense mechanism against both physical and digital attacks, outperforming existing methods and enhancing robustness to unseen threats.

Abstract: Real-world face recognition systems are vulnerable to both physical presentation attacks (PAs) and digital forgery attacks (DFs). We aim to achieve comprehensive protection of biometric data by implementing a unified physical-digital defense framework with advanced detection. Existing approaches primarily employ CLIP with regularization constraints to enhance model generalization across both tasks. However, these methods suffer from conflicting optimization directions between physical and digital attack detection under same category prompt spaces. To overcome this limitation, we propose a Spoofing-aware Prompt Learning for Unified Attack Detection (SPL-UAD) framework, which decouples optimization branches for physical and digital attacks in the prompt space. Specifically, we construct a learnable parallel prompt branch enhanced with adaptive Spoofing Context Prompt Generation, enabling independent control of optimization for each attack type. Furthermore, we design a Cues-awareness Augmentation that leverages the dual-prompt mechanism to generate challenging sample mining tasks on data, significantly enhancing the model's robustness against unseen attack types. Extensive experiments on the large-scale UniAttackDataPlus dataset demonstrate that the proposed method achieves significant performance improvements in unified attack detection tasks.

</details>


### [177] [Human3R: Incorporating Human Priors for Better 3D Dynamic Reconstruction from Monocular Videos](https://arxiv.org/abs/2512.06368)
*Weitao Xiong,Zhiyuan Yuan,Jiahao Lu,Chengfeng Zhao,Peng Li,Yuan Liu*

Main category: cs.CV

TL;DR: Monocular dynamic video reconstruction struggles with geometric inconsistencies and resolution issues in human scenes. This paper introduces Human3R, leveraging structured human priors for geometrically consistent and high-resolution reconstruction.


<details>
  <summary>Details</summary>
Motivation: Current approaches struggle with geometric inconsistencies, distorted limb proportions, unnatural human-object fusion, and boundary drift due to resolution issues when reconstructing dynamic human scenes.

Method: Human3R uses hybrid geometric priors (SMPL models and monocular depth estimation) and a hierarchical pipeline combining full-resolution image processing, strategic cropping, cross-attention fusion, and a Feature Fusion Module for human-specific reconstruction.

Result: Extensive experiments on datasets like TUM Dynamics and GTA-IM show that Human3R achieves superior dynamic human reconstruction results compared to existing methods.

Conclusion: Human3R effectively addresses human scene reconstruction challenges by incorporating structured geometric priors and hierarchical refinement pipelines, ensuring geometrically plausible and detailed results.

Abstract: Monocular dynamic video reconstruction faces significant challenges in dynamic human scenes due to geometric inconsistencies and resolution degradation issues. Existing methods lack 3D human structural understanding, producing geometrically inconsistent results with distorted limb proportions and unnatural human-object fusion, while memory-constrained downsampling causes human boundary drift toward background geometry. To address these limitations, we propose to incorporate hybrid geometric priors that combine SMPL human body models with monocular depth estimation. Our approach leverages structured human priors to maintain surface consistency while capturing fine-grained geometric details in human regions. We introduce Human3R, featuring a hierarchical pipeline with refinement components that processes full-resolution images for overall scene geometry, then applies strategic cropping and cross-attention fusion for human-specific detail enhancement. The method integrates SMPL priors through a Feature Fusion Module to ensure geometrically plausible reconstruction while preserving fine-grained human boundaries. Extensive experiments on TUM Dynamics and GTA-IM datasets demonstrate superior performance in dynamic human reconstruction.

</details>


### [178] [VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning](https://arxiv.org/abs/2512.06373)
*Yuji Wang,Wenlong Liu,Jingxuan Niu,Haoji Zhang,Yansong Tang*

Main category: cs.CV

TL;DR: The paper introduces VG-Refiner, a tool-refined framework for grounded reasoning, addressing limitations in handling unreliable tool outputs.


<details>
  <summary>Details</summary>
Motivation: Existing tool-integrated visual reasoning (TiVR) systems neglect handling unreliable outputs, which leads to hallucinated reasoning in grounded tasks.

Method: The method introduces a two-stage 'think-rethink' approach, a refinement reward mechanism, and improved evaluation metrics to refine reasoning processes.

Result: VG-Refiner improves accuracy and correction ability on grounded reasoning benchmarks, using minimal task-specific data while maintaining general model capabilities.

Conclusion: VG-Refiner successfully addresses unreliable tool outputs in TiVR by refining reasoning mechanisms, enhancing performance in referring and grounding tasks.

Abstract: Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.

</details>


### [179] [Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework](https://arxiv.org/abs/2512.06376)
*Xinhao Xiang,Abhijeet Rastogi,Jiawei Zhang*

Main category: cs.CV

TL;DR: The paper examines whether AI-generated driving videos (AIGVs), created from text prompts, can safely support autonomous driving model training and evaluation. It introduces diagnostic tools and benchmarks highlighting both the risks and advantages of using AIGVs.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need for scalable and low-cost alternatives to real or simulated data for autonomous driving, and the question of whether AI-generated driving videos are reliable enough for this purpose.

Method: The authors developed ADGV-Bench, a benchmark with annotated driving scenes, and ADGVE, an evaluator that assesses AIGVs' quality using multiple metrics like static semantics, temporal cues, and lane obedience signals.

Result: Experiments showed that incorporating raw AIGVs into autonomous driving datasets could harm model performance. However, filtering AIGVs using ADGVE improved video quality metrics and downstream perception models, making AIGVs a useful complement to real-world data.

Conclusion: The study reveals both risks and potentials of AIGVs in autonomous driving pipelines, offering diagnostic tools that enable safe usage and integration for training and evaluation purposes.

Abstract: Recent text-to-video models have enabled the generation of high-resolution driving scenes from natural language prompts. These AI-generated driving videos (AIGVs) offer a low-cost, scalable alternative to real or simulator data for autonomous driving (AD). But a key question remains: can such videos reliably support training and evaluation of AD models? We present a diagnostic framework that systematically studies this question. First, we introduce a taxonomy of frequent AIGV failure modes, including visual artifacts, physically implausible motion, and violations of traffic semantics, and demonstrate their negative impact on object detection, tracking, and instance segmentation. To support this analysis, we build ADGV-Bench, a driving-focused benchmark with human quality annotations and dense labels for multiple perception tasks. We then propose ADGVE, a driving-aware evaluator that combines static semantics, temporal cues, lane obedience signals, and Vision-Language Model(VLM)-guided reasoning into a single quality score for each clip. Experiments show that blindly adding raw AIGVs can degrade perception performance, while filtering them with ADGVE consistently improves both general video quality assessment metrics and downstream AD models, and turns AIGVs into a beneficial complement to real-world data. Our study highlights both the risks and the promise of AIGVs, and provides practical tools for safely leveraging large-scale video generation in future AD pipelines.

</details>


### [180] [VAD-Net: Multidimensional Facial Expression Recognition in Intelligent Education System](https://arxiv.org/abs/2512.06377)
*Yi Huo,Yun Ge*

Main category: cs.CV

TL;DR: The paper introduces VAD annotation to the FER2013 dataset, incorporates Dominance (D), and improves VAD prediction accuracy using orthogonal convolution, also making the dataset and code publicly available.


<details>
  <summary>Details</summary>
Motivation: Current FER datasets mostly lack comprehensive and precise emotion metrics, especially missing Dominance (D) dimension in VAD emotion analysis.

Method: VAD annotation is added to the FER2013 dataset by labeling the Dominance (D) dimension, and orthogonal convolution is applied in the predictive model to enhance feature diversity and accuracy.

Result: The new dataset shows that Dominance can be measured, but it is challenging compared to Valence and Arousal. Orthogonal convolution improves VAD prediction accuracy as verified in an ablation test.

Conclusion: The study provides a fully annotated VAD dataset and proposes a network for VAD prediction using orthogonal convolution, offering benchmarks and publicly available resources for future research.

Abstract: Current FER (Facial Expression Recognition) dataset is mostly labeled by emotion categories, such as happy, angry, sad, fear, disgust, surprise, and neutral which are limited in expressiveness. However, future affective computing requires more comprehensive and precise emotion metrics which could be measured by VAD(Valence-Arousal-Dominance) multidimension parameters. To address this, AffectNet has tried to add VA (Valence and Arousal) information, but still lacks D(Dominance). Thus, the research introduces VAD annotation on FER2013 dataset, takes the initiative to label D(Dominance) dimension. Then, to further improve network capacity, it enforces orthogonalized convolution on it, which extracts more diverse and expressive features and will finally increase the prediction accuracy. Experiment results show that D dimension could be measured but is difficult to obtain compared with V and A dimension no matter in manual annotation or regression network prediction. Secondly, the ablation test by introducing orthogonal convolution verifies that better VAD prediction could be obtained in the configuration of orthogonal convolution. Therefore, the research provides an initiative labelling for D dimension on FER dataset, and proposes a better prediction network for VAD prediction through orthogonal convolution. The newly built VAD annotated FER2013 dataset could act as a benchmark to measure VAD multidimensional emotions, while the orthogonalized regression network based on ResNet could act as the facial expression recognition baseline for VAD emotion prediction. The newly labeled dataset and implementation code is publicly available on https://github.com/YeeHoran/VAD-Net .

</details>


### [181] [OCFER-Net: Recognizing Facial Expression in Online Learning System](https://arxiv.org/abs/2512.06379)
*Yi Huo,Lei Zhang*

Main category: cs.CV

TL;DR: The paper presents OCFER-Net, an improved Facial Expression Recognition (FER) method emphasizing kernel orthogonality for enhanced accuracy and diversity.


<details>
  <summary>Details</summary>
Motivation: Enhance the accuracy and expressiveness of FER methods to improve emotional interaction during online learning.

Method: Introduces a regularizer enforcing orthogonality on convolutional kernels to improve feature diversity and expressiveness in the FER network.

Result: Experiments on FER-2013 dataset demonstrate superior performance with an improvement of 1.087 over baselines.

Conclusion: OCFER-Net outperforms existing methods, offering better FER capabilities, aiding emotional analysis in online learning, and the code is publicly accessible.

Abstract: Recently, online learning is very popular, especially under the global epidemic of COVID-19. Besides knowledge distribution, emotion interaction is also very important. It can be obtained by employing Facial Expression Recognition (FER). Since the FER accuracy is substantial in assisting teachers to acquire the emotional situation, the project explores a series of FER methods and finds that few works engage in exploiting the orthogonality of convolutional matrix. Therefore, it enforces orthogonality on kernels by a regularizer, which extracts features with more diversity and expressiveness, and delivers OCFER-Net. Experiments are carried out on FER-2013, which is a challenging dataset. Results show superior performance over baselines by 1.087. The code of the research project is publicly available on https://github.com/YeeHoran/OCFERNet.

</details>


### [182] [Perceptual Region-Driven Infrared-Visible Co-Fusion for Extreme Scene Enhancement](https://arxiv.org/abs/2512.06400)
*Jing Tao,Yonghong Zong,Banglei Guana,Pengju Sun,Taihang Lei,Yang Shanga,Qifeng Yu*

Main category: cs.CV

TL;DR: The paper addresses the challenge of fusing infrared and visible spectra in photogrammetry under extreme conditions. A region perception-based fusion framework achieving higher image clarity and measurement accuracy was proposed.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to preserve visible imagery quality and measurement accuracy, especially under extreme conditions, thus necessitating a new fusion framework.

Method: The framework includes region perception-based feature fusion for registration, adaptive fusion with contrast enhancement, and structural similarity compensation using saliency maps. It combines multi-exposure and multi-modal techniques using an SVE camera.

Result: The method shows improvements in image clarity and performance, outperforming state-of-the-art methods according to quantitative and visual evaluations on synthetic and real-world data.

Conclusion: The proposed framework effectively fuses infrared and visible spectra, ensuring precise registration and robust performance across diverse conditions, addressing limitations in extreme environments.

Abstract: In photogrammetry, accurately fusing infrared (IR) and visible (VIS) spectra while preserving the geometric fidelity of visible features and incorporating thermal radiation is a significant challenge, particularly under extreme conditions. Existing methods often compromise visible imagery quality, impacting measurement accuracy. To solve this, we propose a region perception-based fusion framework that combines multi-exposure and multi-modal imaging using a spatially varying exposure (SVE) camera. This framework co-fuses multi-modal and multi-exposure data, overcoming single-exposure method limitations in extreme environments. The framework begins with region perception-based feature fusion to ensure precise multi-modal registration, followed by adaptive fusion with contrast enhancement. A structural similarity compensation mechanism, guided by regional saliency maps, optimizes IR-VIS spectral integration. Moreover, the framework adapts to single-exposure scenarios for robust fusion across different conditions. Experiments conducted on both synthetic and real-world data demonstrate superior image clarity and improved performance compared to state-of-the-art methods, as evidenced by both quantitative and visual evaluations.

</details>


### [183] [Rethinking Training Dynamics in Scale-wise Autoregressive Generation](https://arxiv.org/abs/2512.06421)
*Gengze Zhou,Chongjian Ge,Hao Tan,Feng Liu,Yicong Hong*

Main category: cs.CV

TL;DR: The paper addresses issues in scale-wise autoregressive models by proposing Self-Autoregressive Refinement (SAR), which improves generation quality through train-test alignment and better supervision.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome exposure bias in autoregressive models caused by train-test mismatches and varying scale-wise optimization difficulty that hinders generation quality.

Method: The proposed solution, SAR, employs two mechanisms: Stagger-Scale Rollout (SSR) to expose the model to its own intermediate predictions for train-test alignment, and Contrastive Student-Forcing Loss (CSFL) for stable training in self-generated contexts.

Result: SAR improves the generation quality of autoregressive models, showing a 5.2% FID reduction within 10 epochs with minimal computational cost.

Conclusion: SAR proves to be an efficient and scalable solution for enhancing pretrained visual autoregressive models, showing potential as a reliable post-training method.

Abstract: Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.

</details>


### [184] [A Perception CNN for Facial Expression Recognition](https://arxiv.org/abs/2512.06422)
*Chunwei Tian,Jingyuan Xie,Lingjun Li,Wangmeng Zuo,Yanning Zhang,David Zhang*

Main category: cs.CV

TL;DR: This paper introduces the Perception Convolutional Neural Network (PCNN), which improves facial expression recognition (FER) by combining local and global facial features with a novel loss function.


<details>
  <summary>Details</summary>
Motivation: Conventional CNNs for FER can learn facial patterns but often neglect the significance of leveraging segmented facial regions for capturing subtle emotional expressions.

Method: The proposed PCNN employs five parallel networks to learn features from local facial regions (eyes, cheeks, mouth), integrates local and global features using a multi-domain interaction mechanism, and incorporates a two-phase loss function to optimize feature accuracy and reconstructed images.

Result: Experimental results demonstrate that PCNN outperforms existing methods on various FER datasets, including CK+, JAFFE, FER2013, FERPlus, RAF-DB, and Occlusion and Pose Variant Dataset.

Conclusion: PCNN's sensitivity to local and global facial features and its unique loss function make it highly effective for FER tasks. The implementation code is publicly accessible for further research and application.

Abstract: Convolutional neural networks (CNNs) can automatically learn data patterns to express face images for facial expression recognition (FER). However, they may ignore effect of facial segmentation of FER. In this paper, we propose a perception CNN for FER as well as PCNN. Firstly, PCNN can use five parallel networks to simultaneously learn local facial features based on eyes, cheeks and mouth to realize the sensitive capture of the subtle changes in FER. Secondly, we utilize a multi-domain interaction mechanism to register and fuse between local sense organ features and global facial structural features to better express face images for FER. Finally, we design a two-phase loss function to restrict accuracy of obtained sense information and reconstructed face images to guarantee performance of obtained PCNN in FER. Experimental results show that our PCNN achieves superior results on several lab and real-world FER benchmarks: CK+, JAFFE, FER2013, FERPlus, RAF-DB and Occlusion and Pose Variant Dataset. Its code is available at https://github.com/hellloxiaotian/PCNN.

</details>


### [185] [DragMesh: Interactive 3D Generation Made Easy](https://arxiv.org/abs/2512.06424)
*Tianshan Zhang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: DragMesh is a framework for real-time 3D interactive motion that combines decoupled kinematics reasoning and novel motion generation using dual quaternion representations.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in current systems that either lack real-time capabilities or violate kinematic constraints when generating articulated motion.

Method: DragMesh employs decoupled kinematics reasoning to infer joint parameters and uses a novel Dual Quaternion VAE to generate plausible motion trajectories while adhering to strict kinematic consistency.

Result: The proposed framework achieves real-time performance, ensuring plausible and generative motion articulation on novel objects without requiring retraining.

Conclusion: DragMesh marks a practical advancement in achieving generative 3D intelligence, combining real-time interaction with accurate motion articulation constraints.

Abstract: While generative models have excelled at creating static 3D content, the pursuit of systems that understand how objects move and respond to interactions remains a fundamental challenge. Current methods for articulated motion lie at a crossroads: they are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. We present DragMesh, a robust framework for real-time interactive 3D articulation built around a lightweight motion generation core. Our core contribution is a novel decoupled kinematic reasoning and motion generation framework. First, we infer the latent joint parameters by decoupling semantic intent reasoning (which determines the joint type) from geometric regression (which determines the axis and origin using our Kinematics Prediction Network (KPP-Net)). Second, to leverage the compact, continuous, and singularity-free properties of dual quaternions for representing rigid body motion, we develop a novel Dual Quaternion VAE (DQ-VAE). This DQ-VAE receives these predicted priors, along with the original user drag, to generate a complete, plausible motion trajectory. To ensure strict adherence to kinematics, we inject the joint priors at every layer of the DQ-VAE's non-autoregressive Transformer decoder using FiLM (Feature-wise Linear Modulation) conditioning. This persistent, multi-scale guidance is complemented by a numerically-stable cross-product loss to guarantee axis alignment. This decoupled design allows DragMesh to achieve real-time performance and enables plausible, generative articulation on novel objects without retraining, offering a practical step toward generative 3D intelligence. Code: https://github.com/AIGeeksGroup/DragMesh. Website: https://aigeeksgroup.github.io/DragMesh.

</details>


### [186] [When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition](https://arxiv.org/abs/2512.06426)
*Nzakiese Mbongo,Kailash A. Hambarde,Hugo Proença*

Main category: cs.CV

TL;DR: The paper introduces a dual-path transformer framework that utilizes CLIP for accurate gender recognition in long-range imagery, addressing challenges like low resolution and occlusions.


<details>
  <summary>Details</summary>
Motivation: Improving accurate gender recognition under extreme long-range conditions where traditional approaches struggle due to limited spatial resolution, variable viewpoints, and loss of facial features.

Method: The framework uses CLIP-based dual-path learning consisting of (1) a direct visual pathway for fine-tuning CLIP's image encoder and (2) an attribute-mediated pathway leveraging soft-biometric prompts, enhanced with spatial channel attention modules. A new dataset for evaluation, U-DetAGReID, is also introduced.

Result: The framework outperforms state-of-the-art models on various metrics (macro-F1, accuracy, AUC) and demonstrates robustness to challenging conditions, validated through qualitative attention visualizations.

Conclusion: The proposed dual-path learning framework, guided by language and attributes, provides an interpretable, extensible solution for gender recognition in unconstrained, long-range settings.

Abstract: Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.

</details>


### [187] [Automated Deep Learning Estimation of Anthropometric Measurements for Preparticipation Cardiovascular Screening](https://arxiv.org/abs/2512.06434)
*Lucas R. Mareque,Ricardo L. Armentano,Leandro J. Cymberknop*

Main category: cs.CV

TL;DR: This paper introduces a deep-learning approach to accurately estimate anthropometric measurements from synthetic 2D images, aiming to streamline athlete cardiovascular risk screening.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reduce sudden cardiac death (SCD) in athletes by improving the scalability and accuracy of preparticipation cardiovascular examinations using automated anthropometric measurements.

Method: The authors trained VGG19, ResNet50, and DenseNet121 models on 100,000 synthetic 2D images derived from 3D body meshes to predict five anthropometric measurements. ResNet50 achieved the best performance with an MAE of 0.668 cm.

Result: All tested models performed well with sub-centimeter accuracy, and ResNet50 was the most effective model.

Conclusion: This approach proves deep learning's potential in automating anthropometric measurements for athlete screening, with future steps being real-world validation for additional applications.

Abstract: Preparticipation cardiovascular examination (PPCE) aims to prevent sudden cardiac death (SCD) by identifying athletes with structural or electrical cardiac abnormalities. Anthropometric measurements, such as waist circumference, limb lengths, and torso proportions to detect Marfan syndrome, can indicate elevated cardiovascular risk. Traditional manual methods are labor-intensive, operator-dependent, and challenging to scale. We present a fully automated deep-learning approach to estimate five key anthropometric measurements from 2D synthetic human body images. Using a dataset of 100,000 images derived from 3D body meshes, we trained and evaluated VGG19, ResNet50, and DenseNet121 with fully connected layers for regression. All models achieved sub-centimeter accuracy, with ResNet50 performing best, achieving a mean MAE of 0.668 cm across all measurements. Our results demonstrate that deep learning can deliver accurate anthropometric data at scale, offering a practical tool to complement athlete screening protocols. Future work will validate the models on real-world images to extend applicability.

</details>


### [188] [AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars](https://arxiv.org/abs/2512.06438)
*Ramazan Fazylov,Sergey Zagoruyko,Aleksandr Parkin,Stamatis Lefkimmiatis,Ivan Laptev*

Main category: cs.CV

TL;DR: AGORA introduces a novel approach to creating realistic, animatable 3D human avatars with high expression fidelity and real-time rendering speeds, combining 3D Gaussian Splatting and generative adversarial networks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in existing 3D human avatar generation methods, particularly slow rendering and lack of dynamic control in NeRFs and dynamic inconsistency with 3D Gaussian Splatting methods.

Method: The authors propose a framework called AGORA that integrates a lightweight deformation branch conditioned on FLAME, enabling expression control with per-Gaussian residuals. It uses a dual-discriminator training scheme for precise expression fidelity and achieves efficient real-time rendering.

Result: AGORA outperforms NeRF-based methods in expression accuracy, enabling rendering above 250 FPS on GPU and around 9 FPS on CPU. It produces realistic and controllable avatars that are animatable under practical conditions.

Conclusion: AGORA represents a significant advancement in 3D human avatar synthesis, providing a practical solution for creating high-fidelity, animatable, and controllable avatars. This marks an important step toward real-world applications in VR, telepresence, and entertainment.

Abstract: The generation of high-fidelity, animatable 3D human avatars remains a core challenge in computer graphics and vision, with applications in VR, telepresence, and entertainment. Existing approaches based on implicit representations like NeRFs suffer from slow rendering and dynamic inconsistencies, while 3D Gaussian Splatting (3DGS) methods are typically limited to static head generation, lacking dynamic control. We bridge this gap by introducing AGORA, a novel framework that extends 3DGS within a generative adversarial network to produce animatable avatars. Our key contribution is a lightweight, FLAME-conditioned deformation branch that predicts per-Gaussian residuals, enabling identity-preserving, fine-grained expression control while allowing real-time inference. Expression fidelity is enforced via a dual-discriminator training scheme leveraging synthetic renderings of the parametric mesh. AGORA generates avatars that are not only visually realistic but also precisely controllable. Quantitatively, we outperform state-of-the-art NeRF-based methods on expression accuracy while rendering at 250+ FPS on a single GPU, and, notably, at $\sim$9 FPS under CPU-only inference - representing, to our knowledge, the first demonstration of practical CPU-only animatable 3DGS avatar synthesis. This work represents a significant step toward practical, high-performance digital humans. Project website: https://ramazan793.github.io/AGORA/

</details>


### [189] [Towards Stable Cross-Domain Depression Recognition under Missing Modalities](https://arxiv.org/abs/2512.06447)
*Jiuyi Chen,Mingkui Tan,Haifeng Lu,Qiuna Xu,Zhihua Wang,Runhao Zeng,Xiping Hu*

Main category: cs.CV

TL;DR: The paper introduces a framework (SCD-MLLM) for stable cross-domain depression recognition using a multimodal large language model, effectively addressing the challenges of diverse data sources and missing modalities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address public health risks associated with depression by creating a scalable automated solution for depression detection that can handle diverse recognition scenarios and incomplete data inputs, overcoming limitations in traditional audio/video-based methods.

Method: The proposed SCD-MLLM introduces two main components: (i) a Multi-Source Data Input Adapter (MDIA) to unify input data through masking and prompts, and (ii) a Modality-Aware Adaptive Fusion Module (MAFM) for adaptive integration of multimodal features, ensuring robustness to missing modality data.

Result: Through experiments on five datasets (CMDC, AVEC2014, DAIC-WOZ, DVlog, EATD), the framework outperformed current state-of-the-art models and leading commercial language models in generalizability, multimodal depression cue detection, and handling incomplete data.

Conclusion: SCD-MLLM provides a robust, scalable, and generalizable solution for automated depression detection, significantly advancing multimodal analysis and addressing real-world challenges like missing data modalities.

Abstract: Depression poses serious public health risks, including suicide, underscoring the urgency of timely and scalable screening. Multimodal automatic depression detection (ADD) offers a promising solution; however, widely studied audio- and video-based ADD methods lack a unified, generalizable framework for diverse depression recognition scenarios and show limited stability to missing modalities, which are common in real-world data. In this work, we propose a unified framework for Stable Cross-Domain Depression Recognition based on Multimodal Large Language Model (SCD-MLLM). The framework supports the integration and processing of heterogeneous depression-related data collected from varied sources while maintaining stability in the presence of incomplete modality inputs. Specifically, SCD-MLLM introduces two key components: (i) Multi-Source Data Input Adapter (MDIA), which employs masking mechanism and task-specific prompts to transform heterogeneous depression-related inputs into uniform token sequences, addressing inconsistency across diverse data sources; (ii) Modality-Aware Adaptive Fusion Module (MAFM), which adaptively integrates audio and visual features via a shared projection mechanism, enhancing resilience under missing modality conditions. e conduct comprehensive experiments under multi-dataset joint training settings on five publicly available and heterogeneous depression datasets from diverse scenarios: CMDC, AVEC2014, DAIC-WOZ, DVlog, and EATD. Across both complete and partial modality settings, SCD-MLLM outperforms state-of-the-art (SOTA) models as well as leading commercial LLMs (Gemini and GPT), demonstrating superior cross-domain generalization, enhanced ability to capture multimodal cues of depression, and strong stability to missing modality cases in real-world applications.

</details>


### [190] [Sanvaad: A Multimodal Accessibility Framework for ISL Recognition and Voice-Based Interaction](https://arxiv.org/abs/2512.06485)
*Kush Revankar,Shreyas Deshpande,Araham Sayeed,Ansh Tandale,Sarika Bobde*

Main category: cs.CV

TL;DR: Sanvaad is a framework enabling real-time, two-way communication between deaf, visually impaired, and hearing users using lightweight technologies.


<details>
  <summary>Details</summary>
Motivation: To facilitate inclusive communication between deaf, visually impaired, and hearing populations, addressing limitations of one-way interaction tools.

Method: It uses an ISL recognition module with MediaPipe for sign language analysis and voice-to-sign mapping for deaf users, alongside a screen-free multilingual voice interface for visually impaired users.

Result: Sanvaad successfully integrates computer vision and speech processing into a unified system, operable on desktops and mobile devices.

Conclusion: Sanvaad demonstrates the potential for lightweight frameworks in bridging communication barriers across diverse user groups.

Abstract: Communication between deaf users, visually im paired users, and the general hearing population often relies on tools that support only one direction of interaction. To address this limitation, this work presents Sanvaad, a lightweight multimodal accessibility framework designed to support real time, two-way communication. For deaf users, Sanvaad includes an ISL recognition module built on MediaPipe landmarks. MediaPipe is chosen primarily for its efficiency and low computational load, enabling the system to run smoothly on edge devices without requiring dedicated hardware. Spoken input from a phone can also be translated into sign representations through a voice-to-sign component that maps detected speech to predefined phrases and produces corresponding GIFs or alphabet-based visualizations. For visually impaired users, the framework provides a screen free voice interface that integrates multilingual speech recognition, text summarization, and text-to-speech generation. These components work together through a Streamlit-based interface, making the system usable on both desktop and mobile environments. Overall, Sanvaad aims to offer a practical and accessible pathway for inclusive communication by combining lightweight computer vision and speech processing tools within a unified framework.

</details>


### [191] [Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion](https://arxiv.org/abs/2512.06504)
*Andrii Lysyi,Anatoliy Sachenko,Pavlo Radiuk,Mykola Lysyi,Oleksandr Melnychenko,Diana Zahorodnia*

Main category: cs.CV

TL;DR: The paper develops an intelligent and automated system for inspecting photovoltaic (PV) infrastructure, addressing issues like data redundancy and high communication bandwidth.


<details>
  <summary>Details</summary>
Motivation: Address shortcomings in conventional PV inspection methods, such as thermal palette bias and inefficient workflows, to improve plant safety and efficiency.

Method: Introduces a synergistic architecture featuring palette-invariant thermal embeddings, RGB stream fusion with a gated mechanism, adaptive re-acquisition, and geospatial deduplication modules.

Result: The system achieved an mAP@0.5 of 0.903 on the PVF-10 benchmark (12-15% better than baselines), 96% recall, reduced duplicate false positives by 15-20%, and cut data transmission by 60-70%.

Conclusion: This novel framework establishes a new standard for PV inspection with significant improvements in accuracy, efficiency, and operational reliability.

Abstract: The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.

</details>


### [192] [ShadowWolf -- Automatic Labelling, Evaluation and Model Training Optimised for Camera Trap Wildlife Images](https://arxiv.org/abs/2512.06521)
*Jens Dede,Anna Förster*

Main category: cs.CV

TL;DR: The paper introduces ShadowWolf, a unified AI framework for enhancing wildlife monitoring by addressing challenges like environmental variability and minimizing manual labelling efforts.


<details>
  <summary>Details</summary>
Motivation: Human expansion reduces wildlife spaces and increases human-wildlife interaction, necessitating efficient ways to monitor wildlife. Current AI techniques face challenges with environmental variability.

Method: The proposed ShadowWolf framework integrates AI model training and evaluation while enabling dynamic retraining and on-site model adaptation to adjust to environmental changes.

Result: ShadowWolf reduces the need for labelling and increases robustness and efficiency in wildlife monitoring systems.

Conclusion: The framework makes wildlife monitoring systems more accurate and scalable, which benefits conservation efforts.

Abstract: The continuous growth of the global human population is leading to the expansion of human habitats, resulting in decreasing wildlife spaces and increasing human-wildlife interactions. These interactions can range from minor disturbances, such as raccoons in urban waste bins, to more severe consequences, including species extinction. As a result, the monitoring of wildlife is gaining significance in various contexts. Artificial intelligence (AI) offers a solution by automating the recognition of animals in images and videos, thereby reducing the manual effort required for wildlife monitoring. Traditional AI training involves three main stages: image collection, labelling, and model training. However, the variability, for example, in the landscape (e.g., mountains, open fields, forests), weather (e.g., rain, fog, sunshine), lighting (e.g., day, night), and camera-animal distances presents significant challenges to model robustness and adaptability in real-world scenarios.
  In this work, we propose a unified framework, called ShadowWolf, designed to address these challenges by integrating and optimizing the stages of AI model training and evaluation. The proposed framework enables dynamic model retraining to adjust to changes in environmental conditions and application requirements, thereby reducing labelling efforts and allowing for on-site model adaptation. This adaptive and unified approach enhances the accuracy and efficiency of wildlife monitoring systems, promoting more effective and scalable conservation efforts.

</details>


### [193] [On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization](https://arxiv.org/abs/2512.06530)
*Mohammed Wattad,Tamir Shor,Alex Bronstein*

Main category: cs.CV

TL;DR: Learned k-space acquisition patterns improve reconstruction quality in accelerated MRI and generalize across different imaging domains when trained with stochastic perturbations.


<details>
  <summary>Details</summary>
Motivation: Enhancing MRI reconstruction quality while ensuring transferability of acquisition patterns across imaging domains.

Method: Evaluated generalization with learned sampling patterns and introduced stochastic perturbations to simulate scanner variability during training.

Result: Learned k-space sampling showed improved generalization across datasets and imaging paradigms, especially with introduced acquisition uncertainty.

Conclusion: K-space trajectory design should be viewed as an active tool for enhancing domain generalization in MRI reconstruction.

Abstract: Recent work has established learned k-space acquisition patterns as a promising direction for improving reconstruction quality in accelerated Magnetic Resonance Imaging (MRI). Despite encouraging results, most existing research focuses on acquisition patterns optimized for a single dataset or modality, with limited consideration of their transferability across imaging domains. In this work, we demonstrate that the benefits of learned k-space sampling can extend beyond the training domain, enabling superior reconstruction performance under domain shifts. Our study presents two main contributions. First, through systematic evaluation across datasets and acquisition paradigms, we show that models trained with learned sampling patterns exhibitimproved generalization under cross-domain settings. Second, we propose a novel method that enhances domain robustness by introducing acquisition uncertainty during training-stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions. Our results highlight the importance of treating kspace trajectory design not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction.

</details>


### [194] [Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images](https://arxiv.org/abs/2512.06531)
*Sayan Das,Arghadip Biswas*

Main category: cs.CV

TL;DR: This paper proposes two novel deep learning architectures, achieving high accuracy in classifying and segmenting brain tumors.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the difficulty in manually detecting brain tumors from MRI scans due to the growing volume of data and to leverage AI for faster and more accurate detection.

Method: The authors introduced two deep learning models: SAETCN for tumor classification with 99.38% accuracy and SAS-Net for tumor segmentation with 99.23% pixel accuracy.

Result: The models demonstrated highly accurate performance on validation data, surpassing existing methods in their classification and segmentation tasks.

Conclusion: These models effectively provide a robust CAD system for early brain tumor detection, showcasing the potential of deep learning in medical diagnostics.

Abstract: Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.

</details>


### [195] [Bridging spatial awareness and global context in medical image segmentation](https://arxiv.org/abs/2512.06560)
*Dalia Alzu'bi,A. Ben Hamza*

Main category: cs.CV

TL;DR: U-CycleMLP is a U-shaped network for medical image segmentation that achieves high accuracy with lightweight architecture by incorporating multiscale feature extraction and precise boundary delineation methods.


<details>
  <summary>Details</summary>
Motivation: Existing segmentation models often fail to capture local and global contextual information effectively, causing boundary pixel loss and errors in medical image segmentation.

Method: U-CycleMLP employs a novel encoder-decoder architecture using position attention weights, dense atrous blocks for multiscale feature learning, and CycleMLP blocks for refined predictions with lightweight computations.

Result: Experiments on three benchmark datasets show U-CycleMLP’s superior segmentation accuracy, ability to capture fine-grained structures, and robustness across varied medical imaging modalities.

Conclusion: U-CycleMLP demonstrates competitive performance compared to state-of-the-art methods, proving its efficiency in achieving precise segmentation with a lightweight model architecture.

Abstract: Medical image segmentation is a fundamental task in computer-aided diagnosis, requiring models that balance segmentation accuracy and computational efficiency. However, existing segmentation models often struggle to effectively capture local and global contextual information, leading to boundary pixel loss and segmentation errors. In this paper, we propose U-CycleMLP, a novel U-shaped encoder-decoder network designed to enhance segmentation performance while maintaining a lightweight architecture. The encoder learns multiscale contextual features using position attention weight excitation blocks, dense atrous blocks, and downsampling operations, effectively capturing both local and global contextual information. The decoder reconstructs high-resolution segmentation masks through upsampling operations, dense atrous blocks, and feature fusion mechanisms, ensuring precise boundary delineation. To further refine segmentation predictions, channel CycleMLP blocks are incorporated into the decoder along the skip connections, enhancing feature integration while maintaining linear computational complexity relative to input size. Experimental results, both quantitative and qualitative, across three benchmark datasets demonstrate the competitive performance of U-CycleMLP in comparison with state-of-the-art methods, achieving better segmentation accuracy across all datasets, capturing fine-grained anatomical structures, and demonstrating robustness across different medical imaging modalities. Ablation studies further highlight the importance of the model's core architectural components in enhancing segmentation accuracy.

</details>


### [196] [SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities](https://arxiv.org/abs/2512.06562)
*Dung Thuy Nguyen,Quang Nguyen,Preston K. Robinette,Eli Jiang,Taylor T. Johnson,Kevin Leach*

Main category: cs.CV

TL;DR: SUGAR introduces a method to remove specific human identities from generative models without retraining, preserving quality and diversity.


<details>
  <summary>Details</summary>
Motivation: The development of 3D-aware generative models enables impressive identity synthesis but raises ethical concerns around user consent, leading to a need for identity removal from models.

Method: SUGAR removes identities by learning personalized surrogate latents for each identity, redirecting reconstructions without degrading the overall model quality. It employs a continual utility preservation objective to maintain performance as identities are forgotten.

Result: SUGAR removes up to 200 identities in generative models, achieves state-of-the-art unlearning performance, and delivers a 700% improvement in retention utility over existing methods.

Conclusion: SUGAR provides a scalable and efficient approach to generative unlearning, addressing ethical concerns and ensuring model quality and diversity. The code is publicly available for use.

Abstract: Recent advances in 3D-aware generative models have enabled high-fidelity image synthesis of human identities. However, this progress raises urgent questions around user consent and the ability to remove specific individuals from a model's output space. We address this by introducing SUGAR, a framework for scalable generative unlearning that enables the removal of many identities (simultaneously or sequentially) without retraining the entire model. Rather than projecting unwanted identities to unrealistic outputs or relying on static template faces, SUGAR learns a personalized surrogate latent for each identity, diverting reconstructions to visually coherent alternatives while preserving the model's quality and diversity. We further introduce a continual utility preservation objective that guards against degradation as more identities are forgotten. SUGAR achieves state-of-the-art performance in removing up to 200 identities, while delivering up to a 700% improvement in retention utility compared to existing baselines. Our code is publicly available at https://github.com/judydnguyen/SUGAR-Generative-Unlearn.

</details>


### [197] [GNC-Pose: Geometry-Aware GNC-PnP for Accurate 6D Pose Estimation](https://arxiv.org/abs/2512.06565)
*Xiujin Liu*

Main category: cs.CV

TL;DR: GNC-Pose is a robust monocular 6D object pose estimation pipeline, fully learning-free, using geometry-aware weighting and GNC optimization for textured objects.


<details>
  <summary>Details</summary>
Motivation: To develop a robust and learning-free approach for 6D pose estimation for textured objects without relying on extensive training or category-specific priors.

Method: Combines rendering-based initialization, geometry-aware correspondence weighting, and robust Graduated Non-Convexity (GNC) optimization for pose estimation. It uses coarse 2D-3D correspondences and structural consistency to stabilize optimization against outliers. Final LM refinement enhances accuracy.

Result: Tested on YCB Object and Model Set, it achieves competitive accuracy comparable to both learning-based and learning-free methods.

Conclusion: GNC-Pose is a simple, robust, and practical solution for learning-free 6D pose estimation, demonstrating its effectiveness despite no reliance on learned features or training.

Abstract: We present GNC-Pose, a fully learning-free monocular 6D object pose estimation pipeline for textured objects that combines rendering-based initialization, geometry-aware correspondence weighting, and robust GNC optimization. Starting from coarse 2D-3D correspondences obtained through feature matching and rendering-based alignment, our method builds upon the Graduated Non-Convexity (GNC) principle and introduces a geometry-aware, cluster-based weighting mechanism that assigns robust per point confidence based on the 3D structural consistency of the model. This geometric prior and weighting strategy significantly stabilizes the optimization under severe outlier contamination. A final LM refinement further improve accuracy. We tested GNC-Pose on The YCB Object and Model Set, despite requiring no learned features, training data, or category-specific priors, GNC-Pose achieves competitive accuracy compared with both learning-based and learning-free methods, and offers a simple, robust, and practical solution for learning-free 6D pose estimation.

</details>


### [198] [MedGRPO: Multi-Task Reinforcement Learning for Heterogeneous Medical Video Understanding](https://arxiv.org/abs/2512.06581)
*Yuhao Su,Anwesa Choudhuri,Zhongpai Gao,Benjamin Planche,Van Nguyen Nguyen,Meng Zheng,Yuhan Shen,Arun Innanje,Terrence Chen,Ehsan Elhamifar,Ziyan Wu*

Main category: cs.CV

TL;DR: This paper addresses the challenges of medical video understanding by introducing a new benchmark, MedVidBench, and a robust training methodology, MedGRPO.


<details>
  <summary>Details</summary>
Motivation: Large vision-language models struggle with medical video tasks requiring spatial precision, temporal reasoning, and clinical semantics.

Method: The authors introduce MedVidBench, a large-scale medical video-instruction benchmark, and MedGRPO, a reinforcement learning framework with cross-dataset reward normalization and medical LLM for qualitative evaluation.

Result: Supervised fine-tuning on MedVidBench outperforms existing models, and MedGRPO further enhances performance in grounding and captioning tasks.

Conclusion: This work establishes MedVidBench and MedGRPO as essential tools for advancing vision-language models in the medical field, delivering superior performance and a robust training framework.

Abstract: Large vision-language models struggle with medical video understanding, where spatial precision, temporal reasoning, and clinical semantics are critical. To address this, we first introduce \textbf{MedVidBench}, a large-scale benchmark of 531,850 video-instruction pairs across 8 medical sources spanning video, segment, and frame-level tasks, curated through a rigorous quality assurance pipeline with expert-guided prompting and dual-model validation. While supervised fine-tuning on MedVidBench yields noticeable gains, standard Reinforcement Learning (RL) fails due to imbalanced reward scales across datasets, which destabilizes optimization and leads to training collapse. To overcome this, we introduce \textbf{MedGRPO}, a novel RL framework for balanced multi-dataset training with two key innovations: (1) \emph{cross-dataset reward normalization} that maps each dataset's median performance to a common reward value, ensuring fair optimization regardless of difficulty, and (2) a \emph{medical LLM judge} that evaluates caption quality on five clinical dimensions through comparative similarity scoring. Supervised fine-tuning Qwen2.5-VL-7B on MedVidBench substantially outperforms GPT-4.1 and Gemini-2.5-Flash across all tasks, demonstrating MedVidBench's efficacy, while our MedGRPO framework further improves upon the SFT baseline across grounding and captioning tasks. Our work establishes a foundational benchmark and robust training methodology for advancing vision-language models in medical domains. Our project website is available at https://yuhaosu.github.io/MedGRPO/.

</details>


### [199] [From Remote Sensing to Multiple Time Horizons Forecasts: Transformers Model for CyanoHAB Intensity in Lake Champlain](https://arxiv.org/abs/2512.06598)
*Muhammad Adil,Patrick J. Clemins,Andrew W. Schroth,Panagiotis D. Oikonomou,Donna M. Rizzo,Peter D. F. Isles,Xiaohan Zhang,Kareem I. Hannoun,Scott Turnbull,Noah B. Beckage,Asim Zia,Safwan Wshah*

Main category: cs.CV

TL;DR: This paper develops a forecasting framework combining Transformers and BiLSTM to predict CyanoHAB intensities in Lake Champlain, leveraging sparse satellite data and imputation techniques to enable up to 14-day forecasts with high accuracy.


<details>
  <summary>Details</summary>
Motivation: CyanoHABs critically threaten aquatic ecosystems and public health globally. Lake Champlain is highly susceptible due to nutrient load and climatic variability. There is a pressing need for scalable, reliable forecasting tools to support effective management and mitigation of these harmful blooms.

Method: The authors propose a remote sensing-based forecasting framework using Transformers and BiLSTM models. Data preprocessing applied forward fill, weighted temporal imputation, and smoothing for handling substantial data gaps. Features were engineered by binning Cyanobacterial Index data and extracting temperature statistics.

Result: The system achieved strong performance, with F1 scores of 89.5%, 86.4%, and 85.5% for one, two, and three-day forecasts, respectively, and an F1 score of 78.9% with an AUC of 82.6% at the 14-day horizon, showcasing its capability to model spatiotemporal dynamics effectively.

Conclusion: The study confirms the effectiveness of combining Transformers and BiLSTM for forecasting CyanoHABs, even with sparse satellite data. This reliable prediction framework offers promising potential for improved CyanoHAB management and early warning systems.

Abstract: Cyanobacterial Harmful Algal Blooms (CyanoHABs) pose significant threats to aquatic ecosystems and public health globally. Lake Champlain is particularly vulnerable to recurring CyanoHAB events, especially in its northern segment: Missisquoi Bay, St. Albans Bay, and Northeast Arm, due to nutrient enrichment and climatic variability. Remote sensing provides a scalable solution for monitoring and forecasting these events, offering continuous coverage where in situ observations are sparse or unavailable. In this study, we present a remote sensing only forecasting framework that combines Transformers and BiLSTM to predict CyanoHAB intensities up to 14 days in advance. The system utilizes Cyanobacterial Index data from the Cyanobacterial Assessment Network and temperature data from Moderate Resolution Imaging Spectroradiometer satellites to capture long range dependencies and sequential dynamics in satellite time series. The dataset is very sparse, missing more than 30% of the Cyanobacterial Index data and 90% of the temperature data. A two stage preprocessing pipeline addressed data gaps by applying forward fill and weighted temporal imputation at the pixel level, followed by smoothing to reduce the discontinuities of CyanoHAB events. The raw dataset is transformed into meaningful features through equal frequency binning for the Cyanobacterial Index values and extracted temperature statistics. Transformer BiLSTM model demonstrates strong forecasting performance across multiple horizons, achieving F1 scores of 89.5%, 86.4%, and 85.5% at one, two, and three-day forecasts, respectively, and maintaining an F1 score of 78.9% with an AUC of 82.6% at the 14-day horizon. These results confirm the model's ability to capture complex spatiotemporal dynamics from sparse satellite data and to provide reliable early warning for CyanoHABs management.

</details>


### [200] [More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery](https://arxiv.org/abs/2512.07596)
*Wenzhen Dong,Jieming Yu,Yiming Huang,Hongqiu Wang,Lei Zhu,Albert C. S. Chung,Hongliang Ren,Long Bai*

Main category: cs.CV

TL;DR: The Segment Anything Model (SAM) 3 introduces enhanced segmentation capabilities, language prompts, and improved 3D perception, showing advancements but needing further optimization for surgical applications.


<details>
  <summary>Details</summary>
Motivation: To improve segmentation models for robot-assisted surgery, enabling zero-shot segmentation, dynamic video tracking, language-based prompts, and effective 3D reconstruction of surgical scenes.

Method: Performance testing of SAM 3 using spatial prompts, language prompts, and 3D reconstruction benchmarks (e.g., MICCAI EndoVis and others) for assessing segmentation and depth estimation capabilities.

Result: SAM 3 outperformed SAM and SAM 2 on image/video segmentation under spatial prompts, demonstrated strong 3D reconstruction, but showed limitations in highly dynamic surgical scenes and suboptimal performance for language prompts.

Conclusion: SAM 3 represents a significant advancement in segmentation technology, excelling in 3D surgery tasks, but requires domain-specific refinements for optimal use in complex scenarios and language-based segmentation.

Abstract: The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation. While language prompts show potential, their performance in the surgical domain is currently suboptimal, highlighting the need for further domain-specific training. Additionally, we investigate SAM 3's 3D reconstruction abilities, demonstrating its capacity to process surgical scene data and reconstruct 3D anatomical structures from 2D images. Through comprehensive testing on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 3 shows clear improvements over SAM and SAM 2 in both image and video segmentation under spatial prompts, while zero-shot evaluations on SCARED, StereoMIS, and EndoNeRF indicate strong monocular depth estimation and realistic 3D instrument reconstruction, yet also reveal remaining limitations in complex, highly dynamic surgical scenes.

</details>


### [201] [Learning Relative Gene Expression Trends from Pathology Images in Spatial Transcriptomics](https://arxiv.org/abs/2512.06612)
*Kazuya Nishimura,Haruka Hirose,Ryoma Bise,Kaito Shiku,Yasuhiro Kojima*

Main category: cs.CV

TL;DR: This paper proposes STRank, a method for gene expression estimation from pathology images, focusing on learning relative expression patterns rather than absolute values to tackle noise and batch effects.


<details>
  <summary>Details</summary>
Motivation: The current challenge in gene expression estimation lies in inaccuracies due to stochastic noise and batch effects in sequencing techniques, making it hard to predict absolute values accurately.

Method: A novel loss function, STRank, is introduced to learn relative gene expression patterns by modeling the relation and leveraging consistent patterns unaffected by variability and noise.

Result: Experiments with both synthetic and real datasets demonstrate STRank's effectiveness and robustness compared to absolute value prediction methods.

Conclusion: Focusing on relative expression patterns through STRank presents a promising solution for reducing RNA sequencing costs effectively, addressing variability and noise challenges in gene expression estimation.

Abstract: Gene expression estimation from pathology images has the potential to reduce the RNA sequencing cost. Point-wise loss functions have been widely used to minimize the discrepancy between predicted and absolute gene expression values. However, due to the complexity of the sequencing techniques and intrinsic variability across cells, the observed gene expression contains stochastic noise and batch effects, and estimating the absolute expression values accurately remains a significant challenge. To mitigate this, we propose a novel objective of learning relative expression patterns rather than absolute levels. We assume that the relative expression levels of genes exhibit consistent patterns across independent experiments, even when absolute expression values are affected by batch effects and stochastic noise in tissue samples. Based on the assumption, we model the relation and propose a novel loss function called STRank that is robust to noise and batch effects. Experiments using synthetic datasets and real datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/naivete5656/STRank.

</details>


### [202] [Hierarchical Deep Learning for Diatom Image Classification: A Multi-Level Taxonomic Approach](https://arxiv.org/abs/2512.06613)
*Yueying Ke*

Main category: cs.CV

TL;DR: This paper introduces a hierarchical convolutional network for diatom identification, outperforming flat models in taxonomic accuracy and error localization.


<details>
  <summary>Details</summary>
Motivation: Conventional diatom identification requires expert taxonomists, and flat classification methods overlook the taxonomic hierarchy.

Method: A hierarchical convolutional network with cascaded heads predicts multiple taxonomic ranks simultaneously using shared features and constraint masks.

Result: The hierarchical model matches species-level accuracy of flat models while improving accuracy at higher taxonomic levels and reducing misclassification taxonomic distance by 38.2%.

Conclusion: Hierarchical models provide better taxonomic predictions, with biologically aligned outputs and enhanced interpretability compared to flat models.

Abstract: Accurate taxonomic identification of diatoms is essential for aquatic ecosystem monitoring, yet conventional methods depend heavily on expert taxonomists. Recent deep learning approaches improve automation, but most treat diatom recognition as flat classification predicting only one taxonomic rank. We investigate whether embedding taxonomic hierarchy into neural network architectures can improve both accuracy and error locality.
  We introduce a hierarchical convolutional network with five cascaded heads that jointly predict class, order, family, genus, and species. Each head receives shared backbone features and probability distributions from higher levels, with binary masks restricting predictions to valid descendants during training and inference. Using a filtered dataset of 1,456 diatom images covering 82 species, we compare hierarchical and flat models under identical settings.
  The hierarchical model matches flat baselines at species level (69.4% accuracy) while outperforming at all upper taxonomic levels. When species predictions fail, errors remain taxonomically local: 92.5 % of misclassified species are correctly predicted at genus level, versus 67.2% for flat baselines. The hierarchical model reduces mean taxonomic distance by 38.2% (1.209 vs. 1.955).
  Progressive training reveals bidirectional mechanisms: hierarchical constraint masks operate top-down to constrain prediction space, while gradients from fine-grained levels propagate bottom-up through the shared backbone, refining features. This improves class accuracy from 96.2% to 99.5% and yields 6-8% gains at upper levels, producing more robust, interpretable, and biologically aligned predictions for multi-level taxonomic classification.

</details>


### [203] [sim2art: Accurate Articulated Object Modeling from a Single Video using Synthetic Training Data Only](https://arxiv.org/abs/2512.07698)
*Arslan Artykov,Corentin Sautier,Vincent Lepetit*

Main category: cs.CV

TL;DR: The paper introduces a new method to predict part segmentation and joint parameters of articulated objects from monocular video captured with a freely moving camera.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of understanding articulated objects using casually captured video, enabling scalable real-world applications.

Method: A data-driven model trained on synthetic data to predict part segmentation and joint parameters from monocular video.

Result: The method shows strong performance and generalization to real-world objects, despite being trained only on synthetic data.

Conclusion: This approach offers a scalable and practical solution for articulated object understanding, suitable for real-time dynamic environments.

Abstract: Understanding articulated objects is a fundamental challenge in robotics and digital twin creation. To effectively model such objects, it is essential to recover both part segmentation and the underlying joint parameters. Despite the importance of this task, previous work has largely focused on setups like multi-view systems, object scanning, or static cameras. In this paper, we present the first data-driven approach that jointly predicts part segmentation and joint parameters from monocular video captured with a freely moving camera. Trained solely on synthetic data, our method demonstrates strong generalization to real-world objects, offering a scalable and practical solution for articulated object understanding. Our approach operates directly on casually recorded video, making it suitable for real-time applications in dynamic environments. Project webpage: https://aartykov.github.io/sim2art/

</details>


### [204] [Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution](https://arxiv.org/abs/2512.06642)
*Achmad Ardani Prasha,Clavino Ourizqi Rachmadi,Muhamad Fauzan Ibnu Syahlan,Naufal Rahfi Anugerah,Nanda Garin Raditya,Putri Amelia,Sabrina Laila Mutiara,Hilman Syachr Ramadhan*

Main category: cs.CV

TL;DR: This paper proposes a masked autoencoder (MAE) pretraining strategy for analyzing strong gravitational lensing images to classify dark matter models and enhance image resolution.


<details>
  <summary>Details</summary>
Motivation: The goal is to overcome challenges in analyzing noisy, low-resolution gravitational lensing images and develop robust methods to understand dark matter substructures.

Method: The authors pretrained a Vision Transformer encoder using a masked image objective on simulated lensing images, fine-tuning it separately for classification and super-resolution tasks.

Result: MAE pretraining achieved superior results compared to training from scratch. It enhanced classification (macro AUC: 0.968, accuracy: 88.65%) and super-resolution (PSNR ~33 dB, SSIM 0.961), with optimal performance at a 90% mask ratio.

Conclusion: MAE pretraining on simulated strong-lensing images provides a flexible encoder for multiple tasks, balancing trade-offs between classification and reconstruction.

Abstract: Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.

</details>


### [205] [UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction](https://arxiv.org/abs/2512.07756)
*Mayank Anand,Ujair Alam,Surya Prakash,Priya Shukla,Gora Chand Nandi,Domenec Puig*

Main category: cs.CV

TL;DR: The paper introduces UltrasODM, a framework for improving clinical ultrasound acquisition reliability by minimizing reconstruction errors using uncertainty analysis, saliency diagnostics, and physician prompts.


<details>
  <summary>Details</summary>
Motivation: Clinical ultrasound acquisition faces challenges due to rapid probe movement and brightness variations, causing reconstruction errors that reduce trust and utility during clinical workflows.

Method: UltrasODM integrates a dual-stream framework that uses (i) contrastive ranking for motion grouping, (ii) optical-flow processing fused with temporal modules for pose estimation, and (iii) a HITL layer to issue corrective prompts based on Bayesian uncertainty and calibrated thresholds.

Result: UltrasODM achieves reduced drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% compared to previous methods while providing actionable outputs like per-frame uncertainty and saliency maps.

Conclusion: UltrasODM improves clinical ultrasound reliability by reducing reconstruction errors and enhancing transparency and feedback for sonographers, supporting safer clinical workflows.

Abstract: Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM.

</details>


### [206] [TextMamba: Scene Text Detector with Mamba](https://arxiv.org/abs/2512.06657)
*Qiyan Zhao,Yue Yan,Da-Han Wang*

Main category: cs.CV

TL;DR: The paper proposes a novel scene text detector based on the Mamba model, integrating a selection mechanism with attention layers and introducing additional features for better text detection.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Transformer-based approaches in scene text detection, such as forgetting important information or focusing on irrelevant representations when modeling long-range dependencies.

Method: The method combines the Mamba state space model with attention layers, incorporates the Top_k algorithm for key information selection, and introduces a dual-scale feed-forward network and embedding pyramid enhancement module.

Result: The proposed method achieves competitive or state-of-the-art performance on benchmarks, with notable F-measures of 89.7% (CTW1500), 89.2% (TotalText), and 78.5% (ICDAR19ArT).

Conclusion: The integration of the selection mechanism and additional modules significantly improves the encoder's performance in scene text detection, proving effective at managing long-range dependencies and enhancing feature extraction.

Abstract: In scene text detection, Transformer-based methods have addressed the global feature extraction limitations inherent in traditional convolution neural network-based methods. However, most directly rely on native Transformer attention layers as encoders without evaluating their cross-domain limitations and inherent shortcomings: forgetting important information or focusing on irrelevant representations when modeling long-range dependencies for text detection. The recently proposed state space model Mamba has demonstrated better long-range dependencies modeling through a linear complexity selection mechanism. Therefore, we propose a novel scene text detector based on Mamba that integrates the selection mechanism with attention layers, enhancing the encoder's ability to extract relevant information from long sequences. We adopt the Top\_k algorithm to explicitly select key information and reduce the interference of irrelevant information in Mamba modeling. Additionally, we design a dual-scale feed-forward network and an embedding pyramid enhancement module to facilitate high-dimensional hidden state interactions and multi-scale feature fusion. Our method achieves state-of-the-art or competitive performance on various benchmarks, with F-measures of 89.7\%, 89.2\%, and 78.5\% on CTW1500, TotalText, and ICDAR19ArT, respectively. Codes will be available.

</details>


### [207] [Personalized Image Descriptions from Attention Sequences](https://arxiv.org/abs/2512.06662)
*Ruoyu Xue,Hieu Le,Jingyi Xu,Sounak Mondal,Abe Leite,Gregory Zelinsky,Minh Hoai,Dimitris Samaras*

Main category: cs.CV

TL;DR: This paper introduces DEPER, a model that personalizes image descriptions by incorporating individual viewing behavior alongside linguistic style.


<details>
  <summary>Details</summary>
Motivation: Existing models for personalized image description focus only on linguistic style, ignoring the impact of people's varying perception and viewing behaviors.

Method: The authors propose DEPER, a framework that learns a subject embedding combining both linguistic style and viewing behavior. It employs an attention-prediction task and integrates with a frozen vision-language model via a lightweight adapter for few-shot personalization.

Result: DEPER yields a 24% average improvement across diverse datasets, showcasing its ability to deliver human-aligned and high-quality personalized descriptions.

Conclusion: Incorporating individual viewing behaviors into multimodal systems enhances human alignment and improves performance in personalized image description tasks.

Abstract: People can view the same image differently: they focus on different regions, objects, and details in varying orders and describe them in distinct linguistic styles. This leads to substantial variability in image descriptions. However, existing models for personalized image description focus on linguistic style alone, with no prior work leveraging individual viewing patterns. We address this gap by explicitly modeling personalized viewing behavior as a core factor in description generation. Our method, DEPER (DEscription-PERception persona encoder), learns a subject embedding that captures both linguistic style and viewing behavior, guided by an auxiliary attention-prediction task. A lightweight adapter aligns these embeddings with a frozen vision-language model, enabling few-shot personalization without retraining. Across four datasets spanning diverse viewing tasks and both short and detailed descriptions, DEPER achieves a 24% average improvement, showing that modeling personalized attention produces more human-aligned and high-quality descriptions. We posit that understanding how people see helps predict what they say; modeling human diversity in perception can improve both performance and human alignment in multimodal systems.

</details>


### [208] [CoT4Det: A Chain-of-Thought Framework for Perception-Oriented Vision-Language Tasks](https://arxiv.org/abs/2512.06663)
*Yu Qi,Yumeng Zhang,Chenting Gong,Xiao Tan,Weiming Zhang,Wei Zhang,Jingdong Wang*

Main category: cs.CV

TL;DR: The paper introduces a new method, CoT4Det, to improve Large Vision-Language Models' (LVLMs) performance on perception tasks by reformulating them into classification, counting, and grounding steps.


<details>
  <summary>Details</summary>
Motivation: To address the limited performance of LVLMs on perception-centric tasks like object detection and segmentation, surpassing their current inferiority to task-specific models.

Method: The proposed Chain-of-Thought for Detection (CoT4Det) approach reformulates perception tasks into interpretable steps (classification, counting, grounding), leveraging the reasoning strengths of LVLMs.

Result: CoT4Det significantly improved mAP from 19.0% to 33.0% on COCO2017 val using Qwen2.5-VL-7B-Instruct and achieved better results than the baselines on RefCOCO series (+2%) and Flickr30k entities (+19%).

Conclusion: CoT4Det effectively enhances LVLMs' perception capabilities while maintaining their general vision-language abilities, presenting a simple yet efficient strategy for vision-language tasks.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable success in a broad range of vision-language tasks, such as general visual question answering and optical character recognition (OCR). However, their performance on perception-centric tasks -- such as object detection, semantic segmentation, and depth estimation -- remains significantly inferior to that of task-specific expert models. For example, Qwen2.5-VL-7B-Instruct achieves only 19% mAP on COCO2017 val, particularly struggling with dense scenes and small object recall. In this work, we introduce Chain-of-Thought for Detection (CoT4Det), a simple but efficient strategy that reformulates perception tasks into three interpretable steps: classification, counting, and grounding -- each more naturally aligned with the reasoning capabilities of LVLMs. Extensive experiments demonstrate that our method significantly improves perception performance without compromising general vision language capabilities. With a standard Qwen2.5-VL-7B-Instruct, CoT4Det boosts mAP from 19.0% to 33.0% on COCO2017 val and achieves competitive results across a variety of perception benchmarks, outperforming baselines by +2% on RefCOCO series and 19% on Flickr30k entities.

</details>


### [209] [1 + 1 > 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning](https://arxiv.org/abs/2512.06673)
*Shida Gao,Feng Xue,Xiangfeng Wang,Anlong Ming,Teng Long,Yihua Shao,Haozhe Wang,Zhaowen Lin,Wei Wang,Nicu Sebe*

Main category: cs.CV

TL;DR: DEViL couples a Video LLM with an open-vocabulary detector to enhance spatio-temporal grounding and reasoning in videos.


<details>
  <summary>Details</summary>
Motivation: Current autoregressive techniques in MLLMs struggle with spatial errors and progressive localization drift during spatio-temporal reasoning in videos.

Method: DEViL introduces a reference-semantic token (RST) to connect Video LLM with an open-vocabulary detector (OVD) while leveraging tube-mined temporal regularization (TTReg) for better temporal association.

Result: DEViL demonstrates strong performance in spatio-temporal video grounding and reasoning tasks, such as STVG and GroundedVQA, achieving fine-grained understanding.

Conclusion: DEViL effectively addresses spatial errors and temporal consistency challenges in video reasoning and enhances the understanding capabilities of Video LLMs.

Abstract: Spatio-temporal grounding and reasoning aims to locate the temporal segment and spatial region of an event in a video given a user query, while also reasoning about semantics such as causality, temporal order, and action relationships. To achieve this, current MLLMs primarily treats bounding boxes as text tokens and generates them autoregressively. However, such autoregressive spatial decoding leads to very-long output sequences, causing spatial errors to accumulated over time and the localization results to progressively drift across a video. To address this, we present a Detector-Empowered Video LLM, short for DEViL, which couples a Video LLM with an open-vocabulary detector (OVD). Specifically, the MLLM and detector are connected via a reference-semantic token (RST) that distills the user query into a rich semantic representation. Unlike tokens that merely serve as spatial prompts or segmentor switches, the RST functions as both a control signal and a replacement for the OVD's text embedding, enabling end-to-end learning of both referential understanding and spatial localization. Furthermore, we propose a tube-mined temporal regularization (TTReg) within OVD, which drives the OVD to generate temporally-consistent queries for target objects, thereby ensuring effective temporal association. Experiments demonstrate that DEViL achieves strong performance across various fine-grained video understanding tasks, particularly STVG and GroundedVQA. Code will be released on https://github.com/gaostar123/DeViL.

</details>


### [210] [RunawayEvil: Jailbreaking the Image-to-Video Generative Models](https://arxiv.org/abs/2512.06674)
*Songping Wang,Rufan Qian,Yueming Lyu,Qinglong Liu,Linzhuang Zou,Jie Qin,Songhua Liu,Caifeng Shan*

Main category: cs.CV

TL;DR: The paper introduces RunawayEvil, a jailbreak framework for Image-to-Video (I2V) models, showcasing adaptive and self-evolving attacks, significantly outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on the vulnerabilities of multimodal I2V systems to jailbreak attacks.

Method: RunawayEvil framework based on a 'Strategy-Tactic-Action' paradigm, employing reinforcement learning, LLM-based strategy exploration, and multimodal attack coordination.

Result: The framework achieves significantly higher attack success rates on commercial I2V systems (e.g., 58.5-79% improvement on COCO2017) than existing methods.

Conclusion: RunawayEvil highlights critical vulnerabilities in I2V models, offering insights for developing more secure video generation systems.

Abstract: Image-to-Video (I2V) generation synthesizes dynamic visual content from image and text inputs, providing significant creative control. However, the security of such multimodal systems, particularly their vulnerability to jailbreak attacks, remains critically underexplored. To bridge this gap, we propose RunawayEvil, the first multimodal jailbreak framework for I2V models with dynamic evolutionary capability. Built on a "Strategy-Tactic-Action" paradigm, our framework exhibits self-amplifying attack through three core components: (1) Strategy-Aware Command Unit that enables the attack to self-evolve its strategies through reinforcement learning-driven strategy customization and LLM-based strategy exploration; (2) Multimodal Tactical Planning Unit that generates coordinated text jailbreak instructions and image tampering guidelines based on the selected strategies; (3) Tactical Action Unit that executes and evaluates the multimodal coordinated attacks. This self-evolving architecture allows the framework to continuously adapt and intensify its attack strategies without human intervention. Extensive experiments demonstrate RunawayEvil achieves state-of-the-art attack success rates on commercial I2V models, such as Open-Sora 2.0 and CogVideoX. Specifically, RunawayEvil outperforms existing methods by 58.5 to 79 percent on COCO2017. This work provides a critical tool for vulnerability analysis of I2V models, thereby laying a foundation for more robust video generation systems.

</details>


### [211] [EMGauss: Continuous Slice-to-3D Reconstruction via Dynamic Gaussian Modeling in Volume Electron Microscopy](https://arxiv.org/abs/2512.06684)
*Yumeng He,Zanwei Zhou,Yekun Zheng,Chen Liang,Yunbo Wang,Xiaokang Yang*

Main category: cs.CV

TL;DR: This paper introduces EMGauss, a novel framework for 3D reconstruction from 2D slices using Gaussian splatting and a bootstrapping mechanism, which enhances interpolation quality in volume electron microscopy and other domains.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods for vEM struggle with morphologically anisotropic structures and limited axial resolution due to reliance on isotropy assumptions.

Method: The authors propose EMGauss, which models slice-to-3D reconstruction as a rendering problem using Gaussian splatting and a Teacher-Student bootstrapping mechanism for generating pseudo-supervisory signals.

Result: EMGauss significantly outperforms diffusion- and GAN-based methods in interpolation quality, enables continuous slice synthesis, and removes the need for large-scale pretraining.

Conclusion: EMGauss is a promising framework for reconstructing 3D volumes from planar 2D slices, offering generalized applications in vEM and potentially other imaging fields, overcoming limitations of isotropy-based approaches.

Abstract: Volume electron microscopy (vEM) enables nanoscale 3D imaging of biological structures but remains constrained by acquisition trade-offs, leading to anisotropic volumes with limited axial resolution. Existing deep learning methods seek to restore isotropy by leveraging lateral priors, yet their assumptions break down for morphologically anisotropic structures. We present EMGauss, a general framework for 3D reconstruction from planar scanned 2D slices with applications in vEM, which circumvents the inherent limitations of isotropy-based approaches. Our key innovation is to reframe slice-to-3D reconstruction as a 3D dynamic scene rendering problem based on Gaussian splatting, where the progression of axial slices is modeled as the temporal evolution of 2D Gaussian point clouds. To enhance fidelity in data-sparse regimes, we incorporate a Teacher-Student bootstrapping mechanism that uses high-confidence predictions on unobserved slices as pseudo-supervisory signals. Compared with diffusion- and GAN-based reconstruction methods, EMGauss substantially improves interpolation quality, enables continuous slice synthesis, and eliminates the need for large-scale pretraining. Beyond vEM, it potentially provides a generalizable slice-to-3D solution across diverse imaging domains.

</details>


### [212] [Lightweight Wasserstein Audio-Visual Model for Unified Speech Enhancement and Separation](https://arxiv.org/abs/2512.06689)
*Jisoo Park,Seonghak Lee,Guisik Kim,Taewoo Kim,Junseok Kwon*

Main category: cs.CV

TL;DR: UniVoiceLite is a lightweight and unsupervised audio-visual model combining speech enhancement and separation effectively.


<details>
  <summary>Details</summary>
Motivation: Real-world audio often includes both noise and overlapping speakers, highlighting the need for a unified solution to address speech enhancement and separation.

Method: UniVoiceLite uses lip motion and facial identity cues along with Wasserstein distance regularization to unify speech enhancement and separation tasks.

Result: Experiments demonstrate strong performance in handling both noisy environments and multi-speaker scenarios with robust generalization and efficiency.

Conclusion: UniVoiceLite offers an efficient, scalable solution for speech processing tasks without relying on complex architectures or supervised training.

Abstract: Speech Enhancement (SE) and Speech Separation (SS) have traditionally been treated as distinct tasks in speech processing. However, real-world audio often involves both background noise and overlapping speakers, motivating the need for a unified solution. While recent approaches have attempted to integrate SE and SS within multi-stage architectures, these approaches typically involve complex, parameter-heavy models and rely on supervised training, limiting scalability and generalization. In this work, we propose UniVoiceLite, a lightweight and unsupervised audio-visual framework that unifies SE and SS within a single model. UniVoiceLite leverages lip motion and facial identity cues to guide speech extraction and employs Wasserstein distance regularization to stabilize the latent space without requiring paired noisy-clean data. Experimental results demonstrate that UniVoiceLite achieves strong performance in both noisy and multi-speaker scenarios, combining efficiency with robust generalization. The source code is available at https://github.com/jisoo-o/UniVoiceLite.

</details>


### [213] [The Role of Entropy in Visual Grounding: Analysis and Optimization](https://arxiv.org/abs/2512.06726)
*Shuo Li,Jiajun Sun,Zhihao Zhang,Xiaoran Fan,Senjie Jin,Hui Li,Yuming Yang,Junjie Ye,Lixing Shen,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CV

TL;DR: The paper introduces an algorithm ECVGPO focusing on entropy control to optimize visual grounding tasks, leading to performance improvements.


<details>
  <summary>Details</summary>
Motivation: The need to understand entropy's role in perception-oriented tasks like visual grounding and devise effective control strategies.

Method: Analysis of entropy characteristics and introduction of ECVGPO, an interpretable algorithm for entropy control, to balance exploration and exploitation.

Result: Experiments demonstrate ECVGPO improves performance across benchmarks and models.

Conclusion: Entropy control plays a significant role in enhancing visual grounding tasks, and the proposed ECVGPO demonstrates effectiveness in achieving superior results.

Abstract: Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.

</details>


### [214] [Graph Convolutional Long Short-Term Memory Attention Network for Post-Stroke Compensatory Movement Detection Based on Skeleton Data](https://arxiv.org/abs/2512.06736)
*Jiaxing Fan,Jiaojiao Liu,Wenkong Wang,Yang Zhang,Xin Ma,Jichen Zhang*

Main category: cs.CV

TL;DR: The paper proposes a GCN-LSTM-ATT model for detecting compensatory movements in stroke patients during rehabilitation using skeleton data from Kinect. The model outperforms traditional machine learning algorithms.


<details>
  <summary>Details</summary>
Motivation: To improve long-term recovery for stroke patients by accurately detecting compensatory movements during rehabilitation, which are harmful over time.

Method: The study developed a GCN-LSTM-ATT model based on skeleton data from Kinect depth cameras. Sixteen stroke patients' data was used to train the model, and its performance was benchmarked against SVM, KNN, and RF algorithms.

Result: The GCN-LSTM-ATT model achieved a detection accuracy of 0.8580, outperforming traditional methods. Ablation experiments validated the contributions of individual model components.

Conclusion: This model offers a precise and robust approach for identifying compensatory movements, potentially aiding the development of better rehabilitation strategies for stroke patients.

Abstract: Most stroke patients experience upper limb motor dysfunction. Compensatory movements are prevalent during rehabilitation training, which is detrimental to patients' long-term recovery. Therefore, detecting compensatory movements is of great significance. In this study, a Graph Convolutional Long Short-Term Memory Attention Network (GCN-LSTM-ATT) based on skeleton data is proposed for the detection of compensatory movements after stroke. Sixteen stroke patients were selected in the research. The skeleton data of the patients performing specific rehabilitation movements were collected using the Kinect depth camera. After data processing, detection models were constructed respectively using the GCN-LSTM-ATT model, the Support Vector Machine(SVM), the K-Nearest Neighbor algorithm(KNN), and the Random Forest(RF). The results show that the detection accuracy of the GCN-LSTM-ATT model reaches 0.8580, which is significantly higher than that of traditional machine learning algorithms. Ablation experiments indicate that each component of the model contributes significantly to the performance improvement. These findings provide a more precise and powerful tool for the detection of compensatory movements after stroke, and are expected to facilitate the optimization of rehabilitation training strategies for stroke patients.

</details>


### [215] [FedSCAl: Leveraging Server and Client Alignment for Unsupervised Federated Source-Free Domain Adaptation](https://arxiv.org/abs/2512.06738)
*M Yashwanth,Sampath Koti,Arunabh Singh,Shyam Marjit,Anirban Chakraborty*

Main category: cs.CV

TL;DR: The paper proposes FedSCAl, a Federated Learning (FL) framework addressing Federated source-Free Domain Adaptation (FFreeDA) with significant data domain gaps across clients, outperforming state-of-the-art methods in classification tasks.


<details>
  <summary>Details</summary>
Motivation: Address challenges in Federated source-Free Domain Adaptation due to inter-client domain gaps and data heterogeneity, where a pre-trained server model is used as the source dataset is inaccessible.

Method: Introduced FedSCAl framework with a Server-Client Alignment (SCAl) mechanism to align client and server model predictions, improving pseudo-labeling accuracy and mitigating client drift.

Result: FedSCAl improves pseudo-labeling accuracy for clients and outperforms state-of-the-art methods across various benchmark datasets in classification tasks under the FFreeDA setup.

Conclusion: FedSCAl effectively mitigates the client-drift issue in FFreeDA setups by leveraging the SCAl mechanism, showcasing its suitability and robustness in handling severe inter-client domain gaps.

Abstract: We address the Federated source-Free Domain Adaptation (FFreeDA) problem, with clients holding unlabeled data with significant inter-client domain gaps. The FFreeDA setup constrains the FL frameworks to employ only a pre-trained server model as the setup restricts access to the source dataset during the training rounds. Often, this source domain dataset has a distinct distribution to the clients' domains. To address the challenges posed by the FFreeDA setup, adaptation of the Source-Free Domain Adaptation (SFDA) methods to FL struggles with client-drift in real-world scenarios due to extreme data heterogeneity caused by the aforementioned domain gaps, resulting in unreliable pseudo-labels. In this paper, we introduce FedSCAl, an FL framework leveraging our proposed Server-Client Alignment (SCAl) mechanism to regularize client updates by aligning the clients' and server model's predictions. We observe an improvement in the clients' pseudo-labeling accuracy post alignment, as the SCAl mechanism helps to mitigate the client-drift. Further, we present extensive experiments on benchmark vision datasets showcasing how FedSCAl consistently outperforms state-of-the-art FL methods in the FFreeDA setup for classification tasks.

</details>


### [216] [Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2512.06746)
*Ruoxin Chen,Jiahui Gao,Kaiqing Lin,Keyue Zhang,Yandan Zhao,Isabel Guan,Taiping Yao,Shouhong Ding*

Main category: cs.CV

TL;DR: Vision Language Models (VLMs) show limitations in AI-generated image detection due to task-model misalignment. AlignGemini, a two-branch detector leveraging semantic and pixel-artifact supervision, addresses these blind spots, achieving +9.5 accuracy improvement.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Vision Language Models (VLMs) in detecting AI-generated images, particularly their insensitivity to fine-grained pixel artifacts.

Method: Introduced AlignGemini, a two-branch detection system combining semantic consistency checking via fine-tuned VLMs and pixel-artifact detection via specialized models, using orthogonal supervision on simplified datasets.

Result: AlignGemini achieves a +9.5 improvement in average accuracy across five benchmarks by aligning tasks with appropriate models and addressing systematic blind spots.

Conclusion: AlignGemini exemplifies how task-model alignment enhances detection accuracy, suggesting this approach as a generalizable solution for AI-generated image detection.

Abstract: Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs' underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks--semantic consistency checking and pixel-artifact detection--and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.

</details>


### [217] [UARE: A Unified Vision-Language Model for Image Quality Assessment, Restoration, and Enhancement](https://arxiv.org/abs/2512.06750)
*Weiqi Li,Xuanyu Zhang,Bin Chen,Jingfen Xie,Yan Wang,Kexin Zhang,Junlin Li,Li Zhang,Jian Zhang,Shijie Zhao*

Main category: cs.CV

TL;DR: The paper introduces UARE, a unified vision-language model to combine image quality assessment, restoration, and enhancement for improved performance.


<details>
  <summary>Details</summary>
Motivation: The authors aim to integrate image quality assessment (IQA) and restoration into a unified framework as they are conceptually interlinked but often studied separately.

Method: UARE is built on pretrained multimodal models with a two-stage training process: progressive training to handle degradations and unified fine-tuning with text-image co-training.

Result: Experiments show that UARE effectively leverages IQA for superior restoration and enhancement performance across multiple tasks.

Conclusion: UARE demonstrates that unifying IQA and restoration improves performance, offering valuable insights and tools through its approach.

Abstract: Image quality assessment (IQA) and image restoration are fundamental problems in low-level vision. Although IQA and restoration are closely connected conceptually, most existing work treats them in isolation. Recent advances in unified multimodal understanding-generation models demonstrate promising results and indicate that stronger understanding can improve generative performance. This motivates a single model that unifies IQA and restoration and explicitly studies how IQA can guide restoration, a setting that remains largely underexplored yet highly valuable. In this paper, we propose UARE, to our knowledge the first Unified vision-language model for image quality Assessment, Restoration, and Enhancement. Built on pretrained unified understanding and generation models, we introduce a two-stage training framework. First, a progressive, easy-to-hard schedule expands from single-type distortions to higher-order mixed degradations, enabling UARE to handle multiple degradations. Second, we perform unified fine-tuning of quality understanding and restoration with interleaved text-image data, aligning IQA signals with restoration objectives. Through multi-task co-training, UARE leverages IQA to boost restoration and enhancement performance. Extensive experiments across IQA, restoration, and enhancement tasks demonstrate the effectiveness of UARE. The code and models will be available at https://github.com/lwq20020127/UARE.

</details>


### [218] [VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors](https://arxiv.org/abs/2512.06759)
*Wenbo Lyu,Yingjun Du,Jinglin Zhao,Xianton Zhen,Ling Shao*

Main category: cs.CV

TL;DR: This paper introduces VisChainBench, a benchmark for evaluating LVLMs on multi-step visual reasoning with minimal language input.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to evaluate progressive, context-dependent reasoning in LVLMs, especially involving visual-to-visual inference over multiple steps.

Method: The authors created VisChainBench, a large-scale benchmark with 1,457 tasks and over 20,000 images across diverse domains, constructed with a multi-agent generation pipeline to maximize visual diversity and minimize language bias.

Result: The benchmark offers an advanced test bed for assessing LVLMs' ability to handle multi-turn, interdependent reasoning scenarios.

Conclusion: VisChainBench fills the gap in LVLM evaluation by focusing on real-world-like sequential visual reasoning, making it a valuable resource for advancing vision-language model capabilities.

Abstract: Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench

</details>


### [219] [JOCA: Task-Driven Joint Optimisation of Camera Hardware and Adaptive Camera Control Algorithms](https://arxiv.org/abs/2512.06763)
*Chengyang Yan,Mitch Bryson,Donald G. Dansereau*

Main category: cs.CV

TL;DR: The paper proposes a unified framework optimizing camera systems and adaptive control for enhanced vision tasks under challenging conditions.


<details>
  <summary>Details</summary>
Motivation: To enhance image quality and downstream perception performance by integrating camera hardware and runtime adaptive control.

Method: The unified framework includes gradient-based and derivative-free optimization strategies, introducing DF-Grad for handling non-differentiable processes like motion blur.

Result: Experiments show that joint optimization improves perception tasks performance under challenging setups like low light and fast motion.

Conclusion: Integrating camera hardware and adaptive algorithms significantly enhances task-driven camera performance compared to separate optimization techniques.

Abstract: The quality of captured images strongly influences the performance of downstream perception tasks. Recent works on co-designing camera systems with perception tasks have shown improved task performance. However, most prior approaches focus on optimising fixed camera parameters set at manufacturing, while many parameters, such as exposure settings, require adaptive control at runtime. This paper introduces a method that jointly optimises camera hardware and adaptive camera control algorithms with downstream vision tasks. We present a unified optimisation framework that integrates gradient-based and derivative-free methods, enabling support for both continuous and discrete parameters, non-differentiable image formation processes, and neural network-based adaptive control algorithms. To address non-differentiable effects such as motion blur, we propose DF-Grad, a hybrid optimisation strategy that trains adaptive control networks using signals from a derivative-free optimiser alongside unsupervised task-driven learning. Experiments show that our method outperforms baselines that optimise static and dynamic parameters separately, particularly under challenging conditions such as low light and fast motion. These results demonstrate that jointly optimising hardware parameters and adaptive control algorithms improves perception performance and provides a unified approach to task-driven camera system design.

</details>


### [220] [Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding](https://arxiv.org/abs/2512.06769)
*Hang Yin,Xiaomin He,PeiWen Yuan,Yiwei Li,Jiayi Shi,Wenxiao Fan,Shaoxiong Feng,Kan Li*

Main category: cs.CV

TL;DR: The paper introduces Stitch and Tell (SiTe), a method to enhance spatial understanding in vision-language models through structured spatial supervision using stitched image-text pairs, significantly improving performance on spatial and general tasks.


<details>
  <summary>Details</summary>
Motivation: Vision-language models struggle with spatial hallucinations due to asymmetric image and text properties. This paper aims to address this issue with an efficient and annotation-free approach.

Method: The authors propose Stitch and Tell (SiTe), which creates stitched image-text pairs by combining images along a spatial axis and generating spatially-aware captions or QA pairs without needing advanced models or manual intervention.

Result: SiTe was evaluated across three architectures, two training datasets, and eight benchmarks, showing improvements in spatial understanding metrics like MME_Position (+5.50%) and Spatial-MM (+4.19%) while maintaining or improving general tasks such as COCO-QA (+1.02%) and MMBench (+4.76%).

Conclusion: Injecting spatially-aware structures into training data is an effective way to mitigate spatial hallucinations and improve both spatial understanding and general vision-language model capabilities.

Abstract: Existing vision-language models often suffer from spatial hallucinations, i.e., generating incorrect descriptions about the relative positions of objects in an image. We argue that this problem mainly stems from the asymmetric properties between images and text. To enrich the spatial understanding ability of vision-language models, we propose a simple, annotation-free, plug-and-play method named $\text{Stitch and Tell}$ (abbreviated as SiTe), which injects structured spatial supervision into data. It constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or question answer pairs based on the layout of stitched image, without relying on costly advanced models or human involvement. We evaluate SiTe across three architectures including LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B and HALVA-7B, two training datasets, and eight benchmarks. Experiments show that SiTe improves spatial understanding tasks such as $\text{MME}_{\text{Position}}$ (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving performance on general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Our findings suggest that explicitly injecting spatially-aware structure into training data offers an effective way to mitigate spatial hallucinations and improve spatial understanding, while preserving general vision-language capabilities.

</details>


### [221] [RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting](https://arxiv.org/abs/2512.06774)
*Longjie Zhao,Ziming Hong,Zhenyang Ren,Runnan Chen,Mingming Gong,Tongliang Liu*

Main category: cs.CV

TL;DR: The paper introduces RDSplat, a watermarking paradigm for 3D Gaussian Splatting (3DGS) that is resilient to diffusion-based editing, ensuring protection of digital assets.


<details>
  <summary>Details</summary>
Motivation: There is a growing need for robust watermarking in 3D Gaussian Splatting (3DGS) applications to safeguard copyright, but existing methods are ineffective against diffusion-based editing, which easily removes watermarks.

Method: RDSplat uses a multi-domain framework that embeds watermarks in low-frequency Gaussians, preserved during diffusion editing, via covariance regularization, 2D filtering, and adversarial training with a blur-based diffusion proxy.

Result: RDSplat achieves superior watermark robustness against diffusion-based editing while maintaining invisible watermarks, demonstrating state-of-the-art performance in empirical evaluations across benchmark datasets.

Conclusion: RDSplat provides a robust and efficient solution for watermarking in 3DGS, addressing diffusion-editing vulnerabilities while preserving imperceptibility of watermarks.

Abstract: 3D Gaussian Splatting (3DGS) has enabled the creation of digital assets and downstream applications, underscoring the need for robust copyright protection via digital watermarking. However, existing 3DGS watermarking methods remain highly vulnerable to diffusion-based editing, which can easily erase embedded provenance. This challenge highlights the urgent need for 3DGS watermarking techniques that are intrinsically resilient to diffusion-based editing. In this paper, we introduce RDSplat, a Robust watermarking paradigm against Diffusion editing for 3D Gaussian Splatting. RDSplat embeds watermarks into 3DGS components that diffusion-based editing inherently preserve, achieved through (i) proactively targeting low-frequency Gaussians and (ii) adversarial training with a diffusion proxy. Specifically, we introduce a multi-domain framework that operates natively in 3DGS space and embeds watermarks into diffusion-editing-preserved low-frequency Gaussians via coordinated covariance regularization and 2D filtering. In addition, we exploit the low-pass filtering behavior of diffusion-based editing by using Gaussian blur as an efficient training surrogate, enabling adversarial fine-tuning that further enhances watermark robustness against diffusion-based editing. Empirically, comprehensive quantitative and qualitative evaluations on three benchmark datasets demonstrate that RDSplat not only maintains superior robustness under diffusion-based editing, but also preserves watermark invisibility, achieving state-of-the-art performance.

</details>


### [222] [MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2512.06810)
*Yueqian Wang,Songxiang Liu,Disong Wang,Nuo Xu,Guanglu Wan,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: The paper introduces a novel method for proactive interaction in video multimodal large language models, enhancing decision-making capabilities on when to respond during video playback.


<details>
  <summary>Details</summary>
Motivation: Current video multimodal LLMs lack effective real-time interaction abilities, as they mostly rely on turn-based mechanisms. Proactive decision-making for responses during video playback is a promising direction.

Method: The paper proposes a text-to-text proactive interaction approach where the model determines whether to respond or stay silent based on the streaming video and dialogue history. It uses multi-turn reinforcement learning (RL) to improve response timing and accuracy without requiring precisely annotated response times.

Result: The proposed model, MMDuet2, trained with SFT and RL on a dataset of 52k videos, surpasses existing proactive Video MLLM baselines in both response timing and quality, setting a new benchmark for ProactiveVideoQA.

Conclusion: MMDuet2 provides a state-of-the-art solution for proactive interaction in video MLLMs, which could enhance real-time applications through accurate response timing and improved interaction quality.

Abstract: Recent advances in video multimodal large language models (Video MLLMs) have significantly enhanced video understanding and multi-modal interaction capabilities. While most existing systems operate in a turn-based manner where the model can only reply after user turns, proactively deciding when to reply during video playback presents a promising yet challenging direction for real-time applications. In this work, we propose a novel text-to-text approach to proactive interaction, where the model autonomously determines whether to respond or remain silent at each turn based on dialogue history and visual context up to current frame of an streaming video. To overcome difficulties in previous methods such as manually tuning response decision thresholds and annotating precise reply times, we introduce a multi-turn RL based training method that encourages timely and accurate responses without requiring precise response time annotations. We train our model MMDuet2 on a dataset of 52k videos with two types of dialogues via SFT and RL. Experimental results demonstrate that MMDuet2 outperforms existing proactive Video MLLM baselines in response timing and quality, achieving state-of-the-art performance on the ProactiveVideoQA benchmark.

</details>


### [223] [Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos](https://arxiv.org/abs/2512.06783)
*Tobias Leuthold,Michele Xiloyannis,Yves Zimmermann*

Main category: cs.CV

TL;DR: A real-time algorithm refines pose estimation accuracy by incorporating anatomical constraints into BlazePose outputs, ensuring better performance for applications like physical therapy and sports coaching.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve real-time pose estimation systems, such as BlazePose, for applications in physical therapy and sports coaching. This is motivated by the need for accurate and anatomically consistent estimations using monocular video streams.

Method: A weighted optimization approach combining BlazePose 2D and 3D outputs is proposed, penalizing deviations from expected bone lengths based on biomechanical models. The Kalman filter adjusts individual anatomy, maintaining computational efficiency.

Result: The refinement achieves a 10.2% reduction in 3D MPJPE and a 16.6% decrease in body segment angle errors compared to BlazePose 3D using the Physio2.2M dataset.

Conclusion: This approach substantially enhances anatomical accuracy in pose estimation while maintaining computational efficiency, making it well-suited for real-time applications like physiotherapy and automated coaching on consumer-grade devices.

Abstract: Applications providing automated coaching for physical training are increasing in popularity, for example physical therapy. These applications rely on accurate and robust pose estimation using monocular video streams. State-of-the-art models like BlazePose excel in real-time pose tracking, but their lack of anatomical constraints indicates improvement potential by including physical knowledge. We present a real-time post-processing algorithm fusing the strengths of BlazePose 3D and 2D estimations using a weighted optimization, penalizing deviations from expected bone length and biomechanical models. Bone length estimations are refined to the individual anatomy using a Kalman filter with adapting measurement trust. Evaluation using the Physio2.2M dataset shows a 10.2 percent reduction in 3D MPJPE and a 16.6 percent decrease in errors of angles between body segments compared to BlazePose 3D estimation. Our method provides a robust, anatomically consistent pose estimation based on a computationally efficient video-to-3D pose estimation, suitable for automated physiotherapy, healthcare, and sports coaching on consumer-level laptops and mobile devices. The refinement runs on the backend with anonymized data only.

</details>


### [224] [Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior](https://arxiv.org/abs/2512.06866)
*Yulin Li,Haokun Gui,Ziyang Fan,Junjie Wang,Bin Kang,Bin Chen,Zhuotao Tian*

Main category: cs.CV

TL;DR: DyToK is a training-free method for efficient video processing in Video Large Language Models (VLLMs), achieving state-of-the-art efficiency-accuracy tradeoffs by dynamically compressing visual tokens using keyframe priors.


<details>
  <summary>Details</summary>
Motivation: Video processing in VLLMs suffers from inefficiency due to quadratic computational growth in processing long videos, and current keyframe selection methods are suboptimal and add computational burden.

Method: DyToK leverages inherent attention mechanisms in VLLMs to dynamically compress visual tokens by adjusting token retention ratios based on keyframe importance without additional training.

Result: DyToK achieves 4.3x faster inference speed while maintaining accuracy. It also integrates well with existing compression techniques and works across multiple VLLM models.

Conclusion: DyToK provides an effective, plug-and-play solution for improving the efficiency of VLLMs without compromising performance, marking progress in video understanding tasks.

Abstract: Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .

</details>


### [225] [Generalized Geometry Encoding Volume for Real-time Stereo Matching](https://arxiv.org/abs/2512.06793)
*Jiaxin Liu,Gangwei Xu,Xianqi Wang,Chengliang Zhang,Xin Yang*

Main category: cs.CV

TL;DR: This paper introduces GGEV, a real-time stereo matching network, focusing on strong generalization for unseen data while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the lack of generalization in real-time stereo matching methods and the high latency in stereo foundation models.

Method: The authors propose using depth-aware features and a Depth-aware Dynamic Cost Aggregation (DDCA) module to improve generalization. This creates a generalized geometry encoding volume.

Result: GGEV achieves state-of-the-art zero-shot performance on benchmarks including KITTI 2012, KITTI 2015, and ETH3D.

Conclusion: The proposed GGEV method effectively balances efficiency and generalization, surpassing all real-time stereo matching methods in generalization capability.

Abstract: Real-time stereo matching methods primarily focus on enhancing in-domain performance but often overlook the critical importance of generalization in real-world applications. In contrast, recent stereo foundation models leverage monocular foundation models (MFMs) to improve generalization, but typically suffer from substantial inference latency. To address this trade-off, we propose Generalized Geometry Encoding Volume (GGEV), a novel real-time stereo matching network that achieves strong generalization. We first extract depth-aware features that encode domain-invariant structural priors as guidance for cost aggregation. Subsequently, we introduce a Depth-aware Dynamic Cost Aggregation (DDCA) module that adaptively incorporates these priors into each disparity hypothesis, effectively enhancing fragile matching relationships in unseen scenes. Both steps are lightweight and complementary, leading to the construction of a generalized geometry encoding volume with strong generalization capability. Experimental results demonstrate that our GGEV surpasses all existing real-time methods in zero-shot generalization capability, and achieves state-of-the-art performance on the KITTI 2012, KITTI 2015, and ETH3D benchmarks.

</details>


### [226] [NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification](https://arxiv.org/abs/2512.06921)
*Ziyang Song,Zelin Zang,Xiaofan Ye,Boqiang Xu,Long Bai,Jinlin Wu,Hongliang Ren,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.CV

TL;DR: The paper introduces NeuroABench, a benchmark for evaluating anatomical understanding within neurosurgery, revealing significant gaps in machine learning models compared to human performance.


<details>
  <summary>Details</summary>
Motivation: Current research in surgical video understanding largely neglects anatomical comprehension, which is crucial for clinical practice. The paper aims to bridge this gap.

Method: The authors created NeuroABench, consisting of annotated neurosurgical videos, and evaluated 68 anatomical structures using multimodal annotation and testing frameworks.

Result: State-of-the-art MLLMs achieved only 40.87% accuracy, falling behind neurosurgical trainees whose average accuracy was 46.5%. This highlights limitations of current models.

Conclusion: MLLMs show progress in anatomical understanding but require substantial improvements to reach human-level competency in neurosurgical applications.

Abstract: Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.

</details>


### [227] [VDOT: Efficient Unified Video Creation via Optimal Transport Distillation](https://arxiv.org/abs/2512.06802)
*Yutong Wang,Haiyu Zhang,Tianfan Xue,Yu Qiao,Yaohui Wang,Chang Xu,Xinyuan Chen*

Main category: cs.CV

TL;DR: The paper presents VDOT, an efficient video creation model optimizing distribution matching with computational optimal transport (OT), achieving superior performance with fewer steps.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in current video creation models, including limited conditions and long generation times.

Method: Employ a novel computational OT technique in a distribution matching distillation paradigm, utilize a discriminator for real-video perception, and deploy a fully automated pipeline for data annotation and benchmarking.

Result: VDOT, with only 4 denoising steps, outperforms baseline models requiring 100 steps in the video generation task.

Conclusion: VDOT provides an efficient, stable, and high-quality video creation model suitable for diverse applications, with innovative use of OT and automatic testing benchmarking.

Abstract: The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.

</details>


### [228] [RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models](https://arxiv.org/abs/2512.06811)
*Xiang Lin,Weixin Li,Shu Guo,Lihong Wang,Di Huang*

Main category: cs.CV

TL;DR: This paper presents RMAdapter, a novel Reconstruction-based Multimodal Adapter designed to improve few-shot fine-tuning in pre-trained Vision-Language Models (VLMs) like CLIP, addressing critical gaps in balancing task adaptation and generalization.


<details>
  <summary>Details</summary>
Motivation: The work is motivated by the limitations in current approaches for fine-tuning pre-trained VLMs in few-shot scenarios. Prompt-based methods dominate the field, but adapter-based approaches are understudied, and existing methods face challenges in balancing task-specific adaptation with general knowledge retention.

Method: The authors propose the RMAdapter with a dual-branch architecture: one branch adapts task-specific knowledge, and the other reconstructs latent features to preserve general knowledge. By incorporating reconstruction loss and sharing modules to remain lightweight, it effectively balances these needs.

Result: The RMAdapter achieves superior performance across three tasks—generalization to new categories, new target datasets, and domain generalization—surpassing state-of-the-art methods without requiring data augmentation or duplicate prompt designs.

Conclusion: RMAdapter demonstrates that a dual-branch adapter architecture is effective in addressing trade-offs between discriminability and generalization for fine-tuning VLMs in diverse tasks.

Abstract: Pre-trained Vision-Language Models (VLMs), \textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.

</details>


### [229] [MeshSplatting: Differentiable Rendering with Opaque Meshes](https://arxiv.org/abs/2512.06818)
*Jan Held,Sanghyun Son,Renaud Vandeghen,Daniel Rebain,Matheus Gadelha,Yi Zhou,Anthony Cioppa,Ming C. Lin,Marc Van Droogenbroeck,Andrea Tagliasacchi*

Main category: cs.CV

TL;DR: MeshSplatting offers a mesh-based approach that improves novel view synthesis efficiency and compatibility with AR/VR engines, outperforming the current state-of-the-art.


<details>
  <summary>Details</summary>
Motivation: Primitive-based splatting methods, such as 3D Gaussian Splatting, are incompatible with mesh-based pipelines used in AR/VR and game engines, prompting the need for a mesh-based solution.

Method: The paper introduces MeshSplatting, which uses restricted Delaunay triangulation and differentiable rendering to optimize geometry and appearance, producing smooth, high-quality meshes.

Result: MeshSplatting outperforms the state-of-the-art MiLo on Mip-NeRF360 with a +0.69 dB improvement in PSNR, trains twice as fast, and requires significantly less memory.

Conclusion: MeshSplatting bridges the gap between neural rendering and interactive 3D graphics, enabling real-time scene interaction with efficient mesh-based rendering.

Abstract: Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.

</details>


### [230] [SparseCoop: Cooperative Perception with Kinematic-Grounded Queries](https://arxiv.org/abs/2512.06838)
*Jiahao Wang,Zhongwei Jiang,Wenchao Sun,Jiaru Zhong,Haibao Yu,Yuner Zhang,Chenyang Lu,Chuang Zhang,Lei He,Shaobing Xu,Jianqiang Wang*

Main category: cs.CV

TL;DR: The paper introduces SparseCoop, a framework for 3D detection and tracking in cooperative autonomous driving, achieving state-of-the-art performance with efficiency and robustness.


<details>
  <summary>Details</summary>
Motivation: Address limitations of single vehicle perception like occlusions, communication constraints, and lack of flexibility in aligning viewpoints for autonomous vehicles.

Method: SparseCoop discards intermediate BEV features, uses kinematic-grounded queries for spatio-temporal alignment, introduces coarse-to-fine aggregation for fusion, and employs denoising tasks to improve training stability.

Result: SparseCoop achieves state-of-the-art performance in experiments, showcasing computational efficiency, low communication cost, and robustness to latency.

Conclusion: SparseCoop offers a highly efficient and robust framework for cooperative perception in autonomous vehicles, overcoming key challenges in communication and representation.

Abstract: Cooperative perception is critical for autonomous driving, overcoming the inherent limitations of a single vehicle, such as occlusions and constrained fields-of-view. However, current approaches sharing dense Bird's-Eye-View (BEV) features are constrained by quadratically-scaling communication costs and the lack of flexibility and interpretability for precise alignment across asynchronous or disparate viewpoints. While emerging sparse query-based methods offer an alternative, they often suffer from inadequate geometric representations, suboptimal fusion strategies, and training instability. In this paper, we propose SparseCoop, a fully sparse cooperative perception framework for 3D detection and tracking that completely discards intermediate BEV representations. Our framework features a trio of innovations: a kinematic-grounded instance query that uses an explicit state vector with 3D geometry and velocity for precise spatio-temporal alignment; a coarse-to-fine aggregation module for robust fusion; and a cooperative instance denoising task to accelerate and stabilize training. Experiments on V2X-Seq and Griffin datasets show SparseCoop achieves state-of-the-art performance. Notably, it delivers this with superior computational efficiency, low transmission cost, and strong robustness to communication latency. Code is available at https://github.com/wang-jh18-SVM/SparseCoop.

</details>


### [231] [Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models](https://arxiv.org/abs/2512.07141)
*Fenghua Weng,Chaochao Lu,Xia Hu,Wenqi Shao,Wenjie Wang*

Main category: cs.CV

TL;DR: The study introduces a robust three-stage training framework, Think-Reflect-Revise (TRR), to enhance safety alignment in Large Vision Language Models (LVLMs).


<details>
  <summary>Details</summary>
Motivation: Improving safety reasoning in Large Vision Language Models (LVLMs) is crucial because current single-pass think-then-answer methods fail to address harmful content effectively, making models prone to unsafe or jailbreak outputs.

Method: The authors present the Think-Reflect-Revise (TRR) framework consisting of a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples, fine-tuning models for reflective behavior, and policy-guided reflection through reinforcement learning.

Result: The proposed TRR method significantly improves safety in LVLMs, doubling the safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while maintaining performance across general benchmarks (e.g., MMMU and MMStar).

Conclusion: The TRR framework effectively enhances both the safety and interpretability of LVLMs by integrating reflection and self-correction, demonstrating substantial improvement across safety and performance evaluations.

Abstract: As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critical flaw: single-pass reasoning may overlook explicit harmful content in its own output. Our key insight is to exploit this wasted signal through reflection, which can effectively leverage the malicious content revealed in the first-pass reasoning to enable genuine self-correction and prevent unsafe generations. Motivated by this, we propose Think-Reflect-Revise (TRR), a three-stage training framework designed to enhance the safety alignment of LVLMs through policy-guided self-reflection. We first build a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples that follow a think-reflect-revise process. We then fine-tune the target model using the ReSafe dataset to initialize reflective behavior, and finally reinforce policy-guided reflection through reinforcement learning. Experimental results show that TRR substantially improves the safety performance of LVLMs across both safety-awareness benchmarks and jailbreak attack evaluations, increasing the overall safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while preserving stable performance on general benchmarks such as MMMU and MMStar. The project page is available at https://think-reflect-revise.github.io/.

</details>


### [232] [CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles](https://arxiv.org/abs/2512.06840)
*Satoshi Hashimoto,Tatsuya Konishi,Tomoya Kaichi,Kazunori Matsumoto,Mori Kurokawa*

Main category: cs.CV

TL;DR: The paper introduces Continual Anomaly Detection with Ensembles (CADE), a novel approach combining weakly-supervised video anomaly detection and continual learning to address domain shifts and reduce forgetting.


<details>
  <summary>Details</summary>
Motivation: Traditional weakly-supervised video anomaly detection methods fail in scenarios involving domain shifts, leading to performance degradation due to a phenomenon called forgetting.

Method: CADE employs a Dual-Generator for mitigating data imbalance and label uncertainty, and a Multi-Discriminator ensemble approach to address incompleteness caused by forgetting.

Result: Experiments demonstrate that CADE significantly surpasses existing VAD methods in multi-scene datasets like ShanghaiTech and Charlotte Anomaly datasets.

Conclusion: The CADE framework effectively blends weakly-supervised VAD with continual learning methods and demonstrates its superiority in handling domain shifts and enhancing anomaly detection performance across scenes.

Abstract: Video anomaly detection (VAD) has long been studied as a crucial problem in public security and crime prevention. In recent years, weakly-supervised VAD (WVAD) have attracted considerable attention due to their easy annotation process and promising research results. While existing WVAD methods tackle mainly on static datasets, the possibility that the domain of data can vary has been neglected. To adapt such domain-shift, the continual learning (CL) perspective is required because otherwise additional training only with new coming data could easily cause performance degradation for previous data, i.e., forgetting. Therefore, we propose a brand-new approach, called Continual Anomaly Detection with Ensembles (CADE) that is the first work combining CL and WVAD viewpoints. Specifically, CADE uses the Dual-Generator(DG) to address data imbalance and label uncertainty in WVAD. We also found that forgetting exacerbates the "incompleteness'' where the model becomes biased towards certain anomaly modes, leading to missed detections of various anomalies. To address this, we propose to ensemble Multi-Discriminator (MD) that capture missed anomalies in past scenes due to forgetting, using multiple models. Extensive experiments show that CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets.

</details>


### [233] [Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2512.06845)
*Satoshi Hashimoto,Hitoshi Nishimura,Yanan Wang,Mori Kurokawa*

Main category: cs.CV

TL;DR: This paper presents a new method, PA-VAD, for video anomaly detection without using real abnormal videos, achieving state-of-the-art accuracy through synthesized pseudo-abnormal videos and real normal images.


<details>
  <summary>Details</summary>
Motivation: The scarcity and high collection costs of real abnormal footage limit the deployment of video anomaly detection in practical scenarios.

Method: The authors propose PA-VAD, a generation-driven method using synthesized pseudo-abnormal videos derived from real normal images and textual prompts. This is supported by CLIP for image selection, refined textual prompts for better fidelity and scene consistency, and a video diffusion model. Training incorporates a domain-aligned regularized module to mitigate excessive spatiotemporal magnitude.

Result: Their approach achieved 98.2% on ShanghaiTech and 82.5% on UCF-Crime benchmarks, surpassing the strongest real-abnormal and state-of-the-art methods by +0.6% and +1.9%, respectively.

Conclusion: High-accuracy anomaly detection can be achieved using synthesized data instead of real abnormal videos, paving the way for scalable and cost-efficient deployment of video anomaly detection systems.

Abstract: Deploying video anomaly detection in practice is hampered by the scarcity and collection cost of real abnormal footage. We address this by training without any real abnormal videos while evaluating under the standard weakly supervised split, and we introduce PA-VAD, a generation-driven approach that learns a detector from synthesized pseudo-abnormal videos paired with real normal videos, using only a small set of real normal images to drive synthesis. For synthesis, we select class-relevant initial images with CLIP and refine textual prompts with a vision-language model to improve fidelity and scene consistency before invoking a video diffusion model. For training, we mitigate excessive spatiotemporal magnitude in synthesized anomalies by an domain-aligned regularized module that combines domain alignment and memory usage-aware updates. Extensive experiments show that our approach reaches 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming the UVAD state-of-the-art on UCF-Crime by +1.9%. The results demonstrate that high-accuracy anomaly detection can be obtained without collecting real anomalies, providing a practical path toward scalable deployment.

</details>


### [234] [Generating Storytelling Images with Rich Chains-of-Reasoning](https://arxiv.org/abs/2512.07198)
*Xiujie Song,Qi Jia,Shota Watanabe,Xiaoyi Pang,Ruijie Chen,Mengyue Wu,Kenny Q. Zhu*

Main category: cs.CV

TL;DR: The paper introduces StorytellingPainter, a pipeline combining LLMs and T2I models for generating semantically rich images. It presents dedicated evaluation metrics and proposes lightweight LLMs called Mini-Storytellers for enhanced story-image generation.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity and creation challenges of semantically rich Storytelling Images, which convey multi-layered visual stories and have broad applications.

Method: The paper proposes StorytellingPainter, a two-stage pipeline using LLMs for creative reasoning and T2I models for visual synthesis. It includes an evaluation framework with semantic complexity, diversity, and story-image alignment metrics.

Result: Experimental results show the proposed methods are feasible and effective, with tailored training strategies improving LLM performance.

Conclusion: The approaches effectively generate Storytelling Images, bridging gaps in image richness and story alignment. Lightweight models like Mini-Storytellers enhance performance.

Abstract: An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.

</details>


### [235] [Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT](https://arxiv.org/abs/2512.06849)
*Matan Atad,Alexander W. Marka,Lisa Steinhelfer,Anna Curto-Vilalta,Yannik Leonhardt,Sarah C. Foreman,Anna-Sophia Walburga Dietrich,Robert Graf,Alexandra S. Gersing,Bjoern Menze,Daniel Rueckert,Jan S. Kirschke,Hendrik Möller*

Main category: cs.CV

TL;DR: The paper introduces a weakly supervised method to accurately segment vertebral metastasis in CT scans using only vertebra-level labels, achieving high performance without relying on lesion masks.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of accurate segmentation of vertebral metastasis in CT scans, where annotations are scarce and lesions are hard to differentiate from benign changes.

Method: The proposed method employs a Diffusion Autoencoder (DAE) to create classifier-guided edits of vertebrae and uses Hide-and-Seek Attribution to isolate malignancy contributions in candidate lesion regions, resulting in final segmentations.

Result: The method achieves strong performance (F1: 0.91/0.85; Dice: 0.87/0.78) for both lytic and blastic lesions, surpassing baseline methods.

Conclusion: The paper demonstrates that vertebra-level labels combined with generative editing and occlusion techniques can enable reliable lesion segmentation in CT, without relying on voxel-level annotations.

Abstract: Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.

</details>


### [236] [Omni-Referring Image Segmentation](https://arxiv.org/abs/2512.06862)
*Qiancheng Zheng,Yunhang Shen,Gen Luo,Baiyang Song,Xing Sun,Xiaoshuai Sun,Yiyi Zhou,Rongrong Ji*

Main category: cs.CV

TL;DR: This paper introduces OmniRIS, a task that enables generalized image segmentation through multi-modal inputs like text instructions and reference images with masks, boxes, or scribbles.


<details>
  <summary>Details</summary>
Motivation: Current segmentation tasks lack flexibility in conditioning inputs. Leveraging both text and visual prompts can enhance segmentation tasks by combining the strengths of detailed attribute description and object grounding.

Method: The authors propose OmniRIS, a task enabling segmentation through text and reference images as inputs. They develop a large dataset (OmniRef) to support this task and introduce OmniSegNet, a baseline model for handling omni-modal instructions.

Result: Through experiments, OmniSegNet was shown to perform well at following omni-modal instructions, validating OmniRIS as an effective framework for varied segmentation applications.

Conclusion: OmniRIS demonstrates that combining text and visual prompts can improve the adaptability and application of image segmentation technologies.

Abstract: In this paper, we propose a novel task termed Omni-Referring Image Segmentation (OmniRIS) towards highly generalized image segmentation. Compared with existing unimodally conditioned segmentation tasks, such as RIS and visual RIS, OmniRIS supports the input of text instructions and reference images with masks, boxes or scribbles as omni-prompts. This property makes it can well exploit the intrinsic merits of both text and visual modalities, i.e., granular attribute referring and uncommon object grounding, respectively. Besides, OmniRIS can also handle various segmentation settings, such as one v.s. many and many v.s. many, further facilitating its practical use. To promote the research of OmniRIS, we also rigorously design and construct a large dataset termed OmniRef, which consists of 186,939 omni-prompts for 30,956 images, and establish a comprehensive evaluation system. Moreover, a strong and general baseline termed OmniSegNet is also proposed to tackle the key challenges of OmniRIS, such as omni-prompt encoding. The extensive experiments not only validate the capability of OmniSegNet in following omni-modal instructions, but also show the superiority of OmniRIS for highly generalized image segmentation.

</details>


### [237] [Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training](https://arxiv.org/abs/2512.06864)
*Kaixuan Lu,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: AutoQ-VIS introduces a novel unsupervised framework to improve video instance segmentation without human annotations, achieving state-of-the-art performance on YouTubeVIS-2019 data.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome annotation challenges in video instance segmentation, specifically addressing the need for pixel-level masks and temporal consistency labels without relying on manual labeling.

Method: The authors propose AutoQ-VIS, a quality-guided self-training approach that uses a closed-loop system for pseudo-label generation and automatic quality assessment to progressively adapt from synthetic to real videos.

Result: AutoQ-VIS achieves state-of-the-art performance with 52.6 AP50 on YouTubeVIS-2019 val set, outperforming previous methods like VideoCutLER by 4.4% without using human annotations.

Conclusion: The quality-aware self-training framework proves effective for unsupervised video instance segmentation, bridging the synthetic-to-real domain gap and providing a viable alternative to annotation-heavy methods.

Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\text{AP}_{50}$ on YouTubeVIS-2019 $\texttt{val}$ set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.

</details>


### [238] [Spatial Retrieval Augmented Autonomous Driving](https://arxiv.org/abs/2512.06865)
*Xiaosong Jia,Chenhe Zhang,Yule Jiang,Songbur Wong,Zhiyuan Zhang,Chen Chen,Shaofeng Zhang,Xuanhe Zhou,Xue Yang,Junchi Yan,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: The paper introduces a spatial retrieval paradigm using offline geographic images to enhance autonomous driving tasks under challenging conditions.


<details>
  <summary>Details</summary>
Motivation: Current systems rely heavily on onboard sensors which have limitations such as occlusion and poor visibility. Humans naturally recall road structures even in poor conditions, and the paper aims to imbue autonomous systems with a similar ability.

Method: The authors retrieve geographic images via Google Maps APIs and align them with vehicle trajectories. Baselines are established on core autonomous driving tasks to assess performance gains.

Result: Introducing geographic images improves certain tasks like object detection, mapping, and planning. Extensive experiments validate the paradigm's effectiveness.

Conclusion: Using offline retrieved geographic images proves to be a feasible and beneficial extension for autonomous driving systems, highlighting potential for further exploration.

Abstract: Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks.
  For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm.

</details>


### [239] [Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models](https://arxiv.org/abs/2512.07564)
*Kassoum Sanogo,Renzo Ardiccioni*

Main category: cs.CV

TL;DR: The paper presents a training-free self-correction method for VLMs to reduce hallucination by iteratively refining responses based on uncertainty-guided re-attention.


<details>
  <summary>Details</summary>
Motivation: Vision-language models often generate hallucinated content and require methodologies to improve their accuracy without requiring additional training.

Method: A framework is introduced that quantifies uncertainty (token entropy, attention dispersion, semantic consistency, claim confidence) and uses attention-guided cropping for refinement, without gradient updates or retraining.

Result: The approach reduces hallucination rates by 9.8% and improves object existence accuracy by 4.7% on tested benchmarks, validated on the Qwen2.5-VL-7B architecture.

Conclusion: The method shows promise in reducing VLM hallucinations and improving prediction grounded in visual evidence, with plans for broader architecture validation and released code to support future research.

Abstract: Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.

</details>


### [240] [Towards Robust Pseudo-Label Learning in Semantic Segmentation: An Encoding Perspective](https://arxiv.org/abs/2512.06870)
*Wangkai Li,Rui Sun,Zhaoyang Li,Tianzhu Zhang*

Main category: cs.CV

TL;DR: Pseudo-label learning in semantic segmentation often struggles with erroneous labels. This paper proposes ECOCSeg, enhancing pseudo-label learning via error-correcting output codes (ECOC).


<details>
  <summary>Details</summary>
Motivation: Address the challenge of erroneous pseudo-labels in label-scarce semantic segmentation scenarios, including UDA and SSL, which are further worsened by the use of one-hot encoding.

Method: Introduce ECOCSeg, utilizing error-correcting output codes (ECOC) to provide fine-grained class encodings, a bit-level label denoising mechanism, and integration with existing methods.

Result: ECOCSeg improves stability and generalization in pseudo-label learning and consistently outperforms on various UDA and SSL benchmarks across models.

Conclusion: ECOCSeg offers a robust and efficient solution for pseudo-label learning in semantic segmentation, enhancing performance and overcoming key limitations of existing methods.

Abstract: Pseudo-label learning is widely used in semantic segmentation, particularly in label-scarce scenarios such as unsupervised domain adaptation (UDA) and semisupervised learning (SSL). Despite its success, this paradigm can generate erroneous pseudo-labels, which are further amplified during training due to utilization of one-hot encoding. To address this issue, we propose ECOCSeg, a novel perspective for segmentation models that utilizes error-correcting output codes (ECOC) to create a fine-grained encoding for each class. ECOCSeg offers several advantages. First, an ECOC-based classifier is introduced, enabling model to disentangle classes into attributes and handle partial inaccurate bits, improving stability and generalization in pseudo-label learning. Second, a bit-level label denoising mechanism is developed to generate higher-quality pseudo-labels, providing adequate and robust supervision for unlabeled images. ECOCSeg can be easily integrated with existing methods and consistently demonstrates significant improvements on multiple UDA and SSL benchmarks across different segmentation architectures. Code is available at https://github.com/Woof6/ECOCSeg.

</details>


### [241] [SceneMixer: Exploring Convolutional Mixing Networks for Remote Sensing Scene Classification](https://arxiv.org/abs/2512.06877)
*Mohammed Q. Alkhatib,Ali Jamali,Swalpa Kumar Roy*

Main category: cs.CV

TL;DR: The paper introduces a lightweight architecture using convolutional mixers for remote sensing scene classification with higher efficiency and competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges in generalizability of existing models caused by variations such as spatial resolution and background conditions in remote sensing scene classification.

Method: The proposed model combines depthwise convolutions for spatial mixing and pointwise operations for channel mixing to efficiently extract local and contextual information with minimal computational load.

Result: Accuracy of 74.7%, average accuracy of 74.57%, Kappa value of 73.79 on AID dataset; accuracy of 93.90%, average accuracy of 93.93%, Kappa value of 93.22 on EuroSAT dataset.

Conclusion: The method effectively balances computational efficiency and classification accuracy, outperforming standard CNN and transformer models.

Abstract: Remote sensing scene classification plays a key role in Earth observation by enabling the automatic identification of land use and land cover (LULC) patterns from aerial and satellite imagery. Despite recent progress with convolutional neural networks (CNNs) and vision transformers (ViTs), the task remains challenging due to variations in spatial resolution, viewpoint, orientation, and background conditions, which often reduce the generalization ability of existing models. To address these challenges, this paper proposes a lightweight architecture based on the convolutional mixer paradigm. The model alternates between spatial mixing through depthwise convolutions at multiple scales and channel mixing through pointwise operations, enabling efficient extraction of both local and contextual information while keeping the number of parameters and computations low. Extensive experiments were conducted on the AID and EuroSAT benchmarks. The proposed model achieved overall accuracy, average accuracy, and Kappa values of 74.7%, 74.57%, and 73.79 on the AID dataset, and 93.90%, 93.93%, and 93.22 on EuroSAT, respectively. These results demonstrate that the proposed approach provides a good balance between accuracy and efficiency compared with widely used CNN- and transformer-based models. Code will be publicly available on: https://github.com/mqalkhatib/SceneMixer

</details>


### [242] [Hierarchical Image-Guided 3D Point Cloud Segmentation in Industrial Scenes via Multi-View Bayesian Fusion](https://arxiv.org/abs/2512.06882)
*Yu Zhu,Naoya Chiba,Koichi Hashimoto*

Main category: cs.CV

TL;DR: This paper proposes a hierarchical image-guided framework for reliable 3D segmentation in complex industrial environments, addressing challenges like occlusion and scale differences through progressive refinement and multi-view Bayesian fusion.


<details>
  <summary>Details</summary>
Motivation: To improve 3D segmentation in dense and complex industrial scenes, which are often hindered by occlusion, scale disparities, and high annotation costs.

Method: The authors propose a two-stage hierarchical process: (1) top-view image rendering for instance segmentation, using SAM masks guided by YOLO-World, projected onto the 3D point cloud; (2) multi-view part-level segmentation, followed by Bayesian update fusion for semantic consistency.

Result: Experiments on industrial and public datasets show high per-class mIoU scores, showcasing the model's robustness to occlusion, structural complexity, and adaptability to diverse 3D scenarios.

Conclusion: The framework efficiently handles segmentation challenges in 3D scenes, balancing annotation efficiency, semantic consistency, and generalization across different environments.

Abstract: Reliable 3D segmentation is critical for understanding complex scenes with dense layouts and multi-scale objects, as commonly seen in industrial environments. In such scenarios, heavy occlusion weakens geometric boundaries between objects, and large differences in object scale will cause end-to-end models fail to capture both coarse and fine details accurately. Existing 3D point-based methods require costly annotations, while image-guided methods often suffer from semantic inconsistencies across views. To address these challenges, we propose a hierarchical image-guided 3D segmentation framework that progressively refines segmentation from instance-level to part-level. Instance segmentation involves rendering a top-view image and projecting SAM-generated masks prompted by YOLO-World back onto the 3D point cloud. Part-level segmentation is subsequently performed by rendering multi-view images of each instance obtained from the previous stage and applying the same 2D segmentation and back-projection process at each view, followed by Bayesian updating fusion to ensure semantic consistency across views. Experiments on real-world factory data demonstrate that our method effectively handles occlusion and structural complexity, achieving consistently high per-class mIoU scores. Additional evaluations on public dataset confirm the generalization ability of our framework, highlighting its robustness, annotation efficiency, and adaptability to diverse 3D environments.

</details>


### [243] [JoPano: Unified Panorama Generation via Joint Modeling](https://arxiv.org/abs/2512.06885)
*Wancheng Feng,Chen An,Zhenliang He,Meina Kan,Shiguang Shan,Lukun Wang*

Main category: cs.CV

TL;DR: This paper introduces JoPano, a novel approach for panorama generation that jointly handles text-to-panorama and view-to-panorama tasks using a DiT-based model, improving visual quality and workflow efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing panorama generation methods face two main challenges: limited visual quality due to U-Net-based architectures and inefficiency caused by independently handling text-to-panorama and view-to-panorama tasks.

Method: The authors propose a Joint-Face Adapter for transferring DiT backbone generative capabilities to panoramas, utilize Poisson Blending to improve seam consistency, introduce new evaluation metrics Seam-SSIM and Seam-Sobel, and present a unified condition switching mechanism for combining both task types into one model.

Result: JoPano achieves high-quality panorama generation with superior visual quality and consistency across text-to-panorama and view-to-panorama tasks, outperforming state-of-the-art models on various metrics.

Conclusion: This research successfully addresses the challenges in panorama generation, offering a unified, efficient, and effective model that sets new benchmarks in performance for both text-to-panorama and view-to-panorama generation tasks.

Abstract: Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.

</details>


### [244] [Balanced Learning for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2512.06886)
*Wangkai Li,Rui Sun,Bohao Liao,Zhaoyang Li,Tianzhu Zhang*

Main category: cs.CV

TL;DR: The paper proposes BLDA for addressing class imbalance in unsupervised domain adaptation for semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle challenges of class imbalance and distribution shift in both data and label space between domains in UDA.

Method: BLDA identifies over- and under-predicted classes via predicted logits, aligns class distributions using shared anchors, incorporates correction terms into the loss function, and leverages cumulative density as domain-shared knowledge.

Result: BLDA shows consistent performance improvement, particularly for under-predicted classes, across two standard benchmarks when integrated into existing UDA methods.

Conclusion: BLDA effectively addresses class bias in UDA semantic segmentation and achieves better class-balanced learning outcomes. The code is publicly available.

Abstract: Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Despite the effectiveness of self-training techniques in UDA, they struggle to learn each class in a balanced manner due to inherent class imbalance and distribution shift in both data and label space between domains. To address this issue, we propose Balanced Learning for Domain Adaptation (BLDA), a novel approach to directly assess and alleviate class bias without requiring prior knowledge about the distribution shift. First, we identify over-predicted and under-predicted classes by analyzing the distribution of predicted logits. Subsequently, we introduce a post-hoc approach to align the logits distributions across different classes using shared anchor distributions. To further consider the network's need to generate unbiased pseudo-labels during self-training, we estimate logits distributions online and incorporate logits correction terms into the loss function. Moreover, we leverage the resulting cumulative density as domain-shared structural knowledge to connect the source and target domains. Extensive experiments on two standard UDA semantic segmentation benchmarks demonstrate that BLDA consistently improves performance, especially for under-predicted classes, when integrated into various existing methods. Code is available at https://github.com/Woof6/BLDA.

</details>


### [245] [Overcoming Small Data Limitations in Video-Based Infant Respiration Estimation](https://arxiv.org/abs/2512.06888)
*Liyang Song,Hardik Bishnoi,Sai Kumar Reddy Manne,Sarah Ostadabbas,Briana J. Taylor,Michael Wan*

Main category: cs.CV

TL;DR: This paper introduces a novel dataset (AIR-400) and methods for vision-based contactless infant respiration monitoring to advance early detection of breathing irregularities.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the gap in infant-specific respiration monitoring tools, as such advancements could help detect and treat conditions like SIDS, a critical concern for neurodevelopment.

Method: The authors contributed a new dataset, AIR-400, comprised of 400 videos, along with devised infant-specific algorithms leveraging region-of-interest and spatiotemporal neural processing with optical flow.

Result: They presented AIR-400, reproducible pipelines, and benchmarks for vision-based infant respiration monitoring, filling a key void in the domain.

Conclusion: The dataset, algorithms, and benchmarks present a breakthrough for research in infant respiration, providing open-access tools for further development in this critical area.

Abstract: The development of contactless respiration monitoring for infants could enable advances in the early detection and treatment of breathing irregularities, which are associated with neurodevelopmental impairments and conditions like sudden infant death syndrome (SIDS). But while respiration estimation for adults is supported by a robust ecosystem of computer vision algorithms and video datasets, only one small public video dataset with annotated respiration data for infant subjects exists, and there are no reproducible algorithms which are effective for infants. We introduce the annotated infant respiration dataset of 400 videos (AIR-400), contributing 275 new, carefully annotated videos from 10 recruited subjects to the public corpus. We develop the first reproducible pipelines for infant respiration estimation, based on infant-specific region-of-interest detection and spatiotemporal neural processing enhanced by optical flow inputs. We establish, through comprehensive experiments, the first reproducible benchmarks for the state-of-the-art in vision-based infant respiration estimation. We make our dataset, code repository, and trained models available for public use.

</details>


### [246] [Scaling Zero-Shot Reference-to-Video Generation](https://arxiv.org/abs/2512.06905)
*Zijian Zhou,Shikun Liu,Haozhe Liu,Haonan Qiu,Zhaochong An,Weiming Ren,Zhiheng Liu,Xiaoke Huang,Kam Woh Ng,Tian Xie,Xiao Han,Yuren Cong,Hang Li,Chuyan Zhu,Aditya Patel,Tao Xiang,Sen He*

Main category: cs.CV

TL;DR: This paper introduces Saber, a novel framework for reference-to-video generation that eliminates the need for expensive and explicit reference datasets by leveraging video-text pairs and advanced modeling techniques.


<details>
  <summary>Details</summary>
Motivation: Current reference-to-video methods face challenges due to reliance on explicit reference datasets, which are costly and difficult to construct. The study seeks to create a scalable solution for generating identity-preserving videos without needing such data.

Method: Saber employs a zero-shot framework trained on video-text pairs. It uses a masked training strategy, attention-based models, and mask augmentation techniques to ensure identity consistency and reduce copy-paste artifacts.

Result: Saber achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained on explicit R2V datasets, highlighting its generalization strength across varying numbers of references.

Conclusion: The proposed Saber framework is a scalable and effective approach for reference-to-video generation that overcomes the limitations of existing methods, showcasing strong performance and generalization capabilities.

Abstract: Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.

</details>


### [247] [Can We Go Beyond Visual Features? Neural Tissue Relation Modeling for Relational Graph Analysis in Non-Melanoma Skin Histology](https://arxiv.org/abs/2512.06949)
*Shravan Venkatraman,Muthu Subash Kavitha,Joe Dhanith P R,V Manikandarajan,Jia Wu*

Main category: cs.CV

TL;DR: This paper presents Neural Tissue Relation Modeling (NTRM), which integrates CNNs with graph neural networks to improve histopathology image segmentation by encoding spatial and functional tissue relationships, outperforming existing methods on a skin cancer dataset.


<details>
  <summary>Details</summary>
Motivation: Histopathology segmentation is critical for skin cancer diagnosis but faces challenges with regions of overlapping and similar tissues, where current CNN approaches fail to capture biological and spatial contexts.

Method: The authors propose NTRM, a framework combining CNNs with tissue-level graph neural networks to model inter-tissue relationships. NTRM uses a graph over predicted regions, context propagation through message passing, and spatial projection for improved segmentation.

Result: NTRM achieves better performance than state-of-the-art methods with a 4.9% to 31.25% improvement in Dice similarity coefficient on the Histopathology Non-Melanoma Skin Cancer Segmentation Dataset.

Conclusion: Relational modeling like NTRM enables context-aware and structurally coherent histopathology segmentation by encoding inter-tissue dependencies, addressing the limitations of existing CNN-based frameworks.

Abstract: Histopathology image segmentation is essential for delineating tissue structures in skin cancer diagnostics, but modeling spatial context and inter-tissue relationships remains a challenge, especially in regions with overlapping or morphologically similar tissues. Current convolutional neural network (CNN)-based approaches operate primarily on visual texture, often treating tissues as independent regions and failing to encode biological context. To this end, we introduce Neural Tissue Relation Modeling (NTRM), a novel segmentation framework that augments CNNs with a tissue-level graph neural network to model spatial and functional relationships across tissue types. NTRM constructs a graph over predicted regions, propagates contextual information via message passing, and refines segmentation through spatial projection. Unlike prior methods, NTRM explicitly encodes inter-tissue dependencies, enabling structurally coherent predictions in boundary-dense zones. On the benchmark Histopathology Non-Melanoma Skin Cancer Segmentation Dataset, NTRM outperforms state-of-the-art methods, achieving a robust Dice similarity coefficient that is 4.9\% to 31.25\% higher than the best-performing models among the evaluated approaches. Our experiments indicate that relational modeling offers a principled path toward more context-aware and interpretable histological segmentation, compared to local receptive-field architectures that lack tissue-level structural awareness. Our code is available at https://github.com/shravan-18/NTRM.

</details>


### [248] [Selective Masking based Self-Supervised Learning for Image Semantic Segmentation](https://arxiv.org/abs/2512.06981)
*Yuemin Wang,Ian Stavness*

Main category: cs.CV

TL;DR: The paper introduces a novel selective masking method for self-supervised semantic segmentation, which enhances performance compared to random masking techniques.


<details>
  <summary>Details</summary>
Motivation: To improve semantic segmentation by addressing the limitations of random masking in existing masked image modeling pretraining and maximizing results with limited resources.

Method: The method selectively masks patches with the highest reconstruction loss in iterative reconstruction steps, leveraging the model's knowledge during pretraining.

Result: Selective masking increased segmentation accuracy by 2.9% on general datasets (Pascal VOC, Cityscapes) and 2.5% on weed segmentation datasets (Nassar 2020, Sugarbeets 2016), and improved performance for low-performing classes.

Conclusion: Selective Masking Image Reconstruction is a practical self-supervised pretraining method that enhances semantic segmentation performance, especially in low-resource settings.

Abstract: This paper proposes a novel self-supervised learning method for semantic segmentation using selective masking image reconstruction as the pretraining task. Our proposed method replaces the random masking augmentation used in most masked image modelling pretraining methods. The proposed selective masking method selectively masks image patches with the highest reconstruction loss by breaking the image reconstruction pretraining into iterative steps to leverage the trained model's knowledge. We show on two general datasets (Pascal VOC and Cityscapes) and two weed segmentation datasets (Nassar 2020 and Sugarbeets 2016) that our proposed selective masking method outperforms the traditional random masking method and supervised ImageNet pretraining on downstream segmentation accuracy by 2.9% for general datasets and 2.5% for weed segmentation datasets. Furthermore, we found that our selective masking method significantly improves accuracy for the lowest-performing classes. Lastly, we show that using the same pretraining and downstream dataset yields the best result for low-budget self-supervised pretraining. Our proposed Selective Masking Image Reconstruction method provides an effective and practical solution to improve end-to-end semantic segmentation workflows, especially for scenarios that require limited model capacity to meet inference speed and computational resource requirements.

</details>


### [249] [Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues](https://arxiv.org/abs/2512.07034)
*Tuan-Anh Vu,Hai Nguyen-Truong,Ziqiang Zheng,Binh-Son Hua,Qing Guo,Ivor Tsang,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: The paper introduces TransCues, a framework for enhancing transparent object segmentation by incorporating boundary and reflection features, significantly improving performance metrics on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Transparent objects are challenging to segment due to their transparency and reflection properties, which existing methods fail to capture effectively.

Method: The authors propose TransCues, a pyramidal transformer encoder-decoder framework augmented with Boundary Feature Enhancement and Reflection Feature Enhancement modules to improve segmentation.

Result: TransCues significantly outperformed state-of-the-art methods across several benchmark datasets, achieving notable mIoU improvements: +4.2% on Trans10K-v2, +5.6% on MSD, +10.1% on RGBD-Mirror, +13.1% on TROSD, and +8.3% on Stanford2D3D.

Conclusion: Incorporating boundary and reflection cues offers a promising solution to transparent object segmentation challenges, delivering substantial benefits to segmentation accuracy and applicability across datasets.

Abstract: Glass is a prevalent material among solid objects in everyday life, yet segmentation methods struggle to distinguish it from opaque materials due to its transparency and reflection. While it is known that human perception relies on boundary and reflective-object features to distinguish glass objects, the existing literature has not yet sufficiently captured both properties when handling transparent objects. Hence, we propose incorporating both of these powerful visual cues via the Boundary Feature Enhancement and Reflection Feature Enhancement modules in a mutually beneficial way. Our proposed framework, TransCues, is a pyramidal transformer encoder-decoder architecture to segment transparent objects. We empirically show that these two modules can be used together effectively, improving overall performance across various benchmark datasets, including glass object semantic segmentation, mirror object semantic segmentation, and generic segmentation datasets. Our method outperforms the state-of-the-art by a large margin, achieving +4.2% mIoU on Trans10K-v2, +5.6% mIoU on MSD, +10.1% mIoU on RGBD-Mirror, +13.1% mIoU on TROSD, and +8.3% mIoU on Stanford2D3D, showing the effectiveness of our method against glass objects.

</details>


### [250] [Evaluating and Preserving High-level Fidelity in Super-Resolution](https://arxiv.org/abs/2512.07037)
*Josep M. Rocafort,Shaolin Su,Javier Vazquez-Corral,Alexandra Gomez-Villa*

Main category: cs.CV

TL;DR: The paper proposes measuring high-level fidelity in Super-Resolution (SR) models to improve reliability. It introduces a dataset and methodology to evaluate fidelity and enhance SR models' performance.


<details>
  <summary>Details</summary>
Motivation: To address the issue where SR models may hallucinate and alter image content despite achieving high visual quality. Current image metrics inadequately measure such high-level fidelity changes.

Method: The paper constructs a fidelity-annotated dataset, evaluates SR models' performance in preserving high-level fidelity, analyzes existing metric correlations, and demonstrates improvements by fine-tuning SR models using fidelity feedback.

Result: The study shows SR models can improve both semantic fidelity and perceptual quality through fine-tuning. High-level fidelity metrics prove valuable for evaluation and model optimization.

Conclusion: Measuring high-level fidelity is critical for ensuring reliable SR models. The proposed dataset and framework demonstrate potential for enhanced evaluation and optimization of SR models. Dataset, code, and models will be released upon acceptance.

Abstract: Recent image Super-Resolution (SR) models are achieving impressive effects in reconstructing details and delivering visually pleasant outputs. However, the overpowering generative ability can sometimes hallucinate and thus change the image content despite gaining high visual quality. This type of high-level change can be easily identified by humans yet not well-studied in existing low-level image quality metrics. In this paper, we establish the importance of measuring high-level fidelity for SR models as a complementary criterion to reveal the reliability of generative SR models. We construct the first annotated dataset with fidelity scores from different SR models, and evaluate how state-of-the-art (SOTA) SR models actually perform in preserving high-level fidelity. Based on the dataset, we then analyze how existing image quality metrics correlate with fidelity measurement, and further show that this high-level task can be better addressed by foundation models. Finally, by fine-tuning SR models based on our fidelity feedback, we show that both semantic fidelity and perceptual quality can be improved, demonstrating the potential value of our proposed criteria, both in model evaluation and optimization. We will release the dataset, code, and models upon acceptance.

</details>


### [251] [DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation](https://arxiv.org/abs/2512.07051)
*Adnan Munir,Shujaat Khan*

Main category: cs.CV

TL;DR: The paper introduces DAUNet, a lightweight UNet variant, leveraging Deformable V2 Convolutions and Parameter-Free Attention (SimAM) for enhanced medical image segmentation while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve medical image segmentation for clinical use with models that are computationally efficient, robust, and suitable for resource-constrained environments.

Method: DAUNet uses Deformable V2 Convolutions for better spatial adaptability and SimAM attention modules for context-aware feature refinement, integrated in a lightweight design.

Result: DAUNet achieves superior performance on challenging clinical datasets (FH-PS-AoP and FUMPE), outperforming state-of-the-art models in Dice score, HD95, ASD, and parameter efficiency.

Conclusion: DAUNet is highly robust and resource-efficient, making it well-suited for real-time medical applications in environments with limited computational resources.

Abstract: Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.

</details>


### [252] [RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting](https://arxiv.org/abs/2512.07052)
*Hoang-Nhat Tran,Francesco Di Sario,Gabriele Spadaro,Giuseppe Valenzise,Enzo Tartaglione*

Main category: cs.CV

TL;DR: The paper presents a flexible and efficient compression scheme for 3D Gaussian Splatting (3DGS) enabling high-quality rendering with dynamic rate control.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of 3D Gaussian Splatting (3DGS), which suffers from large memory usage and costly training, and to enable adaptable compression for varying device and bandwidth requirements.

Method: The authors propose a computationally lightweight compression scheme that supports interpolation at any rate between predefined bounds, preserving rendering quality without requiring retraining.

Result: The proposed method achieves efficient compression while maintaining high-quality photorealistic rendering across various rates, demonstrating practical utility for immersive applications.

Conclusion: Their approach successfully combines flexibility, computational efficiency, and high-quality rendering, making it ideal for real-world deployment and adaptable to various constraints.

Abstract: Recent advances in neural scene representations have transformed immersive multimedia, with 3D Gaussian Splatting (3DGS) enabling real-time photorealistic rendering. Despite its efficiency, 3DGS suffers from large memory requirements and costly training procedures, motivating efforts toward compression. Existing approaches, however, operate at fixed rates, limiting adaptability to varying bandwidth and device constraints. In this work, we propose a flexible compression scheme for 3DGS that supports interpolation at any rate between predefined bounds. Our method is computationally lightweight, requires no retraining for any rate, and preserves rendering quality across a broad range of operating points. Experiments demonstrate that the approach achieves efficient, high-quality compression while offering dynamic rate control, making it suitable for practical deployment in immersive applications. The code will be provided open-source upon acceptance of the work.

</details>


### [253] [$\mathrm{D}^{\mathrm{3}}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction](https://arxiv.org/abs/2512.07062)
*Changliang Xia,Chengyou Jia,Minnan Luo,Zhuohang Dang,Xin Shen,Bowen Ping*

Main category: cs.CV

TL;DR: The paper presents D³-Predictor, a deterministic, noise-free framework reformed from diffusion models for better dense prediction tasks, avoiding the limitations of stochastic noise.


<details>
  <summary>Details</summary>
Motivation: Diffusion models suffer from stochastic noise disrupting dense predictions, leading to degradation in geometric structure mapping.

Method: The D³-Predictor eliminates stochastic noise by treating diffusion networks as timestep-dependent experts, aggregating geometric priors, and adapting them using task-specific supervision.

Result: D³-Predictor performs competitively or surpasses state-of-the-art methods across diverse dense prediction tasks, requiring less training data and achieving efficient single-step inference.

Conclusion: Reformulating diffusion models for determinism enables robust and efficient dense prediction, overcoming the issues of noise corruption and improving performance.

Abstract: Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\mathrm{D}^{\mathrm{3}}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\mathrm{D}^{\mathrm{3}}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\mathrm{D}^{\mathrm{3}}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.

</details>


### [254] [Persistent Homology-Guided Frequency Filtering for Image Compression](https://arxiv.org/abs/2512.07065)
*Anil Chintapalli,Peter Tenholder,Henry Chen,Arjun Rao*

Main category: cs.CV

TL;DR: The paper introduces a novel method combining discrete Fourier transform and persistent homology for effective feature extraction and compression of noisy images, improving binary classification performance.


<details>
  <summary>Details</summary>
Motivation: Improve feature extraction and image compression reliability in the presence of noisy image datasets, enhancing performance for classification tasks.

Method: Utilized discrete Fourier transform combined with persistent homology analysis to filter frequencies aligned with topological features of images.

Result: Achieved compression quality comparable to JPEG while preserving meaningful data; demonstrated improved binary classification performance with CNNs.

Conclusion: The proposed method enhances reliable image compression and feature extraction under noisy conditions, marking potential in machine learning applications.

Abstract: Feature extraction in noisy image datasets presents many challenges in model reliability. In this paper, we use the discrete Fourier transform in conjunction with persistent homology analysis to extract specific frequencies that correspond with certain topological features of an image. This method allows the image to be compressed and reformed while ensuring that meaningful data can be differentiated. Our experimental results show a level of compression comparable to that of using JPEG using six different metrics. The end goal of persistent homology-guided frequency filtration is its potential to improve performance in binary classification tasks (when augmenting a Convolutional Neural Network) compared to traditional feature extraction and compression methods. These findings highlight a useful end result: enhancing the reliability of image compression under noisy conditions.

</details>


### [255] [Context-measure: Contextualizing Metric for Camouflage](https://arxiv.org/abs/2512.07076)
*Chen-Yang Wang,Gepeng Ji,Song Shao,Ming-Ming Cheng,Deng-Ping Fan*

Main category: cs.CV

TL;DR: This paper introduces Context-measure, a new evaluation metric tailored to context-dependent camouflaged scenarios, improving accuracy in assessing camouflaged object segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing metrics for camouflaged scenarios are insufficient as they are designed for general or salient objects, failing to account for critical spatial context dependencies.

Method: The proposed Context-measure uses a probabilistic pixel-aware correlation framework that incorporates spatial dependencies and quantifies camouflage at the pixel level to improve alignment with human perception.

Result: Experiments on three challenging camouflaged object segmentation datasets demonstrate that Context-measure is more reliable than traditional context-independent metrics.

Conclusion: Context-measure provides a more accurate and context-sensitive evaluation benchmark, applicable to various fields such as agriculture, industry, and medicine.

Abstract: Camouflage is primarily context-dependent yet current metrics for camouflaged scenarios overlook this critical factor. Instead, these metrics are originally designed for evaluating general or salient objects, with an inherent assumption of uncorrelated spatial context. In this paper, we propose a new contextualized evaluation paradigm, Context-measure, built upon a probabilistic pixel-aware correlation framework. By incorporating spatial dependencies and pixel-wise camouflage quantification, our measure better aligns with human perception. Extensive experiments across three challenging camouflaged object segmentation datasets show that Context-measure delivers more reliability than existing context-independent metrics. Our measure can provide a foundational evaluation benchmark for various computer vision applications involving camouflaged patterns, such as agricultural, industrial, and medical scenarios. Code is available at https://github.com/pursuitxi/Context-measure.

</details>


### [256] [DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection](https://arxiv.org/abs/2512.07078)
*Bo Gao,Jingcheng Tong,Xingsheng Chen,Han Yu,Zichen Li*

Main category: cs.CV

TL;DR: The paper presents DFIR-DETR, a transformer-based object detection model that achieves state-of-the-art results in detecting small objects and industrial defects, addressing critical shortcomings in existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in small object detection, such as weak features, cluttered backgrounds, varying object scales, and limitations in current transformer-based detection methods.

Method: The authors propose DFIR-DETR, which incorporates dynamic K-sparse attention, amplitude-normalized upsampling, dual-path shuffle convolution, and frequency-domain processing to enhance object detection.

Result: The model achieves state-of-the-art mAP50 scores of 92.9% on NEU-DET and 51.6% on VisDrone datasets, with an efficient architecture consisting of 11.7M parameters and 41.2 GFLOPs.

Conclusion: DFIR-DETR effectively addresses small object detection challenges, performs well across diverse domains, and is optimized for resource-limited scenarios.

Abstract: Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.
  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.
  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.

</details>


### [257] [COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision](https://arxiv.org/abs/2512.07107)
*Jaeyoon Lee,Hojoon Jung,Sungtae Hwang,Jihyong Oh,Jongwon Choi*

Main category: cs.CV

TL;DR: COREA is a novel framework combining relightable 3D Gaussians and Signed Distance Fields for improved geometry reconstruction, rendering, and relighting.


<details>
  <summary>Details</summary>
Motivation: Current 3D Gaussian Splatting approaches struggle with coarse surfaces and unreliable BRDF-lighting decomposition due to reliance on 2D renderings.

Method: COREA introduces a coarse-to-fine bidirectional alignment strategy in 3D space, enhanced with density-control mechanisms for stabilization.

Result: The framework outperforms in novel-view synthesis, mesh reconstruction, and physically-based rendering.

Conclusion: COREA merges 3D Gaussian Splatting and SDF into a unified system, offering enhanced geometric fidelity and memory efficiency.

Abstract: We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework.

</details>


### [258] [MSN: Multi-directional Similarity Network for Hand-crafted and Deep-synthesized Copy-Move Forgery Detection](https://arxiv.org/abs/2512.07110)
*Liangwei Jiang,Jinluo Xie,Yecheng Huang,Hua Zhang,Hongyu Yang,Di Huang*

Main category: cs.CV

TL;DR: This paper introduces the Multi-directional Similarity Network (MSN) for improved and efficient detection of copy-move image forgery, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Detection of copy-move image forgery has become increasingly difficult due to complex transformations and precise tampering operations, necessitating robust methods for representation and localization.

Method: The proposed method (MSN) features a multi-directional CNN for feature representation and uses a 2-D similarity matrix-based decoder to leverage spatial information for effective localization.

Result: The model outperforms existing methods on classic benchmarks CASIA CMFD, CoMoFoD, and a newly generated dataset, achieving state-of-the-art detection performance.

Conclusion: The MSN model is effective in addressing the key challenges of representation and localization in copy-move forgery detection, while also introducing a new dataset for benchmarking.

Abstract: Copy-move image forgery aims to duplicate certain objects or to hide specific contents with copy-move operations, which can be achieved by a sequence of manual manipulations as well as up-to-date deep generative network-based swapping. Its detection is becoming increasingly challenging for the complex transformations and fine-tuned operations on the tampered regions. In this paper, we propose a novel two-stream model, namely Multi-directional Similarity Network (MSN), to accurate and efficient copy-move forgery detection. It addresses the two major limitations of existing deep detection models in \textbf{representation} and \textbf{localization}, respectively. In representation, an image is hierarchically encoded by a multi-directional CNN network, and due to the diverse augmentation in scales and rotations, the feature achieved better measures the similarity between sampled patches in two streams. In localization, we design a 2-D similarity matrix based decoder, and compared with the current 1-D similarity vector based one, it makes full use of spatial information in the entire image, leading to the improvement in detecting tampered regions. Beyond the method, a new forgery database generated by various deep neural networks is presented, as a new benchmark for detecting the growing deep-synthesized copy-move. Extensive experiments are conducted on two classic image forensics benchmarks, \emph{i.e.} CASIA CMFD and CoMoFoD, and the newly presented one. The state-of-the-art results are reported, which demonstrate the effectiveness of the proposed approach.

</details>


### [259] [Training-free Clothing Region of Interest Self-correction for Virtual Try-On](https://arxiv.org/abs/2512.07126)
*Shengjie Lu,Zhibin Wan,Jiejie Liu,Quan Zhang,Mingjie Sun*

Main category: cs.CV

TL;DR: This paper focuses on improving Virtual Try-ON (VTON) systems by addressing issues with preserving target clothing details. It introduces a novel constraint and attention mechanism, along with a new evaluation metric, to enhance realism and alignment.


<details>
  <summary>Details</summary>
Motivation: Existing VTON methods struggle to accurately maintain clothing details such as patterns, textures, and boundaries when synthesizing clothing on target persons.

Method: The proposed method uses an energy function to guide attention maps, ensuring focus on clothing regions. It also introduces a new evaluation metric called Virtual Try-on Inception Distance (VTID) to better assess alignment with target clothing.

Result: The approach demonstrates superior performance over state-of-the-art methods on VITON-HD and DressCode datasets for both traditional and new metrics. It also improves downstream tasks like Clothing-Change Re-identification across multiple datasets.

Conclusion: This work advances VTON by improving clothing detail preservation and introducing a better evaluation metric, which shows measurable performance gains in both synthesis and related applications.

Abstract: VTON (Virtual Try-ON) aims at synthesizing the target clothing on a certain person, preserving the details of the target clothing while keeping the rest of the person unchanged. Existing methods suffer from the discrepancies between the generated clothing results and the target ones, in terms of the patterns, textures and boundaries. Therefore, we propose to use an energy function to impose constraints on the attention map extracted through the generation process. Thus, at each generation step, the attention can be more focused on the clothing region of interest, thereby influencing the generation results to be more consistent with the target clothing details. Furthermore, to address the limitation that existing evaluation metrics concentrate solely on image realism and overlook the alignment with target elements, we design a new metric, Virtual Try-on Inception Distance (VTID), to bridge this gap and ensure a more comprehensive assessment. On the VITON-HD and DressCode datasets, our approach has outperformed the previous state-of-the-art (SOTA) methods by 1.4%, 2.3%, 12.3%, and 5.8% in the traditional metrics of LPIPS, FID, KID, and the new VTID metrics, respectively. Additionally, by applying the generated data to downstream Clothing-Change Re-identification (CC-Reid) methods, we have achieved performance improvements of 2.5%, 1.1%, and 1.6% on the LTCC, PRCC, VC-Clothes datasets in the metrics of Rank-1. The code of our method is public at https://github.com/MrWhiteSmall/CSC-VTON.git.

</details>


### [260] [MulCLIP: A Multi-level Alignment Framework for Enhancing Fine-grained Long-context CLIP](https://arxiv.org/abs/2512.07128)
*Chau Truong,Hieu Ta Quang,Dung D. Le*

Main category: cs.CV

TL;DR: MulCLIP introduces a novel alignment framework for vision-language models making them capable of handling detailed and lengthy text descriptions by employing multi-level strategies for enhanced text-image connections.


<details>
  <summary>Details</summary>
Motivation: Vision-language models face challenges in effectively aligning images with lengthy and detailed textual descriptions rather than short captions. Addressing these limitations is crucial for broader real-world deployment.

Method: The MulCLIP framework combines global image-text contrastive alignment with extended positional embeddings for longer text sequences. It introduces token reconstruction alignment and subcaption-aggregated patch alignment to link image patches with specific textual contexts.

Result: Experimental results show improved performance across benchmarks and highlight superior fine-grained understanding capabilities, surpassing region-proposal-assisted approaches.

Conclusion: MulCLIP enhances vision-language model capabilities, offering high fine-grained understanding and efficient alignment, proving suitable for diverse practical applications.

Abstract: Vision-language models like CLIP show impressive ability to align images and text, but their training on short, concise captions makes them struggle with lengthy, detailed descriptions. Recent advances mitigate this challenge by leveraging region-proposal information to map visual regions with corresponding sentences from lengthy captions, yet incurring notable deployment costs. We introduce MulCLIP, a novel end-to-end multi-level alignment framework that bridges natural long-text structures with image components. MulCLIP first preserves global contrastive alignment between images and both summary and long captions, while extending positional embeddings for longer text sequences. To further enhance fine-grained understanding, we propose two novel strategies: (1) a token reconstruction alignment over locally calibrated features to strengthen semantic connections between words and image patches, and (2) a subcaption-aggregated patch alignment that automatically extracts and aggregates context-rich patches for each subcaption. Experimental results across diverse benchmarks demonstrate our method consistently improves downstream performance, while ablation studies confirm its multi-scale alignment is the key factor driving better fine-grained capability than region-proposal-assisted approaches, making it particularly suitable for diverse real-world applications.

</details>


### [261] [TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning](https://arxiv.org/abs/2512.07135)
*Zebin Xing,Pengxuan Yang,Linbo Wang,Yichen Zhang,Yiming Hu,Yupeng Zheng,Junli Wang,Yinfeng Gao,Guang Li,Kun Ma,Long Chen,Zhongpu Xia,Qichao Zhang,Hangjun Ye,Dongbin Zhao*

Main category: cs.CV

TL;DR: The paper enhances autonomous driving systems by tailoring trajectory priors using Mixture of Experts (MoE) and fine-tuning trajectory scoring with Reinforcement Learning, achieving improved benchmark performance.


<details>
  <summary>Details</summary>
Motivation: To improve trajectory planning in autonomous driving by addressing the limitations of static trajectory priors and unrefined evaluation mechanisms in current end-to-end frameworks.

Method: Used Mixture of Experts (MoE) for customizing trajectory priors to different driving scenarios and applied Reinforcement Learning to refine trajectory scoring. Enhanced performance by integrating models with advanced perception backbones.

Result: Achieved a score of 51.08 on the navsim ICCV benchmark, securing third place, demonstrating significant performance gains.

Conclusion: The study shows that customizing trajectory priors and refining evaluation mechanisms using advanced techniques like MoE and Reinforcement Learning can significantly enhance autonomous driving systems.

Abstract: Current autonomous driving systems often favor end-to-end frameworks, which take sensor inputs like images and learn to map them into trajectory space via neural networks. Previous work has demonstrated that models can achieve better planning performance when provided with a prior distribution of possible trajectories. However, these approaches often overlook two critical aspects: 1) The appropriate trajectory prior can vary significantly across different driving scenarios. 2) Their trajectory evaluation mechanism lacks policy-driven refinement, remaining constrained by the limitations of one-stage supervised training. To address these issues, we explore improvements in two key areas. For problem 1, we employ MoE to apply different trajectory priors tailored to different scenarios. For problem 2, we utilize Reinforcement Learning to fine-tune the trajectory scoring mechanism. Additionally, we integrate models with different perception backbones to enhance perceptual features. Our integrated model achieved a score of 51.08 on the navsim ICCV benchmark, securing third place.

</details>


### [262] [A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning](https://arxiv.org/abs/2512.07136)
*Siyang Jiang,Mu Yuan,Xiang Ji,Bufang Yang,Zeyu Liu,Lilin Xu,Yang Li,Yuting He,Liran Dong,Wenrui Lu,Zhenyu Yan,Xiaofan Jiang,Wei Gao,Hongkai Chen,Guoliang Xing*

Main category: cs.CV

TL;DR: CUHK-X introduces a multimodal dataset and benchmarks for human action recognition, understanding, and reasoning (HAR, HAU, HARn). It provides high-quality captions using a scene creation method for detailed activity analysis.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in multimodal human action tasks, like the lack of fine-grained annotations in existing datasets, which hampers detailed understanding and reasoning.

Method: CUHK-X collects a large dataset with 58,445 multimodal samples and uses prompt-based scene creation with human validation to generate logical captions for improving data consistency.

Result: Experiments using CUHK-X achieved average accuracies of 76.52% for HAR, 40.76% for HAU, and 70.25% for HARn.

Conclusion: CUHK-X enhances multimodal human action analysis by providing a structured dataset and benchmarks that support data-intensive methods for robust recognition, understanding, and reasoning.

Abstract: Multimodal human action recognition (HAR) leverages complementary sensors for activity classification. Beyond recognition, recent advances in large language models (LLMs) enable detailed descriptions and causal reasoning, motivating new tasks: human action understanding (HAU) and human action reasoning (HARn). However, most LLMs, especially large vision language models (LVLMs), struggle with non-RGB modalities such as depth, IMU, and mmWave due to the lack of large-scale data-caption resources. Existing HAR datasets mainly provide coarse data-label annotations, which are insufficient to capture fine-grained action dynamics needed for HAU and HARn. We consider two ground-truth pair types: (1) data label (discrete category) and (2) data caption (textual description). Naively generating captions from labels often lacks logical and spatiotemporal consistency. We introduce CUHK-X, a large-scale multimodal dataset and benchmark suite for HAR, HAU, and HARn. CUHK-X contains 58,445 samples covering 40 actions performed by 30 participants across two indoor environments. To improve caption consistency, we propose a prompt-based scene creation method that leverages LLMs to generate logically connected activity sequences, followed by human validation. CUHK-X includes three benchmarks with six evaluation tasks. Experiments report average accuracies of 76.52% (HAR), 40.76% (HAU), and 70.25% (HARn). CUHK-X aims to enable the community to apply and develop data-intensive learning methods for robust, multimodal human activity analysis. Project page and code: https://openaiotlab.github.io/CUHK-X/ and https://github.com/openaiotlab/CUHK-X.

</details>


### [263] [CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics](https://arxiv.org/abs/2512.07155)
*Dahyeon Kye,Jeahun Sung,MinKyu Jeon,Jihyong Oh*

Main category: cs.CV

TL;DR: CHIMERA is a diffusion-based framework for image morphing, achieving smoother transitions by utilizing new adaptive mechanisms and evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of smooth and semantically consistent image morphing, as existing approaches lack adaptive structural and semantic alignment.

Method: CHIMERA employs Adaptive Cache Injection (ACI) and Semantic Anchor Prompting (SAP) to achieve semantic and spatial alignment. ACI caches and re-injects features adaptively during denoising, and SAP uses a vision-language model to guide semantic consistency. It also introduces the Global-Local Consistency Score (GLCS) as a new evaluation metric.

Result: CHIMERA outperforms existing methods, producing smoother and more semantically aligned transitions according to experiments and user studies.

Conclusion: CHIMERA establishes a new state of the art in image morphing, offering significant improvements in both smoothness and semantic alignment of transitions.

Abstract: Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.

</details>


### [264] [MuSASplat: Efficient Sparse-View 3D Gaussian Splats via Lightweight Multi-Scale Adaptation](https://arxiv.org/abs/2512.07165)
*Muyu Xu,Fangneng Zhan,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: MuSASplat is a computationally efficient pose-free framework for sparse-view 3D Gaussian splatting, achieving high-quality view rendering with reduced parameters and GPU costs.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of existing 3D Gaussian splatting approaches in terms of computational costs, especially when fine-tuning large ViT backbones for sparse-view 3D rendering.

Method: The authors propose a lightweight Multi-Scale Adapter for efficient fine-tuning and a Feature Fusion Aggregator for effective feature integration and reducing computational/memory requirements.

Result: MuSASplat achieves state-of-the-art rendering quality while drastically reducing training parameters, memory usage, and computational complexity.

Conclusion: The proposed MuSASplat framework provides a substantial efficiency improvement for pose-free 3D rendering, making it suitable for applications with restricted computational resources while sustaining high rendering performance.

Abstract: Sparse-view 3D Gaussian splatting seeks to render high-quality novel views of 3D scenes from a limited set of input images. While recent pose-free feed-forward methods leveraging pre-trained 3D priors have achieved impressive results, most of them rely on full fine-tuning of large Vision Transformer (ViT) backbones and incur substantial GPU costs. In this work, we introduce MuSASplat, a novel framework that dramatically reduces the computational burden of training pose-free feed-forward 3D Gaussian splats models with little compromise of rendering quality. Central to our approach is a lightweight Multi-Scale Adapter that enables efficient fine-tuning of ViT-based architectures with only a small fraction of training parameters. This design avoids the prohibitive GPU overhead associated with previous full-model adaptation techniques while maintaining high fidelity in novel view synthesis, even with very sparse input views. In addition, we introduce a Feature Fusion Aggregator that integrates features across input views effectively and efficiently. Unlike widely adopted memory banks, the Feature Fusion Aggregator ensures consistent geometric integration across input views and meanwhile mitigates the memory usage, training complexity, and computational costs significantly. Extensive experiments across diverse datasets show that MuSASplat achieves state-of-the-art rendering quality but has significantly reduced parameters and training resource requirements as compared with existing methods.

</details>


### [265] [When Privacy Meets Recovery: The Overlooked Half of Surrogate-Driven Privacy Preservation for MLLM Editing](https://arxiv.org/abs/2512.07166)
*Siyuan Xu,Yibing Liu,Peilin Chen,Yung-Hui Li,Shiqi Wang,Sam Kwong*

Main category: cs.CV

TL;DR: This paper addresses privacy leakage in multimodal large language models, examining both privacy authenticity and recovery alongside usability.


<details>
  <summary>Details</summary>
Motivation: Existing efforts obscure private information but neglect to evaluate the authenticity of privacy protection or recovery quality in MLLMs.

Method: The authors introduce the SPPE dataset for assessing privacy recovery quality and propose a guided generation approach for reconstructing private data using multimodal signals.

Result: Both SPPE and InstructPix2Pix experiments demonstrate the introduced method achieves a good balance between privacy protection and MLLM usability.

Conclusion: This study effectively bridges a gap in MLLM privacy research by enabling robust privacy recovery while maintaining the usefulness of edited content.

Abstract: Privacy leakage in Multimodal Large Language Models (MLLMs) has long been an intractable problem. Existing studies, though effectively obscure private information in MLLMs, often overlook the evaluation of the authenticity and recovery quality of user privacy. To this end, this work uniquely focuses on the critical challenge of how to restore surrogate-driven protected data in diverse MLLM scenarios. We first bridge this research gap by contributing the SPPE (Surrogate Privacy Protected Editable) dataset, which includes a wide range of privacy categories and user instructions to simulate real MLLM applications. This dataset offers protected surrogates alongside their various MLLM-edited versions, thus enabling the direct assessment of privacy recovery quality. By formulating privacy recovery as a guided generation task conditioned on complementary multimodal signals, we further introduce a unified approach that reliably reconstructs private content while preserving the fidelity of MLLM-generated edits. The experiments on both SPPE and InstructPix2Pix further show that our approach generalizes well across diverse visual content and editing tasks, achieving a strong balance between privacy protection and MLLM usability.

</details>


### [266] [Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach](https://arxiv.org/abs/2512.07170)
*Jiayang Li,Chengjie Jiang,Junjun Jiang,Pengwei Liang,Jiayi Ma,Liqiang Nie*

Main category: cs.CV

TL;DR: DiTFuse is a novel image fusion framework integrating Diffusion-Transformer models and natural-language instructions for enhanced control, semantic awareness, and adaptability in multimodal image fusion.


<details>
  <summary>Details</summary>
Motivation: Existing image fusion methods lack robustness, adaptability, and the ability to incorporate user intent effectively, especially in challenging scenarios like low-light conditions or exposure imbalance.

Method: DiTFuse uses a unified Diffusion-Transformer-based approach to encode images and natural-language instructions in a shared latent space. It employs a multi-degradation masked image modeling strategy to learn cross-modal alignment, restoration, and task-aware feature selection.

Result: DiTFuse achieves state-of-the-art performance on multiple public benchmarks, offering better texture sharpness, semantic retention, user control, and adaptability for new fusion tasks.

Conclusion: DiTFuse overcomes existing image fusion limitations, enabling robust, semantics-aware, and customizable image fusion while excelling in diverse scenarios and tasks.

Abstract: Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, instruction-driven Diffusion-Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.

</details>


### [267] [TIDE: Two-Stage Inverse Degradation Estimation with Guided Prior Disentanglement for Underwater Image Restoration](https://arxiv.org/abs/2512.07171)
*Shravan Venkatraman,Rakesh Raj Madavan,Pavan Kumar S,Muthu Subash Kavitha*

Main category: cs.CV

TL;DR: TIDE is a novel two-stage framework for underwater image restoration that targets spatially varying degradations and addresses multiple restoration challenges.


<details>
  <summary>Details</summary>
Motivation: To tackle challenges posed by spatially varying and complex underwater image degradations, not effectively addressed by existing methods using uniform restoration strategies.

Method: TIDE employs a two-stage inverse degradation estimation framework, decomposing degradation factors into color distortion, haze, detail loss, and noise. It combines adaptive specialized restoration hypotheses and progressive refinement.

Result: TIDE outperforms state-of-the-art methods in non-reference perceptual quality metrics and achieves competitive fidelity metrics, excelling in color correction and contrast enhancement.

Conclusion: TIDE effectively balances and addresses multiple underwater image degradation factors through targeted restoration strategies, proving its robustness in diverse conditions.

Abstract: Underwater image restoration is essential for marine applications ranging from ecological monitoring to archaeological surveys, but effectively addressing the complex and spatially varying nature of underwater degradations remains a challenge. Existing methods typically apply uniform restoration strategies across the entire image, struggling to handle multiple co-occurring degradations that vary spatially and with water conditions. We introduce TIDE, a $\underline{t}$wo stage $\underline{i}$nverse $\underline{d}$egradation $\underline{e}$stimation framework that explicitly models degradation characteristics and applies targeted restoration through specialized prior decomposition. Our approach disentangles the restoration process into multiple specialized hypotheses that are adaptively fused based on local degradation patterns, followed by a progressive refinement stage that corrects residual artifacts. Specifically, TIDE decomposes underwater degradations into four key factors, namely color distortion, haze, detail loss, and noise, and designs restoration experts specialized for each. By generating specialized restoration hypotheses, TIDE balances competing degradation factors and produces natural results even in highly degraded regions. Extensive experiments across both standard benchmarks and challenging turbid water conditions show that TIDE achieves competitive performance on reference based fidelity metrics while outperforming state of the art methods on non reference perceptual quality metrics, with strong improvements in color correction and contrast enhancement. Our code is available at: https://rakesh-123-cryp.github.io/TIDE.

</details>


### [268] [START: Spatial and Textual Learning for Chart Understanding](https://arxiv.org/abs/2512.07186)
*Zhuoming Liu,Xiaofeng Gao,Feiyang Niu,Qiaozi Gao,Liu Liu,Robinson Piramuthu*

Main category: cs.CV

TL;DR: This paper introduces START, a framework designed to enhance multimodal large language models in chart understanding by focusing on spatial and textual learning.


<details>
  <summary>Details</summary>
Motivation: Chart understanding is essential for effective deployment of MLLMs in tasks requiring precise analysis, pairing visual structure with data representation.

Method: START incorporates chart-element grounding, chart-to-code generation, and introduces START-Dataset and CS-Bench for spatial and textual learning.

Result: The START framework improves performance across models, outperforming previous methods on benchmarks, while addressing challenges in visual and textual chart comprehension.

Conclusion: START effectively advances chart reasoning, introducing tools, datasets, and benchmarks that will be publicly shared for further research.

Abstract: Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.

</details>


### [269] [Integrating Multi-scale and Multi-filtration Topological Features for Medical Image Classification](https://arxiv.org/abs/2512.07190)
*Pengfei Gu,Huimin Li,Haoteng Tang,Dongkuan,Xu,Erik Enriquez,DongChul Kim,Bin Fu,Danny Z. Chen*

Main category: cs.CV

TL;DR: This paper introduces a framework that incorporates advanced topology-based features into vision classifiers for medical image analysis, enhancing both performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing neural networks for medical image classification focus on pixel-intensity or simple topological features, failing to capture multi-scale or deeper topological aspects integral to anatomical structures.

Method: The method computes multi-scale cubical persistence diagrams (PDs), consolidates them with a vineyard algorithm, and processes them using a cross-attention network. These topological embeddings are then fused with CNN or Transformer features in an end-to-end model.

Result: The proposed approach delivers consistent and significant performance improvements over state-of-the-art methods on three public datasets.

Conclusion: Integrating multi-scale and multi-filtration topology into deep learning pipelines effectively enhances the recognition of complex anatomical structures, making medical image analysis more robust and interpretable.

Abstract: Modern deep neural networks have shown remarkable performance in medical image classification. However, such networks either emphasize pixel-intensity features instead of fundamental anatomical structures (e.g., those encoded by topological invariants), or they capture only simple topological features via single-parameter persistence. In this paper, we propose a new topology-guided classification framework that extracts multi-scale and multi-filtration persistent topological features and integrates them into vision classification backbones. For an input image, we first compute cubical persistence diagrams (PDs) across multiple image resolutions/scales. We then develop a ``vineyard'' algorithm that consolidates these PDs into a single, stable diagram capturing signatures at varying granularities, from global anatomy to subtle local irregularities that may indicate early-stage disease. To further exploit richer topological representations produced by multiple filtrations, we design a cross-attention-based neural network that directly processes the consolidated final PDs. The resulting topological embeddings are fused with feature maps from CNNs or Transformers. By integrating multi-scale and multi-filtration topologies into an end-to-end architecture, our approach enhances the model's capacity to recognize complex anatomical structures. Evaluations on three public datasets show consistent, considerable improvements over strong baselines and state-of-the-art methods, demonstrating the value of our comprehensive topological perspective for robust and interpretable medical image classification.

</details>


### [270] [RefLSM: Linearized Structural-Prior Reflectance Model for Medical Image Segmentation and Bias-Field Correction](https://arxiv.org/abs/2512.07191)
*Wenqi Zhao,Jiacheng Sang,Fenghua Cheng,Yonglu Shu,Dong Li,Xiaofeng Yang*

Main category: cs.CV

TL;DR: This paper presents a Reflectance-based Level Set Model (RefLSM) for medical image segmentation, addressing issues like noise and irregular structures with novel methods.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the limitations of traditional methods in handling intensity inhomogeneity, blurred boundaries, and noise in medical image segmentation.

Method: The proposed RefLSM incorporates reflectance decomposition inspired by Retinex theory, linear structural priors, and a relaxed binary level-set, optimized through ADMM.

Result: Experiments on various datasets demonstrated that RefLSM achieved better accuracy, robustness, and efficiency compared to existing level set methods.

Conclusion: RefLSM offers an innovative and effective approach to medical image segmentation, especially in challenging imaging conditions.

Abstract: Medical image segmentation remains challenging due to intensity inhomogeneity, noise, blurred boundaries, and irregular structures. Traditional level set methods, while effective in certain cases, often depend on approximate bias field estimations and therefore struggle under severe non-uniform imaging conditions. To address these limitations, we propose a novel variational Reflectance-based Level Set Model (RefLSM), which explicitly integrates Retinex-inspired reflectance decomposition into the segmentation framework. By decomposing the observed image into reflectance and bias field components, RefLSM directly segments the reflectance, which is invariant to illumination and preserves fine structural details. Building on this foundation, we introduce two key innovations for enhanced precision and robustness. First, a linear structural prior steers the smoothed reflectance gradients toward a data-driven reference, providing reliable geometric guidance in noisy or low-contrast scenes. Second, a relaxed binary level-set is embedded in RefLSM and enforced via convex relaxation and sign projection, yielding stable evolution and avoiding reinitialization-induced diffusion. The resulting variational problem is solved efficiently using an ADMM-based optimization scheme. Extensive experiments on multiple medical imaging datasets demonstrate that RefLSM achieves superior segmentation accuracy, robustness, and computational efficiency compared to state-of-the-art level set methods.

</details>


### [271] [HVQ-CGIC: Enabling Hyperprior Entropy Modeling for VQ-Based Controllable Generative Image Compression](https://arxiv.org/abs/2512.07192)
*Niu Yi,Xu Tianyi,Ma Mingming,Wang Xinkun*

Main category: cs.CV

TL;DR: This paper introduces HVQ-CGIC, a controllable generative image compression framework using a hyperprior for Vector Quantization indices, demonstrating improved rate-distortion performance with significant bitrate reduction.


<details>
  <summary>Details</summary>
Motivation: Current generative image compression methods using Vector Quantization lack adaptability to individual images' content, leading to inefficiencies in bitrate and challenges in flexible rate control.

Method: The paper proposes HVQ-CGIC, which integrates a hyperprior-based entropy model with novel loss design for rate-distortion balance and control, supported by a lightweight hyper-prior estimation network.

Result: HVQ-CGIC achieves a 61.3% average bitrate reduction while maintaining similar perceptual quality to state-of-the-art methods, particularly tested on the Kodak dataset.

Conclusion: HVQ-CGIC provides a foundational improvement in VQGAN-based image compression, potentially setting new benchmarks for efficiency and performance.

Abstract: Generative learned image compression methods using Vector Quantization (VQ) have recently shown impressive potential in balancing distortion and perceptual quality. However, these methods typically estimate the entropy of VQ indices using a static, global probability distribution, which fails to adapt to the specific content of each image. This non-adaptive approach leads to untapped bitrate potential and challenges in achieving flexible rate control. To address this challenge, we introduce a Controllable Generative Image Compression framework based on a VQ Hyperprior, termed HVQ-CGIC. HVQ-CGIC rigorously derives the mathematical foundation for introducing a hyperprior to the VQ indices entropy model. Based on this foundation, through novel loss design, to our knowledge, this framework is the first to introduce RD balance and control into vector quantization-based Generative Image Compression. Cooperating with a lightweight hyper-prior estimation network, HVQ-CGIC achieves a significant advantage in rate-distortion (RD) performance compared to current state-of-the-art (SOTA) generative compression methods. On the Kodak dataset, we achieve the same LPIPS as Control-GIC, CDC and HiFiC with an average of 61.3% fewer bits. We posit that HVQ-CGIC has the potential to become a foundational component for VQGAN-based image compression, analogous to the integral role of the HyperPrior framework in neural image compression.

</details>


### [272] [SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting](https://arxiv.org/abs/2512.07197)
*Seokhyun Youn,Soohyun Lee,Geonho Kim,Weeyoung Kwon,Sung-Ho Bae,Jihyong Oh*

Main category: cs.CV

TL;DR: This survey reviews efficient 3D and 4D Gaussian Splatting techniques for real-time and high-fidelity scene representation, addressing memory and computational challenges.


<details>
  <summary>Details</summary>
Motivation: Massive memory and computational demands in 3D/4D Gaussian Splatting hinder its practical real-time application, especially in dynamic scenes.

Method: The survey categorizes methods into Parameter Compression and Restructuring Compression to improve efficiency and preserve quality, summarizing key ideas and trends.

Result: It provides a unified overview of methods, datasets, benchmarks, and evaluation metrics while examining current limitations and opportunities.

Conclusion: The paper highlights the progress in efficient Gaussian Splatting techniques and outlines future directions for scalable, compact, and real-time 3D/4D scene representation.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.

</details>


### [273] [Understanding Diffusion Models via Code Execution](https://arxiv.org/abs/2512.07201)
*Cheng Yu*

Main category: cs.CV

TL;DR: The paper provides a 300-line implementation of diffusion models to simplify understanding of their practical operation and bridge the gap between theory and coding.


<details>
  <summary>Details</summary>
Motivation: To clarify the practical understanding and implementation of diffusion models, addressing the gap between theoretical formulations and code execution.

Method: The authors offer a minimal, 300-line implementation focusing on core diffusion model concepts like forward diffusion, reverse sampling, noise prediction, and the training loop.

Result: The implementation demonstrates how key diffusion model components function in practice, aiding researchers in understanding theoretical and practical correspondence.

Conclusion: The concise implementation serves as a practical tutorial for researchers, improving accessibility to diffusion model understanding, with code available for further study.

Abstract: Diffusion models have achieved remarkable performance in generative modeling, yet their theoretical foundations are often intricate, and the gap between mathematical formulations in papers and practical open-source implementations can be difficult to bridge. Existing tutorials primarily focus on deriving equations, offering limited guidance on how diffusion models actually operate in code. To address this, we present a concise implementation of approximately 300 lines that explains diffusion models from a code-execution perspective. Our minimal example preserves the essential components -- including forward diffusion, reverse sampling, the noise-prediction network, and the training loop -- while removing unnecessary engineering details. This technical report aims to provide researchers with a clear, implementation-first understanding of how diffusion models work in practice and how code and theory correspond. Our code and pre-trained models are available at: https://github.com/disanda/GM/tree/main/DDPM-DDIM-ClassifierFree.

</details>


### [274] [MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning](https://arxiv.org/abs/2512.07203)
*Xuhui Zheng,Kang An,Ziliang Wang,Yuhang Wang,Faqiang Qian,Yichao Wu*

Main category: cs.CV

TL;DR: MMRPT utilizes reinforcement learning during multimodal pre-training for improved visual reasoning in vision-language models.


<details>
  <summary>Details</summary>
Motivation: To address the descriptive bias in image-caption pairs which limits visual understanding in multimodal models.

Method: Introduces a framework utilizing masked multimodal data, visual dependency estimation, and reinforcement learning to enforce visual-grounded reasoning.

Result: Improved zero-shot and supervised fine-tuning performance across benchmarks, showcasing enhanced robustness and generalizability.

Conclusion: Reinforcement-driven masked reasoning is a more effective pre-training method for multimodal models, improving their ability to ground in visual data.

Abstract: Multimodal pre-training remains constrained by the descriptive bias of image-caption pairs, leading models to favor surface linguistic cues over grounded visual understanding. We introduce MMRPT, a masked multimodal reinforcement pre-training framework that strengthens visual reasoning in MLLMs. We are the first to incorporate reinforcement learning directly into the pre-training of large vision-language models, enabling learning signals that reward visual grounding rather than caption imitation. MMRPT constructs masked multimodal data by estimating sentence-level visual dependency via attention over visual tokens and masking highly vision-dependent segments; the model reconstructs these spans through vision-grounded reasoning guided by a semantic-visual reward. Experiments show consistent zero-shot gains across diverse benchmarks and substantially improved robustness under supervised fine-tuning, demonstrating that reinforcement-driven masked reasoning provides a more reliable and generalizable pre-training objective for multimodal models.

</details>


### [275] [AutoLugano: A Deep Learning Framework for Fully Automated Lymphoma Segmentation and Lugano Staging on FDG-PET/CT](https://arxiv.org/abs/2512.07206)
*Boyang Pan,Zeyu Zhang,Hongyu Meng,Bin Cui,Yingying Zhang,Wenli Hou,Junhao Li,Langdi Zhong,Xiaoxiao Chen,Xiaoyu Xu,Changjin Zuo,Chao Cheng,Nan-Jie Gong*

Main category: cs.CV

TL;DR: AutoLugano is an advanced deep learning system designed for fully automated lymphoma classification, lesion segmentation, and Lugano staging using baseline FDG-PET/CT scans.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create an automated system that simplifies and enhances the accuracy of lymphoma classification and therapeutic stratification based on Lugano staging.

Method: AutoLugano uses a three-step deep learning pipeline: (1) anatomy-informed 3D lesion segmentation with nnU-Net, (2) anatomical localization leveraging the TotalSegmentator toolkit, and (3) transformation of regional distribution into Lugano stages for therapeutic stratification.

Result: The system, validated on an external cohort, achieved an overall accuracy of 88.31% for regional involvement detection and 85.07% for therapeutic stratification, outperforming baseline models.

Conclusion: AutoLugano proves effective as the first end-to-end automated solution for lymphoma staging and treatment planning, offering potential to aid clinical decision-making.

Abstract: Purpose: To develop a fully automated deep learning system, AutoLugano, for end-to-end lymphoma classification by performing lesion segmentation, anatomical localization, and automated Lugano staging from baseline FDG-PET/CT scans. Methods: The AutoLugano system processes baseline FDG-PET/CT scans through three sequential modules:(1) Anatomy-Informed Lesion Segmentation, a 3D nnU-Net model, trained on multi-channel inputs, performs automated lesion detection (2) Atlas-based Anatomical Localization, which leverages the TotalSegmentator toolkit to map segmented lesions to 21 predefined lymph node regions using deterministic anatomical rules; and (3) Automated Lugano Staging, where the spatial distribution of involved regions is translated into Lugano stages and therapeutic groups (Limited vs. Advanced Stage).The system was trained on the public autoPET dataset (n=1,007) and externally validated on an independent cohort of 67 patients. Performance was assessed using accuracy, sensitivity, specificity, F1-scorefor regional involvement detection and staging agreement. Results: On the external validation set, the proposed model demonstrated robust performance, achieving an overall accuracy of 88.31%, sensitivity of 74.47%, Specificity of 94.21% and an F1-score of 80.80% for regional involvement detection,outperforming baseline models. Most notably, for the critical clinical task of therapeutic stratification (Limited vs. Advanced Stage), the system achieved a high accuracy of 85.07%, with a specificity of 90.48% and a sensitivity of 82.61%.Conclusion: AutoLugano represents the first fully automated, end-to-end pipeline that translates a single baseline FDG-PET/CT scan into a complete Lugano stage. This study demonstrates its strong potential to assist in initial staging, treatment stratification, and supporting clinical decision-making.

</details>


### [276] [Object Pose Distribution Estimation for Determining Revolution and Reflection Uncertainty in Point Clouds](https://arxiv.org/abs/2512.07211)
*Frederik Hagelskjær,Dimitrios Arapis,Steffen Madsen,Thorbjørn Mosekjær Iversen*

Main category: cs.CV

TL;DR: This paper introduces a deep learning-based method for estimating object pose uncertainty using 3D colorless data, addressing limitations of existing methods reliant on color input.


<details>
  <summary>Details</summary>
Motivation: Traditional pose estimation methods fail to account for uncertainties due to visual ambiguity and often rely on color, which is unsuitable for many industrial applications.

Method: The authors develop a neural network-based framework to estimate pose uncertainty using only 3D data, focusing on symmetries in reflection and revolution. The method excludes RGB information.

Result: The method was validated in a real-world bin-picking scenario with objects exhibiting geometric ambiguities, demonstrating its effectiveness.

Conclusion: This work offers a significant advancement in pose uncertainty estimation and provides a framework extendable to full SE(3) estimation. Code availability enhances reproducibility.

Abstract: Object pose estimation is crucial to robotic perception and typically provides a single-pose estimate. However, a single estimate cannot capture pose uncertainty deriving from visual ambiguity, which can lead to unreliable behavior. Existing pose distribution methods rely heavily on color information, often unavailable in industrial settings.
  We propose a novel neural network-based method for estimating object pose uncertainty using only 3D colorless data. To the best of our knowledge, this is the first approach that leverages deep learning for pose distribution estimation without relying on RGB input. We validate our method in a real-world bin picking scenario with objects of varying geometric ambiguity. Our current implementation focuses on symmetries in reflection and revolution, but the framework is extendable to full SE(3) pose distribution estimation. Source code available at opde3d.github.io

</details>


### [277] [VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation](https://arxiv.org/abs/2512.07215)
*Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: This paper compares CLIP and DINOv2 for 3D pose estimation in hand object grasping, showing their complementary semantic and geometric strengths.


<details>
  <summary>Details</summary>
Motivation: To understand which vision model is more suitable for tasks like robotic manipulation and grasping, based on semantic and geometric performance.

Method: The study evaluates CLIP-based and DINOv2-based approaches in 6D object pose estimation through extensive experiments on benchmark datasets.

Result: CLIP excels in semantic consistency and language grounding, while DINOv2 offers better geometric precision and competitive 6D pose estimation performance.

Conclusion: The findings highlight the need for selecting models based on specific task requirements, leveraging CLIP for semantics and DINOv2 for geometry.

Abstract: Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.

</details>


### [278] [Towards Robust Protective Perturbation against DeepFake Face Swapping](https://arxiv.org/abs/2512.07228)
*Hengyang Yao,Lin Li,Ke Sun,Jianing Qiu,Huiping Chen*

Main category: cs.CV

TL;DR: This paper focuses on enhancing robustness of invisible perturbations used against DeepFake face swapping by introducing a learnable framework for transformation distributions, achieving significant improvement over existing methods.


<details>
  <summary>Details</summary>
Motivation: DeepFake face swapping poses risks to privacy and security, and current defences embedding invisible perturbations are fragile to transformations like compression and resizing, hence requiring a more robust solution.

Method: The proposed method, Expectation Over Learned distribution of Transformation (EOLT), uses a reinforcement learning-based policy network to adaptively prioritize critical transformations and generate instance-specific perturbations.

Result: EOLT achieves 26% higher average robustness and up to 30% improvements on challenging transformation categories compared to state-of-the-art approaches.

Conclusion: The paper concludes that EOLT successfully addresses limitations of traditional methods by adaptively learning transformation distributions, providing improved protection against DeepFake manipulation.

Abstract: DeepFake face swapping enables highly realistic identity forgeries, posing serious privacy and security risks. A common defence embeds invisible perturbations into images, but these are fragile and often destroyed by basic transformations such as compression or resizing. In this paper, we first conduct a systematic analysis of 30 transformations across six categories and show that protection robustness is highly sensitive to the choice of training transformations, making the standard Expectation over Transformation (EOT) with uniform sampling fundamentally suboptimal. Motivated by this, we propose Expectation Over Learned distribution of Transformation (EOLT), the framework to treat transformation distribution as a learnable component rather than a fixed design choice. Specifically, EOLT employs a policy network that learns to automatically prioritize critical transformations and adaptively generate instance-specific perturbations via reinforcement learning, enabling explicit modeling of defensive bottlenecks while maintaining broad transferability. Extensive experiments demonstrate that our method achieves substantial improvements over state-of-the-art approaches, with 26% higher average robustness and up to 30% gains on challenging transformation categories.

</details>


### [279] [ReLKD: Inter-Class Relation Learning with Knowledge Distillation for Generalized Category Discovery](https://arxiv.org/abs/2512.07229)
*Fang Zhou,Zhiqiang Chen,Martin Pavlovski,Yizhong Zhang*

Main category: cs.CV

TL;DR: ReLKD is a framework addressing the Generalized Category Discovery issue by categorizing both known and novel classes using implicit inter-class relations and achieves superior performance with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of categorizing unlabeled data with mixed known and novel classes by leveraging implicit inter-class relations instead of relying on direct inter-class relations, which are hard to obtain.

Method: ReLKD employs three modules: a target-grained module for learning fine representations, a coarse-grained module for identifying hierarchical class relations, and a distillation module that transfers knowledge to refine representation learning.

Result: ReLKD demonstrates strong performance in categorizing novel and known classes across four datasets, especially when labeled data is sparse.

Conclusion: ReLKD effectively uses inter-class relational information and its components to enhance Generalized Category Discovery tasks while being practical and adaptable in data-scarce environments.

Abstract: Generalized Category Discovery (GCD) faces the challenge of categorizing unlabeled data containing both known and novel classes, given only labels for known classes. Previous studies often treat each class independently, neglecting the inherent inter-class relations. Obtaining such inter-class relations directly presents a significant challenge in real-world scenarios. To address this issue, we propose ReLKD, an end-to-end framework that effectively exploits implicit inter-class relations and leverages this knowledge to enhance the classification of novel classes. ReLKD comprises three key modules: a target-grained module for learning discriminative representations, a coarse-grained module for capturing hierarchical class relations, and a distillation module for transferring knowledge from the coarse-grained module to refine the target-grained module's representation learning. Extensive experiments on four datasets demonstrate the effectiveness of ReLKD, particularly in scenarios with limited labeled data. The code for ReLKD is available at https://github.com/ZhouF-ECNU/ReLKD.

</details>


### [280] [STRinGS: Selective Text Refinement in Gaussian Splatting](https://arxiv.org/abs/2512.07230)
*Abhinav Raundhal,Gaurav Behera,P J Narayanan,Ravi Kiran Sarvadevabhatla,Makarand Tapaswi*

Main category: cs.CV

TL;DR: The paper introduces STRinGS, a method to refine text regions in 3D Gaussian Splatting (3DGS) for better text readability in 3D scene reconstructions.


<details>
  <summary>Details</summary>
Motivation: 3DGS struggles to reconstruct fine-grained text information, leading to semantic loss in real-world scenes rich in textual data.

Method: STRinGS separates text and non-text regions for targeted refinement, optimizing text regions for readability before merging with full-scene reconstructions. It introduces OCR Character Error Rate (CER) as a new metric.

Result: STRinGS shows a 63.6% improvement in text readability over 3DGS within only 7K iterations.

Conclusion: STRinGS advances text-aware 3D reconstruction through selective refinement and a curated dataset, improving understanding and readability in text-rich 3D environments.

Abstract: Text as signs, labels, or instructions is a critical element of real-world scenes as they can convey important contextual information. 3D representations such as 3D Gaussian Splatting (3DGS) struggle to preserve fine-grained text details, while achieving high visual fidelity. Small errors in textual element reconstruction can lead to significant semantic loss. We propose STRinGS, a text-aware, selective refinement framework to address this issue for 3DGS reconstruction. Our method treats text and non-text regions separately, refining text regions first and merging them with non-text regions later for full-scene optimization. STRinGS produces sharp, readable text even in challenging configurations. We introduce a text readability measure OCR Character Error Rate (CER) to evaluate the efficacy on text regions. STRinGS results in a 63.6% relative improvement over 3DGS at just 7K iterations. We also introduce a curated dataset STRinGS-360 with diverse text scenarios to evaluate text readability in 3D reconstruction. Our method and dataset together push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods.

</details>


### [281] [Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models](https://arxiv.org/abs/2512.07234)
*Biao Chen,Lin Zuo,Mengmeng Jing,Kunbin He,Yuchen Wang*

Main category: cs.CV

TL;DR: The paper introduces Dropout Prompt Learning, a method applying dropout for robustness in vision-language models by dropping tokens with context-aware probabilities, achieving improved performance across various benchmarks.


<details>
  <summary>Details</summary>
Motivation: To enhance the robustness and generalization ability of vision-language models using a dropout mechanism.

Method: Dropout is applied to tokens of textual and visual branches with context-aware token significance evaluation. Additionally, residual entropy regularization is proposed for semantic alignment and diverse representation.

Result: The method demonstrates effectiveness in scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization, outperforming other regularization approaches.

Conclusion: Dropout Prompt Learning improves robustness and generalization in vision-language models, outperforming alternative methods on benchmarks.

Abstract: Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.

</details>


### [282] [Unified Camera Positional Encoding for Controlled Video Generation](https://arxiv.org/abs/2512.07237)
*Cheng Zhang,Boying Li,Meng Wei,Yan-Pei Cao,Camilo Cruz Gambardella,Dinh Phung,Jianfei Cai*

Main category: cs.CV

TL;DR: The paper introduces Unified Camera Positional Encoding (UCPE) to address limitations in camera encoding with real-world lens distortions. It achieves high camera controllability and visual fidelity in text-to-video generation, validated on a new extensive video dataset.


<details>
  <summary>Details</summary>
Motivation: Current camera encoding methods rely on simplified assumptions, limiting generalization across diverse real-world circumstances. A robust and universal representation is needed for tasks like autonomous driving and embodied AI.

Method: Develop Relative Ray Encoding for camera geometry consistency and integrate a new Absolute Orientation Encoding for pitch and roll control. Combined with a minimal spatial attention adapter, UCPE is integrated into a pretrained video Diffusion Transformer.

Result: UCPE achieves state-of-the-art results in camera-controllable video generation while introducing minimal additional computational cost. A large dataset showcasing diverse camera motions validates its effectiveness.

Conclusion: UCPE provides a robust, geometry-aware camera representation suitable for future applications in video generation and 3D perception tasks with Transformers. Its code will be publicly available for broader adaptation.

Abstract: Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.

</details>


### [283] [Squeezed-Eff-Net: Edge-Computed Boost of Tomography Based Brain Tumor Classification leveraging Hybrid Neural Network Architecture](https://arxiv.org/abs/2512.07241)
*Md. Srabon Chowdhury,Syeda Fahmida Tanzim,Sheekar Banerjee,Ishtiak Al Mamoon,AKM Muzahidul Islam*

Main category: cs.CV

TL;DR: The paper proposes a hybrid deep learning model for brain tumor classification using MRI data, achieving high accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Brain tumors require accurate diagnosis, which is challenging due to the complexity and time-consuming nature of tumor delineation even with MRI scans.

Method: A hybrid deep learning model combining SqueezeNet v1, EfficientNet-B0, and handcrafted radiomic features was developed and trained on the Nickparvar Brain Tumor MRI dataset.

Result: The model achieved a testing accuracy of 98.93%, reaching 99.08% with Test Time Augmentation (TTA), demonstrating efficiency and diagnostic reliability.

Conclusion: This framework balances computational efficiency with diagnostic accuracy and shows the potential for clinical application in MRI-based tumor classification.

Abstract: Brain tumors are one of the most common and dangerous neurological diseases which require a timely and correct diagnosis to provide the right treatment procedures. Even with the promotion of magnetic resonance imaging (MRI), the process of tumor delineation is difficult and time-consuming, which is prone to inter-observer error. In order to overcome these limitations, this work proposes a hybrid deep learning model based on SqueezeNet v1 which is a lightweight model, and EfficientNet-B0, which is a high-performing model, and is enhanced with handcrafted radiomic descriptors, including Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), Gabor filters and Wavelet transforms. The framework was trained and tested only on publicly available Nickparvar Brain Tumor MRI dataset, which consisted of 7,023 contrast-enhanced T1-weighted axial MRI slices which were categorized into four groups: glioma, meningioma, pituitary tumor, and no tumor. The testing accuracy of the model was 98.93% that reached a level of 99.08% with Test Time Augmentation (TTA) showing great generalization and power. The proposed hybrid network offers a compromise between computation efficiency and diagnostic accuracy compared to current deep learning structures and only has to be trained using fewer than 2.1 million parameters and less than 1.2 GFLOPs. The handcrafted feature addition allowed greater sensitivity in texture and the EfficientNet-B0 backbone represented intricate hierarchical features. The resulting model has almost clinical reliability in automated MRI-based classification of tumors highlighting its possibility of use in clinical decision-support systems.

</details>


### [284] [Zero-Shot Textual Explanations via Translating Decision-Critical Features](https://arxiv.org/abs/2512.07245)
*Toshinori Yamauchi,Hiroshi Kera,Kazuhiko Kawamoto*

Main category: cs.CV

TL;DR: TEXTER is a method for generating natural language explanations of classifier decisions by isolating decision-critical features before alignment with textual descriptions.


<details>
  <summary>Details</summary>
Motivation: To improve interpretability of image classifier decisions by providing faithful and classifier-specific textual explanations.

Method: TEXTER identifies neurons contributing to the prediction and emphasizes their features. It aligns these features into the CLIP feature space and utilizes a sparse autoencoder for better interpretability.

Result: Experiments demonstrate TEXTER produces more accurate and interpretable explanations compared to existing methods.

Conclusion: TEXTER effectively enhances transparency by providing faithful textual explanations for image classifier decisions, bridging the gap between reasoning and interpretation.

Abstract: Textual explanations make image classifier decisions transparent by describing the prediction rationale in natural language. Large vision-language models can generate captions but are designed for general visual understanding, not classifier-specific reasoning. Existing zero-shot explanation methods align global image features with language, producing descriptions of what is visible rather than what drives the prediction. We propose TEXTER, which overcomes this limitation by isolating decision-critical features before alignment. TEXTER identifies the neurons contributing to the prediction and emphasizes the features encoded in those neurons -- i.e., the decision-critical features. It then maps these emphasized features into the CLIP feature space to retrieve textual explanations that reflect the model's reasoning. A sparse autoencoder further improves interpretability, particularly for Transformer architectures. Extensive experiments show that TEXTER generates more faithful and interpretable explanations than existing methods. The code will be publicly released.

</details>


### [285] [AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing](https://arxiv.org/abs/2512.07247)
*Ziming Hong,Tianyu Huang,Runnan Chen,Shanshan Ye,Mingming Gong,Bo Han,Tongliang Liu*

Main category: cs.CV

TL;DR: The paper introduces AdLift, a safeguard mechanism for 3D Gaussian Splatting (3DGS) to prevent unauthorized or malicious editing by leveraging adversarial perturbations optimized via a novel approach.


<details>
  <summary>Details</summary>
Motivation: To address vulnerabilities in 3DGS assets, which are exposed to unauthorized editing due to advances in instruction-driven editing techniques.

Method: AdLift introduces adversarial perturbations lifted from 2D to 3D, optimized using Lifted PGD (Projected Gradient Descent) to balance invisibility and protection across different views and dimensions.

Result: AdLift successfully provides consistent protection against state-of-the-art 2D and 3DGS editing techniques as demonstrated through qualitative and quantitative evaluations.

Conclusion: AdLift ensures security of 3DGS assets by offering cross-view and dimension generalizable protection while maintaining imperceptibility of adversarial perturbations.

Abstract: Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.

</details>


### [286] [See More, Change Less: Anatomy-Aware Diffusion for Contrast Enhancement](https://arxiv.org/abs/2512.07251)
*Junqi Liu,Zejun Wu,Pedro R. A. S. Bassi,Xinze Zhou,Wenxuan Li,Ibrahim E. Hamamci,Sezgin Er,Tianyu Lin,Yi Luo,Szymon Płotka,Bjoern Menze,Daguang Xu,Kai Ding,Kang Wang,Yang Yang,Yucheng Tang,Alan L. Yuille,Zongwei Zhou*

Main category: cs.CV

TL;DR: SMILE is an anatomy-aware diffusion model that enhances medical images while preserving critical details, significantly improving detection and contrast accuracy.


<details>
  <summary>Details</summary>
Motivation: Current medical imaging models often over-edit, leading to distortions and missed diagnoses due to a lack of understanding of anatomy and contrast dynamics.

Method: Proposed a model, SMILE, with structure-aware supervision, registration-free learning, and unified inference to enhance only clinically significant regions.

Result: SMILE exhibited superior image quality (higher SSIM, PSNR, and FID) and improved cancer detection, with a 10% increase in F1 score on non-contrast CT scans.

Conclusion: SMILE is a clinically relevant advancement that provides anatomically accurate and diagnostically meaningful medical imaging improvements.

Abstract: Image enhancement improves visual quality and helps reveal details that are hard to see in the original image. In medical imaging, it can support clinical decision-making, but current models often over-edit. This can distort organs, create false findings, and miss small tumors because these models do not understand anatomy or contrast dynamics. We propose SMILE, an anatomy-aware diffusion model that learns how organs are shaped and how they take up contrast. It enhances only clinically relevant regions while leaving all other areas unchanged. SMILE introduces three key ideas: (1) structure-aware supervision that follows true organ boundaries and contrast patterns; (2) registration-free learning that works directly with unaligned multi-phase CT scans; (3) unified inference that provides fast and consistent enhancement across all contrast phases. Across six external datasets, SMILE outperforms existing methods in image quality (14.2% higher SSIM, 20.6% higher PSNR, 50% better FID) and in clinical usefulness by producing anatomically accurate and diagnostically meaningful images. SMILE also improves cancer detection from non-contrast CT, raising the F1 score by up to 10 percent.

</details>


### [287] [DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement](https://arxiv.org/abs/2512.07253)
*Handing Xu,Zhenguo Nie,Tairan Peng,Huimin Pan,Xin-Jun Liu*

Main category: cs.CV

TL;DR: The paper introduces a real-time framework for enhancing endoscopic video quality using degradation-aware modeling with contrastive learning and fusion mechanisms.


<details>
  <summary>Details</summary>
Motivation: Intraoperative endoscopic videos often suffer from quality degradation due to factors such as uneven illumination and motion blur, essential to address for surgical safety and precision.

Method: A degradation-aware framework using contrastive learning to extract degradation representations, which are fused with input data to guide enhancement. A cycle-consistency constraint improves robustness.

Result: The proposed framework outperforms state-of-the-art methods in balancing performance and efficiency in real-time endoscopic video enhancement.

Conclusion: Degradation-aware modeling and propagating degradation representation enable effective and efficient enhancement of endoscopic videos, making this approach promising for clinical use.

Abstract: Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.

</details>


### [288] [A graph generation pipeline for critical infrastructures based on heuristics, images and depth data](https://arxiv.org/abs/2512.07269)
*Mike Diessner,Yannick Tarant*

Main category: cs.CV

TL;DR: This paper proposes a cost-effective method for generating virtual models of critical infrastructures using photogrammetry and deep learning.


<details>
  <summary>Details</summary>
Motivation: To develop a more affordable and accessible method for creating virtual representations of critical infrastructures, overcoming the limitations of expensive and specialized laser scanners.

Method: The authors used a photogrammetry-based graph generation pipeline, incorporating deep learning for object detection and instance segmentation, combined with user-defined heuristics for determining object relationships.

Result: Tests on two hydraulic systems showed the method produces accurate graphs comparable to ground truth and is adaptable to specific applications.

Conclusion: The presented method offers cost-effectiveness, flexibility, and transparency, making it suitable for resilience planning and high-stakes decision-making for critical infrastructures.

Abstract: Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and depth data generated by a stereo camera. This more cost-effective approach uses deep learning for object detection and instance segmentation of the objects, and employs user-defined heuristics or rules to infer their relations. Results of two hydraulic systems show that this strategy can produce graphs close to the ground truth while its flexibility allows the method to be tailored to specific applications and its transparency qualifies it to be used in the high stakes decision-making that is required for critical infrastructures.

</details>


### [289] [RVLF: A Reinforcing Vision-Language Framework for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2512.07273)
*Zhi Rao,Yucheng Zhou,Benjia Zhou,Yiqing Huang,Sergio Escalera,Jun Wan*

Main category: cs.CV

TL;DR: This paper proposes a framework called RVLF that enhances gloss-free sign language translation by improving sign representation and addressing sentence-level semantic misalignment, achieving significant BLEU-4 score improvements on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Improving the quality of gloss-free sign language translation, which is limited by inadequate sign representation and sentence-level semantic misalignment in existing methods.

Method: The proposed RVLF framework uses a three-stage process, enhancing semantic representation with skeleton-based motion cues and DINOv2 features, followed by instruction tuning and reinforcement learning with GRPO for semantic alignment optimization.

Result: RVLF achieves significant performance improvements, with increased BLEU-4 scores on CSL-Daily, PHOENIX-2014T, How2Sign, and OpenASL datasets by +5.1, +1.11, +1.4, and +1.61, respectively.

Conclusion: The integration of GRPO-based optimization into sign language translation significantly improves translation quality and semantic alignment, demonstrating the effectiveness of the RVLF approach without requiring large-scale pre-training.

Abstract: Gloss-free sign language translation (SLT) is hindered by two key challenges: **inadequate sign representation** that fails to capture nuanced visual cues, and **sentence-level semantic misalignment** in current LLM-based methods, which limits translation quality. To address these issues, we propose a three-stage **r**einforcing **v**ision-**l**anguage **f**ramework (**RVLF**). We build a large vision-language model (LVLM) specifically designed for sign language, and then combine it with reinforcement learning (RL) to adaptively enhance translation performance. First, for a sufficient representation of sign language, RVLF introduces an effective semantic representation learning mechanism that fuses skeleton-based motion cues with semantically rich visual features extracted via DINOv2, followed by instruction tuning to obtain a strong SLT-SFT baseline. Then, to improve sentence-level semantic misalignment, we introduce a GRPO-based optimization strategy that fine-tunes the SLT-SFT model with a reward function combining translation fidelity (BLEU) and sentence completeness (ROUGE), yielding the optimized model termed SLT-GRPO. Our conceptually simple framework yields substantial gains under the gloss-free SLT setting without pre-training on any external large-scale sign language datasets, improving BLEU-4 scores by +5.1, +1.11, +1.4, and +1.61 on the CSL-Daily, PHOENIX-2014T, How2Sign, and OpenASL datasets, respectively. To the best of our knowledge, this is the first work to incorporate GRPO into SLT. Extensive experiments and ablation studies validate the effectiveness of GRPO-based optimization in enhancing both translation quality and semantic consistency.

</details>


### [290] [Effective Attention-Guided Multi-Scale Medical Network for Skin Lesion Segmentation](https://arxiv.org/abs/2512.07275)
*Siyu Wang,Hua Wang,Huiyu Li,Fan Zhang*

Main category: cs.CV

TL;DR: This paper introduces a novel encoder-decoder network architecture for improved skin lesion segmentation, addressing issues of irregular shapes and low contrast.


<details>
  <summary>Details</summary>
Motivation: Accurate skin lesion segmentation remains challenging due to irregular shapes and low contrast of lesions, necessitating advanced techniques for precise detection and diagnosis.

Method: The paper proposes an innovative encoder-decoder network utilizing multi-scale residual structures, combined with MRCF, CMAM modules, and an External Attention Bridge to enhance segmentation accuracy and information retention.

Result: Extensive experiments on various datasets confirm that the proposed model outperforms existing methods in segmentation accuracy and robustness.

Conclusion: The model effectively addresses challenges in skin lesion segmentation, offering significant improvements over current approaches and showcasing its potential for real-world application in healthcare.

Abstract: In the field of healthcare, precise skin lesion segmentation is crucial for the early detection and accurate diagnosis of skin diseases. Despite significant advances in deep learning for image processing, existing methods have yet to effectively address the challenges of irregular lesion shapes and low contrast. To address these issues, this paper proposes an innovative encoder-decoder network architecture based on multi-scale residual structures, capable of extracting rich feature information from different receptive fields to effectively identify lesion areas. By introducing a Multi-Resolution Multi-Channel Fusion (MRCF) module, our method captures cross-scale features, enhancing the clarity and accuracy of the extracted information. Furthermore, we propose a Cross-Mix Attention Module (CMAM), which redefines the attention scope and dynamically calculates weights across multiple contexts, thus improving the flexibility and depth of feature capture and enabling deeper exploration of subtle features. To overcome the information loss caused by skip connections in traditional U-Net, an External Attention Bridge (EAB) is introduced, facilitating the effective utilization of information in the decoder and compensating for the loss during upsampling. Extensive experimental evaluations on several skin lesion segmentation datasets demonstrate that the proposed model significantly outperforms existing transformer and convolutional neural network-based models, showcasing exceptional segmentation accuracy and robustness.

</details>


### [291] [Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery](https://arxiv.org/abs/2512.07276)
*Mai Tsujimoto,Junjue Wang,Weihao Xuan,Naoto Yokoya*

Main category: cs.CV

TL;DR: Geo3DVQA is a benchmark for vision-language models (VLMs) focusing on 3D geospatial reasoning using RGB imagery, emphasizing realistic scenarios over sensor-based methods.


<details>
  <summary>Details</summary>
Motivation: Current geospatial analysis methodologies rely on costly sensors and struggle to integrate 3D cues, answer diverse queries, and provide interpretable results, motivating the need for a scalable, cost-effective, and accessible framework.

Method: Developed Geo3DVQA, a benchmark with 110k curated question-answer pairs across diverse complexity levels, focusing on height-aware reasoning using only RGB remote sensing imagery.

Result: Testing ten state-of-the-art VLMs on Geo3DVQA highlighted the difficulty of RGB-to-3D reasoning. Domain-specific fine-tuning significantly improved results (49.6% accuracy).

Conclusion: Geo3DVQA showcases limitations of current VLMs in RGB-based 3D reasoning and the promise of domain adaptation, paving the way for holistic and accessible geospatial analysis.

Abstract: Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive benchmark for evaluating vision-language models (VLMs) in height-aware, 3D geospatial reasoning using RGB-only remote sensing imagery. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios that integrate elevation, sky view factors, and land cover patterns. The benchmark encompasses 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. The evaluation of ten state-of-the-art VLMs highlights the difficulty of RGB-to-3D reasoning. GPT-4o and Gemini-2.5-Flash achieved only 28.6% and 33.0% accuracy respectively, while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points). These results reveal both the limitations of current VLMs and the effectiveness of domain adaptation. Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis. The dataset and code will be released upon publication at https://github.com/mm1129/Geo3DVQA.

</details>


### [292] [Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts](https://arxiv.org/abs/2512.07302)
*Mingning Guo,Mengwei Wu,Shaoxian Li,Haifeng Li,Chao Tao*

Main category: cs.CV

TL;DR: The paper introduces AerialVP, a framework that improves task prompts for UAV image perception, resulting in better performance across various conditions.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in UAV image perception that traditional VLM-based methods face, such as target confusion, scale variations, and complex backgrounds, which hinder effective alignment between visual and textual content.

Method: AerialVP enhances UAV image perception task prompts in three steps: analyzing task prompts, selecting suitable tools, and generating enhanced prompts. Additionally, a benchmark called AerialSense is introduced for evaluation across different UAV imagery tasks.

Result: Experimental results show AerialVP significantly improves task prompt guidance, enhancing VLM performance in UAV image perception tasks under diverse conditions.

Conclusion: AerialVP effectively overcomes traditional VLM-based limitations in UAV image perception through enhanced task prompts and is validated by notable performance improvements demonstrated by its benchmark, AerialSense.

Abstract: Existing image perception methods based on VLMs generally follow a paradigm wherein models extract and analyze image content based on user-provided textual task prompts. However, such methods face limitations when applied to UAV imagery, which presents challenges like target confusion, scale variations, and complex backgrounds. These challenges arise because VLMs' understanding of image content depends on the semantic alignment between visual and textual tokens. When the task prompt is simplistic and the image content is complex, achieving effective alignment becomes difficult, limiting the model's ability to focus on task-relevant information. To address this issue, we introduce AerialVP, the first agent framework for task prompt enhancement in UAV image perception. AerialVP proactively extracts multi-dimensional auxiliary information from UAV images to enhance task prompts, overcoming the limitations of traditional VLM-based approaches. Specifically, the enhancement process includes three stages: (1) analyzing the task prompt to identify the task type and enhancement needs, (2) selecting appropriate tools from the tool repository, and (3) generating enhanced task prompts based on the analysis and selected tools. To evaluate AerialVP, we introduce AerialSense, a comprehensive benchmark for UAV image perception that includes Aerial Visual Reasoning, Aerial Visual Question Answering, and Aerial Visual Grounding tasks. AerialSense provides a standardized basis for evaluating model generalization and performance across diverse resolutions, lighting conditions, and both urban and natural scenes. Experimental results demonstrate that AerialVP significantly enhances task prompt guidance, leading to stable and substantial performance improvements in both open-source and proprietary VLMs. Our work will be available at https://github.com/lostwolves/AerialVP.

</details>


### [293] [Reevaluating Automated Wildlife Species Detection: A Reproducibility Study on a Custom Image Dataset](https://arxiv.org/abs/2512.07305)
*Tobias Abraham Haider*

Main category: cs.CV

TL;DR: This paper reevaluates a study on automated detection of European wild mammals using pretrained neural networks, achieving similar accuracy but noting limitations in generalization across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: The study seeks to evaluate the reproducibility and generalizability of a previous experiment using pre-trained neural networks for wildlife camera trap images.

Method: The team reimplements the original experiment from scratch using open resources, a different dataset (900 images spanning 90 species), and minimal preprocessing.

Result: Achieved an overall classification accuracy of 62% (compared to 71% in the original study) with varying per-class performance (macro F1 score of 0.28), revealing challenges due to differences in label alignment.

Conclusion: Pretrained neural networks are useful for baseline wildlife species identification, but require species-specific adaptations or transfer learning for consistent accuracy.

Abstract: This study revisits the findings of Carl et al., who evaluated the pre-trained Google Inception-ResNet-v2 model for automated detection of European wild mammal species in camera trap images. To assess the reproducibility and generalizability of their approach, we reimplemented the experiment from scratch using openly available resources and a different dataset consisting of 900 images spanning 90 species. After minimal preprocessing, we obtained an overall classification accuracy of 62%, closely aligning with the 71% reported in the original work despite differences in datasets. As in the original study, per-class performance varied substantially, as indicated by a macro F1 score of 0.28,highlighting limitations in generalization when labels do not align directly with ImageNet classes. Our results confirm that pretrained convolutional neural networks can provide a practical baseline for wildlife species identification but also reinforce the need for species-specific adaptation or transfer learning to achieve consistent, high-quality predictions.

</details>


### [294] [ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation](https://arxiv.org/abs/2512.07328)
*Ziyang Mai,Yu-Wing Tai*

Main category: cs.CV

TL;DR: This paper proposes ContextAnyone, a framework solving character-consistency issues in text-to-video generation using reference images and novel techniques for improved identity preservation.


<details>
  <summary>Details</summary>
Motivation: Achieve character-consistent video generation from text and reference images, addressing current limitations in contextual identity preservation like hairstyle, outfit, and body shape.

Method: Context-aware diffusion framework combining joint reconstruction of reference images and video generation along with novel modules such as Emphasize-Attention, Gap-RoPE positional embedding, and dual-guidance loss.

Result: ContextAnyone surpasses existing methods in identity consistency, visual coherence, and quality of character videos under diverse motions and settings.

Conclusion: The proposed framework effectively maintains contextual and identity fidelity in text-to-video generation, providing enhanced coherence and broader applicability.

Abstract: Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.

</details>


### [295] [The Inductive Bottleneck: Data-Driven Emergence of Representational Sparsity in Vision Transformers](https://arxiv.org/abs/2512.07331)
*Kanishk Awadhiya*

Main category: cs.CV

TL;DR: The paper identifies a U-shaped entropy profile in Vision Transformers, suggesting an adaptive 'Inductive Bottleneck' based on dataset semantics.


<details>
  <summary>Details</summary>
Motivation: Explore why Vision Transformers exhibit a U-shaped entropy profile and if it correlates with dataset semantics.

Method: Investigated layer-wise Effective Encoding Dimension (EED) of DINO-trained Vision Transformers across datasets of varying complexity.

Result: Found that the depth of the bottleneck correlates with task abstraction; texture datasets retain information throughout while object-centric datasets showcase bottleneck formation.

Conclusion: 'Inductive Bottleneck' in Vision Transformers is a data-driven adaptation for better semantic abstraction.

Abstract: Vision Transformers (ViTs) lack the hierarchical inductive biases inherent to Convolutional Neural Networks (CNNs), theoretically allowing them to maintain high-dimensional representations throughout all layers. However, recent observations suggest ViTs often spontaneously manifest a "U-shaped" entropy profile-compressing information in middle layers before expanding it for the final classification. In this work, we demonstrate that this "Inductive Bottleneck" is not an architectural artifact, but a data-dependent adaptation. By analyzing the layer-wise Effective Encoding Dimension (EED) of DINO-trained ViTs across datasets of varying compositional complexity (UC Merced, Tiny ImageNet, and CIFAR-100), we show that the depth of the bottleneck correlates strongly with the semantic abstraction required by the task. We find that while texture-heavy datasets preserve high-rank representations throughout, object-centric datasets drive the network to dampen high-frequency information in middle layers, effectively "learning" a bottleneck to isolate semantic features.

</details>


### [296] [Generalized Referring Expression Segmentation on Aerial Photos](https://arxiv.org/abs/2512.07338)
*Luís Marnoto,Alexandre Bernardino,Bruno Martins*

Main category: cs.CV

TL;DR: The paper introduces Aerial-D, a large-scale referring expression segmentation dataset specifically designed for aerial imagery. The dataset contains 37,288 images with rich referring expressions and spans diverse object classes, overcoming significant challenges in aerial image analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need for precise visual localization in aerial imagery datasets, which face unique challenges such as varying spatial resolutions, inconsistent color use, dense object scenes, small targets, and partial occlusions.

Method: The authors created Aerial-D using an automated pipeline that combines a rule-based referring expression generation process with large language model (LLM) enhancements. Filters simulated historical imaging conditions, and the RSRefSeg architecture was adopted for unified instance and semantic segmentation.

Result: Training models on Aerial-D, combined with prior aerial datasets, achieved strong segmentation performance in modern and historical images. The models maintained high accuracy under visual degradations typical of archival photography.

Conclusion: Aerial-D provides a robust dataset and trained models for advancing referring expression segmentation in aerial imagery. The dataset addresses unique challenges in this domain and offers accessible resources for further research. Public access ensures reproducibility and future exploration.

Abstract: Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions. Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixels, and scenes contain very high object densities and objects with partial occlusions. This work presents Aerial-D, a new large-scale referring expression segmentation dataset for aerial imagery, comprising 37,288 images with 1,522,523 referring expressions that cover 259,709 annotated targets, spanning across individual object instances, groups of instances, and semantic regions covering 21 distinct classes that range from vehicles and infrastructure to land coverage types. The dataset was constructed through a fully automatic pipeline that combines systematic rule-based expression generation with a Large Language Model (LLM) enhancement procedure that enriched both the linguistic variety and the focus on visual details within the referring expressions. Filters were additionally used to simulate historic imaging conditions for each scene. We adopted the RSRefSeg architecture, and trained models on Aerial-D together with prior aerial datasets, yielding unified instance and semantic segmentation from text for both modern and historical images. Results show that the combined training achieves competitive performance on contemporary benchmarks, while maintaining strong accuracy under monochrome, sepia, and grainy degradations that appear in archival aerial photography. The dataset, trained models, and complete software pipeline are publicly available at https://luispl77.github.io/aerial-d .

</details>


### [297] [Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting](https://arxiv.org/abs/2512.07345)
*Shilong Jin,Haoran Duan,Litao Hua,Wentao Huang,Yuan Zhou*

Main category: cs.CV

TL;DR: This paper addresses the limitations of Text-to-Image (T2I) diffusion models in 3D tasks (e.g., generation and editing) due to view inconsistency issues. It proposes a TD-Attn framework to overcome this.


<details>
  <summary>Details</summary>
Motivation: Despite the rising interest in using T2I diffusion models for 3D tasks (without heavy 3D training), they suffer from prior view bias, causing inconsistent appearances in multi-views of objects.

Method: The authors analyze prior view bias mathematically and design the TD-Attn framework. It includes 3D-AAG, which constructs view-consistent attention, and HAM, which uses a tree-guided approach to modulate critical attention layers, ensuring consistency.

Result: TD-Attn significantly enhances multi-view consistency in 3D-related applications and achieves controllable, precise 3D object editing.

Conclusion: This framework is a promising universal enhancement method for T2I diffusion models, addressing critical limitations, particularly multi-view inconsistency in 3D tasks.

Abstract: Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.

</details>


### [298] [MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition](https://arxiv.org/abs/2512.07348)
*Xinyu Wei,Kangrui Cen,Hongyang Wei,Zhen Guo,Bairui Li,Zeqing Wang,Jinrui Zhang,Lei Zhang*

Main category: cs.CV

TL;DR: The paper focuses on Multi-Image Composition (MICo) by building a large-scale dataset (MICo-150K), benchmark (MICo-Bench), and introducing a new evaluation metric (Weighted-Ref-VIEScore).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of synthesizing coherent images from multiple references and the lack of high-quality data for MICo.

Method: They developed MICo-150K, a dataset of synthetic and real-world composite images, and constructed evaluation benchmarks like MICo-Bench with new metrics. Models were fine-tuned and tested on these resources.

Result: The dataset and benchmark enabled models without MICo capabilities to perform better, while models with existing MICo skills were further enhanced. The baseline model, Qwen-MICo, demonstrated strong performance in multi-image composition.

Conclusion: The resources (dataset, benchmark, and new metric) serve as significant advancements for research in MICo, enabling better model performance and further exploration in the field.

Abstract: In controllable image generation, synthesizing coherent and consistent images from multiple reference inputs, i.e., Multi-Image Composition (MICo), remains a challenging problem, partly hindered by the lack of high-quality training data. To bridge this gap, we conduct a systematic study of MICo, categorizing it into 7 representative tasks and curate a large-scale collection of high-quality source images and construct diverse MICo prompts. Leveraging powerful proprietary models, we synthesize a rich amount of balanced composite images, followed by human-in-the-loop filtering and refinement, resulting in MICo-150K, a comprehensive dataset for MICo with identity consistency. We further build a Decomposition-and-Recomposition (De&Re) subset, where 11K real-world complex images are decomposed into components and recomposed, enabling both real and synthetic compositions. To enable comprehensive evaluation, we construct MICo-Bench with 100 cases per task and 300 challenging De&Re cases, and further introduce a new metric, Weighted-Ref-VIEScore, specifically tailored for MICo evaluation. Finally, we fine-tune multiple models on MICo-150K and evaluate them on MICo-Bench. The results show that MICo-150K effectively equips models without MICo capability and further enhances those with existing skills. Notably, our baseline model, Qwen-MICo, fine-tuned from Qwen-Image-Edit, matches Qwen-Image-2509 in 3-image composition while supporting arbitrary multi-image inputs beyond the latter's limitation. Our dataset, benchmark, and baseline collectively offer valuable resources for further research on Multi-Image Composition.

</details>


### [299] [DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection](https://arxiv.org/abs/2512.07351)
*Sayeem Been Zaman,Wasimul Karim,Arefin Ittesafun Abian,Reem E. Mohamed,Md Rafiqul Islam,Asif Karim,Sami Azam*

Main category: cs.CV

TL;DR: The paper introduces 'DeepAgent,' a multi-agent framework for detecting deepfakes by combining visual and audio analysis, achieving high accuracy across various datasets.


<details>
  <summary>Details</summary>
Motivation: To address the growing challenge of deepfake detection, which standard single-model approaches struggle with due to vulnerabilities like noise, modality mismatch, and manipulation.

Method: The DeepAgent framework includes two agents: one detects deepfake manipulation using AlexNet-based CNN (Agent-1), and the second identifies audio-visual inconsistencies through acoustic features and OCR-transcribed image sequences (Agent-2). A Random Forest meta-classifier fuses their decisions for enhanced performance.

Result: The system demonstrates notable performance with Agent-1 achieving 94.35% accuracy, Agent-2 achieving 93.69%, and a combined meta-classifier achieving 97.49% accuracy in cross-dataset testing.

Conclusion: The paper highlights the superiority of a multi-agent approach in deepfake detection, showing improved robustness through modality fusion and applicability across diverse datasets.

Abstract: The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.

</details>


### [300] [Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2512.07360)
*Qiming Huang,Hao Ai,Jianbo Jiao*

Main category: cs.CV

TL;DR: This study identifies and overcomes the limitations of CLIP models in open-vocabulary semantic segmentation by introducing a structure-aware rectification method that improves local region discrimination.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the limitations of vision-language models like CLIP which, due to their global semantic focus during pre-training, struggle with fine-grained visual and textual association, leading to noisy and inconsistent segmentation predictions.

Method: To improve fine-grained segmentation, the authors propose a structure-aware feature rectification method that utilizes region adjacency graphs (RAGs) built upon low-level image features like color and texture, enabling local structural relationships and better refining CLIP features.

Result: The method significantly suppresses segmentation noise, advances region-level consistency, and demonstrates strong performance across various open-vocabulary segmentation benchmarks.

Conclusion: By incorporating instance-specific priors for feature rectification using low-level image structures, this approach effectively enhances the local accuracy and overall performance of open-vocabulary segmentation frameworks.

Abstract: Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.

</details>


### [301] [Enhancing Small Object Detection with YOLO: A Novel Framework for Improved Accuracy and Efficiency](https://arxiv.org/abs/2512.07379)
*Mahila Moghadami,Mohammad Ali Keyvanrad,Melika Sabaghian*

Main category: cs.CV

TL;DR: The paper develops a novel framework, enhancing small object detection in aerial images by refining cropping techniques, integrating advanced modules, and surpassing existing methods.


<details>
  <summary>Details</summary>
Motivation: Given the growing importance of aerial imagery in critical applications, there is a need for robust frameworks to detect small objects effectively, which current approaches struggle to achieve.

Method: The method involves adopting and modifying SW-YOLO with refined cropping dimensions and overlap, introducing architectural modifications in feature extraction (CBAM), and enhancing the network's head for accuracy improvements.

Result: The proposed model achieves an mAP .5.5 accuracy of 61.2 on the VisDrone2019 dataset, outperforming baseline YOLOv5L (35.5 mAP) and CZDet (58.36 mAP).

Conclusion: The research demonstrates substantial improvements in small object detection accuracy using novel architectural modifications and cropping techniques, creating a robust and efficient framework.

Abstract: This paper investigates and develops methods for detecting small objects in large-scale aerial images. Current approaches for detecting small objects in aerial images often involve image cropping and modifications to detector network architectures. Techniques such as sliding window cropping and architectural enhancements, including higher-resolution feature maps and attention mechanisms, are commonly employed. Given the growing importance of aerial imagery in various critical and industrial applications, the need for robust frameworks for small object detection becomes imperative. To address this need, we adopted the base SW-YOLO approach to enhance speed and accuracy in small object detection by refining cropping dimensions and overlap in sliding window usage and subsequently enhanced it through architectural modifications. we propose a novel model by modifying the base model architecture, including advanced feature extraction modules in the neck for feature map enhancement, integrating CBAM in the backbone to preserve spatial and channel information, and introducing a new head to boost small object detection accuracy. Finally, we compared our method with SAHI, one of the most powerful frameworks for processing large-scale images, and CZDet, which is also based on image cropping, achieving significant improvements in accuracy. The proposed model achieves significant accuracy gains on the VisDrone2019 dataset, outperforming baseline YOLOv5L detection by a substantial margin. Specifically, the final proposed model elevates the mAP .5.5 accuracy on the VisDrone2019 dataset from the base accuracy of 35.5 achieved by the YOLOv5L detector to 61.2. Notably, the accuracy of CZDet, which is another classic method applied to this dataset, is 58.36. This research demonstrates a significant improvement, achieving an increase in accuracy from 35.5 to 61.2.

</details>


### [302] [Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects](https://arxiv.org/abs/2512.07381)
*Shuohan Tao,Boyao Zhou,Hanzhang Tu,Yuwang Wang,Yebin Liu*

Main category: cs.CV

TL;DR: The paper introduces Tessellation GS, a method enhancing 3D Gaussian Splatting for better generalization in sparse-view and dynamic scene reconstruction by utilizing structured 2D GS on mesh faces.


<details>
  <summary>Details</summary>
Motivation: To address limitations in traditional 3D Gaussian Splatting, particularly its poor generalization and overfitting to sparse-view and dynamic scenes.

Method: Introduces Tessellation GS that leverages mesh face-anchored 2D Gaussian distributions with adaptive face subdivision and foundation model priors for better dynamic scene reconstruction.

Result: The method reduces LPIPS by 29.1% and Chamfer distance by 49.2% compared to previous state-of-the-art methods on dynamic scene reconstruction tasks.

Conclusion: Tessellation GS significantly improves appearance and mesh reconstruction quality, making it viable for challenging scenarios such as single static camera reconstructions of dynamic objects.

Abstract: 3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.

</details>


### [303] [LogicCBMs: Logic-Enhanced Concept-Based Learning](https://arxiv.org/abs/2512.07383)
*Deepika SN Vemuri,Gautham Bellamkonda,Aditya Pola,Vineeth N Balasubramanian*

Main category: cs.CV

TL;DR: Concept Bottleneck Models (CBMs) are improved using propositional logic, creating a LogicCBM that connects learned concepts with differentiable logic operations, enabling higher model expressivity and better outcomes.


<details>
  <summary>Details</summary>
Motivation: Current CBMs lack flexibility and are restricted to linear combinations of concepts, limiting their expressive capability. There is a need to enhance their ability to model inter-concept relations and increase the expressivity of predictions.

Method: A logic module is integrated into CBMs to link learned concepts through differentiable logic operations, enabling the use of logic operators for deriving predictions while keeping the model learnable end-to-end.

Result: The enhanced LogicCBM demonstrates superior accuracy, effective interventions, and improved interpretability compared to traditional CBMs on benchmark and synthetic datasets.

Conclusion: LogicCBM broadens the expressive scope of CBMs by embedding logical operations, offering improved interpretability and accuracy, and is suited for applications requiring effective inter-concept relationship modeling.

Abstract: Concept Bottleneck Models (CBMs) provide a basis for semantic abstractions within a neural network architecture. Such models have primarily been seen through the lens of interpretability so far, wherein they offer transparency by inferring predictions as a linear combination of semantic concepts. However, a linear combination is inherently limiting. So we propose the enhancement of concept-based learning models through propositional logic. We introduce a logic module that is carefully designed to connect the learned concepts from CBMs through differentiable logic operations, such that our proposed LogicCBM can go beyond simple weighted combinations of concepts to leverage various logical operations to yield the final predictions, while maintaining end-to-end learnability. Composing concepts using a set of logic operators enables the model to capture inter-concept relations, while simultaneously improving the expressivity of the model in terms of logic operations. Our empirical studies on well-known benchmarks and synthetic datasets demonstrate that these models have better accuracy, perform effective interventions and are highly interpretable.

</details>


### [304] [How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline](https://arxiv.org/abs/2512.07385)
*Chunhui Zhang,Li Liu,Zhipeng Zhang,Yong Wang,Hao Wen,Xi Zhou,Shiming Ge,Yanfeng Wang*

Main category: cs.CV

TL;DR: The paper introduces UAV-Anti-UAV, a novel multi-modal visual tracking task where a UAV tracks another UAV, addressing challenges caused by dual-dynamic disturbances. They provide a million-scale annotated dataset and propose the MambaSTS baseline model for enhanced spatial-temporal-semantic learning.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings in existing Anti-UAV research, which mainly focus on videos captured by stationary ground cameras, and to introduce a method for tracking UAVs from a moving UAV under complex conditions.

Method: The authors propose UAV-Anti-UAV with a comprehensive dataset of 1,810 annotated videos. They introduce MambaSTS, using Mamba and Transformer models for semantic and spatial learning, and a state space model for long-term temporal context with token propagation.

Result: Experimental evaluations on the UAV-Anti-UAV dataset show gaps in the performance of 50 modern deep tracking algorithms, emphasizing UAV-Anti-UAV's challenges and validating MambaSTS's effectiveness.

Conclusion: The study highlights the challenges and potential room for improvement in UAV-Anti-UAV tasks. Their novel dataset and baseline method set a foundation for future advancements.

Abstract: Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.

</details>


### [305] [GlimmerNet: A Lightweight Grouped Dilated Depthwise Convolutions for UAV-Based Emergency Monitoring](https://arxiv.org/abs/2512.07391)
*Đorđe Nedeljković*

Main category: cs.CV

TL;DR: The paper introduces GlimmerNet, a lightweight CNN designed for efficient edge and mobile vision tasks, achieving state-of-the-art performance with minimal computational costs.


<details>
  <summary>Details</summary>
Motivation: The paper aims to reduce computational overhead in CNNs for edge and mobile vision tasks while maintaining or improving performance, addressing inefficiencies in existing methods such as Vision Transformers.

Method: The authors propose GlimmerNet, which utilizes Grouped Dilated Depthwise Convolutions for multi-scale feature extraction and a novel Aggregator module for efficient feature recombination, minimizing parameters and FLOPs.

Result: GlimmerNet delivers a weighted F1-score of 0.966 on the AIDERv2 dataset, with only 31K parameters and 29% fewer FLOPs compared to the recent baseline, establishing notable efficiency and accuracy.

Conclusion: The study establishes GlimmerNet as an advanced solution for emergency monitoring on resource-constrained UAV platforms, setting a new benchmark for accuracy and efficiency balance in convolutional networks.

Abstract: Convolutional Neural Networks (CNNs) have proven highly effective for edge and mobile vision tasks due to their computational efficiency. While many recent works seek to enhance CNNs with global contextual understanding via self-attention-based Vision Transformers, these approaches often introduce significant computational overhead. In this work, we demonstrate that it is possible to retain strong global perception without relying on computationally expensive components. We present GlimmerNet, an ultra-lightweight convolutional network built on the principle of separating receptive field diversity from feature recombination. GlimmerNet introduces Grouped Dilated Depthwise Convolutions(GDBlocks), which partition channels into groups with distinct dilation rates, enabling multi-scale feature extraction at no additional parameter cost. To fuse these features efficiently, we design a novel Aggregator module that recombines cross-group representations using grouped pointwise convolution, significantly lowering parameter overhead. With just 31K parameters and 29% fewer FLOPs than the most recent baseline, GlimmerNet achieves a new state-of-the-art weighted F1-score of 0.966 on the UAV-focused AIDERv2 dataset. These results establish a new accuracy-efficiency trade-off frontier for real-time emergency monitoring on resource-constrained UAV platforms. Our implementation is publicly available at https://github.com/djordjened92/gdd-cnn.

</details>


### [306] [Reconstructing Objects along Hand Interaction Timelines in Egocentric Video](https://arxiv.org/abs/2512.07394)
*Zhifan Zhu,Siddhant Bansal,Shashank Tripathi,Dima Damen*

Main category: cs.CV

TL;DR: The paper introduces the ROHIT task for reconstructing objects along Hand Interaction Timelines, using a novel framework (COP) for pose constraint modeling and propagation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of reconstructing objects during hand interactions in videos without relying on 3D ground truth, with a focus on stable grasps.

Method: The authors propose a Constrained Optimisation and Propagation (COP) framework to model and propagate object's pose constraints along Hand Interaction Timelines.

Result: Evaluated on HOT3D and EPIC-Kitchens datasets, COP improves object reconstruction performance significantly, with improvements of 6.2-11.3% for stable grasp reconstructions and up to 24.5% for HIT reconstruction.

Conclusion: The paper highlights the success of COP in enhancing object reconstruction accuracy on egocentric video datasets without requiring 3D annotations, focusing on timelines with stable grasps.

Abstract: We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object's perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object's pose along the HIT enabling superior reconstruction using our proposed Constrained Optimisation and Propagation (COP) framework. Importantly, we focus on timelines with stable grasps - i.e. where the hand is stably holding an object, effectively maintaining constant contact during use. This allows us to efficiently annotate, study, and evaluate object reconstruction in videos without 3D ground truth. We evaluate our proposed task, ROHIT, over two egocentric datasets, HOT3D and in-the-wild EPIC-Kitchens. In HOT3D, we curate 1.2K clips of stable grasps. In EPIC-Kitchens, we annotate 2.4K clips of stable grasps including 390 object instances across 9 categories from videos of daily interactions in 141 environments. Without 3D ground truth, we utilise 2D projection error to assess the reconstruction. Quantitatively, COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% with constrained pose propagation.

</details>


### [307] [InterAgent: Physics-based Multi-agent Command Execution via Diffusion on Interaction Graphs](https://arxiv.org/abs/2512.07410)
*Bin Li,Ruichi Zhang,Han Liang,Jingyan Zhang,Juze Zhang,Xin Chen,Lan Xu,Jingyi Yu,Jingya Wang*

Main category: cs.CV

TL;DR: Humanoid multi-agent systems face challenges in emulating human-like social behavior. InterAgent introduces an advanced framework for text-driven, physics-based humanoid multi-agent control, leveraging novel architectures and mechanisms.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of prior work that focused on single-agent behaviors and neglected the dynamics of multi-agent interactions essential for realistic and physically plausible coordination.

Method: The paper introduces InterAgent, featuring an autoregressive diffusion transformer with multi-stream blocks for isolating sensory modes and a novel interaction graph representation with edge-based attention for robust spatial relation modeling.

Result: InterAgent achieves state-of-the-art performance, consistently outperforming baselines in generating coherent and realistic multi-agent interactions from simple text prompts.

Conclusion: InterAgent advances the capability of humanoid agents by offering robust multi-agent coordination, fostering future development and research in realistic social behavior modeling.

Abstract: Humanoid agents are expected to emulate the complex coordination inherent in human social behaviors. However, existing methods are largely confined to single-agent scenarios, overlooking the physically plausible interplay essential for multi-agent interactions. To bridge this gap, we propose InterAgent, the first end-to-end framework for text-driven physics-based multi-agent humanoid control. At its core, we introduce an autoregressive diffusion transformer equipped with multi-stream blocks, which decouples proprioception, exteroception, and action to mitigate cross-modal interference while enabling synergistic coordination. We further propose a novel interaction graph exteroception representation that explicitly captures fine-grained joint-to-joint spatial dependencies to facilitate network learning. Additionally, within it we devise a sparse edge-based attention mechanism that dynamically prunes redundant connections and emphasizes critical inter-agent spatial relations, thereby enhancing the robustness of interaction modeling. Extensive experiments demonstrate that InterAgent consistently outperforms multiple strong baselines, achieving state-of-the-art performance. It enables producing coherent, physically plausible, and semantically faithful multi-agent behaviors from only text prompts. Our code and data will be released to facilitate future research.

</details>


### [308] [Data-driven Exploration of Mobility Interaction Patterns](https://arxiv.org/abs/2512.07415)
*Gabriele Galatolo,Mirco Nanni*

Main category: cs.CV

TL;DR: This paper analyzes human movement behaviors using data-driven methods to improve simulation models in human dynamics.


<details>
  <summary>Details</summary>
Motivation: The research aims to understand individual movement behaviors and their interaction with others, which is essential for accurate modeling in areas like crowd simulation and emergency management.

Method: A data-mining based approach is employed to extract mobility events from data, identify mutual interactions, and analyze complex, persistent patterns within these interactions.

Result: The proposed method is tested on two real case studies involving cars and pedestrians, with evaluations on performance, sensitivity analysis, and sample result interpretations.

Conclusion: Data-driven insights can enhance simulation models by uncovering underlying interaction mechanics in human mobility scenarios.

Abstract: Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.

</details>


### [309] [When normalization hallucinates: unseen risks in AI-powered whole slide image processing](https://arxiv.org/abs/2512.07426)
*Karel Moens,Matthew B. Blaschko,Tinne Tuytelaars,Bart Diricx,Jonas De Vylder,Mustafa Yousif*

Main category: cs.CV

TL;DR: This paper identifies the risks of hallucinations in whole slide image (WSI) normalization using deep learning, presenting a novel detection measure to identify these issues in real-world data.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the overlooked issue of hallucinated content and artifacts during WSI normalization in computational pathology that threaten data accuracy and downstream analysis.

Method: The authors propose a novel image comparison measure to detect hallucinations in WSI normalization outputs and use this metric systematically to evaluate normalization methods on real-world clinical data.

Result: The analysis demonstrates frequent hallucinations in retrained models evaluated on clinical data, highlighting failures and inconsistencies undetected by conventional metrics.

Conclusion: The paper emphasizes the necessity for robust, interpretable normalization methods and stricter evaluation frameworks to ensure reliability in clinical applications.

Abstract: Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to downstream analysis. These hallucinations are nearly impossible to detect visually, and current evaluation practices often overlook them. In this work, we demonstrate that the risk of hallucinations is real and underappreciated. While many methods perform adequately on public datasets, we observe a concerning frequency of hallucinations when these same models are retrained and evaluated on real-world clinical data. To address this, we propose a novel image comparison measure designed to automatically detect hallucinations in normalized outputs. Using this measure, we systematically evaluate several well-cited normalization methods retrained on real-world data, revealing significant inconsistencies and failures that are not captured by conventional metrics. Our findings underscore the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment.

</details>


### [310] [Unified Video Editing with Temporal Reasoner](https://arxiv.org/abs/2512.07469)
*Xiangpeng Yang,Ji Xie,Yiyuan Yang,Yan Huang,Min Xu,Qiang Wu*

Main category: cs.CV

TL;DR: VideoCoF is a novel video editing method addressing precision versus unification trade-offs using a Chain-of-Frames approach, enabling precise edits without user-provided masks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for video editing either rely on expert models with task-specific priors like masks or unified models without explicit spatial cues, leading to issues with precision and instruction-region alignment.

Method: VideoCoF uses a "see, reason, then edit" procedure in its video diffusion model by predicting reasoning tokens as edit-region latents before generating the target video tokens. It also employs a RoPE alignment strategy for motion alignment and duration extrapolation.

Result: VideoCoF achieves state-of-the-art results on VideoCoF-Bench with only 50k video pairs, demonstrating high efficiency and effectiveness.

Conclusion: The study presents VideoCoF as a powerful approach to achieve precise and fine-grained video editing while eliminating the need for user-provided masks, addressing a critical gap in video editing methodologies.

Abstract: Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.

</details>


### [311] [Single-step Diffusion-based Video Coding with Semantic-Temporal Guidance](https://arxiv.org/abs/2512.07480)
*Naifu Xue,Zhaoyang Jia,Jiahao Li,Bin Li,Zihan Zheng,Yuan Zhang,Yan Lu*

Main category: cs.CV

TL;DR: This paper introduces S2VC, a video codec using single-step diffusion to improve low bitrate video quality efficiently.


<details>
  <summary>Details</summary>
Motivation: Traditional and neural video codecs struggle with high perceptual video quality at low bitrates due to limited generation capacity or high sampling complexity.

Method: The authors propose S2VC using a single-step diffusion generator with conditional coding, incorporating Contextual Semantic Guidance and Temporal Consistency Guidance to enhance video reconstruction quality and temporal coherence efficiently.

Result: S2VC achieves state-of-the-art perceptual quality with a 52.73% bitrate savings compared to prior methods.

Conclusion: S2VC demonstrates the potential of single-step diffusion techniques for efficient and high-quality video compression, especially at low bitrates.

Abstract: While traditional and neural video codecs (NVCs) have achieved remarkable rate-distortion performance, improving perceptual quality at low bitrates remains challenging. Some NVCs incorporate perceptual or adversarial objectives but still suffer from artifacts due to limited generation capacity, whereas others leverage pretrained diffusion models to improve quality at the cost of heavy sampling complexity. To overcome these challenges, we propose S2VC, a Single-Step diffusion based Video Codec that integrates a conditional coding framework with an efficient single-step diffusion generator, enabling realistic reconstruction at low bitrates with reduced sampling cost. Recognizing the importance of semantic conditioning in single-step diffusion, we introduce Contextual Semantic Guidance to extract frame-adaptive semantics from buffered features. It replaces text captions with efficient, fine-grained conditioning, thereby improving generation realism. In addition, Temporal Consistency Guidance is incorporated into the diffusion U-Net to enforce temporal coherence across frames and ensure stable generation. Extensive experiments show that S2VC delivers state-of-the-art perceptual quality with an average 52.73% bitrate saving over prior perceptual methods, underscoring the promise of single-step diffusion for efficient, high-quality video compression.

</details>


### [312] [Towards Robust DeepFake Detection under Unstable Face Sequences: Adaptive Sparse Graph Embedding with Order-Free Representation and Explicit Laplacian Spectral Prior](https://arxiv.org/abs/2512.07498)
*Chih-Chung Hsu,Shao-Ning Chen,Chia-Ming Lee,Yi-Fang Wang,Yi-Shiuan Chou*

Main category: cs.CV

TL;DR: DeepFake detection faces challenges under real-world disruptions. LR-GCN addresses these issues using graph-based techniques for robustness.


<details>
  <summary>Details</summary>
Motivation: DeepFake detection is hindered by real-world disruptions such as occlusions, artifacts, and adversarial attacks.

Method: A Laplacian-Regularized Graph Convolutional Network (LR-GCN) using an Order-Free Temporal Graph Embedding (OF-TGE) and graph spectral analysis.

Result: LR-GCN achieves state-of-the-art detection performance under severe disruptions in datasets FF++, Celeb-DFv2, and DFDC.

Conclusion: The proposed LR-GCN enhances robustness and reliability in detecting manipulated videos under challenging conditions.

Abstract: Ensuring the authenticity of video content remains challenging as DeepFake generation becomes increasingly realistic and robust against detection. Most existing detectors implicitly assume temporally consistent and clean facial sequences, an assumption that rarely holds in real-world scenarios where compression artifacts, occlusions, and adversarial attacks destabilize face detection and often lead to invalid or misdetected faces. To address these challenges, we propose a Laplacian-Regularized Graph Convolutional Network (LR-GCN) that robustly detects DeepFakes from noisy or unordered face sequences, while being trained only on clean facial data. Our method constructs an Order-Free Temporal Graph Embedding (OF-TGE) that organizes frame-wise CNN features into an adaptive sparse graph based on semantic affinities. Unlike traditional methods constrained by strict temporal continuity, OF-TGE captures intrinsic feature consistency across frames, making it resilient to shuffled, missing, or heavily corrupted inputs. We further impose a dual-level sparsity mechanism on both graph structure and node features to suppress the influence of invalid faces. Crucially, we introduce an explicit Graph Laplacian Spectral Prior that acts as a high-pass operator in the graph spectral domain, highlighting structural anomalies and forgery artifacts, which are then consolidated by a low-pass GCN aggregation. This sequential design effectively realizes a task-driven spectral band-pass mechanism that suppresses background information and random noise while preserving manipulation cues. Extensive experiments on FF++, Celeb-DFv2, and DFDC demonstrate that LR-GCN achieves state-of-the-art performance and significantly improved robustness under severe global and local disruptions, including missing faces, occlusions, and adversarially perturbed face detections.

</details>


### [313] [MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer](https://arxiv.org/abs/2512.07500)
*Penghui Liu,Jiangshan Wang,Yutong Shen,Shanhui Mo,Chenyang Qi,Yue Ma*

Main category: cs.CV

TL;DR: The study introduces MultiMotion, a framework that improves multi-object video motion transfer in Diffusion Transformer architectures via a focus on motion disentanglement and control.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges of motion entanglement and lack of object-level control in multi-object video motion transfer using Diffusion Transformer architectures.

Method: The authors propose Maskaware Attention Motion Flow (AMF) for disentangling motion features using masks, and RectPC as a high-order predictor-corrector for efficient sampling. Additionally, they create a benchmark dataset for evaluation.

Result: The framework demonstrates precise, semantically aligned, and temporally coherent motion transfer for multiple objects with high quality and scalability.

Conclusion: MultiMotion successfully enables controlled and scalable multi-object motion transfer, overcoming limitations of existing methods within Diffusion Transformer workflows.

Abstract: Multi-object video motion transfer poses significant challenges for Diffusion Transformer (DiT) architectures due to inherent motion entanglement and lack of object-level control. We present MultiMotion, a novel unified framework that overcomes these limitations. Our core innovation is Maskaware Attention Motion Flow (AMF), which utilizes SAM2 masks to explicitly disentangle and control motion features for multiple objects within the DiT pipeline. Furthermore, we introduce RectPC, a high-order predictor-corrector solver for efficient and accurate sampling, particularly beneficial for multi-entity generation. To facilitate rigorous evaluation, we construct the first benchmark dataset specifically for DiT-based multi-object motion transfer. MultiMotion demonstrably achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects, maintaining DiT's high quality and scalability. The code is in the supp.

</details>


### [314] [SJD++: Improved Speculative Jacobi Decoding for Training-free Acceleration of Discrete Auto-regressive Text-to-Image Generation](https://arxiv.org/abs/2512.07503)
*Yao Teng,Zhihuan Jiang,Han Shi,Xian Liu,Xuefei Ning,Guohao Dai,Yu Wang,Zhenguo Li,Xihui Liu*

Main category: cs.CV

TL;DR: The paper introduces Speculative Jacobi Decoding++ (SJD++), a method to speed up autoregressive text-to-image generation by multi-token predictions in parallel, significantly accelerating production without loss of quality.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models generate high-quality images but are hindered by slow inference speeds due to sequential token predictions. This research seeks a method to overcome these decoding inefficiencies.

Method: SJD++ combines multi-token prediction (from Jacobi decoding) with a probabilistic drafting-and-verification process (from speculative sampling) and optimizes generation by reusing verified high-confidence tokens.

Result: Experiments show SJD++ reduces inference latency by 2–3× and step compression by 2–7×, maintaining image quality without degradation.

Conclusion: SJD++ offers an effective solution to accelerate text-to-image autoregressive models, combining speed improvements with maintained output quality, making it practical for faster inference.

Abstract: Large autoregressive models can generate high-quality, high-resolution images but suffer from slow generation speed, because these models require hundreds to thousands of sequential forward passes for next-token prediction during inference. To accelerate autoregressive text-to-image generation, we propose Speculative Jacobi Decoding++ (SJD++), a training-free probabilistic parallel decoding algorithm. Unlike traditional next-token prediction, SJD++ performs multi-token prediction in each forward pass, drastically reducing generation steps. Specifically, it integrates the iterative multi-token prediction mechanism from Jacobi decoding, with the probabilistic drafting-and-verification mechanism from speculative sampling. More importantly, for further acceleration, SJD++ reuses high-confidence draft tokens after each verification phase instead of resampling them all. We conduct extensive experiments on several representative autoregressive text-to-image generation models and demonstrate that SJD++ achieves $2\times$ to $3\times$ inference latency reduction and $2\times$ to $7\times$ step compression, while preserving visual quality with no observable degradation.

</details>


### [315] [ControlVP: Interactive Geometric Refinement of AI-Generated Images with Consistent Vanishing Points](https://arxiv.org/abs/2512.07504)
*Ryota Okumura,Kaede Shiohara,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: The paper introduces ControlVP to address geometric inconsistencies in text-to-image models, particularly improving vanishing points for structural realism.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models produce impressive visuals but struggle with spatial realism, especially vanishing point inconsistencies in architectural scenes.

Method: ControlVP extends pre-trained diffusion models, incorporating structural guidance from building contours and enforcing geometric constraints.

Result: ControlVP improves geometric consistency without compromising visual fidelity, enhancing accuracy for applications like image-to-3D reconstruction.

Conclusion: ControlVP successfully corrects vanishing point errors in text-to-image outputs, with dataset and code available for use.

Abstract: Recent text-to-image models, such as Stable Diffusion, have achieved impressive visual quality, yet they often suffer from geometric inconsistencies that undermine the structural realism of generated scenes. One prominent issue is vanishing point inconsistency, where projections of parallel lines fail to converge correctly in 2D space. This leads to structurally implausible geometry that degrades spatial realism, especially in architectural scenes. We propose ControlVP, a user-guided framework for correcting vanishing point inconsistencies in generated images. Our approach extends a pre-trained diffusion model by incorporating structural guidance derived from building contours. We also introduce geometric constraints that explicitly encourage alignment between image edges and perspective cues. Our method enhances global geometric consistency while maintaining visual fidelity comparable to the baselines. This capability is particularly valuable for applications that require accurate spatial structure, such as image-to-3D reconstruction. The dataset and source code are available at https://github.com/RyotaOkumura/ControlVP .

</details>


### [316] [MeshRipple: Structured Autoregressive Generation of Artist-Meshes](https://arxiv.org/abs/2512.07514)
*Junkai Lin,Hang Long,Huipeng Guo,Jielei Zhang,JiaYi Yang,Tianle Guo,Yang Yang,Jianwen Li,Wenxiao Zhang,Matthias Nießner,Wei Yang*

Main category: cs.CV

TL;DR: The paper introduces MeshRipple, a novel method for improving autoregressive mesh generation by focusing on smooth surface expansion and addressing topology completeness issues.


<details>
  <summary>Details</summary>
Motivation: Existing autoregressive mesh generation methods suffer from fragmented results due to mismatched training and inference strategies, breaking geometric dependencies.

Method: MeshRipple proposes frontier-aware BFS tokenization, expansive prediction for coherent surface growth, and sparse-attention global memory for resolving long-range dependencies.

Result: MeshRipple produces meshes with improved surface fidelity and topological completeness, outperforming recent state-of-the-art techniques.

Conclusion: MeshRipple resolves key limitations of traditional methods, offering a more robust and accurate approach to autoregressive 3D mesh generation.

Abstract: Meshes serve as a primary representation for 3D assets. Autoregressive mesh generators serialize faces into sequences and train on truncated segments with sliding-window inference to cope with memory limits. However, this mismatch breaks long-range geometric dependencies, producing holes and fragmented components. To address this critical limitation, we introduce MeshRipple, which expands a mesh outward from an active generation frontier, akin to a ripple on a surface.MeshRipple rests on three key innovations: a frontier-aware BFS tokenization that aligns the generation order with surface topology; an expansive prediction strategy that maintains coherent, connected surface growth; and a sparse-attention global memory that provides an effectively unbounded receptive field to resolve long-range topological dependencies.This integrated design enables MeshRipple to generate meshes with high surface fidelity and topological completeness, outperforming strong recent baselines.

</details>


### [317] [From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images](https://arxiv.org/abs/2512.07527)
*Fei Yu,Yu Liu,Luyang Tang,Mingchao Sun,Zengye Ge,Rui Bu,Yuchao Jin,Haisen Zhao,He Sun,Yangyan Li,Mu Xu,Wenzheng Chen,Baoquan Chen*

Main category: cs.CV

TL;DR: This paper addresses city-scale 3D reconstruction from sparse satellite images using novel methods for handling extreme viewpoint gaps and degraded textures.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of synthesizing ground-level novel views from satellite images which lack parallax and detailed textures, causing failures in existing 3D reconstruction technologies like NeRF.

Method: The authors use a 2.5D height map modeled as a Z-monotonic signed distance field (SDF) for geometry and employ a generative texture restoration network to enhance image details, combining these with differentiable rendering techniques.

Result: The method effectively reconstructs large-scale urban areas and achieves photorealistic ground views from minimal satellite input, outperforming prior methods in detail and scalability.

Conclusion: The proposed approach produces visually accurate, high-fidelity reconstructions that are suitable for applications such as urban planning and simulation.

Abstract: City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax. This requires inferring nearly $90^\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail.
  To address this problem, we propose two design choices tailored for city structures and satellite inputs. First, we model city geometry as a 2.5D height map, implemented as a Z-monotonic signed distance field (SDF) that matches urban building layouts from top-down viewpoints. This stabilizes geometry optimization under sparse, off-nadir satellite views and yields a watertight mesh with crisp roofs and clean, vertically extruded facades. Second, we paint the mesh appearance from satellite images via differentiable rendering techniques. While the satellite inputs may contain long-range, blurry captures, we further train a generative texture restoration network to enhance the appearance, recovering high-frequency, plausible texture details from degraded inputs.
  Our method's scalability and robustness are demonstrated through extensive experiments on large-scale urban reconstruction. For example, in our teaser figure, we reconstruct a $4\,\mathrm{km}^2$ real-world region from only a few satellite images, achieving state-of-the-art performance in synthesizing photorealistic ground views. The resulting models are not only visually compelling but also serve as high-fidelity, application-ready assets for downstream tasks like urban planning and simulation.

</details>


### [318] [Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation](https://arxiv.org/abs/2512.07568)
*Xuecheng Li,Weikuan Jia,Alisher Kurbonaliev,Qurbonaliev Alisher,Khudzhamkulov Rustam,Ismoilov Shuhratjon,Eshmatov Javhariddin,Yuanjie Zheng*

Main category: cs.CV

TL;DR: The paper introduces DSRSD-Net, a framework for disentangling cross-modal representation learning challenges using residual decomposition and semantic decorrelation constraints.


<details>
  <summary>Details</summary>
Motivation: To address issues of modality dominance, redundant information coupling, and spurious correlations in multimodal representations, which limit generalization and interpretability.

Method: DSRSD-Net employs a dual-stream module for modality-specific and shared factor separation, residual alignment using semantic alignment head, and loss functions for decorrelation and orthogonality to suppress redundancy and feature collapse.

Result: The framework achieves improved generalization in multimodal learning tasks on educational datasets when compared to existing fusion and attention-based methods.

Conclusion: DSRSD-Net effectively enhances interpretability and robustness in multimodal learning by disentangling shared and private latent factors across modalities.

Abstract: Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while naïve fusion strategies entangle modality-shared and modality-specific factors in an uncontrolled manner. This makes it difficult to understand which modality actually drives a prediction and to maintain robustness when some modalities are noisy or missing. To address these challenges, we propose a Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net), a simple yet effective framework that disentangles modality-specific and modality-shared information through residual decomposition and explicit semantic decorrelation constraints. DSRSD-Net introduces: (1) a dual-stream representation learning module that separates intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) a residual semantic alignment head that maps shared factors from different modalities into a common space using a combination of contrastive and regression-style objectives; and (3) a decorrelation and orthogonality loss that regularizes the covariance structure of the shared space while enforcing orthogonality between shared and private streams, thereby suppressing cross-modal redundancy and preventing feature collapse. Experimental results on two large-scale educational benchmarks demonstrate that DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.

</details>


### [319] [All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs](https://arxiv.org/abs/2512.07580)
*Yahong Wang,Juncheng Wu,Zhangkai Ni,Longzhen Yang,Yihang Liu,Chengmei Yang,Ying Wen,Xianfeng Tang,Hui Liu,Yuyin Zhou,Lianghua He*

Main category: cs.CV

TL;DR: The paper analyzes inefficiencies in Vision Large Language Models (VLLMs) caused by their reliance on numerous visual tokens and proposes random deep-layer token pruning to improve efficiency while maintaining strong performance.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency in Vision Large Language Models (VLLMs) caused by their dependence on hundreds of visual tokens, and to improve inference speed without significant performance losses.

Method: The authors performed an information analysis of visual tokens across different layers using a proposed metric that measures changes in model output probabilities upon token removal. They identified trends in information loss across layers, introduced the concept of an "information horizon," and evaluated random pruning methods.

Result: The study found that visual token information diminishes in deeper layers beyond an "information horizon" and is influenced by task complexity and model capacity. Random pruning in deep layers achieves better efficiency and performance balance, with DivPrune achieving state-of-the-art results through random token pruning.

Conclusion: Strategic token pruning is essential to improve the efficiency of VLLMs. Random pruning in deep layers is a competitive solution, delivering state-of-the-art performance while significantly reducing computational costs. This approach opens new avenues for efficient VLLM adaptation.

Abstract: Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by "vanishing token information", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as "information horizon", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon.

</details>


### [320] [LongCat-Image Technical Report](https://arxiv.org/abs/2512.07584)
*Meituan LongCat Team,Hanghang Ma,Haoxian Tan,Jiale Huang,Junqiang Wu,Jun-Yan He,Lishuai Gao,Songlin Xiao,Xiaoming Wei,Xiaoqi Ma,Xunliang Cai,Yayong Guan,Jie Hu*

Main category: cs.CV

TL;DR: LongCat-Image is an open-source, bilingual Chinese-English image generation model offering state-of-the-art text-rendering, photorealism, and efficiency, while facilitating community access through a comprehensive ecosystem.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in multilingual text rendering, photorealism, efficiency, and accessibility found in current image generation models.

Method: Implemented optimized data curation strategies across training phases (pre-training, mid-training, fine-tuning), coordinated reward models, and utilized a compact 6B parameter architecture for efficiency.

Result: Achieved state-of-the-art performance in text rendering, especially for Chinese characters, high photorealism, low VRAM usage, fast inference, and superior image editing abilities.

Conclusion: Open-sourced a powerful, efficient, and accessible model with a comprehensive ecosystem, offering significant value for developers and researchers in visual content creation.

Abstract: We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.

</details>


### [321] [Robust Variational Model Based Tailored UNet: Leveraging Edge Detector and Mean Curvature for Improved Image Segmentation](https://arxiv.org/abs/2512.07590)
*Kaili Qi,Zhongyi Huang,Wenli Yang*

Main category: cs.CV

TL;DR: This paper introduces a robust hybrid framework, VM_TUNet, for segmenting noisy images by combining variational methods with deep learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the segmentation of noisy images with blurred or fragmented boundaries by integrating physical priors from variational methods with deep learning capabilities.

Method: The method uses a hybrid approach combining variational PDEs, incorporating an edge detector and a mean curvature term into a tailored Cahn-Hilliard equation, with deep learning in a modified UNet. The architecture includes F and T modules for preprocessing and stable computations, respectively.

Result: Extensive experiments on three benchmark datasets show the proposed approach balances performance and computational efficiency, delivering competitive quantitative results and better visual quality than CNN-based models, matching transformer-based methods with lower costs.

Conclusion: VM_TUNet successfully integrates variational PDEs and deep learning, offering interpretable, efficient, and high-quality segmentation for noisy images.

Abstract: To address the challenge of segmenting noisy images with blurred or fragmented boundaries, this paper presents a robust version of Variational Model Based Tailored UNet (VM_TUNet), a hybrid framework that integrates variational methods with deep learning. The proposed approach incorporates physical priors, an edge detector and a mean curvature term, into a modified Cahn-Hilliard equation, aiming to combine the interpretability and boundary-smoothing advantages of variational partial differential equations (PDEs) with the strong representational ability of deep neural networks. The architecture consists of two collaborative modules: an F module, which conducts efficient frequency domain preprocessing to alleviate poor local minima, and a T module, which ensures accurate and stable local computations, backed by a stability estimate. Extensive experiments on three benchmark datasets indicate that the proposed method achieves a balanced trade-off between performance and computational efficiency, which yields competitive quantitative results and improved visual quality compared to pure convolutional neural network (CNN) based models, while achieving performance close to that of transformer-based method with reasonable computational expense.

</details>


### [322] [Online Segment Any 3D Thing as Instance Tracking](https://arxiv.org/abs/2512.07599)
*Hanshi Wang,Zijian Cai,Jin Gao,Yiwei Zhang,Weiming Hu,Ke Wang,Zhipeng Zhang*

Main category: cs.CV

TL;DR: The paper introduces AutoSeg3D, a method to enhance online 3D segmentation by treating it as an instance tracking problem, integrating temporal and spatial consistency for better object understanding.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked temporal aspect in dynamic perception processes of embodied intelligent agents and rectify limitations of current query-based pipelines in 3D segmentation.

Method: AutoSeg3D uses sparse object queries to propagate temporal information, ensuring long-term instance association and short-term updates while integrating spatial consistency learning.

Result: The proposed method outperforms previous state-of-the-art techniques like ESAM by 2.8 AP on ScanNet200 and shows improvements on datasets such as ScanNet, SceneNN, and 3RScan.

Conclusion: The approach enhances embodied agents' temporal and spatial understanding with less computational overhead, advancing 3D segmentation and environmental comprehension capabilities.

Abstract: Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.

</details>


### [323] [Decomposition Sampling for Efficient Region Annotations in Active Learning](https://arxiv.org/abs/2512.07606)
*Jingna Qiu,Frauke Wilm,Mathias Öttl,Jonas Utz,Maja Schlereth,Moritz Schillinger,Marc Aubreville,Katharina Breininger*

Main category: cs.CV

TL;DR: DECOMP enhances annotation efficiency in dense prediction tasks by utilizing class-specific pseudo-label decomposition and confidence-based sampling, outperforming existing methods in diverse medical imaging scenarios.


<details>
  <summary>Details</summary>
Motivation: Annotation for dense prediction tasks is costly and time-consuming, especially in medical imaging, due to the need for region-level precision. Existing methods struggle with computational and selection inefficiencies, limiting their applicability.

Method: DECOMP is an active learning method that decomposes images into class-specific components using pseudo-labels and incorporates region sampling based on class-wise predictive confidence for diverse and informed annotation.

Result: DECOMP demonstrates superior performance over baseline methods in ROI classification, 2-D segmentation, and 3-D segmentation, particularly in improving minority-class region sampling and boosting performance on challenging classes.

Conclusion: The study concludes that DECOMP significantly improves annotation efficiency and model performance by focusing on challenging and minority-class regions, addressing key limitations of existing representative annotation techniques.

Abstract: Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods for representative annotation region selection suffer from high computational and memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling. We propose decomposition sampling (DECOMP), a new active learning sampling strategy that addresses these limitations. It enhances annotation diversity by decomposing images into class-specific components using pseudo-labels and sampling regions from each class. Class-wise predictive confidence further guides the sampling process, ensuring that difficult classes receive additional annotations. Across ROI classification, 2-D segmentation, and 3-D segmentation, DECOMP consistently surpasses baseline methods by better sampling minority-class regions and boosting performance on these challenging classes. Code is in https://github.com/JingnaQiu/DECOMP.git.

</details>


### [324] [MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation](https://arxiv.org/abs/2512.07628)
*Zhiqi Li,Wenhuan Li,Tengfei Wang,Zhenwei Wang,Junta Wu,Haoyuan Wang,Yunhan Yang,Zehuan Huang,Yang Li,Peidong Liu,Chunchao Guo*

Main category: cs.CV

TL;DR: This paper introduces MoCA, a compositional 3D generative model optimizing scalability and efficiency using innovative attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: To improve scalability and efficiency in part-aware 3D generative models, which often face high computational costs with the increase in object components.

Method: MoCA integrates two designs: importance-based component routing for sparse attention and unimportant components compression to retain priors while reducing computational complexity.

Result: MoCA demonstrates superior performance over baseline methods in generating compositional 3D objects and scenes.

Conclusion: MoCA offers an efficient and scalable solution for fine-grained 3D generation, addressing prior limitations in computational costs.

Abstract: Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of unselected components while reducing computational complexity of global attention. With these designs, MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components. Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks. Project page: https://lizhiqi49.github.io/MoCA

</details>


### [325] [Liver Fibrosis Quantification and Analysis: The LiQA Dataset and Baseline Method](https://arxiv.org/abs/2512.07651)
*Yuanye Liu,Hanxiao Zhang,Nannan Shi,Yuxin Shi,Arif Mahmood,Murtaza Taj,Xiahai Zhuang*

Main category: cs.CV

TL;DR: The paper presents the LiQA dataset designed for benchmarking liver segmentation and fibrosis staging algorithms in challenging real-world scenarios, and highlights a high-performing methodology using multi-source data and advanced techniques.


<details>
  <summary>Details</summary>
Motivation: To address the global health burden of liver fibrosis by establishing a reliable dataset and methodologies for accurate staging and segmentation under real-world complexities.

Method: The authors developed the LiQA dataset with MRI scans from 440 patients, introduced a semi-supervised learning framework for segmentation, and a multi-view consensus approach with CAM-based regularization for staging.

Result: Results indicate that using multi-source data and anatomical constraints improves the robustness and accuracy of models in clinical applications.

Conclusion: The LiQA dataset and proposed methodologies enhance the evaluation of liver fibrosis algorithms, making them more effective under real-world clinical challenges.

Abstract: Liver fibrosis represents a significant global health burden, necessitating accurate staging for effective clinical management. This report introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, established as part of the CARE 2024 challenge. Comprising $440$ patients with multi-phase, multi-center MRI scans, the dataset is curated to benchmark algorithms for Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS) under complex real-world conditions, including domain shifts, missing modalities, and spatial misalignment. We further describe the challenge's top-performing methodology, which integrates a semi-supervised learning framework with external data for robust segmentation, and utilizes a multi-view consensus approach with Class Activation Map (CAM)-based regularization for staging. Evaluation of this baseline demonstrates that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings.

</details>


### [326] [An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research](https://arxiv.org/abs/2512.07652)
*Hamad Almazrouei,Mariam Al Nasseri,Maha Alzaabi*

Main category: cs.CV

TL;DR: The paper introduces an AI-powered autonomous underwater vehicle (AUV) system that automates underwater object detection, clustering, and report generation to improve the efficiency of sea exploration.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to address the limitations of traditional underwater exploration methods, such as extreme conditions, poor visibility, and high costs, which leave large areas of oceans unexplored.

Method: The system utilizes YOLOv12 Nano for real-time object detection, ResNet50 for feature extraction, PCA for dimensionality reduction, K-Means++ for clustering marine objects, and GPT-4o Mini for report generation. These components are applied to datasets from Australian marine environments.

Result: The system achieved notable performance metrics, including mAP@0.5 of 0.512, precision of 0.535, and recall of 0.438. PCA reduced dimensionality efficiently while maintaining 98% of data variance, and clustering successfully grouped objects based on visual features. The LLM provided effective summary generation backed by location data.

Conclusion: This integrated AI system reduces human diving risk, improves research efficiency, and accelerates underwater data analysis, promoting advances in marine science and exploration.

Abstract: Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction, Principal Component Analysis (PCA) for dimensionality reduction, and K-Means++ clustering for grouping marine objects based on visual characteristics. Furthermore, a Large Language Model (LLM) (GPT-4o Mini) is employed to generate structured reports and summaries of underwater findings, enhancing data interpretation. The system was trained and evaluated on a combined dataset of over 55,000 images from the DeepFish and OzFish datasets, capturing diverse Australian marine environments. Experimental results demonstrate the system's capability to detect marine objects with a mAP@0.5 of 0.512, a precision of 0.535, and a recall of 0.438. The integration of PCA effectively reduced feature dimensionality while preserving 98% variance, facilitating K-Means clustering which successfully grouped detected objects based on visual similarities. The LLM integration proved effective in generating insightful summaries of detections and clusters, supported by location data. This integrated approach significantly reduces the risks associated with human diving, increases mission efficiency, and enhances the speed and depth of underwater data analysis, paving the way for more effective scientific research and discovery in challenging marine environments.

</details>


### [327] [Optimization-Guided Diffusion for Interactive Scene Generation](https://arxiv.org/abs/2512.07661)
*Shiaho Li,Naisheng Ye,Tianyu Li,Kashyap Chitta,Tuo An,Peng Su,Boyang Wang,Haiou Liu,Chen Lv,Hongyang Li*

Main category: cs.CV

TL;DR: The paper introduces OMEGA, a training-free framework to enhance multi-agent driving scene generation, ensuring realism, consistency, and controllability in safety-critical scenarios.


<details>
  <summary>Details</summary>
Motivation: The need to evaluate autonomous vehicles under realistic and diverse driving scenes, especially with rare safety-critical events, which are underrepresented in existing driving datasets.

Method: OMEGA is a diffusion-based sampling framework that uses constrained optimization to reinforce physical plausibility and social behaviors, and applies game-theoretic optimization to simulate ego-attacker scenarios.

Result: OMEGA significantly improves the validity of generated scenes, increasing physically and behaviorally valid scenes up to 72.27%, improves controllability to 80%, and generates more near-collision scenarios while retaining realism.

Conclusion: OMEGA offers an effective solution for generating realistic and diverse driving scenarios, enhancing safety-critical evaluations for autonomous vehicles.

Abstract: Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, an optimization-guided, training-free framework that enforces structural consistency and interaction awareness during diffusion-based sampling from a scene generation model. OMEGA re-anchors each reverse diffusion step via constrained optimization, steering the generation towards physically plausible and behaviorally coherent trajectories. Building on this framework, we formulate ego-attacker interactions as a game-theoretic optimization in the distribution space, approximating Nash equilibria to generate realistic, safety-critical adversarial scenarios. Experiments on nuPlan and Waymo show that OMEGA improves generation realism, consistency, and controllability, increasing the ratio of physically and behaviorally valid scenes from 32.35% to 72.27% for free exploration capabilities, and from 11% to 80% for controllability-focused generation. Our approach can also generate $5\times$ more near-collision frames with a time-to-collision under three seconds while maintaining the overall scene realism.

</details>


### [328] [EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset](https://arxiv.org/abs/2512.07668)
*Ronan John,Aditya Kesari,Vincenzo DiMatteo,Kristin Dana*

Main category: cs.CV

TL;DR: The paper introduces the EgoCampus dataset for studying egocentric pedestrian eye gaze during outdoor navigation and proposes the EgoCampusNet model for gaze prediction.


<details>
  <summary>Details</summary>
Motivation: To address the lack of datasets and methods focused on predicting human visual attention in real-world outdoor navigation scenarios, unlike existing indoor-focused datasets.

Method: The authors created the EgoCampus dataset using Meta's Project Aria glasses, capturing eye gaze, RGB video, and sensor data from pedestrians across outdoor campus paths. They developed EgoCampusNet, a novel gaze prediction model, using this data.

Result: The study provides a diverse dataset with 6 km of outdoor path recordings from over 80 pedestrians. This forms the basis for EgoCampusNet, which effectively predicts eye gaze in outdoor navigation contexts.

Conclusion: The dataset and model offer valuable resources for studying real-world visual attention and advancing gaze prediction in navigation, with dataset access promised to the public in the near future.

Abstract: We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at https://github.com/ComputerVisionRutgers/EgoCampus .

</details>


### [329] [DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations](https://arxiv.org/abs/2512.07674)
*Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: The paper introduces DIST-CLIP, a framework for MRI harmonization addressing heterogeneous medical imaging data by disentangling anatomy from image contrast.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations posed by data heterogeneity in medical imaging, especially in MRI, due to scanners, protocols, and parameters that can obscure diagnostic signals.

Method: Proposes DIST-CLIP, a framework disentangling anatomical content from image contrast using pre-trained CLIP encoders and an Adaptive Style Transfer module, guided by either target images or DICOM metadata.

Result: DIST-CLIP significantly improves style translation fidelity and anatomical preservation, outperforming state-of-the-art methods on diverse real-world clinical datasets.

Conclusion: DIST-CLIP offers a robust, flexible solution for MRI harmonization and data standardization, aiding diagnostic analysis. The code and weights will be shared publicly.

Abstract: Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variability, but existing approaches remain insufficient. When applied to imaging data, image-based harmonization approaches are often restricted by the need for target images, while existing text-guided methods rely on simplistic labels that fail to capture complex acquisition details or are typically restricted to datasets with limited variability, failing to capture the heterogeneity of real-world clinical environments. To address these limitations, we propose DIST-CLIP (Disentangled Style Transfer with CLIP Guidance), a unified framework for MRI harmonization that flexibly uses either target images or DICOM metadata for guidance. Our framework explicitly disentangles anatomical content from image contrast, with the contrast representations being extracted using pre-trained CLIP encoders. These contrast embeddings are then integrated into the anatomical content via a novel Adaptive Style Transfer module. We trained and evaluated DIST-CLIP on diverse real-world clinical datasets, and showed significant improvements in performance when compared against state-of-the-art methods in both style translation fidelity and anatomical preservation, offering a flexible solution for style transfer and standardizing MRI data. Our code and weights will be made publicly available upon publication.

</details>


### [330] [Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment](https://arxiv.org/abs/2512.07702)
*Sangha Park,Eunji Kim,Yeongtak Oh,Jooyoung Choi,Sungroh Yoon*

Main category: cs.CV

TL;DR: This paper introduces Negative Prompting for Image Correction (NPC), a pipeline that improves text-image alignment for text-to-image generation by suppressing unintended content via negative prompts.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in achieving strong text-image alignment, especially for prompts with complex or imaginative elements.

Method: NPC utilizes cross-attention patterns to identify negative prompts that suppress unintended content. A verifier-captioner-proposer framework generates and ranks candidate prompts based on a salient text-space score.

Result: NPC outperformed existing baselines, showing superior scores on benchmarks like GenEval++ (0.571 vs. 0.371) and achieving the best overall results on Imagine-Bench.

Conclusion: NPC provides a fully automated, effective method for enhancing text-image alignment, offering significant advancements in diffusion model performance.

Abstract: Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.

</details>


### [331] [PVeRA: Probabilistic Vector-Based Random Matrix Adaptation](https://arxiv.org/abs/2512.07703)
*Leo Fillioux,Enzo Ferrante,Paul-Henry Cournède,Maria Vakalopoulou,Stergios Christodoulidis*

Main category: cs.CV

TL;DR: This paper introduces PVeRA, a probabilistic enhancement of the VeRA adapter, achieving superior parameter-efficient adaptation performance in large models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of scarce datasets and costly computational resources needed for fine-tuning large models, the authors aim to enhance adaptation methods with an efficient approach.

Method: The authors propose PVeRA, which builds upon VeRA by incorporating probabilistic adjustments to its low-rank matrices, enabling better handling of ambiguities and adaptability in input data.

Result: PVeRA showed superior performance compared to VeRA and other methods on the VTAB-1k benchmark.

Conclusion: PVeRA effectively enhances adaptation efficiency, refining large foundation models with minimal resources; code is made publicly available for reproducibility.

Abstract: Large foundation models have emerged in the last years and are pushing performance boundaries for a variety of tasks. Training or even finetuning such models demands vast datasets and computational resources, which are often scarce and costly. Adaptation methods provide a computationally efficient solution to address these limitations by allowing such models to be finetuned on small amounts of data and computing power. This is achieved by appending new trainable modules to frozen backbones with only a fraction of the trainable parameters and fitting only these modules on novel tasks. Recently, the VeRA adapter was shown to excel in parameter-efficient adaptations by utilizing a pair of frozen random low-rank matrices shared across all layers. In this paper, we propose PVeRA, a probabilistic version of the VeRA adapter, which modifies the low-rank matrices of VeRA in a probabilistic manner. This modification naturally allows handling inherent ambiguities in the input and allows for different sampling configurations during training and testing. A comprehensive evaluation was performed on the VTAB-1k benchmark and seven adapters, with PVeRA outperforming VeRA and other adapters. Our code for training models with PVeRA and benchmarking all adapters is available https://github.com/leofillioux/pvera.

</details>


### [332] [UnCageNet: Tracking and Pose Estimation of Caged Animal](https://arxiv.org/abs/2512.07712)
*Sayak Dutta,Harish Katti,Shashikant Verma,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: The paper introduces a preprocessing pipeline to improve animal tracking and pose estimation systems affected by cage structures and occlusions through cage segmentation, inpainting, and evaluation.


<details>
  <summary>Details</summary>
Motivation: The goal is to address performance drops in tracking systems caused by cage structures and occlusions that hinder pose estimation and tracking in animal studies.

Method: The pipeline consists of three stages: (1) a segmentation model (Gabor-enhanced ResNet-UNet) identifies cage structures, (2) inpainting with CRFill reconstructs occluded regions, and (3) performance is evaluated on uncaged frames.

Result: Experimental results show the pipeline significantly improves pose estimation and tracking accuracy, making performance comparable to occlusion-free environments.

Conclusion: The introduced pipeline effectively mitigates occlusion issues, enhancing keypoint detection and trajectory tracking consistency in animal studies.

Abstract: Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.

</details>


### [333] [ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation](https://arxiv.org/abs/2512.07720)
*Fan Yang,Heyuan Li,Peihao Li,Weihao Yuan,Lingteng Qiu,Chaoyue Song,Cheng Chen,Yisheng He,Shifeng Zhang,Xiaoguang Han,Steven Hoi,Guosheng Lin*

Main category: cs.CV

TL;DR: This paper introduces a method for creating realistic upper-body 3D avatars from a single image by integrating 3D reconstruction models with video diffusion models.


<details>
  <summary>Details</summary>
Motivation: Current methods either have structural stability issues or lack realistic textures and motions.

Method: The approach combines 3D reconstruction for structural reliability with a real-time autoregressive video diffusion model for rendering high-quality, dynamic results.

Result: The proposed method reduces artifacts, enhances visual quality, and achieves temporally coherent motion in avatars.

Conclusion: The method successfully leverages the strengths of both paradigms to create high-fidelity, realistic 3D avatars suitable for real-time applications like gaming and VR.

Abstract: Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa

</details>


### [334] [Improving action classification with brain-inspired deep networks](https://arxiv.org/abs/2512.07729)
*Aidas Aglinskas,Stefano Anzellotti*

Main category: cs.CV

TL;DR: The paper investigates the role of body and background information in action recognition by comparing neural network performance with human capabilities and proposes a brain-inspired neural architecture.


<details>
  <summary>Details</summary>
Motivation: To explore how information from both the body and background is utilized in action recognition by AI systems compared to humans and propose improvements inspired by human brain functionality.

Method: The research involved testing DNNs on the HAA500 dataset with stimuli variations (including body-only, background-only, and combined versions), evaluating human performance, and implementing a brain-inspired neural architecture with separate streams for body and background.

Result: Human participants outperformed DNNs in recognizing actions across stimuli conditions, emphasizing the importance of body information. The brain-inspired architecture improved DNN recognition performance and exhibited accuracy patterns similar to humans.

Conclusion: Separating body and background information processing in DNNs, as inspired by the human brain, enhances action recognition and aligns DNN accuracy with human patterns.

Abstract: Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks (DNNs) make use of information about the body and information about the background remains unclear. Since these two sources of information may be correlated within a training dataset, DNNs might learn to rely predominantly on one of them, without taking full advantage of the other. Unlike DNNs, humans have domain-specific brain regions selective for perceiving bodies, and regions selective for perceiving scenes. The present work tests whether humans are thus more effective at extracting information from both body and background, and whether building brain-inspired deep network architectures with separate domain-specific streams for body and scene perception endows them with more human-like performance. We first demonstrate that DNNs trained using the HAA500 dataset perform almost as accurately on versions of the stimuli that show both body and background and on versions of the stimuli from which the body was removed, but are at chance-level for versions of the stimuli from which the background was removed. Conversely, human participants (N=28) can recognize the same set of actions accurately with all three versions of the stimuli, and perform significantly better on stimuli that show only the body than on stimuli that show only the background. Finally, we implement and test a novel architecture patterned after domain specificity in the brain with separate streams to process body and background information. We show that 1) this architecture improves action recognition performance, and 2) its accuracy across different versions of the stimuli follows a pattern that matches more closely the pattern of accuracy observed in human participants.

</details>


### [335] [SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination](https://arxiv.org/abs/2512.07730)
*Sangha Park,Seungryong Yoo,Jisoo Mok,Sungroh Yoon*

Main category: cs.CV

TL;DR: SAVE enhances visual information processing in MLLMs to address object hallucination by utilizing Sparse Autoencoder latent features.


<details>
  <summary>Details</summary>
Motivation: Address limitations in MLLMs, particularly hallucination issues caused by language priors and loss of visual information.

Method: Utilize Sparse Autoencoder-driven latent features and steering techniques to reinforce grounded visual understanding.

Result: SAVE achieves a 10%p improvement on CHAIR_S, gains on POPE and MMHal-Bench, and robust performance across models and layers.

Conclusion: The framework mitigates hallucination effectively, improves attention to image tokens, and generalizes well across benchmarks.

Abstract: Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\%p improvement in CHAIR\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.

</details>


### [336] [SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery](https://arxiv.org/abs/2512.07733)
*Meng Cao,Xingyu Li,Xue Liu,Ian Reid,Xiaodan Liang*

Main category: cs.CV

TL;DR: SpatialDreamer introduces improved spatial reasoning for MLLMs, incorporating active exploration with reinforcement learning and a novel reward optimization method.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with tasks requiring complex spatial reasoning and mental simulation as they only passively observe spatial data, lacking an active mental imagery process.

Method: The SpatialDreamer framework utilizes reinforcement learning, integrating active exploration, visual imagination through a world model, and evidence-grounded reasoning. GeoPO is introduced to enhance reward supervision in reasoning tasks.

Result: SpatialDreamer achieves strong performance across various challenging benchmarks, demonstrating its capability in human-like spatial reasoning.

Conclusion: SpatialDreamer marks significant progress in enabling MLLMs to perform active spatial simulation tasks, enhancing their mental simulation capacities.

Abstract: Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visual imagination via a world model, and evidence-grounded reasoning. To address the lack of fine-grained reward supervision in longhorizontal reasoning tasks, we propose Geometric Policy Optimization (GeoPO), which introduces tree-structured sampling and step-level reward estimation with geometric consistency constraints. Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, signifying a critical advancement in human-like active spatial mental simulation for MLLMs.

</details>


### [337] [HLTCOE Evaluation Team at TREC 2025: VQA Track](https://arxiv.org/abs/2512.07738)
*Dengjia Zhang,Charles Weng,Katherine Guerrerio,Yi Lu,Kenton Murray,Alexander Martin,Reno Kriz,Benjamin Van Durme*

Main category: cs.CV

TL;DR: The paper introduces a listwise learning framework for answer generation in video-question pairs, focusing on improving semantic precision and ranking using a new Masked Pointer Cross-Entropy Loss with Rank Weights.


<details>
  <summary>Details</summary>
Motivation: To address limitations in semantic precision and ranking consistency in video-based question answering tasks.

Method: The authors propose a two-step method: (1) generate candidate answers using a multimodal model and (2) rerank the answers using a model trained with Masked Pointer Cross-Entropy Loss with Rank Weights, which combines pointer-based selection, rank-dependent weighting, and vocabulary-restricted optimization.

Result: Experiments show improved accuracy and ranking consistency, particularly for questions involving temporal reasoning and semantic disambiguation.

Conclusion: By integrating generative models and discriminative ranking techniques, the proposed method enhances the quality and interpretability of answer lists in video-question answering systems.

Abstract: The HLTCOE Evaluation team participated in TREC VQA's Answer Generation (AG) task, for which we developed a listwise learning framework that aims to improve semantic precision and ranking consistency in answer generation. Given a video-question pair, a base multimodal model first generates multiple candidate answers, which are then reranked using a model trained with a novel Masked Pointer Cross-Entropy Loss with Rank Weights. This objective integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction, enabling stable and interpretable listwise optimization. By bridging generative modeling with discriminative ranking, our method produces coherent, fine-grained answer lists. Experiments reveal consistent gains in accuracy and ranking stability, especially for questions requiring temporal reasoning and semantic disambiguation.

</details>


### [338] [DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving](https://arxiv.org/abs/2512.07745)
*Jialv Zou,Shaoyu Chen,Bencheng Liao,Zhiyu Zheng,Yuehao Song,Lefei Zhang,Qian Zhang,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: DiffusionDriveV2 uses reinforcement learning to improve autonomous driving trajectory generation, achieving record-breaking results by balancing diversity and quality.


<details>
  <summary>Details</summary>
Motivation: Generative diffusion models for autonomous driving struggle with mode collapse, leading to homogeneous behaviors and a trade-off between diversity and quality.

Method: The paper introduces diffusion models with reinforcement learning techniques like adaptive noise and GRPO to manage trajectory generation and prevent mode collapse.

Result: DiffusionDriveV2 achieves high performance metrics on NAVSIM datasets, surpassing previous benchmarks with improved trajectory planning.

Conclusion: The model successfully resolves the diversity-quality trade-off, setting new standards for autonomous driving systems, and offers open-source access.

Abstract: Generative diffusion models for end-to-end autonomous driving often suffer from mode collapse, tending to generate conservative and homogeneous behaviors. While DiffusionDrive employs predefined anchors representing different driving intentions to partition the action space and generate diverse trajectories, its reliance on imitation learning lacks sufficient constraints, resulting in a dilemma between diversity and consistent high quality. In this work, we propose DiffusionDriveV2, which leverages reinforcement learning to both constrain low-quality modes and explore for superior trajectories. This significantly enhances the overall output quality while preserving the inherent multimodality of its core Gaussian Mixture Model. First, we use scale-adaptive multiplicative noise, ideal for trajectory planning, to promote broad exploration. Second, we employ intra-anchor GRPO to manage advantage estimation among samples generated from a single anchor, and inter-anchor truncated GRPO to incorporate a global perspective across different anchors, preventing improper advantage comparisons between distinct intentions (e.g., turning vs. going straight), which can lead to further mode collapse. DiffusionDriveV2 achieves 91.2 PDMS on the NAVSIM v1 dataset and 85.5 EPDMS on the NAVSIM v2 dataset in closed-loop evaluation with an aligned ResNet-34 backbone, setting a new record. Further experiments validate that our approach resolves the dilemma between diversity and consistent high quality for truncated diffusion models, achieving the best trade-off. Code and model will be available at https://github.com/hustvl/DiffusionDriveV2

</details>


### [339] [Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation](https://arxiv.org/abs/2512.07747)
*Shihao Zhao,Yitong Chen,Zeyinzi Jiang,Bojia Zi,Shaozhe Hao,Yu Liu,Chaojie Mao,Kwan-Yee K. Wong*

Main category: cs.CV

TL;DR: The paper presents Unison, a two-stage multimodal learning model that offers diverse understanding and generation tasks with low training cost and high automation, eliminating the need for manual intervention.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in multimodal learning, such as high computational demands and limited task coverage or poor generation quality in existing systems, along with the lack of automated parsing of input meta-information.

Method: The authors propose Unison, a model built on a two-stage scheme that effectively utilizes pre-trained models while also automating input parsing and task determination to perform understanding and generation tasks across various modalities.

Result: With only 500k training samples and 50 GPU hours, Unison demonstrates accurate task identification and parameter extraction, achieving superior performance in text, image, and video understanding as well as diverse generation tasks.

Conclusion: Unison effectively tackles limitations in previous multimodal models by offering low-cost, high-performance, automated solutions for understanding and generation tasks across multiple modalities.

Abstract: Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.

</details>


### [340] [Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2512.07760)
*Menglin Wang,Xiaojin Gong,Jiachen Li,Genlin Ji*

Main category: cs.CV

TL;DR: This paper addresses unsupervised visible-infrared person re-identification (USVI-ReID) by mitigating modality bias for better cross-modality associations and representation learning.


<details>
  <summary>Details</summary>
Motivation: The paper focuses on the challenge of addressing the large gap in association learning for identifying individuals across visible and infrared modalities in USVI-ReID, targeting bias and modality discrepancy.

Method: They propose modality-aware Jaccard distance to reduce modality bias for reliable global clustering and implement a 'split-and-contrast' strategy to align modality-specific prototypes for robust representation learning.

Result: The method achieves state-of-the-art (SOTA) performance on visible-infrared person re-ID benchmarks, showing significant improvements over previous methods.

Conclusion: The proposed approach demonstrates effectiveness in mitigating cross-modality bias and achieving modality-invariant representation learning, advancing the unsupervised VI-ReID field.

Abstract: Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-level relations. By mining and attending to the visible-infrared modality bias, this paper focuses on addressing cross-modality learning from two aspects: bias-mitigated global association and modality-invariant representation learning. Motivated by the camera-aware distance rectification in single-modality re-ID, we propose modality-aware Jaccard distance to mitigate the distance bias caused by modality discrepancy, so that more reliable cross-modality associations can be estimated through global clustering. To further improve cross-modality representation learning, a `split-and-contrast' strategy is designed to obtain modality-specific global prototypes. By explicitly aligning these prototypes under global association guidance, modality-invariant yet ID-discriminative representation learning can be achieved. While conceptually simple, our method obtains state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin, validating its effectiveness.

</details>


### [341] [GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring](https://arxiv.org/abs/2512.07776)
*Maximilian Schall,Felix Leonard Knöfel,Noah Elias König,Jan Jonas Kubeler,Maximilian von Klinski,Joan Wilhelm Linnemann,Xiaoshi Liu,Iven Jelle Schlegelmilch,Ole Woyciniuk,Alexandra Schild,Dante Wasmuht,Magdalena Bermejo Espinet,German Illera Basas,Gerard de Melo*

Main category: cs.CV

TL;DR: The paper tackles the challenge of monitoring critically endangered western lowland gorillas by introducing new datasets and a pipeline, GorillaWatch, to automate re-identification in video footage.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation of manual effort in re-identifying gorillas from camera footage and the lack of large-scale video datasets to train deep learning models.

Method: The authors developed three novel datasets and proposed GorillaWatch, a pipeline combining detection, tracking, and re-identification with a self-supervised pretraining strategy. They also implemented a verification method to ensure unbiased model reliance.

Result: Extensive benchmarking showed improved re-identification through aggregated image features, better than specialized video architectures. They tackled population counting issues with spatiotemporal constraints in clustering.

Conclusion: The approach facilitates scalable, non-invasive monitoring of endangered species, with datasets and code made available publicly to support research and conservation efforts.

Abstract: Monitoring critically endangered western lowland gorillas is currently hampered by the immense manual effort required to re-identify individuals from vast archives of camera trap footage. The primary obstacle to automating this process has been the lack of large-scale, "in-the-wild" video datasets suitable for training robust deep learning models. To address this gap, we introduce a comprehensive benchmark with three novel datasets: Gorilla-SPAC-Wild, the largest video dataset for wild primate re-identification to date; Gorilla-Berlin-Zoo, for assessing cross-domain re-identification generalization; and Gorilla-SPAC-MoT, for evaluating multi-object tracking in camera trap footage. Building on these datasets, we present GorillaWatch, an end-to-end pipeline integrating detection, tracking, and re-identification. To exploit temporal information, we introduce a multi-frame self-supervised pretraining strategy that leverages consistency in tracklets to learn domain-specific features without manual labels. To ensure scientific validity, a differentiable adaptation of AttnLRP verifies that our model relies on discriminative biometric traits rather than background correlations. Extensive benchmarking subsequently demonstrates that aggregating features from large-scale image backbones outperforms specialized video architectures. Finally, we address unsupervised population counting by integrating spatiotemporal constraints into standard clustering to mitigate over-segmentation. We publicly release all code and datasets to facilitate scalable, non-invasive monitoring of endangered species

</details>


### [342] [Distribution Matching Variational AutoEncoder](https://arxiv.org/abs/2512.07778)
*Sen Ye,Jianning Pei,Mengde Xu,Shuyang Gu,Chunyu Wang,Liwei Wang,Han Hu*

Main category: cs.CV

TL;DR: The paper introduces DMVAE, a model that aligns latent distribution with reference distributions to explore optimal structures for visual generative modeling, achieving high performance results.


<details>
  <summary>Details</summary>
Motivation: Existing visual generative models compress images into latent spaces without explicitly shaping their distribution, leaving uncertainty about the optimal latent space structures.

Method: DMVAE aligns the encoder's latent distribution with arbitrary reference distributions using a distribution matching constraint, enabling generalized latent structures beyond traditional Gaussian priors.

Result: DMVAE demonstrates that self-supervised learning derived distributions optimize the balance between reconstruction fidelity and modeling efficiency, achieving gFID 3.2 on ImageNet in only 64 epochs.

Conclusion: Choosing suitable latent distribution structures, rather than relying on fixed priors, is essential for high-fidelity and efficient image synthesis in visual generative models.

Abstract: Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \textbf{Distribution-Matching VAE} (\textbf{DMVAE}), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.

</details>


### [343] [OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory](https://arxiv.org/abs/2512.07802)
*Zhaochong An,Menglin Jia,Haonan Qiu,Zijian Zhou,Xiaoke Huang,Zhiheng Liu,Weiming Ren,Kumara Kahatapitiya,Ding Liu,Sen He,Chenyang Zhang,Tao Xiang,Fanny Yang,Serge Belongie,Tian Xie*

Main category: cs.CV

TL;DR: OneStory is a new method for multi-shot video generation that enhances narrative coherence by using compact global context modeling and leveraging pretrained models, alongside a curated dataset, for superior storytelling in videos.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in existing multi-shot video generation methods, which struggle with understanding long-range cross-shot context and performing effectively on complex narratives.

Method: OneStory reformulates multi-shot video generation as a next-shot generation task, incorporating a Frame Selection module for semantically-relevant global memory and an Adaptive Conditioner for importance-guided patchification. It uses a curated 60K dataset and finetunes pretrained image-to-video models for enhanced results.

Result: OneStory achieved state-of-the-art performance in narrative coherence across diverse and complex scenes in both text- and image-conditioned multi-shot video generation tasks.

Conclusion: OneStory significantly improves multi-shot video generation by enabling controllable, scalable, and more coherent long-form storytelling via global context modeling and enhanced architectural innovations.

Abstract: Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.

</details>


### [344] [Multi-view Pyramid Transformer: Look Coarser to See Broader](https://arxiv.org/abs/2512.07806)
*Gyeongjin Kang,Seungkwon Yang,Seungtae Nam,Younggeun Lee,Jungwoo Kim,Eunbyung Park*

Main category: cs.CV

TL;DR: The paper introduces Multi-view Pyramid Transformer (MVP), a scalable architecture for reconstructing large 3D scenes from multiple images efficiently.


<details>
  <summary>Details</summary>
Motivation: The need to efficiently reconstruct large-scale, complex 3D scenes from numerous images, balancing computational efficiency and high-quality results.

Method: Two hierarchies for reconstruction: a local-to-global inter-view hierarchy widening the perspective and a fine-to-coarse intra-view hierarchy compressing spatial detail into compact tokens.

Result: The proposed MVP achieves state-of-the-art reconstruction quality using 3D Gaussian Splatting, demonstrating high efficiency and scalability across various datasets and view configurations.

Conclusion: MVP establishes a new standard for scalable and effective 3D scene reconstruction from images, combining computational efficiency with representational richness.

Abstract: We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details," MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.

</details>


### [345] [Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes](https://arxiv.org/abs/2512.07807)
*Shai Krakovsky,Gal Fiebelman,Sagie Benaim,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: The paper proposes an efficient method for embedding language fields in 3D representations to enhance human-computer interaction and multimodal reasoning in large-scale scenes, addressing inefficiencies and feature misalignment faced by current methods.


<details>
  <summary>Details</summary>
Motivation: To improve semantic understanding and enable intuitive interaction with 3D spatial environments, while addressing the inefficiency and misalignment issues in learning from large-scale Internet data.

Method: They propose using low-dimensional semantic bottleneck features within a 3D Gaussian representation combined with a hash encoder, and introduce an Attenuated Downsampler module with regularizations to tackle semantic misalignment.

Result: The proposed method demonstrated superior performance and efficiency on the HolyScenes dataset compared to existing approaches.

Conclusion: The approach effectively enhances accuracy and efficiency in 3D semantic representation, particularly for large-scale scenes, enabling better human-computer interaction and applications like navigation and multimodal reasoning.

Abstract: Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.

</details>


### [346] [WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling](https://arxiv.org/abs/2512.07821)
*Shaoheng Fang,Hanwen Jiang,Yunpeng Bai,Niloy J. Mitra,Qixing Huang*

Main category: cs.CV

TL;DR: WorldReel is a 4D video generator achieving spatiotemporal consistency, enabling coherent geometry and appearance over time in dynamic scenes and moving cameras.


<details>
  <summary>Details</summary>
Motivation: To address the inconsistency in 3D modeling of recent video generators, producing videos that remain temporally coherent and geometrically stable.

Method: WorldReel integrates explicit 4D representations, combining synthetic data for precise supervision with real videos for diversity and realism. This results in consistent geometry and appearance over viewpoints and time.

Result: WorldReel achieves state-of-the-art performance in geometric consistency, motion coherence, and reduces artifacts. It generalizes well to real-world footage.

Conclusion: WorldReel advances video generation, achieving 4D-consistent modeling of dynamic scenes for applications in rendering, interaction, and reasoning in stable spatiotemporal representations.

Abstract: Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.

</details>


### [347] [OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing](https://arxiv.org/abs/2512.07826)
*Haoyang He,Jie Wang,Jiangning Zhang,Zhucun Xue,Xingyuan Bu,Qiangpeng Yang,Shilei Wen,Lei Xie*

Main category: cs.CV

TL;DR: The paper introduces OpenVE-3M, a large-scale, high-quality instruction-based video editing dataset, and OpenVE-Bench, a benchmark for evaluating video editing. The proposed OpenVE-Edit model achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The scarcity of large-scale, high-quality datasets for instruction-based video editing limits progress in this field, despite advancements in image editing.

Method: The authors developed OpenVE-3M with spatially and non-spatially aligned video edit categories using a rigorous data pipeline. They also created OpenVE-Bench for evaluation, and trained OpenVE-Edit, a 5B model, on OpenVE-3M.

Result: OpenVE-3M outperforms existing datasets in scale, diversity, quality, and instruction length. OpenVE-Edit sets a new state-of-the-art on OpenVE-Bench, surpassing larger open-source models.

Conclusion: OpenVE-3M and OpenVE-Bench fill a critical gap in instruction-based video editing resources. The OpenVE-Edit model demonstrates the potential for high performance even with smaller model sizes.

Abstract: The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://github.com/lewandofskee/OpenVE.

</details>


### [348] [One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation](https://arxiv.org/abs/2512.07829)
*Yuan Gao,Chen Chen,Tianrong Chen,Jiatao Gu*

Main category: cs.CV

TL;DR: FAE (Feature Auto-Encoder) adapts high-quality pre-trained visual representations into low-dimensional latent spaces for generative models using a simple framework with two deep decoders, achieving strong performance in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current visual generative models face challenges in integrating pre-trained visual representations into effective latent spaces due to mismatches between the encoding preferences for understanding-oriented features and the requirements for generative-friendly dimensions.

Method: FAE employs a dual-decoder approach: one for reconstructing the original feature space and the other for leveraging these reconstructed features in image generation, requiring only a single attention layer for adaptation.

Result: FAE shows strong generative performance on benchmarks, achieving near or state-of-the-art FID scores for diffusion models on ImageNet, both with and without conditional guidance function (CFG).

Conclusion: FAE improves compatibility between pre-trained visual representations and generative models, simplifying adaptation while maintaining high reconstruction, understanding capabilities, and generation quality.

Abstract: Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.

</details>


### [349] [UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation](https://arxiv.org/abs/2512.07831)
*Jiehui Huang,Yuechen Zhang,Xu He,Yuan Gao,Zhi Cen,Bin Xia,Yan Zhou,Xin Tao,Pengfei Wan,Jiaya Jia*

Main category: cs.CV

TL;DR: UnityVideo is a unified video generation framework addressing limitations in single-modality conditioning by learning across multiple modalities, enhanced by a modality switcher and dynamic noising.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome limitations in current video generation models that struggle to integrate holistic world understanding due to insufficient cross-modal interaction and limited modal diversity.

Method: The researchers propose UnityVideo, featuring dynamic noising for integrating heterogeneous training paradigms and a modality switcher for modular and context-aware learning. It uses a unified dataset with 1.3M samples for joint optimization.

Result: UnityVideo accelerates convergence, improves zero-shot generalization for unseen data, and achieves superior video quality and consistency while aligning with physical world constraints.

Conclusion: UnityVideo advances world-aware video generation by jointly leveraging multiple modalities for enhanced synthesis capabilities and generalization, creating a more comprehensive and robust framework.

Abstract: Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo

</details>


### [350] [Relational Visual Similarity](https://arxiv.org/abs/2512.07833)
*Thao Nguyen,Sicheng Mo,Krishna Kumar Singh,Yilin Wang,Jing Shi,Nicholas Kolkin,Eli Shechtman,Yong Jae Lee,Yuheng Li*

Main category: cs.CV

TL;DR: The paper discusses modeling relational similarity in images, focusing on how humans perceive relations beyond visual attributes. Researchers propose a new vision-language model to measure this similarity.


<details>
  <summary>Details</summary>
Motivation: Humans excel at recognizing relational similarities (e.g., Earth resembling a peach by structure), yet current image-similarity models focus only on visual attributes, neglecting relational logic.

Method: The authors created a 114k image-caption dataset emphasizing relational logic over surface content. They fine-tuned a Vision-Language model using this dataset to measure relational image similarity.

Result: The fine-tuned model can discern relational similarity, bridging a gap ignored by existing visual similarity models.

Conclusion: This study highlights the need to capture relational similarities in images and offers an initial step with models that focus on relational logic rather than just appearance.

Abstract: Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.

</details>


### [351] [Voxify3D: Pixel Art Meets Volumetric Rendering](https://arxiv.org/abs/2512.07834)
*Yi-Chuan Huang,Jiewen Chan,Hao-Jen Chien,Yu-Lun Liu*

Main category: cs.CV

TL;DR: This paper introduces Voxify3D, a framework for generating voxel art from 3D meshes while preserving semantics and aesthetics.


<details>
  <summary>Details</summary>
Motivation: To overcome the difficulty in generating automated voxel art from 3D meshes, which requires a balance between geometric abstraction, semantic preservation, and discrete color coherence.

Method: The authors propose a two-stage differentiable framework—combining orthographic pixel art supervision, patch-based CLIP alignment, and palette-constrained Gumbel-Softmax quantization—to optimize voxel art generation while addressing aesthetic and semantic challenges.

Result: The method achieves superior voxel art generation with high semantic and aesthetic quality (CLIP-IQA 37.12, 77.90% user preference), supporting controllable abstraction levels in terms of resolution and color palette.

Conclusion: Voxify3D effectively bridges the gap between 3D mesh input and voxel art output, addressing critical challenges and outperforming existing methods in aesthetics and semantic preservation.

Abstract: Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [352] [Vec-LUT: Vector Table Lookup for Parallel Ultra-Low-Bit LLM Inference on Edge Devices](https://arxiv.org/abs/2512.06443)
*Xiangyu Li,Chengyu Yin,Weijun Wang,Jianyu Wei,Ting Cao,Yunxin Liu*

Main category: cs.DC

TL;DR: The paper introduces a vector lookup table (Vec-LUT) paradigm to improve the efficiency of ultra-low-bit large language models (LLMs) deployed on edge devices, achieving up to 4.2× speedup compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Ultra-low-bit large language models (LLMs) are increasingly being used on edge devices to meet strict resource constraints. However, current lookup table (LUT)-based inference methods underutilize memory bandwidth in parallel inference scenarios, hindering performance.

Method: The paper proposes the vector lookup table (Vec-LUT) paradigm, which unifies LUT across parallel tokens and performs a single $1 \rightarrow N$ lookup per index. It introduces two additional techniques: (1) Vector LUT-Centric Tensor Layout and (2) Cache-Aware Streamed Lookup techniques for efficient realization.

Result: The proposed Vec-LUT paradigm improves performance on 5 edge devices across 3 different LLMs, achieving up to 4.2× speedup over state-of-the-art baselines.

Conclusion: The Vec-LUT paradigm addresses inefficiencies in memory bandwidth utilization during LUT-based inference for ultra-low-bit LLMs on edge devices. Its integration into llama.cpp demonstrates significant performance gains, making it a valuable advancement for edge applications.

Abstract: Large language models (LLMs) are increasingly deployed on edge devices. To meet strict resource constraints, real-world deployment has pushed LLM quantization from 8-bit to 4-bit, 2-bit, and now 1.58-bit. Combined with lookup table (LUT)-based inference, CPUs run these ultra-low-bit LLMs even faster than NPUs, opening new opportunities for ubiquitous on-device intelligence.
  However, this paper identifies that LUT-based inference underutilizes memory bandwidth during parallel inference, which is required for prefilling, test-time scaling, and other multi-token scenarios. The root cause is the scalar LUT paradigm, which performs repetitive and non-contiguous memory accesses for each token.
  To solve the issue, we propose vector LUT, a new lookup paradigm that constructs a unified LUT across parallel tokens, and performs a single $1 \rightarrow N$ lookup per index. To realize it efficiently, we further introduce (1) Vector LUT-Centric Tensor Layout, and (2) Cache-Aware Streamed Lookup techniques. Evaluations on 5 edge devices across 3 LLMs show that Vec-LUT outperforms state-of-the-art baselines by up to $4.2\times$. Our implementation is integrated into llama.cpp. The code is available at https://github.com/Cipherxzc/vlut.cpp.

</details>


### [353] [Stable-MoE: Lyapunov-based Token Routing for Distributed Mixture-of-Experts Training over Edge Networks](https://arxiv.org/abs/2512.06784)
*Long Shi,Bingyan Ou,Kang Wei,Weihao Zhu,Zhe Wang,Zhiyong Chen*

Main category: cs.DC

TL;DR: Stable-MoE introduces a Lyapunov-based token routing mechanism to optimize mixture of experts model training in resource-constrained edge networks, achieving superior throughput and accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional token routing methods struggle with workload backlog and resource inefficiencies, especially in resource-heterogeneous and stochastic edge network environments.

Method: The paper proposes Stable-MoE, a framework using a Lyapunov optimization approach to transform long-term optimization issues into manageable per-slot subproblems, enhancing token routing and resource allocation dynamically.

Result: Stable-MoE shows at least 40% improvement in system throughput and a minimum of 5% increase in test accuracy compared to baseline methods on SVHN and CIFAR-100 datasets.

Conclusion: Stable-MoE effectively addresses edge network constraints in distributed MoE training while maintaining performance stability and optimization of computational resources.

Abstract: The sparse activation mechanism of mixture of experts (MoE) model empowers edge intelligence with enhanced training efficiency and reduced computational resource consumption. However, traditional token routing in distributed MoE training faces significant challenges in resource-constrained edge networks characterized by heterogeneous computing capabilities and stochastic token arrivals, which inevitably suffer from workload backlog, resource inefficiency, and performance degradation. To address this issue, we propose a novel Lyapunov-based token routing framework for distributed MoE training over resource-heterogeneous edge networks, termed Stable-MoE. Specifically, we formulate a stochastic optimization problem to maximize both system throughput and gating consistency via optimizing the token routing strategy and computational resource allocation, while ensuring long-term stability of both token and energy queues at the edge devices. Using the Lyapunov optimization, we transform the intractable long-term optimization problem into tractable per-slot subproblems by enabling online decision-making of token routing and computation frequency utilization without the knowledge of future system states. Experimental results on the SVHN and CIFAR-100 datasets demonstrate that Stable-MoE outperforms the baselines with at least 40% and 5% gains in system throughput and test accuracy, respectively.

</details>


### [354] [Cloud Revolution: Tracing the Origins and Rise of Cloud Computing](https://arxiv.org/abs/2512.06800)
*Deepa Gurung,S M Zia Ur Rashid,Zain ul Abdeen,Suman Rath*

Main category: cs.DC

TL;DR: The paper revisits cloud computing's historical evolution, its technological and economic impacts, limitations, and transformative trends like AI and quantum computing, outlining an evolving cloud paradigm.


<details>
  <summary>Details</summary>
Motivation: To analyze the historical, technological, and economic evolution of cloud computing and assess its impacts, challenges, and trends shaping its future.

Method: Historical reevaluation, examination of technological forces, analysis of adoption limitations, and discussion of emerging technological trends in cloud computing.

Result: It identifies cloud computing's organizational impacts (e.g., lowering the entry barrier for applications), challenges like vendor dependency and security risks, and transformative trends like edge-cloud convergence and AI-driven architectures.

Conclusion: Cloud computing is a rapidly evolving paradigm influenced by scalability, openness, and trust, shaped by both ongoing challenges and emerging technologies such as AI and quantum computing.

Abstract: The history behind the development of cloud computing is more than several decades of technological progress in the fields of virtualization, distributed systems, and high-speed networking, but its current application is much broader than the underlying technologies that made it possible. This paper reexamines the historical evolution of the field, including the initial ideas of resource sharing and utility-based computing approaches and the development of hyperscale data centers and modern globally federated cloud ecosystems. We also analyze the technological and economic forces and point to the way cloud platforms altered the organizational computing habits, decreasing the entrance-level to the data-intensive and computation-heavy apps. The study also takes into account the ongoing limitations which have come with the large-scale adoption of clouds which include exposure to security due to the weaknesses in configuration, particular establishment regulations, and structural reliance on the single vendors. Lastly, we address some of the new trends that are transforming the cloud environment, including the convergence of edge and cloud infrastructure, the increased prominence of AI-optimised architectures and the initial adoption of quantum computing services. Collectively, the developments above describe an emerging but quickly changing paradigm with its future direction being determined by a strike of balancing between scalability, openness, and trust.

</details>


### [355] [Optimizing video analytics inference pipelines: a case study](https://arxiv.org/abs/2512.07009)
*Saeid Ghafouri,Yuming Ding,Katerine Diaz Chito,Jesús Martinez del Rincón,Niamh O'Connell,Hans Vandierendonck*

Main category: cs.DC

TL;DR: This paper focuses on cost-effective and fast video analytics for precision livestock monitoring, presenting optimizations that enhance processing speed without affecting accuracy.


<details>
  <summary>Details</summary>
Motivation: Achieving high-throughput and cost-efficient video analytics for real-time monitoring in commercial farming, which involves extensive computational resources due to high-resolution footage.

Method: Optimizations across various system modules, using multi-level parallelization, GPU-accelerated code, vectorized clustering, and memory-efficient post-processing.

Result: Real-world evaluations reveal up to 2x faster processing pipeline speeds without compromising accuracy.

Conclusion: Practical strategies were developed for scalable, low-latency video analytics, enabling improvements in agriculture and other large-scale video analytics applications.

Abstract: Cost-effective and scalable video analytics are essential for precision livestock monitoring, where high-resolution footage and near-real-time monitoring needs from commercial farms generates substantial computational workloads. This paper presents a comprehensive case study on optimizing a poultry welfare monitoring system through system-level improvements across detection, tracking, clustering, and behavioral analysis modules. We introduce a set of optimizations, including multi-level parallelization, Optimizing code with substituting CPU code with GPU-accelerated code, vectorized clustering, and memory-efficient post-processing. Evaluated on real-world farm video footage, these changes deliver up to a 2x speedup across pipelines without compromising model accuracy. Our findings highlight practical strategies for building high-throughput, low-latency video inference systems that reduce infrastructure demands in agricultural and smart sensing deployments as well as other large-scale video analytics applications.

</details>


### [356] [PIR-DSN: A Decentralized Storage Network Supporting Private Information Retrieval](https://arxiv.org/abs/2512.07189)
*Jiahao Zhang,Minghui Xu,Hechuan Guo,Xiuzhen Cheng*

Main category: cs.DC

TL;DR: The paper introduces PIR-DSN, a DSN protocol that integrates Private Information Retrieval to ensure user privacy during file retrieval in decentralized systems.


<details>
  <summary>Details</summary>
Motivation: To address the critical user privacy vulnerability during file retrieval in existing Decentralized Storage Networks (DSNs).

Method: The paper proposes PIR-DSN, utilizing innovations such as secure mapping for compact indexes and Byzantine-robust private retrieval using file replication across miners. PIR-DSN supports both single and multi-server Private Information Retrieval settings.

Result: PIR-DSN demonstrates comparable overhead for file upload and deletion while incurring higher retrieval latency due to PIR but maintains comparable throughput. Evaluations show its viability against existing industrial DSN systems.

Conclusion: PIR-DSN is a practical solution for enhancing user privacy in privacy-sensitive applications within decentralized storage networks, despite the computational cost of PIR.

Abstract: Decentralized Storage Networks (DSNs) are emerging as a foundational infrastructure for Web 3.0, offering global peer-to-peer storage. However, a critical vulnerability persists: user privacy during file retrieval remains largely unaddressed, risking the exposure of sensitive information. To overcome this, we introduce PIR-DSN, the first DSN protocol to integrate Private Information Retrieval (PIR) for both single and multi-server settings. Our key innovations include a novel secure mapping method that transforms sparse file identifiers into compact integer indexes, enabling both public verifiability of file operations and efficient private retrieval. Furthermore, PIR-DSN guarantees Byzantine-robust private retrieval through file replication across multiple miners. We implement and rigorously evaluate PIR-DSN against three prominent industrial DSN systems. Experimental results demonstrate that PIR-DSN achieves comparable overhead for file upload and deletion. While PIR inherently introduces an additional computational cost leading to higher retrieval latency, PIR-DSN maintains comparable throughput. These findings underscore PIR-DSN's practical viability for privacy-sensitive applications within DSN environments.

</details>


### [357] [ContinuumConductor : Decentralized Process Mining on the Edge-Cloud Continuum](https://arxiv.org/abs/2512.07280)
*Hendrik Reiter,Janick Edinger,Martin Kabierski,Agnes Koschmider,Olaf Landsiedel,Arvid Lepsien,Xixi Lu,Andrea Marrella,Estefania Serral,Stefan Schulte,Florian Tschorsch,Matthias Weidlich,Wilhelm Hasselbring*

Main category: cs.DC

TL;DR: This paper proposes a decentralized approach for process mining in IoT edge-cloud systems by introducing a decision framework, ContinuumConductor, demonstrated in a real-world inland port use-case.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies and challenges of centralized process mining in the context of distributed, resource-constrained IoT edge-cloud infrastructures.

Method: Introduced ContinuumConductor, a layered decision framework that analyzes decentralization versus centralization trade-offs in process mining tasks and proposes criteria for guiding these decisions.

Result: Applied ContinuumConductor to a real-world use-case in inland ports, demonstrating its effectiveness in optimizing decentralized process mining tasks under real-world constraints.

Conclusion: The study provides foundational insights for privacy-aware, responsive, and resource-efficient process mining in cyber-physical and Industrial Internet of Things (IIoT) systems.

Abstract: Process mining traditionally assumes centralized event data collection and analysis. However, modern Industrial Internet of Things systems increasingly operate over distributed, resource-constrained edge-cloud infrastructures. This paper proposes a structured approach for decentralizing process mining by enabling event data to be mined directly within the IoT systems edge-cloud continuum. We introduce ContinuumConductor a layered decision framework that guides when to perform process mining tasks such as preprocessing, correlation, and discovery centrally or decentrally. Thus, enabling privacy, responsive and resource-efficient process mining. For each step in the process mining pipeline, we analyze the trade-offs of decentralization versus centralization across these layers and propose decision criteria. We demonstrate ContinuumConductor at a real-world use-case of process optimazition in inland ports. Our contributions lay the foundation for computing-aware process mining in cyber-physical and IIoT systems.

</details>


### [358] [Venus: An Efficient Edge Memory-and-Retrieval System for VLM-based Online Video Understanding](https://arxiv.org/abs/2512.07344)
*Shengyuan Ye,Bei Ouyang,Tianyi Qian,Liekang Zeng,Mu Yuan,Xiaowen Chu,Weijie Hong,Xu Chen*

Main category: cs.DC

TL;DR: Venus is a system designed for efficient online video understanding, addressing deployment constraints through edge-cloud disaggregation and achieving significant speedups while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The need for efficient deployment of vision-language models (VLMs) in online video applications, as these models often face system overhead issues.

Method: Proposes Venus, which uses an edge-cloud disaggregated system with stages for ingestion and querying, employing techniques like scene segmentation, clustering, and progressive sampling for memory construction and keyframe retrieval.

Result: Venus delivers a 15x-131x improvement in response latency compared to state-of-the-art methods, supporting real-time responses with comparable or superior reasoning accuracy.

Conclusion: Venus effectively addresses deployment inefficiencies in video understanding tasks, balancing cost, speed, and accuracy.

Abstract: Vision-language models (VLMs) have demonstrated impressive multimodal comprehension capabilities and are being deployed in an increasing number of online video understanding applications. While recent efforts extensively explore advancing VLMs' reasoning power in these cases, deployment constraints are overlooked, leading to overwhelming system overhead in real-world deployments. To address that, we propose Venus, an on-device memory-and-retrieval system for efficient online video understanding. Venus proposes an edge-cloud disaggregated architecture that sinks memory construction and keyframe retrieval from cloud to edge, operating in two stages. In the ingestion stage, Venus continuously processes streaming edge videos via scene segmentation and clustering, where the selected keyframes are embedded with a multimodal embedding model to build a hierarchical memory for efficient storage and retrieval. In the querying stage, Venus indexes incoming queries from memory, and employs a threshold-based progressive sampling algorithm for keyframe selection that enhances diversity and adaptively balances system cost and reasoning accuracy. Our extensive evaluation shows that Venus achieves a 15x-131x speedup in total response latency compared to state-of-the-art methods, enabling real-time responses within seconds while maintaining comparable or even superior reasoning accuracy.

</details>


### [359] [Communication-Efficient Serving for Video Diffusion Models with Latent Parallelism](https://arxiv.org/abs/2512.07350)
*Zhiyuan Wu,Shuai Wang,Li Chen,Kaihui Gao,Dan Li,Yanyu Ren,Qiming Zhang,Yong Wang*

Main category: cs.DC

TL;DR: The paper introduces Latent Parallelism (LP), a novel approach to video diffusion models that reduces communication overhead by up to 97% without affecting generation quality, by utilizing local spatio-temporal dependencies.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the high memory consumption and severe communication bottlenecks in video diffusion models due to their 3D spatio-temporal attention computations, requiring efficient parallelism strategies.

Method: Latent Parallelism (LP) decomposes global denoising into parallelizable sub-problems by dynamically rotating spatio-temporal partitioning dimensions. It includes a patch-aligned overlapping partition strategy and position-aware latent reconstruction to ensure quality and reduce communication.

Result: LP achieves up to 97% reduction in communication overhead across three benchmarks compared to traditional methods, while maintaining comparable video generation quality.

Conclusion: LP offers a scalable, non-intrusive parallelism approach for video diffusion models, which can integrate seamlessly with existing strategies to enhance efficiency in video generation services.

Abstract: Video diffusion models (VDMs) perform attention computation over the 3D spatio-temporal domain. Compared to large language models (LLMs) processing 1D sequences, their memory consumption scales cubically, necessitating parallel serving across multiple GPUs. Traditional parallelism strategies partition the computational graph, requiring frequent high-dimensional activation transfers that create severe communication bottlenecks. To tackle this issue, we exploit the local spatio-temporal dependencies inherent in the diffusion denoising process and propose Latent Parallelism (LP), the first parallelism strategy tailored for VDM serving. \textcolor{black}{LP decomposes the global denoising problem into parallelizable sub-problems by dynamically rotating the partitioning dimensions (temporal, height, and width) within the compact latent space across diffusion timesteps, substantially reducing the communication overhead compared to prevailing parallelism strategies.} To ensure generation quality, we design a patch-aligned overlapping partition strategy that matches partition boundaries with visual patches and a position-aware latent reconstruction mechanism for smooth stitching. Experiments on three benchmarks demonstrate that LP reduces communication overhead by up to 97\% over baseline methods while maintaining comparable generation quality. As a non-intrusive plug-in paradigm, LP can be seamlessly integrated with existing parallelism strategies, enabling efficient and scalable video generation services.

</details>


### [360] [Otus Supercomputer](https://arxiv.org/abs/2512.07401)
*Sadaf Ehtesabi,Manoar Hossain,Tobias Kenter,Andreas Krawinkel,Holger Nitsche,Lukas Ostermann,Christian Plessl,Heinrich Riebler,Stefan Rohde,Robert Schade,Michael Schwarz,Jens Simon,Nils Winnwa,Alex Wiens,Xin Wu*

Main category: cs.DC

TL;DR: Otus is a high-performance computing cluster operated in Germany, part of the NHR initiative, ranking well on Top500 and Green500 lists due to its computational and energy efficiency. The paper covers its technical details and aims to offer insights to both users and operators.


<details>
  <summary>Details</summary>
Motivation: To provide a powerful, energy-efficient computing resource within the NHR initiative while offering insights into its operation and setup for scientific users and HPC operators.

Method: The article comprehensively describes Otus' hardware, software setup, system integration, and building integration for energy-efficient operation, and it plans to keep updating these details.

Result: Otus achieved high rankings: 164th and 255th on Top500, and 5th on Green500 in June 2025, showcasing its computational power and energy efficiency.

Conclusion: Otus stands as a benchmark in efficient and powerful HPC systems, serving researchers while informing HPC cluster operators of best practices.

Abstract: Otus is a high-performance computing cluster that was launched in 2025 and is operated by the Paderborn Center for Parallel Computing (PC2) at Paderborn University in Germany. The system is part of the National High Performance Computing (NHR) initiative. Otus complements the previous supercomputer Noctua 2, offering approximately twice the computing power while retaining the three node types that were characteristic of Noctua 2: 1) CPU compute nodes with different memory capacities, 2) high-end GPU nodes, and 3) HPC-grade FPGA nodes. On the Top500 list, which ranks the 500 most powerful supercomputers in the world, Otus is in position 164 with the CPU partition and in position 255 with the GPU partition (June 2025). On the Green500 list, ranking the 500 most energy-efficient supercomputers in the world, Otus is in position 5 with the GPU partition (June 2025).
  This article provides a comprehensive overview of the system in terms of its hardware, software, system integration, and its overall integration into the data center building to ensure energy-efficient operation. The article aims to provide unique insights for scientists using the system and for other centers operating HPC clusters. The article will be continuously updated to reflect the latest system setup and measurements.

</details>


### [361] [Bandwidth-Aware Network Topology Optimization for Decentralized Learning](https://arxiv.org/abs/2512.07536)
*Yipeng Shen,Zehan Zhu,Yan Huang,Changzhi Yan,Cheng Zhuo,Jinming Xu*

Main category: cs.DC

TL;DR: The paper proposes a bandwidth-aware network topology optimization framework to enhance parameter synchronization speed in distributed learning under bandwidth constraints.


<details>
  <summary>Details</summary>
Motivation: To address the lack of consideration for bandwidth limitations in existing network topology designs for distributed learning.

Method: Introduced a maximum bandwidth allocation strategy for heterogeneous scenarios, reformulated the optimization problem as a Mixed-Integer SDP, and used an ADMM-based method with conjugate gradient for efficiency.

Result: Optimized network topologies outperformed benchmarks, achieving significant speedups and reducing training times for decentralized learning tasks.

Conclusion: Bandwidth-aware network topology optimization improves consensus speed and reduces training time in distributed learning settings, ensuring better task performance.

Abstract: Network topology is critical for efficient parameter synchronization in distributed learning over networks. However, most existing studies do not account for bandwidth limitations in network topology design. In this paper, we propose a bandwidth-aware network topology optimization framework to maximize consensus speed under edge cardinality constraints. For heterogeneous bandwidth scenarios, we introduce a maximum bandwidth allocation strategy for the edges to ensure efficient communication among nodes. By reformulating the problem into an equivalent Mixed-Integer SDP problem, we leverage a computationally efficient ADMM-based method to obtain topologies that yield the maximum consensus speed. Within the ADMM substep, we adopt the conjugate gradient method to efficiently solve large-scale linear equations to achieve better scalability. Experimental results demonstrate that the resulting network topologies outperform the benchmark topologies in terms of consensus speed, and reduce the training time required for decentralized learning tasks on real-world datasets to achieve the target test accuracy, exhibiting speedups of more than $1.11\times$ and $1.21\times$ for homogeneous and heterogeneous bandwidth settings, respectively.

</details>


### [362] [A Performance Analyzer for a Public Cloud's ML-Augmented VM Allocator](https://arxiv.org/abs/2512.07750)
*Roozbeh Bostandoost,Pooria Namyar,Siva Kesava Reddy Kakarla,Ryan Beckett,Santiago Segarra,Eli Cortez,Ankur Mallick,Kevin Hsieh,Rodrigo Fonseca,Mohammad Hajiesmaili,Behnaz Arzani*

Main category: cs.DC

TL;DR: SANJESH is a tool for analyzing the impact of machine learning models on end-to-end performance of operational cloud systems. It uses a bi-level optimization technique, solving challenges prior approaches couldn't.


<details>
  <summary>Details</summary>
Motivation: Operators lack tools to understand the impact of machine learning models and their interactions on cloud system performance.

Method: SANJESH employs bi-level optimization and introduces novel mechanisms to speed up solving performance-related queries.

Result: SANJESH applied in VM placement systems uncovered scenarios of significantly worse performance (${~4\times}$) undetected by simulation methods.

Conclusion: SANJESH demonstrates improved ability to diagnose and evaluate interactions of ML models on cloud system performance compared to existing methods.

Abstract: Many operational cloud systems use one or more machine learning models that help them achieve better efficiency and performance. But operators do not have tools to help them understand how each model and the interaction between them affect the end-to-end system performance. SANJESH is such a tool. SANJESH supports a diverse set of performance-related queries which we answer through a bi-level optimization. We invent novel mechanisms to solve this optimization more quickly. These techniques allow us to solve an optimization which prior work failed to solve even after $24$ hours.
  As a proof of concept, we apply SANJESH to an example production system that uses multiple ML models to optimize virtual machine (VM) placement. These models impact how many servers the operators uses to host VMs and the frequency with which it has to live-migrate them because the servers run out of resources. SANJESH finds scenarios where these models cause $~4\times$ worse performance than what simulation-based approaches detect.

</details>


### [363] [Designing Co-operation in Systems of Hierarchical, Multi-objective Schedulers for Stream Processing](https://arxiv.org/abs/2512.07792)
*Animesh Dangwal,Yufeng Jiang,Charlie Arnold,Jun Fan,Mohamed Bassem,Aish Rajagopal*

Main category: cs.DC

TL;DR: Meta's advanced stream processing framework handles TBs of data in seconds by leveraging efficient schedulers and a hierarchical infrastructure, now enhanced for robust load balancing to meet increasing complexity.


<details>
  <summary>Details</summary>
Motivation: To address the need for improved and proactive load balancing due to growing application complexity and user demands in Meta's stream processing ecosystem.

Method: Designing a system focused on load balancing key compute resources by integrating new schedulers into Meta’s existing hierarchical infrastructure.

Result: The proposed system enhances load balancing by enabling multiple schedulers to collaborate effectively within the infrastructure hierarchy.

Conclusion: The integration of new schedulers within the existing system improves stream processing efficiency, allowing Meta to meet evolving user demands and application complexity.

Abstract: Stream processing is a computing paradigm that supports real-time data processing for a wide variety of applications. At Meta, it's used across the company for various tasks such as deriving product insights, providing and improving user services, and enabling AI at scale for our ever-growing user base. Meta's current stream processing framework supports processing TerraBytes(TBs) of data in mere seconds. This is enabled by our efficient schedulers and multi-layered infrastructure, which allocate workloads across various compute resources, working together in hierarchies across various parts of the infrastructure. But with the ever growing complexity of applications, and user needs, areas of the infrastructure that previously required minimal load balancing, now must be made more robust and proactive to application load. In our work we explore how to build and design such a system that focuses on load balancing over key compute resources and properties of these applications. We also showcase how to integrate new schedulers into the hierarchy of the existing ones, allowing multiple schedulers to work together and perform load balancing, at their infrastructure level, effectively.

</details>


### [364] [Quantifying the Carbon Reduction of DAG Workloads: A Job Shop Scheduling Perspective](https://arxiv.org/abs/2512.07799)
*Roozbeh Bostandoost,Adam Lechowicz,Walid A. Hanafy,Prashant Shenoy,Mohammad Hajiesmaili*

Main category: cs.DC

TL;DR: The study explores carbon-aware schedulers for data centers, focusing on how leveraging task dependencies in batch workloads can decrease carbon emissions by up to 25% without increasing the total job completion time.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address inefficiencies in current carbon-aware schedulers by incorporating the specific dependencies and resource needs of smaller tasks within larger jobs to improve carbon efficiency in data centers.

Method: The authors model the scheduling problem as a flexible job-shop scheduling variant and use an offline solver to determine the upper bounds of carbon and energy savings achievable by dependency-aware approaches.

Result: The approach achieves up to 25% lower carbon emissions on average without increasing the total job completion time; however, in heterogeneous server setups, this may result in higher energy usage compared to energy-optimal schedules.

Conclusion: The study highlights the trade-offs between carbon savings, energy usage, and makespan. Allowing more flexibility in total job completion time can lead to significantly higher carbon savings, and factors such as job structure and server count play a crucial role in achievable reductions.

Abstract: Carbon-aware schedulers aim to reduce the operational carbon footprint of data centers by running flexible workloads during periods of low carbon intensity. Most schedulers treat workloads as single monolithic tasks, ignoring that many jobs, like video encoding or offline inference, consist of smaller tasks with specific dependencies and resource needs; however, knowledge of this structure enables opportunities for greater carbon efficiency.
  We quantify the maximum benefit of a dependency-aware approach for batch workloads. We model the problem as a flexible job-shop scheduling variant and use an offline solver to compute upper bounds on carbon and energy savings. Results show up to $25\%$ lower carbon emissions on average without increasing the optimal makespan (total job completion time) compared to a makespan-only baseline. Although in heterogeneous server setup, these schedules may use more energy than energy-optimal ones. Our results also show that allowing twice the optimal makespan nearly doubles the carbon savings, underscoring the tension between carbon, energy, and makespan. We also highlight key factors such as job structure and server count influence the achievable carbon reductions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [365] [A self-driving lab for solution-processed electrochromic thin films](https://arxiv.org/abs/2512.05989)
*Selma Dahms,Luca Torresi,Shahbaz Tareq Bandesha,Jan Hansmann,Holger Röhm,Alexander Colsmann,Marco Schott,Pascal Friederich*

Main category: cs.LG

TL;DR: The study focuses on leveraging self-driving laboratories using machine learning and automation to accelerate the optimization of electrochromic thin film coatings for smart windows and displays.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in optimizing solution-processed electrochromic thin film electrodes, as their performance is influenced by material properties and fabrication conditions.

Method: A self-driving laboratory combining automation, image processing, spectral analysis, and Bayesian optimization to efficiently optimize spin-coated electrochromic thin films.

Result: The system increases data throughput and enables precise exploration of processing parameters for improved electrochromic film development.

Conclusion: Self-driving laboratories can significantly enhance materials discovery and process optimization in electrochromics and other solution-processed materials.

Abstract: Solution-processed electrochromic materials offer high potential for energy-efficient smart windows and displays. Their performance varies with material choice and processing conditions. Electrochromic thin film electrodes require a smooth, defect-free coating for optimal contrast between bleached and colored states. The complexity of optimizing the spin-coated electrochromic thin layer poses challenges for rapid development. This study demonstrates the use of self-driving laboratories to accelerate the development of electrochromic coatings by coupling automation with machine learning. Our system combines automated data acquisition, image processing, spectral analysis, and Bayesian optimization to explore processing parameters efficiently. This approach not only increases throughput but also enables a pointed search for optimal processing parameters. The approach can be applied to various solution-processed materials, highlighting the potential of self-driving labs in enhancing materials discovery and process optimization.

</details>


### [366] [Memory-Amortized Inference: A Topological Unification of Search, Closure, and Structure](https://arxiv.org/abs/2512.05990)
*Xin Li*

Main category: cs.LG

TL;DR: The paper introduces Memory-Amortized Inference (MAI), a framework combining learning and memory via topological principles for computational efficiency and biological-like cognition.


<details>
  <summary>Details</summary>
Motivation: The goal is to address the inefficiencies in contemporary machine learning systems compared to biological cognition, specifically in terms of sample efficiency and thermodynamic frugality.

Method: The authors propose a theoretical framework, Memory-Amortized Inference (MAI), based on algebraic topology. It operates through the Homological Parity Principle, distinguishing dynamic context (odd-dimensional homology) and stable content (even-dimensional homology).

Result: They demonstrate how high-complexity search processes can be reduced to low-complexity lookups through topological mechanisms, particularly cycle closure.

Conclusion: The framework provides insights into cognition, linking fast-thinking and slow-thinking via topological processes and serving as a potential foundation for advanced computational architectures.

Abstract: Contemporary ML separates the static structure of parameters from the dynamic flow of inference, yielding systems that lack the sample efficiency and thermodynamic frugality of biological cognition. In this theoretical work, we propose \textbf{Memory-Amortized Inference (MAI)}, a formal framework rooted in algebraic topology that unifies learning and memory as phase transitions of a single geometric substrate. Central to our theory is the \textbf{Homological Parity Principle}, which posits a fundamental dichotomy: even-dimensional homology ($H_{even}$) physically instantiates stable \textbf{Content} (stable scaffolds or ``what''), while odd-dimensional homology ($H_{odd}$) instantiates dynamic \textbf{Context} (dynamic flows or ``where''). We derive the logical flow of MAI as a topological trinity transformation: \textbf{Search $\to$ Closure $\to$ Structure}. Specifically, we demonstrate that cognition operates by converting high-complexity recursive search (modeled by \textit{Savitch's Theorem} in NPSPACE) into low-complexity lookup (modeled by \textit{Dynamic Programming} in P) via the mechanism of \textbf{Topological Cycle Closure}. We further show that this consolidation process is governed by a topological generalization of the Wake-Sleep algorithm, functioning as a coordinate descent that alternates between optimizing the $H_{odd}$ flow (inference/wake) and condensing persistent cycles into the $H_{even}$ scaffold (learning/sleep). This framework offers a rigorous explanation for the emergence of fast-thinking (intuition) from slow-thinking (reasoning) and provides a blueprint for post-Turing architectures that compute via topological resonance.

</details>


### [367] [Deep learning recognition and analysis of Volatile Organic Compounds based on experimental and synthetic infrared absorption spectra](https://arxiv.org/abs/2512.06059)
*Andrea Della Valle,Annalisa D'Arco,Tiziana Mancini,Rosanna Mosetti,Maria Chiara Paolozzi,Stefano Lupi,Sebastiano Pilati,Andrea Perali*

Main category: cs.LG

TL;DR: The study develops a method using neural networks and augmented IR spectra data to accurately identify and quantify VOC concentrations in real-time.


<details>
  <summary>Details</summary>
Motivation: Accurate and real-time detection of VOCs in the atmosphere is crucial due to their health risks, yet traditional IR spectroscopy faces limitations in VOC recognition due to the complexity of spectra.

Method: An experimental dataset of VOCs' IR spectra was created, augmented with synthetic data using conditional generative neural networks, and used to train a discriminative neural network.

Result: The trained neural network can reliably classify nine types of VOCs and accurately predict their concentrations.

Conclusion: This work demonstrates that neural networks, bolstered by data augmentation, can enhance real-time analysis of VOCs, offering potential for integration into sensing devices.

Abstract: Volatile Organic Compounds (VOCs) are organic molecules that have low boiling points and therefore easily evaporate into the air. They pose significant risks to human health, making their accurate detection the crux of efforts to monitor and minimize exposure. Infrared (IR) spectroscopy enables the ultrasensitive detection at low-concentrations of VOCs in the atmosphere by measuring their IR absorption spectra. However, the complexity of the IR spectra limits the possibility to implement VOC recognition and quantification in real-time. While deep neural networks (NNs) are increasingly used for the recognition of complex data structures, they typically require massive datasets for the training phase. Here, we create an experimental VOC dataset for nine different classes of compounds at various concentrations, using their IR absorption spectra. To further increase the amount of spectra and their diversity in term of VOC concentration, we augment the experimental dataset with synthetic spectra created via conditional generative NNs. This allows us to train robust discriminative NNs, able to reliably identify the nine VOCs, as well as to precisely predict their concentrations. The trained NN is suitable to be incorporated into sensing devices for VOCs recognition and analysis.

</details>


### [368] [Block Sparse Flash Attention](https://arxiv.org/abs/2512.07011)
*Daniel Ohayon,Itay Lamprecht,Itay Hubara,Israel Cohen,Daniel Soudry,Noam Elata*

Main category: cs.LG

TL;DR: The paper introduces Block-Sparse FlashAttention (BSFA), which accelerates long-context processing in language models by efficiently pruning irrelevant computations while preserving output quality.


<details>
  <summary>Details</summary>
Motivation: As modern large language models increasingly rely on long contexts for reasoning and multi-document tasks, attention's quadratic complexity results in computational inefficiency.

Method: BSFA computes exact query-key similarities to identify and prune irrelevant value blocks, skipping about 50% of computation and memory transfers by using thresholds calibrated on a small dataset.

Result: BSFA achieves significant speedups (up to 1.24x) in reasoning and retrieval tasks on Llama-3.1-8B, maintaining above 99% baseline accuracy while outperforming existing sparse attention techniques.

Conclusion: BSFA is an effective and efficient solution for accelerating long-context inference in large language models, with advantages in both computational cost and model accuracy.

Abstract: Modern large language models increasingly require long contexts for reasoning and multi-document tasks, but attention's quadratic complexity creates a severe computational bottleneck. We present Block-Sparse FlashAttention (BSFA), a drop-in replacement that accelerates long-context inference while preserving model quality. Unlike methods that predict importance before computing scores, BSFA computes exact query-key similarities to select the top-k most important value blocks for each query. By comparing per-block maximum scores against calibrated thresholds, we skip approximately 50% of the computation and memory transfers for pruned blocks. Our training-free approach requires only a one-time threshold calibration on a small dataset to learn the per-layer and per-head attention score distributions. We provide a CUDA kernel implementation that can be used as a drop-in replacement for FlashAttention. On Llama-3.1-8B, BSFA achieves up to 1.10x speedup on real-world reasoning benchmarks and up to 1.24x for needle-in-a-haystack retrieval tasks while maintaining above 99% baseline accuracy, with certain configurations even improving accuracy by focusing on the most relevant content, substantially outperforming existing sparse attention methods. The implementation is available at https://github.com/Danielohayon/Block-Sparse-Flash-Attention

</details>


### [369] [When Privacy Isn't Synthetic: Hidden Data Leakage in Generative AI Models](https://arxiv.org/abs/2512.06062)
*S. M. Mustaqim,Anantaa Kotal,Paul H. Yi*

Main category: cs.LG

TL;DR: Synthetic data from generative models can still leak sensitive training data.


<details>
  <summary>Details</summary>
Motivation: To identify vulnerabilities in privacy-preserving synthetic data generation methods.

Method: A black-box membership inference attack using clustering and dense region analysis.

Result: Membership leakage occurs despite differential privacy or noise mechanisms.

Conclusion: Synthetic data pipelines need stronger privacy guarantees focused on distributional neighborhood inference.

Abstract: Generative models are increasingly used to produce privacy-preserving synthetic data as a safe alternative to sharing sensitive training datasets. However, we demonstrate that such synthetic releases can still leak information about the underlying training samples through structural overlap in the data manifold. We propose a black-box membership inference attack that exploits this vulnerability without requiring access to model internals or real data. The attacker repeatedly queries the generative model to obtain large numbers of synthetic samples, performs unsupervised clustering to identify dense regions of the synthetic distribution, and then analyzes cluster medoids and neighborhoods that correspond to high-density regions in the original training data. These neighborhoods act as proxies for training samples, enabling the adversary to infer membership or reconstruct approximate records. Our experiments across healthcare, finance, and other sensitive domains show that cluster overlap between real and synthetic data leads to measurable membership leakage-even when the generator is trained with differential privacy or other noise mechanisms. The results highlight an under-explored attack surface in synthetic data generation pipelines and call for stronger privacy guarantees that account for distributional neighborhood inference rather than sample-level memorization alone, underscoring its role in privacy-preserving data publishing. Implementation and evaluation code are publicly available at:github.com/Cluster-Medoid-Leakage-Attack.

</details>


### [370] [Physics-Informed Neural Koopman Machine for Interpretable Longitudinal Personalized Alzheimer's Disease Forecasting](https://arxiv.org/abs/2512.06134)
*Georgi Hrusanov,Duy-Thanh Vu,Duy-Cat Can,Sophie Tascedda,Margaret Ryan,Julien Bodelet,Katarzyna Koscielska,Carsten Magnus,Oliver Y. Chén*

Main category: cs.LG

TL;DR: This paper introduces the Neural Koopman Machine (NKM), a novel machine learning model designed for interpreting and predicting personalized cognitive decline in Alzheimer's Disease using multimodal data.


<details>
  <summary>Details</summary>
Motivation: To address challenges in integrating multimodal data for longitudinal personalized forecasting in Alzheimer's Disease while maintaining interpretability.

Method: The Neural Koopman Machine (NKM) leverages dynamical systems and hierarchical attention mechanisms to integrate multimodal genetic, neuroimaging, proteomic, and demographic data. Using the Koopman operator framework, it transforms nonlinear trajectories into interpretable linear representations.

Result: NKM outperforms traditional machine learning and deep learning models in forecasting cognitive decline. It predicts changes across multiple cognitive scores, quantifies biomarker contributions, and identifies brain regions predictive of cognitive deterioration.

Conclusion: NKM offers an interpretable and effective forecasting tool for personalized cognitive decline in Alzheimer's Disease, advancing understanding of multimodal biological factors underlying disease progression.

Abstract: Early forecasting of individual cognitive decline in Alzheimer's disease (AD) is central to disease evaluation and management. Despite advances, it is as of yet challenging for existing methodological frameworks to integrate multimodal data for longitudinal personalized forecasting while maintaining interpretability. To address this gap, we present the Neural Koopman Machine (NKM), a new machine learning architecture inspired by dynamical systems and attention mechanisms, designed to forecast multiple cognitive scores simultaneously using multimodal genetic, neuroimaging, proteomic, and demographic data. NKM integrates analytical ($α$) and biological ($β$) knowledge to guide feature grouping and control the hierarchical attention mechanisms to extract relevant patterns. By implementing Fusion Group-Aware Hierarchical Attention within the Koopman operator framework, NKM transforms complex nonlinear trajectories into interpretable linear representations. To demonstrate NKM's efficacy, we applied it to study the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Our results suggest that NKM consistently outperforms both traditional machine learning methods and deep learning models in forecasting trajectories of cognitive decline. Specifically, NKM (1) forecasts changes of multiple cognitive scores simultaneously, (2) quantifies differential biomarker contributions to predicting distinctive cognitive scores, and (3) identifies brain regions most predictive of cognitive deterioration. Together, NKM advances personalized, interpretable forecasting of future cognitive decline in AD using past multimodal data through an explainable, explicit system and reveals potential multimodal biological underpinnings of AD progression.

</details>


### [371] [JaxWildfire: A GPU-Accelerated Wildfire Simulator for Reinforcement Learning](https://arxiv.org/abs/2512.06102)
*Ufuk Çakır,Victor-Alexandru Darvariu,Bruno Lacerda,Nick Hawes*

Main category: cs.LG

TL;DR: The paper introduces JaxWildfire, a fast wildfire simulator in JAX enabling efficient reinforcement learning (RL) for wildfire management.


<details>
  <summary>Details</summary>
Motivation: The need for effective training methods to improve uncertain decision-making scenarios in managing wildfires and overcoming current simulator speed limitations.

Method: Developing JaxWildfire, a wildfire simulator built on cellular automata with vectorized simulations on GPUs using JAX.

Result: JaxWildfire achieves significant speed upgrades (6-35x faster than existing tools) and successfully trains RL agents for wildfire suppression.

Conclusion: JaxWildfire facilitates advancements in RL for managing natural hazards, addressing limitations of current simulation tools.

Abstract: Artificial intelligence methods are increasingly being explored for managing wildfires and other natural hazards. In particular, reinforcement learning (RL) is a promising path towards improving outcomes in such uncertain decision-making scenarios and moving beyond reactive strategies. However, training RL agents requires many environment interactions, and the speed of existing wildfire simulators is a severely limiting factor. We introduce $\texttt{JaxWildfire}$, a simulator underpinned by a principled probabilistic fire spread model based on cellular automata. It is implemented in JAX and enables vectorized simulations using $\texttt{vmap}$, allowing high throughput of simulations on GPUs. We demonstrate that $\texttt{JaxWildfire}$ achieves 6-35x speedup over existing software and enables gradient-based optimization of simulator parameters. Furthermore, we show that $\texttt{JaxWildfire}$ can be used to train RL agents to learn wildfire suppression policies. Our work is an important step towards enabling the advancement of RL techniques for managing natural hazards.

</details>


### [372] [ARC-AGI Without Pretraining](https://arxiv.org/abs/2512.06104)
*Isaac Liao,Albert Gu*

Main category: cs.LG

TL;DR: This paper introduces CompressARC, a small model solving 20% of ARC-AGI puzzles without pretraining, using the principle of minimizing description length (MDL).


<details>
  <summary>Details</summary>
Motivation: To challenge the prevailing notion that solving ARC-AGI puzzles requires massive pretraining and explore alternative methods for achieving generalization.

Method: The authors introduce CompressARC, a 76K parameter model that minimizes the description length of puzzles during inference, solving them without pretraining or training on the provided ARC-AGI training set.

Result: CompressARC solves 20% of ARC-AGI puzzles under highly data-limited conditions, achieving significant generalization despite not relying on pretraining.

Conclusion: The results demonstrate that intelligence can emerge through the MDL principle as an alternative to massive pretraining, providing a new perspective on solving such visual puzzles.

Abstract: Conventional wisdom in the age of LLMs dictates that solving IQ-test-like visual puzzles from the ARC-AGI-1 benchmark requires capabilities derived from massive pretraining. To counter this, we introduce CompressARC, a 76K parameter model without any pretraining that solves 20% of evaluation puzzles by minimizing the description length (MDL) of the target puzzle purely during inference time. The MDL endows CompressARC with extreme generalization abilities typically unheard of in deep learning. To our knowledge, CompressARC is the only deep learning method for ARC-AGI where training happens only on a single sample: the target inference puzzle itself, with the final solution information removed. Moreover, CompressARC does not train on the pre-provided ARC-AGI "training set". Under these extremely data-limited conditions, we do not ordinarily expect any puzzles to be solvable at all. Yet CompressARC still solves a diverse distribution of creative ARC-AGI puzzles, suggesting MDL to be an alternative feasible way to produce intelligence, besides conventional pretraining.

</details>


### [373] [A Prescriptive Framework for Determining Optimal Days for Short-Term Traffic Counts](https://arxiv.org/abs/2512.06111)
*Arthur Mukwaya,Nancy Kasamala,Nana Kankam Gyimah,Judith Mwakalonge,Gurcan Comert,Saidi Siuhi,Denis Ruganuza,Mark Ngotonie*

Main category: cs.LG

TL;DR: This paper introduces a new machine learning approach to optimize short-duration traffic data collection for improved AADT prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: State DOTs face challenges in obtaining accurate AADT, especially for roads without continuous traffic monitoring, prompting the need for alternative methods.

Method: The methodology involves utilizing Texas traffic volume data, iteratively selecting optimal representative days, and applying leave-one-out techniques for unbiased feature generation.

Result: The proposed approach significantly outperformed the baseline, achieving reduced prediction errors (e.g., lower RMSE, MAE, MAPE, and higher R^2 values).

Conclusion: The study provides a cost-effective, accurate alternative for DOTs to enhance AADT estimation, ensuring compliance with monitoring requirements and reducing operational costs.

Abstract: The Federal Highway Administration (FHWA) mandates that state Departments of Transportation (DOTs) collect reliable Annual Average Daily Traffic (AADT) data. However, many U.S. DOTs struggle to obtain accurate AADT, especially for unmonitored roads. While continuous count (CC) stations offer accurate traffic volume data, their implementation is expensive and difficult to deploy widely, compelling agencies to rely on short-duration traffic counts. This study proposes a machine learning framework, the first to our knowledge, to identify optimal representative days for conducting short count (SC) data collection to improve AADT prediction accuracy. Using 2022 and 2023 traffic volume data from the state of Texas, we compare two scenarios: an 'optimal day' approach that iteratively selects the most informative days for AADT estimation and a 'no optimal day' baseline reflecting current practice by most DOTs. To align with Texas DOT's traffic monitoring program, continuous count data were utilized to simulate the 24 hour short counts. The actual field short counts were used to enhance feature engineering through using a leave-one-out (LOO) technique to generate unbiased representative daily traffic features across similar road segments. Our proposed methodology outperforms the baseline across the top five days, with the best day (Day 186) achieving lower errors (RMSE: 7,871.15, MAE: 3,645.09, MAPE: 11.95%) and higher R^2 (0.9756) than the baseline (RMSE: 11,185.00, MAE: 5,118.57, MAPE: 14.42%, R^2: 0.9499). This research offers DOTs an alternative to conventional short-duration count practices, improving AADT estimation, supporting Highway Performance Monitoring System compliance, and reducing the operational costs of statewide traffic data collection.

</details>


### [374] [QL-LSTM: A Parameter-Efficient LSTM for Stable Long-Sequence Modeling](https://arxiv.org/abs/2512.06582)
*Isaac Kofi Nti*

Main category: cs.LG

TL;DR: The paper introduces the Quantum-Leap LSTM (QL-LSTM), which addresses parameter redundancy and long-term information retention issues in traditional LSTMs, achieving competitive accuracy while reducing parameters significantly.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of traditional LSTMs, which suffer from redundant parameters and diminished ability to retain long-range information.

Method: QL-LSTM introduces two key mechanisms: Parameter-Shared Unified Gating (PSUG) to reduce redundant parameters and Hierarchical Gated Recurrence with Additive Skip Connections (HGR-ASC) to enhance long-term information flow, evaluated on the IMDB sentiment classification dataset.

Result: QL-LSTM delivers competitive accuracy while reducing the number of parameters by approximately 48 percent compared to traditional models.

Conclusion: While QL-LSTM improves efficiency and accuracy, further kernel-level optimization is needed to achieve real-time speed improvements due to its sequential nature.

Abstract: Recurrent neural architectures such as LSTM and GRU remain widely used in sequence modeling, but they continue to face two core limitations: redundant gate-specific parameters and reduced ability to retain information across long temporal distances. This paper introduces the Quantum-Leap LSTM (QL-LSTM), a recurrent architecture designed to address both challenges through two independent components. The Parameter-Shared Unified Gating mechanism replaces all gate-specific transformations with a single shared weight matrix, reducing parameters by approximately 48 percent while preserving full gating behavior. The Hierarchical Gated Recurrence with Additive Skip Connections component adds a multiplication-free pathway that improves long-range information flow and reduces forget-gate degradation. We evaluate QL-LSTM on sentiment classification using the IMDB dataset with extended document lengths, comparing it to LSTM, GRU, and BiLSTM reference models. QL-LSTM achieves competitive accuracy while using substantially fewer parameters. Although the PSUG and HGR-ASC components are more efficient per time step, the current prototype remains limited by the inherent sequential nature of recurrent models and therefore does not yet yield wall-clock speed improvements without further kernel-level optimization.

</details>


### [375] [gp2Scale: A Class of Compactly-Supported Non-Stationary Kernels and Distributed Computing for Exact Gaussian Processes on 10 Million Data Points](https://arxiv.org/abs/2512.06143)
*Marcus M. Noack,Mark D. Risser,Hengrui Luo,Vardaan Tekriwal,Ronald J. Pandolfi*

Main category: cs.LG

TL;DR: The paper introduces gp2Scale, a method enabling exact Gaussian processes on over 10 million data points without relying on common approximations, using flexible and non-stationary kernel designs to exploit sparse covariance structures.


<details>
  <summary>Details</summary>
Motivation: To overcome the trade-off in Gaussian processes between computational speed, accuracy, and its loss of customizability, especially in applications requiring expressive non-stationary kernels.

Method: The proposed gp2Scale method uses compactly supported and non-stationary kernels to find sparse structures in covariance matrices, enabling fast computation without approximations like inducing points or kernel interpolation.

Result: gp2Scale achieves superior approximation performance on multiple real-world datasets, demonstrating scalability and flexibility without compromising accuracy or customizability.

Conclusion: gp2Scale eliminates traditional trade-offs in Gaussian process scalability while maintaining robust performance and flexibility for modern applications, making it suitable for a variety of inputs and complex designs.

Abstract: Despite a large corpus of recent work on scaling up Gaussian processes, a stubborn trade-off between computational speed, prediction and uncertainty quantification accuracy, and customizability persists. This is because the vast majority of existing methodologies exploit various levels of approximations that lower accuracy and limit the flexibility of kernel and noise-model designs -- an unacceptable drawback at a time when expressive non-stationary kernels are on the rise in many fields. Here, we propose a methodology we term \emph{gp2Scale} that scales exact Gaussian processes to more than 10 million data points without relying on inducing points, kernel interpolation, or neighborhood-based approximations, and instead leveraging the existing capabilities of a GP: its kernel design. Highly flexible, compactly supported, and non-stationary kernels lead to the identification of naturally occurring sparse structure in the covariance matrix, which is then exploited for the calculations of the linear system solution and the log-determinant for training. We demonstrate our method's functionality on several real-world datasets and compare it with state-of-the-art approximation algorithms. Although we show superior approximation performance in many cases, the method's real power lies in its agnosticism toward arbitrary GP customizations -- core kernel design, noise, and mean functions -- and the type of input space, making it optimally suited for modern Gaussian process applications.

</details>


### [376] [Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics](https://arxiv.org/abs/2512.06737)
*Nikhil Verma,Joonas Linnosmaa,Espinosa-Leal Leonardo,Napat Vajragupta*

Main category: cs.LG

TL;DR: This paper introduces the ArcGD optimizer, evaluates it on challenging benchmarks and real-world ML tasks, and finds it outperforms or matches state-of-the-art optimizers like Adam, SGD, and Lion in terms of convergence and generalization.


<details>
  <summary>Details</summary>
Motivation: To address the need for more effective optimizers in ML tasks that can generalize better, resist overfitting, and handle challenging optimization landscapes.

Method: The authors formulated and implemented the ArcGD optimizer and evaluated it against state-of-the-art optimizers on a non-convex benchmark function (Rosenbrock function) as well as on the CIFAR-10 image classification task across diverse architectures.

Result: ArcGD outperformed Adam in handling the highly challenging 50,000D non-convex function and achieved the highest average test accuracy (50.7%) on CIFAR-10 across diverse architectures, showcasing its generalization ability and resistance to overfitting.

Conclusion: ArcGD shows promising results on both geometric and ML benchmarks. It not only demonstrates broad applicability but also hints at deeper connections with existing optimizers, such as Lion, warranting future exploration.

Abstract: The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved super ior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a variant of ArcGD can be interpreted as a special case of the Lion optimiser, highlighting connections between the inherent mechanisms of such optimisation methods.

</details>


### [377] [Learning Invariant Graph Representations Through Redundant Information](https://arxiv.org/abs/2512.06154)
*Barproda Halder,Pasan Dissanayake,Sanghamitra Dutta*

Main category: cs.LG

TL;DR: The paper presents the RIG framework which uses Partial Information Decomposition to focus on redundant information in graph representations for improved out-of-distribution generalization.


<details>
  <summary>Details</summary>
Motivation: Existing invariant graph representation learning approaches struggle with retaining spurious components, which hinders OOD generalization.

Method: Introduced the concept of Partial Information Decomposition (PID) and developed a multi-level optimization framework, Redundancy-guided Invariant Graph learning (RIG). This framework alternates between estimating a lower bound of redundant information and maximizing it alongside additional objectives.

Result: The RIG framework demonstrated its superior generalization capabilities across synthetic and real-world graph datasets under various distribution shifts.

Conclusion: The study establishes PID as a valuable tool for invariant representation learning and showcases the effectiveness of redundancy-guided methods in achieving OOD generalization.

Abstract: Learning invariant graph representations for out-of-distribution (OOD) generalization remains challenging because the learned representations often retain spurious components. To address this challenge, this work introduces a new tool from information theory called Partial Information Decomposition (PID) that goes beyond classical information-theoretic measures. We identify limitations in existing approaches for invariant representation learning that solely rely on classical information-theoretic measures, motivating the need to precisely focus on redundant information about the target $Y$ shared between spurious subgraphs $G_s$ and invariant subgraphs $G_c$ obtained via PID. Next, we propose a new multi-level optimization framework that we call -- Redundancy-guided Invariant Graph learning (RIG) -- that maximizes redundant information while isolating spurious and causal subgraphs, enabling OOD generalization under diverse distribution shifts. Our approach relies on alternating between estimating a lower bound of redundant information (which itself requires an optimization) and maximizing it along with additional objectives. Experiments on both synthetic and real-world graph datasets demonstrate the generalization capabilities of our proposed RIG framework.

</details>


### [378] [Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search](https://arxiv.org/abs/2512.07142)
*Tanay Arora,Christof Teuscher*

Main category: cs.LG

TL;DR: The study addresses the inefficiencies in Lottery Ticket Rewinding (LTR) and the accuracy-sparsity trade-off of Pruning-at-Initialization (PaI) by introducing Concrete Ticket Search (CTS), which identifies effective sparse subnetworks efficiently.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and accuracy of detecting sparse, trainable subnetworks in neural networks, overcoming the computational challenges of LTR and the shortcomings of PaI techniques.

Method: CTS employs a holistic optimization approach using Concrete relaxation and a gradient balancing scheme (GRADBALANCE). It also proposes pruning objectives inspired by knowledge distillation, such as minimizing reverse Kullback-Leibler divergence.

Result: CTS significantly reduces computation time while producing subnetworks that perform better or equally as well as LTR, particularly at high sparsity levels. It also surpasses saliency-based PaI methods in performance.

Conclusion: CTS effectively combines efficiency and accuracy, offering a superior method for sparse subnetwork discovery, especially in highly sparse scenarios.

Abstract: The Lottery Ticket Hypothesis asserts the existence of highly sparse, trainable subnetworks ('winning tickets') within dense, randomly initialized neural networks. However, state-of-the-art methods of drawing these tickets, like Lottery Ticket Rewinding (LTR), are computationally prohibitive, while more efficient saliency-based Pruning-at-Initialization (PaI) techniques suffer from a significant accuracy-sparsity trade-off and fail basic sanity checks. In this work, we argue that PaI's reliance on first-order saliency metrics, which ignore inter-weight dependencies, contributes substantially to this performance gap, especially in the sparse regime. To address this, we introduce Concrete Ticket Search (CTS), an algorithm that frames subnetwork discovery as a holistic combinatorial optimization problem. By leveraging a Concrete relaxation of the discrete search space and a novel gradient balancing scheme (GRADBALANCE) to control sparsity, CTS efficiently identifies high-performing subnetworks near initialization without requiring sensitive hyperparameter tuning. Motivated by recent works on lottery ticket training dynamics, we further propose a knowledge distillation-inspired family of pruning objectives, finding that minimizing the reverse Kullback-Leibler divergence between sparse and dense network outputs (CTS-KL) is particularly effective. Experiments on varying image classification tasks show that CTS produces subnetworks that robustly pass sanity checks and achieve accuracy comparable to or exceeding LTR, while requiring only a small fraction of the computation. For example, on ResNet-20 on CIFAR10, it reaches 99.3% sparsity with 74.0% accuracy in 7.9 minutes, while LTR attains the same sparsity with 68.3% accuracy in 95.2 minutes. CTS's subnetworks outperform saliency-based methods across all sparsities, but its advantage over LTR is most pronounced in the highly sparse regime.

</details>


### [379] [PMA-Diffusion: A Physics-guided Mask-Aware Diffusion Framework for TSE from Sparse Observations](https://arxiv.org/abs/2512.06183)
*Lindong Liu,Zhixiong Jin,Seongjin Choi*

Main category: cs.LG

TL;DR: PMA-Diffusion is a novel framework to estimate highway traffic states from sparse and noisy traffic data using guided diffusion techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of sparse and noisy traffic data, ensuring high-resolution traffic state estimation necessary for Intelligent Transportation Systems.

Method: The PMA-Diffusion framework reconstructs missing highway speed fields using mask-aware diffusion training approaches (Single-Mask and Double-Mask) and physics-guided posterior samplers.

Result: The method was tested on the I-24 MOTION dataset, showing superior reconstruction performance under extreme sparsity (5% visibility) compared to other baselines.

Conclusion: The combination of mask-aware diffusion priors with physics-guided posterior samplers proves to be an effective solution for traffic state estimation in conditions of sparse data.

Abstract: High-resolution highway traffic state information is essential for Intelligent Transportation Systems, but typical traffic data acquired from loop detectors and probe vehicles are often too sparse and noisy to capture the detailed dynamics of traffic flow. We propose PMA-Diffusion, a physics-guided mask-aware diffusion framework that reconstructs unobserved highway speed fields from sparse, incomplete observations. Our approach trains a diffusion prior directly on sparsely observed speed fields using two mask-aware training strategies: Single-Mask and Double-Mask. At the inference phase, the physics-guided posterior sampler alternates reverse-diffusion updates, observation projection, and physics-guided projection based on adaptive anisotropic smoothing to reconstruct the missing speed fields. The proposed framework is tested on the I-24 MOTION dataset with varying visibility ratios. Even under severe sparsity, with only 5% visibility, PMA-Diffusion outperforms other baselines across three reconstruction error metrics. Furthermore, PMA-diffusion trained with sparse observation nearly matches the performance of the baseline model trained on fully observed speed fields. The results indicate that combining mask-aware diffusion priors with a physics-guided posterior sampler provides a reliable and flexible solution for traffic state estimation under realistic sensing sparsity.

</details>


### [380] [How Should We Evaluate Data Deletion in Graph-Based ANN Indexes?](https://arxiv.org/abs/2512.06200)
*Tomohiro Yamashita,Daichi Amagata,Yusuke Matsui*

Main category: cs.LG

TL;DR: The paper proposes a framework to evaluate data deletion methods in Approximate Nearest Neighbor Search (ANNS) using practical use cases and suggests a novel method called Deletion Control.


<details>
  <summary>Details</summary>
Motivation: ANNS is widely used in applications like Retrieval-Augmented Generation, and there exists a lack of a standardized evaluation methodology for dynamic data deletion in such searches.

Method: The paper categorizes and mathematically formalizes data deletion methods in graph-based ANNS, proposes comprehensive evaluation metrics, and applies them practically using Hierarchical Navigable Small World.

Result: The study provides an analysis of the effects of data deletion on ANNS performance and introduces Deletion Control, which dynamically selects optimal deletion methods based on search accuracy requirements.

Conclusion: This framework and its evaluation enable a better understanding of data deletion in ANNS and offer practical solutions for dynamic data support.

Abstract: Approximate Nearest Neighbor Search (ANNS) has recently gained significant attention due to its many applications, such as Retrieval-Augmented Generation. Such applications require ANNS algorithms that support dynamic data, so the ANNS problem on dynamic data has attracted considerable interest. However, a comprehensive evaluation methodology for data deletion in ANNS has yet to be established. This study proposes an experimental framework and comprehensive evaluation metrics to assess the efficiency of data deletion for ANNS indexes under practical use cases. Specifically, we categorize data deletion methods in graph-based ANNS into three approaches and formalize them mathematically. The performance is assessed in terms of accuracy, query speed, and other relevant metrics. Finally, we apply the proposed evaluation framework to Hierarchical Navigable Small World, one of the state-of-the-art ANNS methods, to analyze the effects of data deletion, and propose Deletion Control, a method which dynamically selects the appropriate deletion method under a required search accuracy.

</details>


### [381] [K2-V2: A 360-Open, Reasoning-Enhanced LLM](https://arxiv.org/abs/2512.06201)
*K2 Team,Zhengzhong Liu,Liping Tang,Linghao Jin,Haonan Li,Nikhil Ranjan,Desai Fan,Shaurya Rohatgi,Richard Fan,Omkar Pangarkar,Huijuan Wang,Zhoujun Cheng,Suqi Sun,Seungwook Han,Bowen Tan,Gurpreet Gosal,Xudong Han,Varad Pimpalkhute,Shibo Hao,Ming Shan Hee,Joel Hestness,Haolong Jia,Liqun Ma,Aaryamonvikram Singh,Daria Soboleva,Natalia Vassilieva,Renxi Wang,Yingquan Wu,Yuekai Sun,Taylor Killian,Alexander Moreno,John Maggs,Hector Ren,Guowei He,Hongyi Wang,Xuezhe Ma,Yuqi Wang,Mikhail Yurochkin,Eric P. Xing*

Main category: cs.LG

TL;DR: K2-V2 is an advanced open large language model designed for reasoning and other tasks, rivaling top-performing models in its class.


<details>
  <summary>Details</summary>
Motivation: To create a 360-open large language model (LLM) that excels in reasoning adaptation, long-context understanding, and tool use, beyond general LLM functions like conversation and knowledge retrieval.

Method: The model was built from scratch and infused with domain knowledge, reasoning, and tool capabilities during training. Complete training data and history are shared for transparency and community use.

Result: K2-V2 rivals leading models in its size class, surpasses Qwen2.5-72B, and approaches Qwen3-235B in performance, establishing a strong baseline for further tuning.

Conclusion: K2-V2's open approach, superior reasoning capabilities, and shared resources create a powerful foundation for community-driven advancements in LLM development.

Abstract: We introduce K2-V2, a 360-open LLM built from scratch as a superior base for reasoning adaptation, in addition to functions such as conversation and knowledge retrieval from general LLMs. It stands as the strongest fully open model, rivals open-weight leaders in its size class, outperforms Qwen2.5-72B and approaches the performance of Qwen3-235B. We actively infuse domain knowledge, reasoning, long-context, and tool use throughout the training process. This explicitly prepares the model for complex reasoning tasks. We demonstrate this potential using simple supervised fine-tuning, establishing a strong baseline that indicates significant headroom for advanced alignment. By releasing the full training history and data composition, we maximize the effectiveness of continuous training, a key open source production scenario. We release the model weights and signature LLM360 artifacts, such as complete training data, to empower the community with a capable, reasoning-centric foundation.

</details>


### [382] [KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models](https://arxiv.org/abs/2512.07437)
*Chenwei Shi,Xueyu Luan*

Main category: cs.LG

TL;DR: This paper introduces KAN-Dreamer, a hybrid algorithm combining KAN architectures with the DreamerV3 reinforcement learning framework to enhance efficiency and interpretability.


<details>
  <summary>Details</summary>
Motivation: To improve parameter efficiency and interpretability in reinforcement learning frameworks by integrating Kolmogorov-Arnold Networks (KANs) into DreamerV3.

Method: KAN-Dreamer incorporates KAN and FastKAN layers into DreamerV3's architecture, replacing specific elements like MLPs and convolutional layers. The approach is tested on three subsystems and includes a fully vectorized JAX implementation.

Result: Experimental results on the DeepMind Control Suite (walker_walk task) show that the adapted FastKAN maintains comparable performance in sample efficiency and training speed to the original MLP-based DreamerV3.

Conclusion: KAN-Dreamer achieves equivalent performance to DreamerV3 while introducing improved parameter efficiency and interpretability, setting the stage for further exploration of KAN-based models in reinforcement learning.

Abstract: DreamerV3 is a state-of-the-art online model-based reinforcement learning (MBRL) algorithm known for remarkable sample efficiency. Concurrently, Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to Multi-Layer Perceptrons (MLPs), offering superior parameter efficiency and interpretability. To mitigate KANs' computational overhead, variants like FastKAN leverage Radial Basis Functions (RBFs) to accelerate inference. In this work, we investigate integrating KAN architectures into the DreamerV3 framework. We introduce KAN-Dreamer, replacing specific MLP and convolutional components of DreamerV3 with KAN and FastKAN layers. To ensure efficiency within the JAX-based World Model, we implement a tailored, fully vectorized version with simplified grid management. We structure our investigation into three subsystems: Visual Perception, Latent Prediction, and Behavior Learning. Empirical evaluations on the DeepMind Control Suite (walker_walk) analyze sample efficiency, training time, and asymptotic performance. Experimental results demonstrate that utilizing our adapted FastKAN as a drop-in replacement for the Reward and Continue predictors yields performance on par with the original MLP-based architecture, maintaining parity in both sample efficiency and training speed. This report serves as a preliminary study for future developments in KAN-based world models.

</details>


### [383] [A-3PO: Accelerating Asynchronous LLM Training with Staleness-aware Proximal Policy Approximation](https://arxiv.org/abs/2512.06547)
*Xiaocan Li,Shiliang Wu,Zheng Shen*

Main category: cs.LG

TL;DR: The paper introduces A-3PO, a reinforcement learning algorithm that replaces the computationally expensive proximal policy approximation with a simple interpolation, significantly reducing training time.


<details>
  <summary>Details</summary>
Motivation: To address computational inefficiency caused by the proximal policy in decoupled loss reinforcement learning, particularly in large language models.

Method: A-3PO relies on interpolated approximation of the proximal policy rather than requiring its explicit computation, reducing computational overhead without sacrificing performance.

Result: A-3PO cuts training time by 18% while maintaining performance levels comparable to traditional approaches.

Conclusion: A-3PO simplifies reinforcement learning training by replacing computationally costly steps with efficient approximations, offering a faster method without performance trade-offs.

Abstract: Decoupled loss has been a successful reinforcement learning (RL) algorithm to deal with the high data staleness under the asynchronous RL setting. Decoupled loss improves coupled-loss style of algorithms' (e.g., PPO, GRPO) learning stability by introducing a proximal policy to decouple the off-policy corrections (importance weight) from the controlling policy updates (trust region). However, the proximal policy requires an extra forward pass through the network at each training step, creating a computational bottleneck for large language models. We observe that since the proximal policy only serves as a trust region anchor between the behavior and target policies, we can approximate it through simple interpolation without explicit computation. We call this approach A-3PO (APproximated Proximal Policy Optimization). A-3PO eliminates this overhead, reducing training time by 18% while maintaining comparable performance. Code & off-the-shelf example are available at: https://github.com/inclusionAI/AReaL/blob/main/docs/algorithms/prox_approx.md

</details>


### [384] [Quantifying Memory Use in Reinforcement Learning with Temporal Range](https://arxiv.org/abs/2512.06204)
*Rodney Lafuente-Mercado,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: The paper introduces Temporal Range, a metric analyzing how reinforcement learning policies utilize past observations in decision-making.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify the extent of memory usage of trained RL policies, providing insights into task-level memory and agent-environment dynamics.

Method: Temporal Range uses reverse-mode automatic differentiation to compute sensitivities of vector outputs over a temporal window, summarized by magnitude-weighted average lag.

Result: Temporal Range effectively scales with ground-truth lag in memory-intensive tasks and aligns with optimal history window sizes, validated across diagnostic tasks with various RL architectures.

Conclusion: Temporal Range is a practical metric for measuring agent memory dependence, useful for comparing agents and environments and selecting appropriate context lengths.

Abstract: How much does a trained RL policy actually use its past observations? We propose \emph{Temporal Range}, a model-agnostic metric that treats first-order sensitivities of multiple vector outputs across a temporal window to the input sequence as a temporal influence profile and summarizes it by the magnitude-weighted average lag. Temporal Range is computed via reverse-mode automatic differentiation from the Jacobian blocks $\partial y_s/\partial x_t\in\mathbb{R}^{c\times d}$ averaged over final timesteps $s\in\{t+1,\dots,T\}$ and is well-characterized in the linear setting by a small set of natural axioms. Across diagnostic and control tasks (POPGym; flicker/occlusion; Copy-$k$) and architectures (MLPs, RNNs, SSMs), Temporal Range (i) remains small in fully observed control, (ii) scales with the task's ground-truth lag in Copy-$k$, and (iii) aligns with the minimum history window required for near-optimal return as confirmed by window ablations. We also report Temporal Range for a compact Long Expressive Memory (LEM) policy trained on the task, using it as a proxy readout of task-level memory. Our axiomatic treatment draws on recent work on range measures, specialized here to temporal lag and extended to vector-valued outputs in the RL setting. Temporal Range thus offers a practical per-sequence readout of memory dependence for comparing agents and environments and for selecting the shortest sufficient context.

</details>


### [385] [Average-reward reinforcement learning in semi-Markov decision processes via relative value iteration](https://arxiv.org/abs/2512.06218)
*Huizhen Yu,Yi Wan,Richard S. Sutton*

Main category: cs.LG

TL;DR: This paper proves convergence for RVI Q-learning in average-reward semi-Markov decision processes and introduces improved estimation conditions for optimal reward rates.


<details>
  <summary>Details</summary>
Motivation: The integration of asynchronous stochastic approximation into reinforcement learning for better convergence in average-reward scenarios of semi-Markov decision processes.

Method: Employing asynchronous SA in the Borkar-Meyn framework, the study establishes conditions for RVI Q-learning's stability and convergence under average-reward semi-Markov decision processes.

Result: The RVI Q-learning algorithm converges almost surely to appropriate solutions of the average-reward optimality equation, with uniqueness under specific conditions. Additionally, new monotonicity conditions for estimating the optimal reward rate were introduced.

Conclusion: This work broadens the algorithmic scope of average-reward reinforcement learning and strengthens the theoretical guarantees of RVI Q-learning under asynchronous settings.

Abstract: This paper applies the authors' recent results on asynchronous stochastic approximation (SA) in the Borkar-Meyn framework to reinforcement learning in average-reward semi-Markov decision processes (SMDPs). We establish the convergence of an asynchronous SA analogue of Schweitzer's classical relative value iteration algorithm, RVI Q-learning, for finite-space, weakly communicating SMDPs. In particular, we show that the algorithm converges almost surely to a compact, connected subset of solutions to the average-reward optimality equation, with convergence to a unique, sample path-dependent solution under additional stepsize and asynchrony conditions. Moreover, to make full use of the SA framework, we introduce new monotonicity conditions for estimating the optimal reward rate in RVI Q-learning. These conditions substantially expand the previously considered algorithmic framework and are addressed through novel arguments in the stability and convergence analysis of RVI Q-learning.

</details>


### [386] [Back to Author Console Empowering GNNs for Domain Adaptation via Denoising Target Graph](https://arxiv.org/abs/2512.06236)
*Haiyang Yu,Meng-Chieh Lee,Xiang song,Qi Zhu,Christos Faloutsos*

Main category: cs.LG

TL;DR: The paper addresses the challenge of node classification on graphs with domain adaptation, proposing a framework called GraphDeT that integrates an auxiliary edge denoising task to improve graph neural networks' (GNNs) performance on target graphs.


<details>
  <summary>Details</summary>
Motivation: Graph Neural Networks often perform poorly on target graphs due to domain shifts in graph structures caused by factors like temporal or regional variances. This paper aims to enhance GNN generalization on target graphs by leveraging both the source and target graphs' structures and source labels.

Method: The authors propose the GraphDeT framework, which incorporates an auxiliary edge denoising task into GNN training to address structure domain shifts and tighten graph generalization bounds via -distance.

Result: GraphDeT demonstrates superior performance against existing baselines in mitigating the effects of domain shifts in both temporal and regional graph contexts, as validated through experimental results.

Conclusion: Integrating auxiliary edge tasks into GNN training is effective in improving generalization and addressing structural domain shifts in node classification problems on graphs.

Abstract: We explore the node classification task in the context of graph domain adaptation, which uses both source and target graph structures along with source labels to enhance the generalization capabilities of Graph Neural Networks (GNNs) on target graphs. Structure domain shifts frequently occur, especially when graph data are collected at different times or from varying areas, resulting in poor performance of GNNs on target graphs. Surprisingly, we find that simply incorporating an auxiliary loss function for denoising graph edges on target graphs can be extremely effective in enhancing GNN performance on target graphs. Based on this insight, we propose our framework, GraphDeT, a framework that integrates this auxiliary edge task into GNN training for node classification under domain adaptation. Our theoretical analysis connects this auxiliary edge task to the graph generalization bound with -distance, demonstrating such auxiliary task can imposes a constraint which tightens the bound and thereby improves generalization. The experimental results demonstrate superior performance compared to the existing baselines in handling both time and regional domain graph shifts.

</details>


### [387] [Quantization Blindspots: How Model Compression Breaks Backdoor Defenses](https://arxiv.org/abs/2512.06243)
*Rohan Pandey,Eric Ye*

Main category: cs.LG

TL;DR: The paper investigates how existing backdoor defenses perform under standard post-training quantization techniques, finding that quantization significantly reduces detection effectiveness while preserving attack success rates.


<details>
  <summary>Details</summary>
Motivation: To assess whether backdoor defenses remain effective when models are deployed in real-world settings with quantized precision (e.g., INT8, INT4).

Method: The authors systematically evaluated five backdoor defenses across different quantization levels (FP32, INT8, INT4) using standard benchmarks (CIFAR-10, GTSRB) and a canonical backdoor attack (BadNet).

Result: Quantization reduced backdoor detection rates to 0% for INT8 while maintaining high attack success (99%). INT4 showed dependency on datasets, with backdoors still effective but defenses failing in some cases.

Conclusion: The mismatched evaluation of backdoor defenses in full versus quantized precision models suggests that future backdoor defense designs need to consider robustness under quantization as a critical evaluation metric.

Abstract: Backdoor attacks embed input-dependent malicious behavior into neural networks while preserving high clean accuracy, making them a persistent threat for deployed ML systems. At the same time, real-world deployments almost never serve full-precision models: post-training quantization to INT8 or lower precision is now standard practice for reducing memory and latency. This work asks a simple question: how do existing backdoor defenses behave under standard quantization pipelines? We conduct a systematic empirical study of five representative defenses across three precision settings (FP32, INT8 dynamic, INT4 simulated) and two standard vision benchmarks using a canonical BadNet attack. We observe that INT8 quantization reduces the detection rate of all evaluated defenses to 0% while leaving attack success rates above 99%. For INT4, we find a pronounced dataset dependence: Neural Cleanse remains effective on GTSRB but fails on CIFAR-10, even though backdoors continue to survive quantization with attack success rates above 90%. Our results expose a mismatch between how defenses are commonly evaluated (on FP32 models) and how models are actually deployed (in quantized form), and they highlight quantization robustness as a necessary axis in future evaluations and designs of backdoor defenses.

</details>


### [388] [Auto-exploration for online reinforcement learning](https://arxiv.org/abs/2512.06244)
*Caleb Ju,Guanghui Lan*

Main category: cs.LG

TL;DR: The paper addresses exploration-exploitation challenges in reinforcement learning by introducing parameter-free auto-exploration algorithms for tabular and linear function approximation settings, achieving efficient sample complexity.


<details>
  <summary>Details</summary>
Motivation: To tackle the limitations of inefficient exploration and impractical prior algorithms in reinforcement learning, which rely on problem-specific parameters.

Method: Proposes two parameter-free algorithms for auto-exploration in reinforcement learning by utilizing novel techniques such as dynamic mixing time, discounted state distributions, robust gradient estimation, and an advantage gap function.

Result: The proposed methods achieve a sample complexity of $O(ε^{-2})$ under general assumptions, without depending on problem-specific algorithm parameters, making them easy to implement and effective.

Conclusion: The methods provide a significant advancement in reinforcement learning by balancing exploration and exploitation through parameter-free approaches, ensuring efficiency and practical applicability.

Abstract: The exploration-exploitation dilemma in reinforcement learning (RL) is a fundamental challenge to efficient RL algorithms. Existing algorithms for finite state and action discounted RL problems address this by assuming sufficient exploration over both state and action spaces. However, this yields non-implementable algorithms and sub-optimal performance. To resolve these limitations, we introduce a new class of methods with auto-exploration, or methods that automatically explore both state and action spaces in a parameter-free way, i.e.,~without a priori knowledge of problem-dependent parameters. We present two variants: one for the tabular setting and one for linear function approximation. Under algorithm-independent assumptions on the existence of an exploring optimal policy, both methods attain $O(ε^{-2})$ sample complexity to solve to $ε$ error. Crucially, these complexities are novel since they are void of algorithm-dependent parameters seen in prior works, which may be arbitrarily large. The methods are also simple to implement because they are parameter-free and do not directly estimate the unknown parameters. These feats are achieved by new algorithmic innovations for RL, including a dynamic mixing time, a discounted state distribution for sampling, a simple robust gradient estimator, and a recent advantage gap function to certify convergence.

</details>


### [389] [Entropic Confinement and Mode Connectivity in Overparameterized Neural Networks](https://arxiv.org/abs/2512.06297)
*Luca Di Carlo,Chase Goddard,David J. Schwab*

Main category: cs.LG

TL;DR: The paper explores why optimization dynamics in neural networks do not explore low-loss paths despite their connectivity, identifying entropic barriers caused by curvature variations and noise.


<details>
  <summary>Details</summary>
Motivation: To understand why optimization dynamics in neural networks remain confined to specific basins despite the existence of low-loss connecting paths within the loss landscape.

Method: The authors analyze curvature variations and their interplay with noisy optimization dynamics, using empirical evaluations to observe how these factors affect the loss landscape and parameter localization.

Result: The researchers found that curvature increases away from minima, creating effective forces that confine noisy dynamics to endpoints of low-loss paths despite flat losses. This highlights the role of entropic barriers in deep learning.

Conclusion: Curvature-induced entropic barriers explain the confinement of optimization dynamics within convex basins, impacting solution localization and connectivity in neural network loss landscapes.

Abstract: Modern neural networks exhibit a striking property: basins of attraction in the loss landscape are often connected by low-loss paths, yet optimization dynamics generally remain confined to a single convex basin and rarely explore intermediate points. We resolve this paradox by identifying entropic barriers arising from the interplay between curvature variations along these paths and noise in optimization dynamics. Empirically, we find that curvature systematically rises away from minima, producing effective forces that bias noisy dynamics back toward the endpoints - even when the loss remains nearly flat. These barriers persist longer than energetic barriers, shaping the late-time localization of solutions in parameter space. Our results highlight the role of curvature-induced entropic forces in governing both connectivity and confinement in deep learning landscapes.

</details>


### [390] [Learning When to Switch: Adaptive Policy Selection via Reinforcement Learning](https://arxiv.org/abs/2512.06250)
*Chris Tava*

Main category: cs.LG

TL;DR: This paper introduces a Q-learning technique to adaptively switch between exploration and pathfinding strategies for maze navigation, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of determining optimal strategy-switching moments in autonomous systems handling complex tasks.

Method: A Q-learning-based adaptive reinforcement learning method that adjusts strategy-switch thresholds by discretizing state space into coverage and distance buckets.

Result: Adaptive threshold learning showed significant improvements in completion times, variance reduction, and worst-case scenarios over fixed strategy and threshold baselines, scaling better with increasing maze complexity.

Conclusion: Adaptive switching is more effective than fixed thresholds or single strategies, with significant generalization and scalability benefits as maze task complexity increases.

Abstract: Autonomous agents often require multiple strategies to solve complex tasks, but determining when to switch between strategies remains challenging. This research introduces a reinforcement learning technique to learn switching thresholds between two orthogonal navigation policies. Using maze navigation as a case study, this work demonstrates how an agent can dynamically transition between systematic exploration (coverage) and goal-directed pathfinding (convergence) to improve task performance. Unlike fixed-threshold approaches, the agent uses Q-learning to adapt switching behavior based on coverage percentage and distance to goal, requiring only minimal domain knowledge: maze dimensions and target location. The agent does not require prior knowledge of wall positions, optimal threshold values, or hand-crafted heuristics; instead, it discovers effective switching strategies dynamically during each run. The agent discretizes its state space into coverage and distance buckets, then adapts which coverage threshold (20-60\%) to apply based on observed progress signals. Experiments across 240 test configurations (4 maze sizes from 16$\times$16 to 128$\times$128 $\times$ 10 unique mazes $\times$ 6 agent variants) demonstrate that adaptive threshold learning outperforms both single-strategy agents and fixed 40\% threshold baselines. Results show 23-55\% improvements in completion time, 83\% reduction in runtime variance, and 71\% improvement in worst-case scenarios. The learned switching behavior generalizes within each size class to unseen wall configurations. Performance gains scale with problem complexity: 23\% improvement for 16$\times$16 mazes, 34\% for 32$\times$32, and 55\% for 64$\times$64, demonstrating that as the space of possible maze structures grows, the value of adaptive policy selection over fixed heuristics increases proportionally.

</details>


### [391] [Zero Generalization Error Theorem for Random Interpolators via Algebraic Geometry](https://arxiv.org/abs/2512.06347)
*Naoki Yoshida,Isao Ishikawa,Masaaki Imaizumi*

Main category: cs.LG

TL;DR: This paper establishes that interpolators achieve zero generalization error beyond a certain number of training samples under teacher-student settings using algebraic geometry.


<details>
  <summary>Details</summary>
Motivation: To address why large-scale machine learning models like deep neural networks exhibit high generalization ability, which remains a central unresolved issue in machine learning theory.

Method: The researchers use algebraic geometry tools to analyze the geometric structure of interpolator sets in the parameter space under teacher-student frameworks.

Result: It is proven that the generalization error of randomly sampled interpolators becomes strictly 0 when the number of training samples surpasses a certain threshold defined by the geometry of the parameter space.

Conclusion: The paper suggests that generalization properties of models stem more from inherent structural characteristics of model architectures rather than biases introduced by optimization techniques like SGD.

Abstract: We theoretically demonstrate that the generalization error of interpolators for machine learning models under teacher-student settings becomes 0 once the number of training samples exceeds a certain threshold. Understanding the high generalization ability of large-scale models such as deep neural networks (DNNs) remains one of the central open problems in machine learning theory. While recent theoretical studies have attributed this phenomenon to the implicit bias of stochastic gradient descent (SGD) toward well-generalizing solutions, empirical evidences indicate that it primarily stems from properties of the model itself. Specifically, even randomly sampled interpolators, which are parameters that achieve zero training error, have been observed to generalize effectively. In this study, under a teacher-student framework, we prove that the generalization error of randomly sampled interpolators becomes exactly zero once the number of training samples exceeds a threshold determined by the geometric structure of the interpolator set in parameter space. As a proof technique, we leverage tools from algebraic geometry to mathematically characterize this geometric structure.

</details>


### [392] [Learning Without Time-Based Embodiment Resets in Soft-Actor Critic](https://arxiv.org/abs/2512.06252)
*Homayoon Farrahi,A. Rupam Mahmood*

Main category: cs.LG

TL;DR: The paper explores challenges and strategies in reinforcement learning when typical task accessories like episode terminations and embodiment resets are removed.


<details>
  <summary>Details</summary>
Motivation: Improve reinforcement learning's real-world applicability by removing accessory components that create artificial task setups but hinder long-term real-world performance.

Method: A modified algorithm, Continuing Soft Actor-Critic (SAC), was developed and tested under conditions without episodic terminations or environment resets. Adjustments to reward functions and entropy were key interventions.

Result: Continuing SAC matched or outperformed episodic SAC in tasks without terminations but highlighted challenges, such as poor exploration and slower learning, in the absence of embodiment resets. Increasing policy entropy was effective in mitigating performance losses.

Conclusion: Reinforcement learning without resets and terminations can yield competitive results with appropriate modifications, though embodiment resets are critical for efficient state exploration.

Abstract: When creating new reinforcement learning tasks, practitioners often accelerate the learning process by incorporating into the task several accessory components, such as breaking the environment interaction into independent episodes and frequently resetting the environment. Although they can enable the learning of complex intelligent behaviors, such task accessories can result in unnatural task setups and hinder long-term performance in the real world. In this work, we explore the challenges of learning without episode terminations and robot embodiment resets using the Soft Actor-Critic (SAC) algorithm. To learn without terminations, we present a continuing version of the SAC algorithm and show that, with simple modifications to the reward functions of existing tasks, continuing SAC can perform as well as or better than episodic SAC while reducing the sensitivity of performance to the value of the discount rate $γ$. On a modified Gym Reacher task, we investigate possible explanations for the failure of continuing SAC when learning without embodiment resets. Our results suggest that embodiment resets help with exploration of the state space in the SAC algorithm, and removing embodiment resets can lead to poor exploration of the state space and failure of or significantly slower learning. Finally, on additional simulated tasks and a real-robot vision task, we show that increasing the entropy of the policy when performance trends worse or remains static is an effective intervention for recovering the performance lost due to not using embodiment resets.

</details>


### [393] [Optimizing Optimizers for Fast Gradient-Based Learning](https://arxiv.org/abs/2512.06370)
*Jaerin Lee,Kyoung Mu Lee*

Main category: cs.LG

TL;DR: The paper proposes a theoretical framework for automating and optimizing gradient-based learning optimizers.


<details>
  <summary>Details</summary>
Motivation: To establish a systematic approach for designing and fine-tuning optimizers in gradient-based learning, exploiting gradient statistics.

Method: Formulated optimizer design as a convex optimization problem aiming to maximize the instantaneous loss reduction while deriving closed-form solutions and hyperparameters.

Result: The paper recovers popular optimizers, determines their optimal hyperparameters, and enables dynamic optimization of optimizers during training.

Conclusion: The proposed framework enhances the design, hyperparameter tuning, and dynamic adaptation of gradient-based learning optimizers, streamlining their development.

Abstract: We lay the theoretical foundation for automating optimizer design in gradient-based learning. Based on the greedy principle, we formulate the problem of designing optimizers as maximizing the instantaneous decrease in loss. By treating an optimizer as a function that translates loss gradient signals into parameter motions, the problem reduces to a family of convex optimization problems over the space of optimizers. Solving these problems under various constraints not only recovers a wide range of popular optimizers as closed-form solutions, but also produces the optimal hyperparameters of these optimizers with respect to the problems at hand. This enables a systematic approach to design optimizers and tune their hyperparameters according to the gradient statistics that are collected during the training process. Furthermore, this optimization of optimization can be performed dynamically during training.

</details>


### [394] [Networked Restless Multi-Arm Bandits with Reinforcement Learning](https://arxiv.org/abs/2512.06274)
*Hanmo Zhang,Zenghui Sun,Kai Wang*

Main category: cs.LG

TL;DR: This paper develops a new framework, Networked RMAB, which incorporates network interactions into traditional RMAB models for decision-making, and validates its effectiveness experimentally.


<details>
  <summary>Details</summary>
Motivation: Traditional RMABs inadequately model interactions between individuals in networked environments, limiting their effectiveness for real-world applications such as public health interventions.

Method: The authors extend RMABs by integrating the independent cascade model, define the Bellman equation for this networked setting, prove submodularity, and use a hill-climbing algorithm for efficient updates with a performance guarantee. They also validate convergence through a modified contraction analysis and design a tailored Q-learning algorithm.

Result: Experimental results using real-world graph data show the Q-learning algorithm outperforms other approaches, emphasizing the benefit of accounting for network effects.

Conclusion: Incorporating network interactions into RMABs allows for more accurate and effective decision-making in sequential optimization tasks in networked environments.

Abstract: Restless Multi-Armed Bandits (RMABs) are a powerful framework for sequential decision-making, widely applied in resource allocation and intervention optimization challenges in public health. However, traditional RMABs assume independence among arms, limiting their ability to account for interactions between individuals that can be common and significant in a real-world environment. This paper introduces Networked RMAB, a novel framework that integrates the RMAB model with the independent cascade model to capture interactions between arms in networked environments. We define the Bellman equation for networked RMAB and present its computational challenge due to exponentially large action and state spaces. To resolve the computational challenge, we establish the submodularity of Bellman equation and apply the hill-climbing algorithm to achieve a $1-\frac{1}{e}$ approximation guarantee in Bellman updates. Lastly, we prove that the approximate Bellman updates are guaranteed to converge by a modified contraction analysis. We experimentally verify these results by developing an efficient Q-learning algorithm tailored to the networked setting. Experimental results on real-world graph data demonstrate that our Q-learning approach outperforms both $k$-step look-ahead and network-blind approaches, highlighting the importance of capturing and leveraging network effects where they exist.

</details>


### [395] [Theoretical Compression Bounds for Wide Multilayer Perceptrons](https://arxiv.org/abs/2512.06288)
*Houssam El Cheairi,David Gamarnik,Rahul Mazumder*

Main category: cs.LG

TL;DR: This paper analyzes a randomized greedy compression algorithm for post-training pruning and quantization in neural networks, giving theoretical backing for their empirical success.


<details>
  <summary>Details</summary>
Motivation: The goal is to address the lack of theoretical justification for the empirical success of pruning and quantization in reducing parameters of large neural networks.

Method: The authors utilize a randomized greedy compression algorithm, analyze its performance, and extend results to structured pruning for both MLPs and CNNs without relying on data assumptions.

Result: Theoretical results demonstrate that pruned and quantized subnetworks can achieve competitive performance and reveal a tradeoff between network width and compressibility.

Conclusion: The study bridges the theoretical gap in pruning/quantization techniques, providing a rigorous foundation for their empirical success and showcasing their effectiveness in wide networks.

Abstract: Pruning and quantization techniques have been broadly successful in reducing the number of parameters needed for large neural networks, yet theoretical justification for their empirical success falls short. We consider a randomized greedy compression algorithm for pruning and quantization post-training and use it to rigorously show the existence of pruned/quantized subnetworks of multilayer perceptrons (MLPs) with competitive performance. We further extend our results to structured pruning of MLPs and convolutional neural networks (CNNs), thus providing a unified analysis of pruning in wide networks. Our results are free of data assumptions, and showcase a tradeoff between compressibility and network width. The algorithm we consider bears some similarities with Optimal Brain Damage (OBD) and can be viewed as a post-training randomized version of it. The theoretical results we derive bridge the gap between theory and application for pruning/quantization, and provide a justification for the empirical success of compression in wide multilayer perceptrons.

</details>


### [396] [Importance-aware Topic Modeling for Discovering Public Transit Risk from Noisy Social Media](https://arxiv.org/abs/2512.06293)
*Fatima Ashraf,Muhammad Ayub Sabir,Jiaxin Deng,Junbiao Pang,Haitao Yu*

Main category: cs.LG

TL;DR: The paper presents a model for analyzing urban transit-related social media data to identify emerging service risks using a novel Poisson Deconvolution Factorization (PDF) approach.


<details>
  <summary>Details</summary>
Motivation: The need to analyze sparse and often noisy signals from social media data regarding urban transit issues, such as crowding and delays, to monitor and mitigate service risks effectively.

Method: The method involves building an influence-weighted keyword co-occurrence graph, applying a Poisson Deconvolution Factorization (PDF) to extract topics and topic importance, using a decorrelation regularizer to ensure distinct topics, and selecting the number of topics through coherence-based evaluation.

Result: The model achieves state-of-the-art topic coherence and diversity when applied to large-scale social media data streams, outperforming existing baselines.

Conclusion: The proposed framework offers an effective and interpretable way to analyze social media streams for urban transit systems, identifying impactful insights to address service risks effectively.

Abstract: Urban transit agencies increasingly turn to social media to monitor emerging service risks such as crowding, delays, and safety incidents, yet the signals of concern are sparse, short, and easily drowned by routine chatter. We address this challenge by jointly modeling linguistic interactions and user influence. First, we construct an influence-weighted keyword co-occurrence graph from cleaned posts so that socially impactful posts contributes proportionally to the underlying evidence. The core of our framework is a Poisson Deconvolution Factorization (PDF) that decomposes this graph into a low-rank topical structure and topic-localized residual interactions, producing an interpretable topic--keyword basis together with topic importance scores. A decorrelation regularizer \emph{promotes} distinct topics, and a lightweight optimization procedure ensures stable convergence under nonnegativity and normalization constraints. Finally, the number of topics is selected through a coherence-driven sweep that evaluates the quality and distinctness of the learned topics. On large-scale social streams, the proposed model achieves state-of-the-art topic coherence and strong diversity compared with leading baselines. The code and dataset are publicly available at https://github.com/pangjunbiao/Topic-Modeling_ITS.git

</details>


### [397] [Prediction with Expert Advice under Local Differential Privacy](https://arxiv.org/abs/2512.06971)
*Ben Jacobsen,Kassem Fawaz*

Main category: cs.LG

TL;DR: This paper addresses prediction with expert advice under local differential privacy, proposing two algorithms (RW-AdaBatch and RW-Meta) and demonstrating their improved performance over baseline and state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To improve prediction with expert advice while maintaining local differential privacy, as motivated by applications like sensitive, real-world data.

Method: The paper introduces two algorithms: RW-AdaBatch, leveraging privacy amplification and random walks for improved analytical results, and RW-Meta, which allows private selection among complex expert algorithms at no additional privacy cost.

Result: Proposed algorithms achieve enhanced utility, with RW-Meta outperforming both classical baselines and state-of-the-art central DP algorithms by 1.5-3x in forecasting hospital COVID patient density.

Conclusion: The presented methods highlight the feasibility of combining strong privacy guarantees with competitive performance, extending current work on differential privacy in prediction tasks.

Abstract: We study the classic problem of prediction with expert advice under the constraint of local differential privacy (LDP). In this context, we first show that a classical algorithm naturally satisfies LDP and then design two new algorithms that improve it: RW-AdaBatch and RW-Meta. For RW-AdaBatch, we exploit the limited-switching behavior induced by LDP to provide a novel form of privacy amplification that grows stronger on easier data, analogous to the shuffle model in offline learning. Drawing on the theory of random walks, we prove that this improvement carries essentially no utility cost. For RW-Meta, we develop a general method for privately selecting between experts that are themselves non-trivial learning algorithms, and we show that in the context of LDP this carries no extra privacy cost. In contrast, prior work has only considered data-independent experts. We also derive formal regret bounds that scale inversely with the degree of independence between experts. Our analysis is supplemented by evaluation on real-world data reported by hospitals during the COVID-19 pandemic; RW-Meta outperforms both the classical baseline and a state-of-the-art \textit{central} DP algorithm by 1.5-3$\times$ on the task of predicting which hospital will report the highest density of COVID patients each week.

</details>


### [398] [Chemistry Integrated Language Model using Hierarchical Molecular Representation for Polymer Informatics](https://arxiv.org/abs/2512.06301)
*Jihun Ahn,Gabriella Pasya Irianti,Vikram Thapar,Su-Mi Hur*

Main category: cs.LG

TL;DR: The paper presents CI-LLM, a machine learning framework improving polymer property prediction and inverse design using strategic molecular representations.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in applying machine learning to polymers due to data scarcity, enabling better property prediction and inverse design.

Method: The approach combines HAPPY, a polymer-specific representation, with descriptor-enriched transformer models, including De$^3$BERTa for predictions and GPT-based tools for design.

Result: The framework delivers 3.5x faster inference, outperforms SMILES models in accuracy, and showcases successful polymer generation with properties optimization.

Conclusion: Strategic molecular representations can significantly advance machine learning in polymer science, enabling interpretable and efficient property prediction and design.

Abstract: Machine learning has transformed material discovery for inorganic compounds and small molecules, yet polymers remain largely inaccessible to these methods. While data scarcity is often cited as the primary bottleneck, we demonstrate that strategic molecular representations can overcome this limitation. We introduce CI-LLM (Chemically Informed Language Model), a framework combining HAPPY (Hierarchically Abstracted rePeat unit of PolYmer), which encodes chemical substructures as tokens, with numerical descriptors within transformer architectures. For property prediction, De$^3$BERTa, our descriptor-enriched encoder, achieves 3.5x faster inference than SMILES-based models with improved accuracy ($R^2$ score gains of 0.9-4.1 percent across four properties), while providing interpretable structure-property insights at the subgroup level. For inverse design, our GPT-based generator produces polymers with targeted properties, achieving 100 percent scaffold retention and successful multi-property optimization for negatively correlated objectives. This comprehensive framework demonstrates both forward prediction and inverse design capabilities, showcasing how strategic molecular representation advances machine learning applications in polymer science.

</details>


### [399] [Multimodal Graph Neural Networks for Prognostic Modeling of Brain Network Reorganization](https://arxiv.org/abs/2512.06303)
*Preksha Girish,Rachana Mysore,Kiran K. N.,Hiranmayee R.,Shipra Prashanth,Shrey Kumar*

Main category: cs.LG

TL;DR: The paper introduces a graph neural network framework to analyze brain network reorganization using multi-modal imaging data, aiming to identify interpretable biomarkers and predict clinical outcomes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand dynamic brain network reorganization for predicting neurological conditions, cognitive decline, and individual clinical variability using existing imaging data.

Method: The method involves building longitudinal brain graphs where nodes represent brain regions and edges represent structural and functional connectivity from MRI, DTI, and fMRI data. Temporal evolution is modeled with fractional stochastic operators in graph-based recurrent networks, and attention mechanisms fuse multimodal data.

Result: The framework demonstrated strong predictive accuracy and interpretability in experiments using longitudinal neuroimaging datasets, providing clinically meaningful biomarkers.

Conclusion: The study proves the potential of multimodal graph-based approaches to predict brain network changes, offering interpretable and mathematically rigorous biomarkers without requiring new data collection.

Abstract: Understanding the dynamic reorganization of brain networks is critical for predicting cognitive decline, neurological progression, and individual variability in clinical outcomes. This work proposes a multimodal graph neural network framework that integrates structural MRI, diffusion tensor imaging, and functional MRI to model spatiotemporal brain network reorganization. Brain regions are represented as nodes and structural and functional connectivity as edges, forming longitudinal brain graphs for each subject. Temporal evolution is captured via fractional stochastic differential operators embedded within graph-based recurrent networks, enabling the modeling of long-term dependencies and stochastic fluctuations in network dynamics. Attention mechanisms fuse multimodal information and generate interpretable biomarkers, including network energy entropy, graph curvature, fractional memory indices, and modality-specific attention scores. These biomarkers are combined into a composite prognostic index to quantify individual risk of network instability or cognitive decline. Experiments on longitudinal neuroimaging datasets demonstrate both predictive accuracy and interpretability. The results highlight the potential of mathematically rigorous, multimodal graph-based approaches for deriving clinically meaningful biomarkers from existing imaging data without requiring new data collection.

</details>


### [400] [Interpretive Efficiency: Information-Geometric Foundations of Data Usefulness](https://arxiv.org/abs/2512.06341)
*Ronald Katende*

Main category: cs.LG

TL;DR: This paper introduces Interpretive Efficiency, a theoretical metric for assessing how well a given machine learning representation channels task-relevant information.


<details>
  <summary>Details</summary>
Motivation: Interpretability is critical to trustworthy machine learning, but current measures often fail to quantify how well task-related data support interpretive functions.

Method: The authors propose a formal definition of Interpretive Efficiency based on five axioms and relate it to mutual information. They utilize Fisher-geometric expansions and empirical-process tools for finite-sample and asymptotic guarantees.

Result: Experiments show that the metric aligns with theoretical expectations, identifies representational redundancy, and correlates with robustness in tasks involving images and signals.

Conclusion: Interpretive Efficiency offers a practical and theoretically grounded diagnostic for evaluating and designing interpretive representations in machine learning.

Abstract: Interpretability is central to trustworthy machine learning, yet existing metrics rarely quantify how effectively data support an interpretive representation. We propose Interpretive Efficiency, a normalized, task-aware functional that measures the fraction of task-relevant information transmitted through an interpretive channel. The definition is grounded in five axioms ensuring boundedness, Blackwell-style monotonicity, data-processing stability, admissible invariance, and asymptotic consistency. We relate the functional to mutual information and derive a local Fisher-geometric expansion, then establish asymptotic and finite-sample estimation guarantees using standard empirical-process tools. Experiments on controlled image and signal tasks demonstrate that the measure recovers theoretical orderings, exposes representational redundancy masked by accuracy, and correlates with robustness, making it a practical, theory-backed diagnostic for representation design.

</details>


### [401] [When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models](https://arxiv.org/abs/2512.06343)
*Tong Xie,Andrew Bai,Yuanhao Ban,Yunqi Hong,Haoyu Li,Cho-jui Hsieh*

Main category: cs.LG

TL;DR: The paper identifies a limitation in the standard Bradley-Terry (BT) loss used for reward modeling in RLHF for LLMs, and proposes an improved method called NormBT that addresses the issue.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of the BT loss function, which can result in misaligned learning due to imbalanced gradient updates caused by representation distance in LLM outputs.

Method: The paper introduces NormBT, an adaptive pair-wise normalization strategy that focuses updates on prediction errors while balancing the representation distance impact.

Result: NormBT significantly enhances reward model performance across various LLMs and datasets, achieving over 5% improvement in the Reasoning category of RewardBench.

Conclusion: NormBT corrects a key limitation in the BT loss and is an effective, lightweight improvement for LLM reward modeling in various applications.

Abstract: Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of a pair of chosen and rejected responses. In this work, we analyze the per-sample gradient of BT-loss and show that its norm scales with two distinct components: (1) the difference in predicted rewards between chosen and rejected responses, which reflects the prediction error, and critically, (2) representation distance between the pair measured in the output space of the final layer. While the first term captures the intended training signal, we show that the second term can significantly impact the update magnitude and misalign learning. Specifically, pairs with small representation distance often receive vanishingly weak updates, even when misranked, while pairs with large distance receive disproportionately strong updates. This leads to gradients from large-distance pairs to overshadow those from small-distance pairs, where fine-grained distinctions are especially important. To overcome this limitation, we propose NormBT, an adaptive pair-wise normalization scheme that balances representation-driven effects and focuses learning signals on prediction error. NormBT is a lightweight, drop-in integration to BT loss with negligible overhead. Across various LLM backbones and datasets, NormBT improves reward model performance consistently, with notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs. This work reveals a key limitation in the widely used BT objective and provides a simple, effective correction.

</details>


### [402] [A Bootstrap Perspective on Stochastic Gradient Descent](https://arxiv.org/abs/2512.07676)
*Hongjian Lan,Yucong Liu,Florian Schäfer*

Main category: cs.LG

TL;DR: The paper explores how SGD improves generalization by relating it to bootstrap principles and demonstrates theoretical and experimental evidence to support the assertion.


<details>
  <summary>Details</summary>
Motivation: To understand why models trained with SGD generalize better compared to models trained with deterministic gradient descent (GD).

Method: The study connects SGD’s behavior to statistical bootstrap, uses both theoretical analysis and empirical experiments, proves SGD implicitly regularizes the gradient covariance trace, and shows improvements in neural network training using these insights.

Result: SGD is shown to focus on parameter choices robust to resampling, avoid spurious solutions, reduce sensitivity to sampling noise, and improve generalization. Numerical experiments back up the framework’s implications.

Conclusion: SGD’s connection to bootstrap estimation effectively contributes to its generalization advantages; explicitly regularizing algorithmic variability further enhances performance.

Abstract: Machine learning models trained with \emph{stochastic} gradient descent (SGD) can generalize better than those trained with deterministic gradient descent (GD). In this work, we study SGD's impact on generalization through the lens of the statistical bootstrap: SGD uses gradient variability under batch sampling as a proxy for solution variability under the randomness of the data collection process. We use empirical results and theoretical analysis to substantiate this claim. In idealized experiments on empirical risk minimization, we show that SGD is drawn to parameter choices that are robust under resampling and thus avoids spurious solutions even if they lie in wider and deeper minima of the training loss. We prove rigorously that by implicitly regularizing the trace of the gradient covariance matrix, SGD controls the algorithmic variability. This regularization leads to solutions that are less sensitive to sampling noise, thereby improving generalization. Numerical experiments on neural network training show that explicitly incorporating the estimate of the algorithmic variability as a regularizer improves test performance. This fact supports our claim that bootstrap estimation underpins SGD's generalization advantages.

</details>


### [403] [LLM-Upgraded Graph Reinforcement Learning for Carbon-Aware Job Scheduling in Smart Manufacturing](https://arxiv.org/abs/2512.06351)
*Zhiying Yang,Fang Liu,Wei Zhang,Xin Lou,Malcolm Yoke Hean Low,Boon Ping Gan*

Main category: cs.LG

TL;DR: The paper introduces LUCA, a graph reinforcement learning framework integrating large language models for carbon-efficient and dynamic scheduling in smart manufacturing, demonstrating superior results compared to alternatives.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of real-time and sustainable scheduling in smart manufacturing systems that prioritize both operational efficiency and carbon emission reduction.

Method: LUCA combines a graph neural network and a large language model, supported by a custom prompting strategy, to produce fused embeddings processed by a deep reinforcement learning policy. This is further reinforced by a dual-objective reward function targeting both makespan and energy efficiency.

Result: LUCA outperforms existing algorithms, achieving superior results in both makespan reduction and carbon efficiency across synthetic and public datasets, with notable improvements like 4.1%–12.2% reduced makespan on synthetic data.

Conclusion: The LUCA framework proves to be an effective and practical solution for dynamic, sustainable scheduling in smart manufacturing, balancing efficiency and carbon-awareness.

Abstract: This paper presents \textsc{Luca}, a \underline{l}arge language model (LLM)-\underline{u}pgraded graph reinforcement learning framework for \underline{c}arbon-\underline{a}ware flexible job shop scheduling. \textsc{Luca} addresses the challenges of dynamic and sustainable scheduling in smart manufacturing systems by integrating a graph neural network and an LLM, guided by a carefully designed in-house prompting strategy, to produce a fused embedding that captures both structural characteristics and contextual semantics of the latest scheduling state. This expressive embedding is then processed by a deep reinforcement learning policy network, which generates real-time scheduling decisions optimized for both makespan and carbon emission objectives. To support sustainability goals, \textsc{Luca} incorporates a dual-objective reward function that encourages both energy efficiency and scheduling timeliness. Experimental results on both synthetic and public datasets demonstrate that \textsc{Luca} consistently outperforms comparison algorithms. For instance, on the synthetic dataset, it achieves an average of 4.1\% and up to 12.2\% lower makespan compared to the best-performing comparison algorithm while maintaining the same emission level. On public datasets, additional gains are observed for both makespan and emission. These results demonstrate that \textsc{Luca} is effective and practical for carbon-aware scheduling in smart manufacturing.

</details>


### [404] [Provable Long-Range Benefits of Next-Token Prediction](https://arxiv.org/abs/2512.07818)
*Xinyuan Cao,Santosh S. Vempala*

Main category: cs.LG

TL;DR: The paper proves that next-token prediction with RNNs can approximate training data distributions, explaining long-range coherence in generated language.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand why modern language models trained for next-token prediction exhibit coherence and capture long-range structure in text.

Method: They mathematically prove that optimizing next-token prediction with recurrent neural networks results in models capable of generating long-range coherent text, indistinguishable from training data within specific constraints.

Result: The research provides polynomial bounds for model size required to achieve indistinguishability of generated token sequences from training distributions, independent of document length.

Conclusion: Next-token prediction is a theoretically powerful method for learning and generating coherent text that captures long-range structure efficiently.

Abstract: Why do modern language models, trained to do well on next-word prediction, appear to generate coherent documents and capture long-range structure? Here we show that next-token prediction is provably powerful for learning longer-range structure, even with common neural network architectures. Specifically, we prove that optimizing next-token prediction over a Recurrent Neural Network (RNN) yields a model that closely approximates the training distribution: for held-out documents sampled from the training distribution, no algorithm of bounded description length limited to examining the next $k$ tokens, for any $k$, can distinguish between $k$ consecutive tokens of such documents and $k$ tokens generated by the learned language model following the same prefix. We provide polynomial bounds (in $k$, independent of the document length) on the model size needed to achieve such $k$-token indistinguishability, offering a complexity-theoretic explanation for the long-range coherence observed in practice.

</details>


### [405] [DDFI: Diverse and Distribution-aware Missing Feature Imputation via Two-step Reconstruction](https://arxiv.org/abs/2512.06356)
*Yifan Song,Fenglin Yu,Yihong Luo,Xingjian Tao,Siya Qiu,Kai Han,Jing Tang*

Main category: cs.LG

TL;DR: The paper addresses the issue of incomplete node features degrading Graph Neural Networks' performance and proposes DDFI, a novel imputation method to overcome current limitations.


<details>
  <summary>Details</summary>
Motivation: Incomplete node features are common in real-world graphs, leading to reduced GNN performance, and existing solutions like Feature Propagation face limitations such as struggles with disconnected graphs, over-smoothing, and inefficiency for inductive tasks.

Method: DDFI (Diverse and Distribution-aware Missing Feature Imputation) integrates feature propagation with a graph-based Masked AutoEncoder (MAE). It incorporates Co-Label Linking (CLL) to handle disconnected graphs and employs a two-step feature reconstruction process during inference for inductive tasks.

Result: Experiments on six public datasets and the new Sailing dataset demonstrate that DDFI surpasses state-of-the-art methods in both transductive and inductive scenarios.

Conclusion: DDFI effectively addresses challenges in missing node feature imputation and showcases high performance under diverse settings, improving upon existing approaches.

Abstract: Incomplete node features are ubiquitous in real-world scenarios, e.g., the attributes of web users may be partly private, which causes the performance of Graph Neural Networks (GNNs) to decline significantly. Feature propagation (FP) is a well-known method that performs well for imputation of missing node features on graphs, but it still has the following three issues: 1) it struggles with graphs that are not fully connected, 2) imputed features face the over-smoothing problem, and 3) FP is tailored for transductive tasks, overlooking the feature distribution shift in inductive tasks. To address these challenges, we introduce DDFI, a Diverse and Distribution-aware Missing Feature Imputation method that combines feature propagation with a graph-based Masked AutoEncoder (MAE) in a nontrivial manner. It first designs a simple yet effective algorithm, namely Co-Label Linking (CLL), that randomly connects nodes in the training set with the same label to enhance the performance on graphs with numerous connected components. Then we develop a novel two-step representation generation process at the inference stage. Specifically, instead of directly using FP-imputed features as input during inference, DDFI further reconstructs the features through the whole MAE to reduce feature distribution shift in the inductive tasks and enhance the diversity of node features. Meanwhile, since existing feature imputation methods for graphs only evaluate by simulating the missing scenes with manually masking the features, we collect a new dataset called Sailing from the records of voyages that contains naturally missing features to help better evaluate the effectiveness. Extensive experiments conducted on six public datasets and Sailing show that DDFI outperforms the state-of-the-art methods under both transductive and inductive settings.

</details>


### [406] [Proportional integral derivative booster for neural networks-based time-series prediction: Case of water demand prediction](https://arxiv.org/abs/2512.06357)
*Tony Sallooma,Okyay Kaynak,Xinbo Yub,Wei He*

Main category: cs.LG

TL;DR: The paper proposes using a PID control-inspired method to improve neural network models' accuracy for multi-step periodic time-series prediction with minimal impact on system complexity. Case studies demonstrate successful applications in water demand and energy consumption forecasting.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of prediction accuracy hindered by complex neural network structures when applied to multi-step periodic time-series prediction.

Method: It introduces a PID control-inspired booster, applied to the predicted value at each time step, refining it to align closer with real data values.

Result: The method improved accuracy across different periodic time-series problems, comparing favorably to original models while maintaining low system complexity.

Conclusion: The PID-based method effectively enhances prediction accuracy for neural networks across different periodic time-series applications without significantly increasing system complexity.

Abstract: Multi-step time-series prediction is an essential supportive step for decision-makers in several industrial areas. Artificial intelligence techniques, which use a neural network component in various forms, have recently frequently been used to accomplish this step. However, the complexity of the neural network structure still stands up as a critical problem against prediction accuracy. In this paper, a method inspired by the proportional-integral-derivative (PID) control approach is investigated to enhance the performance of neural network models used for multi-step ahead prediction of periodic time-series information while maintaining a negligible impact on the complexity of the system. The PID-based method is applied to the predicted value at each time step to bring that value closer to the real value. The water demand forecasting problem is considered as a case study, where two deep neural network models from the literature are used to prove the effectiveness of the proposed boosting method. Furthermore, to prove the applicability of this PID-based booster to other types of periodic time-series prediction problems, it is applied to enhance the accuracy of a neural network model used for multi-step forecasting of hourly energy consumption. The comparison between the results of the original prediction models and the results after using the proposed technique demonstrates the superiority of the proposed method in terms of prediction accuracy and system complexity.

</details>


### [407] [RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs](https://arxiv.org/abs/2512.06392)
*Runlong Zhou,Lefan Zhang,Shang-Chen Wu,Kelvin Zou,Hanzhi Zhou,Ke Ye,Yihao Feng,Dong Yin,Alex Guillen Garcia,Dmytro Babych,Rohit Chatterjee,Matthew Hopkins,Xiang Kong,Chang Lan,Lezhi Li,Yiping Ma,Daniele Molinari,Senyu Tong,Yanchao Sun,Thomas Voice,Jianyu Wang,Chong Wang,Simon Wang,Floris Weers,Yechen Xu,Guolin Yin,Muyang Yu,Yi Zhang,Zheng Zhou,Danyang Zhuo,Ruoming Pang,Cheng Leong*

Main category: cs.LG

TL;DR: RLAX leverages reinforcement learning (RL) techniques to enhance reasoning abilities of large language models using a scalable TPU-based framework.


<details>
  <summary>Details</summary>
Motivation: Advancing the reasoning capabilities of large language models by introducing a scalable and efficient RL system.

Method: A parameter-server architecture enabling preemptible and scalable RL operations, paired with dataset curation and alignment techniques.

Result: RLAX improves QwQ-32B's pass@8 accuracy by 12.8% in just 12 hours 48 minutes on 1024 TPUs, showcasing robustness during training interruptions.

Conclusion: The RLAX framework advances RL scalability while demonstrating significant improvements in performance and robustness for state-of-the-art models.

Abstract: Reinforcement learning (RL) has emerged as the de-facto paradigm for improving the reasoning capabilities of large language models (LLMs). We have developed RLAX, a scalable RL framework on TPUs. RLAX employs a parameter-server architecture. A master trainer periodically pushes updated model weights to the parameter server while a fleet of inference workers pull the latest weights and generates new rollouts. We introduce a suite of system techniques to enable scalable and preemptible RL for a diverse set of state-of-art RL algorithms. To accelerate convergence and improve model quality, we have devised new dataset curation and alignment techniques. Large-scale evaluations show that RLAX improves QwQ-32B's pass@8 accuracy by 12.8% in just 12 hours 48 minutes on 1024 v5p TPUs, while remaining robust to preemptions during training.

</details>


### [408] [Hankel-FNO: Fast Underwater Acoustic Charting Via Physics-Encoded Fourier Neural Operator](https://arxiv.org/abs/2512.06417)
*Yifan Sun,Lei Cheng,Jianlong Li,Peter Gerstoft*

Main category: cs.LG

TL;DR: The study proposes Hankel-FNO, a Fourier Neural Operator-based model for underwater acoustic charting, achieving high accuracy and speed, while addressing scalability and generalization challenges of existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of traditional numerical solvers and deep-learning methods, which either lack scalability or face constraints such as fixed resolution, to enable faster and accurate underwater acoustic modeling.

Method: The paper introduces Hankel-FNO, a data-driven model that integrates Fourier Neural Operators with sound propagation knowledge and bathymetry to perform rapid and precise acoustic charting.

Result: Hankel-FNO surpasses traditional solvers in computational speed and outperforms other deep-learning models in predictive accuracy, particularly in long-range scenarios.

Conclusion: Hankel-FNO offers a scalable and generalizable solution for acoustic charting in diverse underwater environments, making it suitable for tasks like sensor placement and autonomous navigation.

Abstract: Fast and accurate underwater acoustic charting is crucial for downstream tasks such as environment-aware sensor placement optimization and autonomous vehicle path planning. Conventional methods rely on computationally expensive while accurate numerical solvers, which are not scalable for large-scale or real-time applications. Although deep learning-based surrogate models can accelerate these computations, they often suffer from limitations such as fixed-resolution constraints or dependence on explicit partial differential equation formulations. These issues hinder their applicability and generalization across diverse environments. We propose Hankel-FNO, a Fourier Neural Operator (FNO)-based model for efficient and accurate acoustic charting. By incorporating sound propagation knowledge and bathymetry, our method has high accuracy while maintaining high computational speed. Results demonstrate that Hankel-FNO outperforms traditional solvers in speed and surpasses data-driven alternatives in accuracy, especially in long-range predictions. Experiments show the model's adaptability to diverse environments and sound source settings with minimal fine-tuning.

</details>


### [409] [A new initialisation to Control Gradients in Sinusoidal Neural network](https://arxiv.org/abs/2512.06427)
*Andrea Combette,Antoine Venaille,Nelly Pustelnik*

Main category: cs.LG

TL;DR: This paper introduces a new initialization strategy for neural networks with sinusoidal activations, surpassing existing methods in performance across tasks like function fitting and image reconstruction.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing initialization strategies, which lack sufficient control over gradients and generalization in sinusoidal activation networks.

Method: The proposed method derives a closed-form initialisation expression targeting gradient control and preventing vanishing pre-activations, leveraging fixed-point analysis and Jacobian variance.

Result: The new initialization enhances the training dynamics (analyzed through the NTK framework) and outperforms current methods in tasks such as function fitting, image reconstruction, and physics-informed neural networks.

Conclusion: The study demonstrates how improved initialization can mitigate gradient issues, enhance training, and achieve superior generalization, setting a new benchmark for sinusoidal activation networks.

Abstract: Proper initialisation strategy is of primary importance to mitigate gradient explosion or vanishing when training neural networks. Yet, the impact of initialisation parameters still lacks a precise theoretical understanding for several well-established architectures. Here, we propose a new initialisation for networks with sinusoidal activation functions such as \texttt{SIREN}, focusing on gradients control, their scaling with network depth, their impact on training and on generalization. To achieve this, we identify a closed-form expression for the initialisation of the parameters, differing from the original \texttt{SIREN} scheme. This expression is derived from fixed points obtained through the convergence of pre-activation distribution and the variance of Jacobian sequences. Controlling both gradients and targeting vanishing pre-activation helps preventing the emergence of inappropriate frequencies during estimation, thereby improving generalization. We further show that this initialisation strongly influences training dynamics through the Neural Tangent Kernel framework (NTK). Finally, we benchmark \texttt{SIREN} with the proposed initialisation against the original scheme and other baselines on function fitting and image reconstruction. The new initialisation consistently outperforms state-of-the-art methods across a wide range of reconstruction tasks, including those involving physics-informed neural networks.

</details>


### [410] [Neural expressiveness for beyond importance model compression](https://arxiv.org/abs/2512.06440)
*Angelos-Christos Maroudis,Sotirios Xydis*

Main category: cs.LG

TL;DR: This paper introduces "Expressiveness," a new criterion for neural network pruning, achieving better compression efficiency and reducing computational demand, while improving performance in specific tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current pruning methods that rely predominantly on neuron weight importance, and to propose a more effective data-agnostic approach based on activation overlap.

Method: The authors introduce "Expressiveness" as a metric that evaluates neurons' ability to redistribute informational resources based on activation overlap, independent of learning states. They also combine expressiveness with traditional importance-based methods for hybrid pruning strategies.

Result: The proposed approach achieved up to 10x higher parameter compression with minimal performance degradation. It outperformed traditional methods in compression efficiency and demonstrated a 46.1% MACs reduction in YOLOv8 with a parameter removal of 55.4% and a 3% increase in performance for object detection tasks.

Conclusion: Expressiveness provides a state-independent and versatile criterion for neural network pruning, offering both standalone and hybrid benefits, and highlights a new perspective on data-agnostic compression strategies.

Abstract: Neural Network Pruning has been established as driving force in the exploration of memory and energy efficient solutions with high throughput both during training and at test time. In this paper, we introduce a novel criterion for model compression, named "Expressiveness". Unlike existing pruning methods that rely on the inherent "Importance" of neurons' and filters' weights, ``Expressiveness" emphasizes a neuron's or group of neurons ability to redistribute informational resources effectively, based on the overlap of activations. This characteristic is strongly correlated to a network's initialization state, establishing criterion autonomy from the learning state stateless and thus setting a new fundamental basis for the expansion of compression strategies in regards to the "When to Prune" question. We show that expressiveness is effectively approximated with arbitrary data or limited dataset's representative samples, making ground for the exploration of Data-Agnostic strategies. Our work also facilitates a "hybrid" formulation of expressiveness and importance-based pruning strategies, illustrating their complementary benefits and delivering up to 10x extra gains w.r.t. weight-based approaches in parameter compression ratios, with an average of 1% in performance degradation. We also show that employing expressiveness (independently) for pruning leads to an improvement over top-performing and foundational methods in terms of compression efficiency. Finally, on YOLOv8, we achieve a 46.1% MACs reduction by removing 55.4\% of the parameters, with an increase of 3% in the mean Absolute Precision ($mAP_{50-95}$) for object detection on COCO dataset.

</details>


### [411] [BitStopper: An Efficient Transformer Attention Accelerator via Stage-fusion and Early Termination](https://arxiv.org/abs/2512.06457)
*Huizheng Wang,Hongbin Wang,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.LG

TL;DR: BitStopper, a new algorithm-architecture co-design, addresses inefficiencies in LLM attention by eliminating sparsity predictors, integrating execution stages, and optimizing memory utilization.


<details>
  <summary>Details</summary>
Motivation: Address the computational and hardware inefficiencies of self-attention mechanisms in large language models, particularly focusing on limitations of dynamic sparsity techniques.

Method: Introduce BitStopper with features like bit-serial enable stage fusion, lightweight adaptive token selection, and bit-level asynchronous processing, alongside an efficient architecture.

Result: BitStopper outperforms state-of-the-art Transformer accelerators by achieving up to 2.03x speedup in performance and 2.4x improvement in energy efficiency compared to competing designs.

Conclusion: BitStopper effectively reduces computation complexity and hardware overhead in self-attention mechanisms, offering significant improvements in performance and energy efficiency.

Abstract: Attention-based large language models (LLMs) have transformed modern AI applications, but the quadratic cost of self-attention imposes significant compute and memory overhead. Dynamic sparsity (DS) attention mitigates this, yet its hardware efficiency is limited by the added prediction stage and the heavy memory traffic it entails. To address these limitations, this paper proposes BitStopper, a fine-grained algorithm-architecture co-design that operates without a sparsity predictor. First, a bit-serial enable stage fusion (BESF) mechanism is proposed to reuse and minimize the memory access by progressively terminating trivial tokens and merging the prediction stage into the execution stage. Second, a lightweight and adaptive token selection (LATS) strategy is developed to work in concert with the bit-level sparsity speculation. Third, a bit-level asynchronous processing (BAP) strategy is employed to improve compute utilization during the on-demand bit-grained memory fetching. Finally, an elaborate architecture is designed to translate the theoretical complexity reduction into practical performance improvement. Extensive evaluations demonstrate that, compared to state-of-the-art (SOTA) Transformer accelerators, BitStopper achieves 2.03x and 1.89x speedups over Sanger and SOFA, respectively, while delivering 2.4x and 2.1x improvements in energy efficiency.

</details>


### [412] [Why Goal-Conditioned Reinforcement Learning Works: Relation to Dual Control](https://arxiv.org/abs/2512.06471)
*Nathan P. Lawrence,Ali Mesbah*

Main category: cs.LG

TL;DR: The paper analyzes goal-conditioned reinforcement learning (RL) through an optimal control framework and highlights its advantages in nonlinear and uncertain environments.


<details>
  <summary>Details</summary>
Motivation: To address why goal-conditioned RL succeeds while classical dense reward approaches can face challenges, especially in noisy and uncertain setups.

Method: The authors derived an optimality gap between classical objectives and goal-conditioned rewards, analyzed partially observed Markov decision processes, and validated their framework on nonlinear and uncertain systems using RL and predictive control methods.

Result: The framework demonstrates advantages of goal-conditioned policies through experiments in uncertain and nonlinear environments.

Conclusion: Goal-conditioned RL is better suited for complex settings, such as dual control problems, and outperforms classical approaches in challenging scenarios.

Abstract: Goal-conditioned reinforcement learning (RL) concerns the problem of training an agent to maximize the probability of reaching target goal states. This paper presents an analysis of the goal-conditioned setting based on optimal control. In particular, we derive an optimality gap between more classical, often quadratic, objectives and the goal-conditioned reward, elucidating the success of goal-conditioned RL and why classical ``dense'' rewards can falter. We then consider the partially observed Markov decision setting and connect state estimation to our probabilistic reward, further making the goal-conditioned reward well suited to dual control problems. The advantages of goal-conditioned policies are validated on nonlinear and uncertain environments using both RL and predictive control techniques.

</details>


### [413] [Optimizing LLMs Using Quantization for Mobile Execution](https://arxiv.org/abs/2512.06490)
*Agatsya Yadav,Renta Chintala Bhargavi*

Main category: cs.LG

TL;DR: The paper explores compressing large language models (LLMs) for mobile devices through post-training quantization (PTQ) to 4-bits, achieving significant size reduction while maintaining inference performance.


<details>
  <summary>Details</summary>
Motivation: Large Language Models have powerful capabilities but are often too large and computationally intensive for mobile devices. This work aims to enable efficient LLM deployment on resource-constrained devices.

Method: The authors utilized 4-bit post-training quantization (PTQ) through the BitsAndBytes library and the Hugging Face Transformers framework. They converted the quantized model into GGUF format with llama.cpp for mobile optimization.

Result: The 4-bit quantization reduced the model size by 68.66%, making it feasible for execution on Android devices. The quantized model successfully performed inference tasks within the Termux and Ollama frameworks.

Conclusion: The study demonstrates that PTQ at 4-bit precision, paired with mobile-optimized formats, allows LLMs like Llama 3.2 3B to achieve a balance between reduced size and satisfactory performance, enabling practical mobile deployment.

Abstract: Large Language Models (LLMs) offer powerful capabilities, but their significant size and computational requirements hinder deployment on resource-constrained mobile devices. This paper investigates Post-Training Quantization (PTQ) for compressing LLMs for mobile execution. We apply 4-bit PTQ using the BitsAndBytes library with the Hugging Face Transformers framework to Meta's Llama 3.2 3B model. The quantized model is converted to GGUF format using llama.cpp tools for optimized mobile inference. The PTQ workflow achieves a 68.66% reduction in model size through 4-bit quantization, enabling the Llama 3.2 3B model to run efficiently on an Android device. Qualitative validation shows that the 4-bit quantized model can perform inference tasks successfully. We demonstrate the feasibility of running the quantized GGUF model on an Android device using the Termux environment and the Ollama framework. PTQ, especially at 4-bit precision combined with mobile-optimized formats like GGUF, provides a practical pathway for deploying capable LLMs on mobile devices, balancing model size and performance.

</details>


### [414] [Diagnosis-based mortality prediction for intensive care unit patients via transfer learning](https://arxiv.org/abs/2512.06511)
*Mengqi Xu,Subha Maity,Joel Dubin*

Main category: cs.LG

TL;DR: This paper evaluates the use of transfer learning for improving diagnosis-specific mortality predictions in ICUs, outperforming traditional models and scoring methods.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve mortality prediction models in ICUs by addressing diagnostic heterogeneity that current models often overlook.

Method: The researchers used GLM- and XGBoost-based models with transfer learning techniques to analyze data from the eICU Collaborative Research Database.

Result: Transfer learning not only outperformed diagnosis-specific and traditional pooled data models but also showed better calibration and higher predictive performance across different cutoff criteria, making it more effective.

Conclusion: Transfer learning is a superior approach for mortality prediction in the ICU setting, especially when addressing diagnostic heterogeneity.

Abstract: In the intensive care unit, the underlying causes of critical illness vary substantially across diagnoses, yet prediction models accounting for diagnostic heterogeneity have not been systematically studied. To address the gap, we evaluate transfer learning approaches for diagnosis-specific mortality prediction and apply both GLM- and XGBoost-based models to the eICU Collaborative Research Database. Our results demonstrate that transfer learning consistently outperforms models trained only on diagnosis-specific data and those using a well-known ICU severity-of-illness score, i.e., APACHE IVa, alone, while also achieving better calibration than models trained on the pooled data. Our findings also suggest that the Youden cutoff is a more appropriate decision threshold than the conventional 0.5 for binary outcomes, and that transfer learning maintains consistently high predictive performance across various cutoff criteria.

</details>


### [415] [Hierarchical geometric deep learning enables scalable analysis of molecular dynamics](https://arxiv.org/abs/2512.06520)
*Zihan Pengmei,Spencer C. Guo,Chatipat Lorpaiboon,Aaron R. Dinner*

Main category: cs.LG

TL;DR: This paper addresses challenges in analyzing molecular dynamics simulations of large biomolecular systems using graph neural networks by introducing a method to aggregate local information, enhancing efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome the difficulties in analyzing molecular dynamics simulations due to limitations in feature engineering, memory runtime requirements, and capturing long-range interactions efficiently.

Method: The paper proposes a technique that aggregates local information in graph neural networks to reduce computational resource demands while maintaining atomic detail during analysis.

Result: The method enables analysis of protein-nucleic acid complexes with thousands of residues efficiently on single GPUs within minutes and shows improved performance and interpretability for systems with sufficient data.

Conclusion: The approach expands the applicability of graph neural networks for biomolecular dynamics simulations by resolving scalability and efficiency issues.

Abstract: Molecular dynamics simulations can generate atomically detailed trajectories of complex systems, but analyzing these dynamics can be challenging when systems lack well-established quantitative descriptors (features). Graph neural networks (GNNs) in which messages are passed between nodes that represent atoms that are spatial neighbors promise to obviate manual feature engineering, but the use of GNNs with biomolecular systems of more than a few hundred residues has been limited in the context of analyzing dynamics by both difficulties in capturing the details of long-range interactions with message passing and the memory and runtime requirements associated with large graphs. Here, we show how local information can be aggregated to reduce memory and runtime requirements without sacrificing atomic detail. We demonstrate that this approach opens the door to analyzing simulations of protein-nucleic acid complexes with thousands of residues on single GPUs within minutes. For systems with hundreds of residues, for which there are sufficient data to make quantitative comparisons, we show that the approach improves performance and interpretability.

</details>


### [416] [Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning](https://arxiv.org/abs/2512.06533)
*Ming Chen,Sheng Tang,Rong-Xi Tan,Ziniu Li,Jiacheng Chen,Ke Xue,Chao Qian*

Main category: cs.LG

TL;DR: This paper introduces a reinforcement learning-based approach to improve decoding-based regression for numerical predictions, addressing misalignment in current methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of decoding-based regression methods which struggle with aligning token-level objectives with global numerical precision.

Method: The authors propose formulating regression as a Markov Decision Process and introduce RL-based techniques (ReMax and GRPO) for sequence-level rewards to improve numerical coherence.

Result: Experiments on tabular regression and code metric regression datasets show the RL-based method consistently outperforms state-of-the-art alternatives in precision and generalization.

Conclusion: The RL-based approach establishes a stronger paradigm for decoding-based regression, enhancing predictive precision and sampling efficiency in numerical predictions.

Abstract: Decoding-based regression, which reformulates regression as a sequence generation task, has emerged as a promising paradigm of applying large language models for numerical prediction. However, its progress is hindered by the misalignment between discrete token-level objectives (e.g., cross-entropy) and continuous numerical values. Existing approaches relying on token-level constraints often fail to capture the global magnitude of the target value, limiting their precision and generalization. In this paper, we propose to unlock the potential of decoding-based regression via Reinforcement Learning (RL). We formulate the generation process as a Markov Decision Process, utilizing sequence-level rewards to enforce global numerical coherence. Extensive experiments on tabular regression and code metric regression demonstrate that our method (specifically with ReMax and GRPO) consistently outperforms both state-of-the-art token-level baselines and traditional regression heads, showing the superiority of introducing sequence-level signals. Our analysis further reveals that RL significantly enhances sampling efficiency and predictive precision, establishing decoding-based regression as a robust and accurate paradigm for general-purpose numerical prediction.

</details>


### [417] [Deep Manifold Part 2: Neural Network Mathematics](https://arxiv.org/abs/2512.06563)
*Max Y. Ma,Gen-Hua Shi*

Main category: cs.LG

TL;DR: The paper explores neural networks through manifold complexity and fixed-point theory, highlighting how learnability emerges when fixed points stabilize, while advocating for distributed models to better handle real-world complexities.


<details>
  <summary>Details</summary>
Motivation: The study seeks to understand the fundamental basis of neural network behavior, focusing on the dynamics of learnability under fixed points and complexity constraints arising from data and geometry.

Method: The authors employ geometry and fixed-point theory to develop equations governing neural networks, analyzing their behavior using boundary-conditioned iterations and residual dynamics.

Result: The research illuminates constraints like data-induced plasticity and shifting node covers, showing that neural networks build fixed points iteratively to stabilize learning.

Conclusion: The work advocates for distributed neural network architectures, promoting designs that distribute manifold complexity and enhance modeling under real-world data complexities.

Abstract: This work develops the global equations of neural networks through stacked piecewise manifolds, fixed-point theory, and boundary-conditioned iteration. Once fixed coordinates and operators are removed, a neural network appears as a learnable numerical computation shaped by manifold complexity, high-order nonlinearity, and boundary conditions. Real-world data impose strong data complexity, near-infinite scope, scale, and minibatch fragmentation, while training dynamics produce learning complexity through shifting node covers, curvature accumulation, and the rise and decay of plasticity. These forces constrain learnability and explain why capability emerges only when fixed-point regions stabilize. Neural networks do not begin with fixed points; they construct them through residual-driven iteration. This perspective clarifies the limits of monolithic models under geometric and data-induced plasticity and motivates architectures and federated systems that distribute manifold complexity across many elastic models, forming a coherent world-modeling framework grounded in geometry, algebra, fixed points, and real-data complexity.

</details>


### [418] [On fine-tuning Boltz-2 for protein-protein affinity prediction](https://arxiv.org/abs/2512.06592)
*James King,Lewis Cornwall,Andrei Cristian Nica,James Day,Aaron Sim,Neil Dalchau,Lilly Wollman,Joshua Meyers*

Main category: cs.LG

TL;DR: The paper evaluates Boltz-2-PPI, a structure-based protein-protein affinity predictor, revealing its underperformance compared to sequence-based models and exploring complementary integration methods.


<details>
  <summary>Details</summary>
Motivation: Understanding protein-protein interactions' binding affinity is crucial for molecular biology and therapeutic design.

Method: Boltz-2 was adapted for protein-protein affinity regression and tested on two datasets, comparing structure- and sequence-based methods.

Result: Boltz-2-PPI had high structural accuracy but underperformed in affinity prediction compared to sequence models; combined embeddings improved weaker models.

Conclusion: Structure-based models are currently suboptimal for affinity prediction and benefit from complementary integration with sequence-based data.

Abstract: Accurate prediction of protein-protein binding affinity is vital for understanding molecular interactions and designing therapeutics. We adapt Boltz-2, a state-of-the-art structure-based protein-ligand affinity predictor, for protein-protein affinity regression and evaluate it on two datasets, TCR3d and PPB-affinity. Despite high structural accuracy, Boltz-2-PPI underperforms relative to sequence-based alternatives in both small- and larger-scale data regimes. Combining embeddings from Boltz-2-PPI with sequence-based embeddings yields complementary improvements, particularly for weaker sequence models, suggesting different signals are learned by sequence- and structure-based models. Our results echo known biases associated with training with structural data and suggest that current structure-based representations are not primed for performant affinity prediction.

</details>


### [419] [A Fast and Effective Solution to the Problem of Look-ahead Bias in LLMs](https://arxiv.org/abs/2512.06607)
*Humzah Merchant,Bradford Levy*

Main category: cs.LG

TL;DR: This paper addresses the challenge of applying large language models (LLMs) to predictive tasks in finance by introducing an inference-time method to mitigate look-ahead bias without expensive retraining.


<details>
  <summary>Details</summary>
Motivation: Applying LLMs in finance faces issues such as look-ahead bias due to their training on long time-series data. This prevents conventional backtesting methods used in finance.

Method: The paper proposes a novel technique where inference is guided using logits adjustment. This involves employing two smaller, specialized models: one fine-tuned on forgetting specific information and the other on retaining necessary information.

Result: The method effectively removes unwanted knowledge, biases, and improves on prior approaches for finance prediction tasks.

Conclusion: This approach offers a fast, effective, and low-cost way to address look-ahead bias in financial applications of LLM without needing retraining, outperforming existing methods.

Abstract: Applying LLMs to predictive tasks in finance is challenging due to look-ahead bias resulting from their training on long time-series data. This precludes the backtests typically employed in finance since retraining frontier models from scratch with a specific knowledge cutoff is prohibitive. In this paper, we introduce a fast, effective, and low-cost alternative. Our method guides generation at inference time by adjusting the logits of a large base model using a pair of smaller, specialized models -- one fine-tuned on information to be forgotten and another on information to be retained. We demonstrate that our method effectively removes both verbatim and semantic knowledge, corrects biases, and outperforms prior methods.

</details>


### [420] [Vector Quantization using Gaussian Variational Autoencoder](https://arxiv.org/abs/2512.06609)
*Tongda Xu,Wendi Zheng,Jiajun He,Jose Miguel Hernandez-Lobato,Yan Wang,Ya-Qin Zhang,Jie Tang*

Main category: cs.LG

TL;DR: This paper proposes Gaussian Quant (GQ), a methodology to convert Gaussian VAEs into VQ-VAEs without retraining, solving training challenges of discrete auto-encoders.


<details>
  <summary>Details</summary>
Motivation: The complexity of training VQ-VAE due to discretization motivated the development of a more straightforward way to achieve effective training and performance using Gaussian VAEs.

Method: The method involves generating Gaussian noise as a codebook and matching it to posterior means for quantization. A new heuristic constraint, Target Divergence Constraint (TDC), is developed to enhance performance.

Result: GQ outperforms prior techniques like VQGAN and TokenBridge using UNet and ViT architectures. TDC training further improves Gaussian VAE discretization methods.

Conclusion: By introducing GQ and TDC, the study shows significant advancements in VQ-VAE training techniques, achieving better results and reduced complexity without retraining.

Abstract: Vector quantized variational autoencoder (VQ-VAE) is a discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose a simple yet effective technique, dubbed Gaussian Quant (GQ), that converts a Gaussian VAE with certain constraint into a VQ-VAE without training. GQ generates random Gaussian noise as a codebook and finds the closest noise to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE.

</details>


### [421] [Quantum Temporal Convolutional Neural Networks for Cross-Sectional Equity Return Prediction: A Comparative Benchmark Study](https://arxiv.org/abs/2512.06630)
*Chi-Sheng Chen,Xinyu Zhang,Rong Fu,Qiuzhe Xie,Fan Zhang*

Main category: cs.LG

TL;DR: The paper proposes a Quantum Temporal Convolutional Neural Network (QTCNN) for stock market prediction, demonstrating its superiority over classical models in processing complex financial data.


<details>
  <summary>Details</summary>
Motivation: Classical forecasting models face shortcomings like noisy input handling, regime shifts, and limited generalization, particularly in dynamic financial environments.

Method: A hybrid Quantum Temporal Convolutional Neural Network (QTCNN) that combines a classical temporal encoder with quantum convolution circuits for cross-sectional equity return prediction.

Result: The QTCNN achieved a Sharpe ratio of 0.538 on the JPX Tokyo Stock Exchange dataset, outperforming the best classical model by 72%.

Conclusion: QTCNN demonstrates significant potential as a robust and practical tool for quantum-enhanced financial forecasting, addressing key deficiencies of classical models.

Abstract: Quantum machine learning offers a promising pathway for enhancing stock market prediction, particularly under complex, noisy, and highly dynamic financial environments. However, many classical forecasting models struggle with noisy input, regime shifts, and limited generalization capacity. To address these challenges, we propose a Quantum Temporal Convolutional Neural Network (QTCNN) that combines a classical temporal encoder with parameter-efficient quantum convolution circuits for cross-sectional equity return prediction. The temporal encoder extracts multi-scale patterns from sequential technical indicators, while the quantum processing leverages superposition and entanglement to enhance feature representation and suppress overfitting. We conduct a comprehensive benchmarking study on the JPX Tokyo Stock Exchange dataset and evaluate predictions through long-short portfolio construction using out-of-sample Sharpe ratio as the primary performance metric. QTCNN achieves a Sharpe ratio of 0.538, outperforming the best classical baseline by approximately 72\%. These results highlight the practical potential of quantum-enhanced forecasting model, QTCNN, for robust decision-making in quantitative finance.

</details>


### [422] [The Impact of Data Characteristics on GNN Evaluation for Detecting Fake News](https://arxiv.org/abs/2512.06638)
*Isha Karn,David Jensen*

Main category: cs.LG

TL;DR: This paper highlights limitations in the widely-used graph neural network (GNN) datasets GossipCop and PolitiFact for fake news detection, showing their shallow graph structures fail to test the utility of GNNs.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that current benchmark datasets effectively evaluate the structural modeling capabilities of GNNs in fake news detection.

Method: The study systematically compares the performance of five GNN architectures with a structure-agnostic multilayer perceptron (MLP) using the same node features. Controlled experiments are conducted by shuffling node features and randomizing edge structures.

Result: MLPs match or slightly trail GNNs on these benchmarks, with performance gaps within 1-2%. Structural analysis shows shallow topologies with one-hop nodes and negligible structural diversity. GNNs outperform MLPs only on synthetic datasets with noisy features and informative structure.

Conclusion: Widely used benchmarks fail to meaningfully test the structural utility of GNN models, necessitating the design of datasets with richer and more diverse graph topologies for accurate evaluation.

Abstract: Graph neural networks (GNNs) are widely used for the detection of fake news by modeling the content and propagation structure of news articles on social media. We show that two of the most commonly used benchmark data sets - GossipCop and PolitiFact - are poorly suited to evaluating the utility of models that use propagation structure. Specifically, these data sets exhibit shallow, ego-like graph topologies that provide little or no ability to differentiate among modeling methods. We systematically benchmark five GNN architectures against a structure-agnostic multilayer perceptron (MLP) that uses the same node features. We show that MLPs match or closely trail the performance of GNNs, with performance gaps often within 1-2% and overlapping confidence intervals. To isolate the contribution of structure in these datasets, we conduct controlled experiments where node features are shuffled or edge structures randomized. We find that performance collapses under feature shuffling but remains stable under edge randomization. This suggests that structure plays a negligible role in these benchmarks. Structural analysis further reveals that over 75% of nodes are only one hop from the root, exhibiting minimal structural diversity. In contrast, on synthetic datasets where node features are noisy and structure is informative, GNNs significantly outperform MLPs. These findings provide strong evidence that widely used benchmarks do not meaningfully test the utility of modeling structural features, and they motivate the development of datasets with richer, more diverse graph topologies.

</details>


### [423] [Financial Fraud Identification and Interpretability Study for Listed Companies Based on Convolutional Neural Network](https://arxiv.org/abs/2512.06648)
*Xiao Li*

Main category: cs.LG

TL;DR: The paper introduces a CNN-based financial fraud detection framework for Chinese A-share firms to predict fraud in advance, overcoming traditional model limitations.


<details>
  <summary>Details</summary>
Motivation: Fraud in listed firms damages capital markets, yet detection is costly and challenging due to traditional models' limitations in handling nonlinear features and timeliness.

Method: A CNN model using panel data transformed into image-like representations was developed to capture temporal and cross-sectional fraud patterns, improving interpretability with local explanation techniques.

Result: The CNN model demonstrated superior accuracy, robustness, and early-warning capability over logistic regression and LightGBM, with tailored classification thresholds enhancing high-risk settings.

Conclusion: The framework reliably detects financial fraud, identifies key predictors, and delivers explanations, making it a robust solution for early fraud detection efforts.

Abstract: Since the emergence of joint-stock companies, financial fraud by listed firms has repeatedly undermined capital markets. Fraud is difficult to detect because of covert tactics and the high labor and time costs of audits. Traditional statistical models are interpretable but struggle with nonlinear feature interactions, while machine learning models are powerful but often opaque. In addition, most existing methods judge fraud only for the current year based on current year data, limiting timeliness.
  This paper proposes a financial fraud detection framework for Chinese A-share listed companies based on convolutional neural networks (CNNs). We design a feature engineering scheme that transforms firm-year panel data into image like representations, enabling the CNN to capture cross-sectional and temporal patterns and to predict fraud in advance. Experiments show that the CNN outperforms logistic regression and LightGBM in accuracy, robustness, and early-warning performance, and that proper tuning of the classification threshold is crucial in high-risk settings.
  To address interpretability, we analyze the model along the dimensions of entity, feature, and time using local explanation techniques. We find that solvency, ratio structure, governance structure, and internal control are general predictors of fraud, while environmental indicators matter mainly in high-pollution industries. Non-fraud firms share stable feature patterns, whereas fraud firms exhibit heterogeneous patterns concentrated in short time windows. A case study of Guanong Shares in 2022 shows that cash flow analysis, social responsibility, governance structure, and per-share indicators are the main drivers of the model's fraud prediction, consistent with the company's documented misconduct.

</details>


### [424] [Estimating Black Carbon Concentration from Urban Traffic Using Vision-Based Machine Learning](https://arxiv.org/abs/2512.06649)
*Camellia Zakaria,Aryan Sadeghi,Weaam Jaafar,Junshi Xu,Alex Mariakakis,Marianne Hatzopoulou*

Main category: cs.LG

TL;DR: This paper develops a machine learning system using traffic video and weather data to estimate urban black carbon (BC) emissions, a pollutant primarily stemming from traffic and disproportionately impacting marginalized communities.


<details>
  <summary>Details</summary>
Motivation: The lack of localized data on urban BC emissions impedes effective policy-making, especially given that existing monitoring relies on costly instruments. At the same time, cities have extensive traffic monitoring infrastructure, creating an opportunity to understand environmental impacts using these resources.

Method: The authors use a machine learning model to extract visual vehicle data from traffic videos and combine it with weather data to estimate street-level BC concentrations. Model performance is validated using metrics like R-squared (0.72) and RMSE (129.42 ng/m3).

Result: The proposed model successfully estimates urban BC concentrations at a street level, validating its accuracy and potential usefulness for understanding localized traffic emissions.

Conclusion: This method leverages existing urban infrastructure to generate actionable BC data, informing urban planning, pollution mitigation, public health, and environmental justice.

Abstract: Black carbon (BC) emissions in urban areas are primarily driven by traffic, with hotspots near major roads disproportionately affecting marginalized communities. Because BC monitoring is typically performed using costly and specialized instruments. there is little to no available data on BC from local traffic sources that could help inform policy interventions targeting local factors. By contrast, traffic monitoring systems are widely deployed in cities around the world, highlighting the imbalance between what we know about traffic conditions and what do not know about their environmental consequences. To bridge this gap, we propose a machine learning-driven system that extracts visual information from traffic video to capture vehicles behaviors and conditions. Combining these features with weather data, our model estimates BC at street level, achieving an R-squared value of 0.72 and RMSE of 129.42 ng/m3 (nanogram per cubic meter). From a sustainability perspective, this work leverages resources already supported by urban infrastructure and established modeling techniques to generate information relevant to traffic emission. Obtaining BC concentration data provides actionable insights to support pollution reduction, urban planning, public health, and environmental justice at the local municipal level.

</details>


### [425] [Adaptive Test-Time Training for Predicting Need for Invasive Mechanical Ventilation in Multi-Center Cohorts](https://arxiv.org/abs/2512.06652)
*Xiaolei Lu,Shamim Nemati*

Main category: cs.LG

TL;DR: The paper introduces Adaptive Test-Time Training (AdaTTT), an enhanced method for predicting invasive mechanical ventilation (IMV) in ICU patients, overcoming domain shift challenges with improved self-supervised learning techniques.


<details>
  <summary>Details</summary>
Motivation: Predicting the need for IMV in ICU patients is essential for better interventions and resource allocation, but domain shifts across institutions reduce model reliability.

Method: AdaTTT leverages self-supervised learning tasks, dynamic masking strategies, prototype learning, and Partial Optimal Transport (POT) for flexible feature alignment and robust predictions.

Result: AdaTTT achieves competitive classification performance on prediction tasks across multi-center ICU settings despite domain shifts.

Conclusion: AdaTTT improves the adaptability and robustness of predictive models for ICU applications, addressing domain shift challenges effectively.

Abstract: Accurate prediction of the need for invasive mechanical ventilation (IMV) in intensive care units (ICUs) patients is crucial for timely interventions and resource allocation. However, variability in patient populations, clinical practices, and electronic health record (EHR) systems across institutions introduces domain shifts that degrade the generalization performance of predictive models during deployment. Test-Time Training (TTT) has emerged as a promising approach to mitigate such shifts by adapting models dynamically during inference without requiring labeled target-domain data. In this work, we introduce Adaptive Test-Time Training (AdaTTT), an enhanced TTT framework tailored for EHR-based IMV prediction in ICU settings. We begin by deriving information-theoretic bounds on the test-time prediction error and demonstrate that it is constrained by the uncertainty between the main and auxiliary tasks. To enhance their alignment, we introduce a self-supervised learning framework with pretext tasks: reconstruction and masked feature modeling optimized through a dynamic masking strategy that emphasizes features critical to the main task. Additionally, to improve robustness against domain shifts, we incorporate prototype learning and employ Partial Optimal Transport (POT) for flexible, partial feature alignment while maintaining clinically meaningful patient representations. Experiments across multi-center ICU cohorts demonstrate competitive classification performance on different test-time adaptation benchmarks.

</details>


### [426] [GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering](https://arxiv.org/abs/2512.06655)
*Jehyeok Yeon,Federico Cinus,Yifan Wu,Luca Luceri*

Main category: cs.LG

TL;DR: This paper presents Graph-Regularized Sparse Autoencoders (GSAEs) to address safety challenges in large language models by creating smoother, distributed safety representations and achieving high selective refusal rates while maintaining task accuracy.


<details>
  <summary>Details</summary>
Motivation: Large language models often generate harmful content due to adversarial prompts or jailbreak attacks, and existing safety mechanisms are insufficient—either basic filtering or oversimplified latent steering.

Method: The paper proposes GSAEs, which extend Sparse Autoencoders by introducing a Laplacian smoothness penalty to co-activation graphs, distributing safety representations across multiple features. A two-stage gating mechanism is used to steer safety at runtime.

Result: GSAE steering achieved an 82% selective refusal rate, outperforming standard SAE (42%), while maintaining good task accuracy (e.g., 70% on TriviaQA). It consistently showed over 90% refusal of harmful content across various LLMs and proved robust against jailbreak attacks.

Conclusion: GSAEs provide an effective and generalizable framework for enhancing language model safety by enabling adaptive safety enforcement while preserving task performance.

Abstract: Large language models (LLMs) face critical safety challenges, as they can be manipulated to generate harmful content through adversarial prompts and jailbreak attacks. Many defenses are typically either black-box guardrails that filter outputs, or internals-based methods that steer hidden activations by operationalizing safety as a single latent feature or dimension. While effective for simple concepts, this assumption is limiting, as recent evidence shows that abstract concepts such as refusal and temporality are distributed across multiple features rather than isolated in one. To address this limitation, we introduce Graph-Regularized Sparse Autoencoders (GSAEs), which extends SAEs with a Laplacian smoothness penalty on the neuron co-activation graph. Unlike standard SAEs that assign each concept to a single latent feature, GSAEs recover smooth, distributed safety representations as coherent patterns spanning multiple features. We empirically demonstrate that GSAE enables effective runtime safety steering, assembling features into a weighted set of safety-relevant directions and controlling them with a two-stage gating mechanism that activates interventions only when harmful prompts or continuations are detected during generation. This approach enforces refusals adaptively while preserving utility on benign queries. Across safety and QA benchmarks, GSAE steering achieves an average 82% selective refusal rate, substantially outperforming standard SAE steering (42%), while maintaining strong task accuracy (70% on TriviaQA, 65% on TruthfulQA, 74% on GSM8K). Robustness experiments further show generalization across LLaMA-3, Mistral, Qwen, and Phi families and resilience against jailbreak attacks (GCG, AutoDAN), consistently maintaining >= 90% refusal of harmful content.

</details>


### [427] [Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods](https://arxiv.org/abs/2512.06665)
*Panagiota Kiourti,Anu Singh,Preeti Duraipandian,Weichao Zhou,Wenchao Li*

Main category: cs.LG

TL;DR: The paper introduces a new definition and evaluation approach for the robustness of feature attribution methods in deep neural networks, addressing limitations in existing metrics.


<details>
  <summary>Details</summary>
Motivation: Current attribution robustness evaluations fail to consider differences in model outputs and their impact on feature attribution robustness.

Method: The paper proposes a novel definition of similar inputs, a new robustness metric, and employs generative adversarial networks to generate such inputs for evaluation.

Result: The study highlights weaknesses in current attribution methods, suggesting the need for objective metrics that evaluate the attribution method itself rather than the neural network.

Conclusion: Future evaluations of attribution robustness should adopt objective and refined metrics to ensure reliable and accurate assessments, enhancing the understanding of attribution methods.

Abstract: This paper studies the robustness of feature attribution methods for deep neural networks. It challenges the current notion of attributional robustness that largely ignores the difference in the model's outputs and introduces a new way of evaluating the robustness of attribution methods. Specifically, we propose a new definition of similar inputs, a new robustness metric, and a novel method based on generative adversarial networks to generate these inputs. In addition, we present a comprehensive evaluation with existing metrics and state-of-the-art attribution methods. Our findings highlight the need for a more objective metric that reveals the weaknesses of an attribution method rather than that of the neural network, thus providing a more accurate evaluation of the robustness of attribution methods.

</details>


### [428] [The Meta-Learning Gap: Combining Hydra and Quant for Large-Scale Time Series Classification](https://arxiv.org/abs/2512.06666)
*Urav Maniar*

Main category: cs.LG

TL;DR: The paper addresses the trade-off between accuracy and computational efficiency in time series classification by combining two efficient algorithms, achieving slight mean accuracy improvement but highlighting challenges in effectively combining algorithm outputs.


<details>
  <summary>Details</summary>
Motivation: The authors aim to balance the high accuracy of ensemble models with computational practicality for large-scale datasets, addressing inefficiency in current state-of-the-art methods.

Method: Two complementary algorithms, Hydra and Quant, are combined in different ensemble configurations and applied to large-scale datasets to evaluate their accuracy and computational efficiency.

Result: The study achieved a mean accuracy improvement (0.829 to 0.836) but found limited meta-learning performance, with significant potential for improved combination strategies.

Conclusion: The challenge lies not in ensuring algorithm diversity but in developing effective strategies to combine them, suggesting opportunities for better meta-learning solutions to enhance performance.

Abstract: Time series classification faces a fundamental trade-off between accuracy and computational efficiency. While comprehensive ensembles like HIVE-COTE 2.0 achieve state-of-the-art accuracy, their 340-hour training time on the UCR benchmark renders them impractical for large-scale datasets. We investigate whether targeted combinations of two efficient algorithms from complementary paradigms can capture ensemble benefits while maintaining computational feasibility. Combining Hydra (competing convolutional kernels) and Quant (hierarchical interval quantiles) across six ensemble configurations, we evaluate performance on 10 large-scale MONSTER datasets (7,898 to 1,168,774 training instances). Our strongest configuration improves mean accuracy from 0.829 to 0.836, succeeding on 7 of 10 datasets. However, prediction-combination ensembles capture only 11% of theoretical oracle potential, revealing a substantial meta-learning optimization gap. Feature-concatenation approaches exceeded oracle bounds by learning novel decision boundaries, while prediction-level complementarity shows moderate correlation with ensemble gains. The central finding: the challenge has shifted from ensuring algorithms are different to learning how to combine them effectively. Current meta-learning strategies struggle to exploit the complementarity that oracle analysis confirms exists. Improved combination strategies could potentially double or triple ensemble gains across diverse time series classification applications.

</details>


### [429] [GradientSpace: Unsupervised Data Clustering for Improved Instruction Tuning](https://arxiv.org/abs/2512.06678)
*Shrihari Sridharan,Deepak Ravikumar,Anand Raghunathan,Kaushik Roy*

Main category: cs.LG

TL;DR: GradientSpace improves large language model instruction tuning by clustering samples in gradient space, training specialized experts, and significantly reducing latency while enhancing accuracy.


<details>
  <summary>Details</summary>
Motivation: Real-world datasets for LLM instruction tuning are diverse and cause gradient interference, which degrades performance. Current approaches to group data are suboptimal because they misrepresent how data affects model parameters.

Method: The GradientSpace framework clusters samples in full-dimensional gradient space using an online SVD-based algorithm. Each cluster trains specialized LoRA experts, with a lightweight router selecting the best expert during inference.

Result: Improved coherent specialization among experts and consistent accuracy gains across diverse tasks (math, code generation, finance, creative writing), outperforming state-of-the-art methods and reducing inference latency.

Conclusion: GradientSpace effectively addresses gradient interference and boosts LLM performance while being computationally efficient during inference.

Abstract: Instruction tuning is one of the key steps required for adapting large language models (LLMs) to a broad spectrum of downstream applications. However, this procedure is difficult because real-world datasets are rarely homogeneous; they consist of a mixture of diverse information, causing gradient interference, where conflicting gradients pull the model in opposing directions, degrading performance. A common strategy to mitigate this issue is to group data based on semantic or embedding similarity. However, this fails to capture how data influences model parameters during learning. While recent works have attempted to cluster gradients directly, they randomly project gradients into lower dimensions to manage memory, which leads to accuracy loss. Moreover, these methods rely on expert ensembles which necessitates multiple inference passes and expensive on-the-fly gradient computations during inference. To address these limitations, we propose GradientSpace, a framework that clusters samples directly in full-dimensional gradient space. We introduce an online SVD-based algorithm that operates on LoRA gradients to identify latent skills without the infeasible cost of storing all sample gradients. Each cluster is used to train a specialized LoRA expert along with a lightweight router trained to select the best expert during inference. We show that routing to a single, appropriate expert outperforms expert ensembles used in prior work, while significantly reducing inference latency. Our experiments across mathematical reasoning, code generation, finance, and creative writing tasks demonstrate that GradientSpace leads to coherent expert specialization and consistent accuracy gains over state-of-the-art clustering methods and finetuning techniques.

</details>


### [430] [State Diversity Matters in Offline Behavior Distillation](https://arxiv.org/abs/2512.06692)
*Shiye Lei,Zhihao Cheng,Dacheng Tao*

Main category: cs.LG

TL;DR: This paper uncovers a misalignment between high-quality original datasets and their distilled synthetic counterparts in Offline Behavior Distillation (OBD), emphasizing the importance of state diversity and proposing a novel algorithm (SDW OBD) to address this.


<details>
  <summary>Details</summary>
Motivation: The study seeks to optimize Offline Behavior Distillation (OBD) for efficient policy training by addressing the observed misalignment between original and synthetic datasets, particularly focusing on state diversity's role.

Method: The authors conduct empirical analyses to examine policy performance under various training loss conditions and propose a novel algorithm, SDW OBD, which incorporates state density weighting to increase state diversity in the distilled synthetic dataset.

Result: The SDW OBD algorithm demonstrates significant performance improvements across multiple D4RL datasets, particularly in scenarios where the original dataset lacks state diversity.

Conclusion: State diversity is shown to be a critical factor in the success of OBD, and the proposed SDW OBD effectively addresses this, enhancing policy performance on diverse tasks.

Abstract: Offline Behavior Distillation (OBD), which condenses massive offline RL data into a compact synthetic behavioral dataset, offers a promising approach for efficient policy training and can be applied across various downstream RL tasks. In this paper, we uncover a misalignment between original and distilled datasets, observing that a high-quality original dataset does not necessarily yield a superior synthetic dataset. Through an empirical analysis of policy performance under varying levels of training loss, we show that datasets with greater state diversity outperforms those with higher state quality when training loss is substantial, as is often the case in OBD, whereas the relationship reverses under minimal loss, which contributes to the misalignment. By associating state quality and diversity in reducing pivotal and surrounding error, respectively, our theoretical analysis establishes that surrounding error plays a more crucial role in policy performance when pivotal error is large, thereby highlighting the importance of state diversity in OBD scenario. Furthermore, we propose a novel yet simple algorithm, state density weighted (SDW) OBD, which emphasizes state diversity by weighting the distillation objective using the reciprocal of state density, thereby distilling a more diverse state information into synthetic data. Extensive experiments across multiple D4RL datasets confirm that SDW significantly enhances OBD performance when the original dataset exhibits limited state diversity.

</details>


### [431] [Mitigating Barren plateaus in quantum denoising diffusion probabilistic models](https://arxiv.org/abs/2512.06695)
*Haipeng Cao,Kaining Zhang,Dacheng Tao,Zhaofeng Su*

Main category: cs.LG

TL;DR: The paper addresses issues in the Quantum Denoising Diffusion Probabilistic Model (QuDDPM) and proposes an improved model to mitigate barren plateaus, enhancing the quality of quantum generative learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance quantum generative models' ability to efficiently learn and generate quantum data while addressing the barren plateau problem found in QuDDPMs.

Method: The authors introduce a revised QuDDPM using a distribution that maintains a distance from the Haar distribution to overcome barren plateaus, improving model trainability.

Result: The improved QuDDPM successfully mitigates barren plateaus and generates higher-quality quantum data, which is confirmed through both theoretical analysis and experimental validation.

Conclusion: The research demonstrates the feasibility of an enhanced QuDDPM framework, offering scalable and efficient quantum generative learning with better sample quality.

Abstract: Quantum generative models leverage quantum superposition and entanglement to enhance learning efficiency for both classical and quantum data. The quantum denoising diffusion probabilistic model (QuDDPM), inspired by its classical counterpart, has been proposed as a promising framework for quantum generative learning. QuDDPM is capable of efficiently learning and generating quantum data, and it demonstrates excellent performance in learning correlated quantum noise models, quantum many-body phases, and the topological structure of quantum data. However, we show that barren plateaus emerge in QuDDPMs due to the use of 2-design states as the input for the denoising process, which severely undermines the performance of QuDDPM. Through theoretical analysis and experimental validation, we confirm the presence of barren plateaus in the original QuDDPM. To address this issue, we introduce an improved QuDDPM that utilizes a distribution maintaining a certain distance from the Haar distribution, ensuring better trainability. Experimental results demonstrate that our approach effectively mitigates the barren plateau problem and generates samples with higher quality, paving the way for scalable and efficient quantum generative learning.

</details>


### [432] [Pathway to $O(\sqrt{d})$ Complexity bound under Wasserstein metric of flow-based models](https://arxiv.org/abs/2512.06702)
*Xiangjun Meng,Zhongjian Wang*

Main category: cs.LG

TL;DR: The paper provides tools for estimating error in flow-based generative models under the Wasserstein metric and establishes dimension-based optimal sampling complexity as $O(\sqrt{d})$.


<details>
  <summary>Details</summary>
Motivation: To address error estimation and sampling complexity bound for flow-based generative models using grounded mathematical analysis.

Method: Analytical tools focus on controlling error via Lipschitz push-forward maps and local discretization bounded by $O(\sqrt{d})$. These aspects are validated under Gaussian tail assumptions in specific models.

Result: Error is split into dimension-independent Lipschitzness of backward flows and dimension-scaling local discretization. Sampling complexity correlates with square-root behavior of covariance operator's trace.

Conclusion: The established results provide a clear, dimension-scalable approach to generative modeling under theoretical guarantees of stability and efficiency.

Abstract: We provide attainable analytical tools to estimate the error of flow-based generative models under the Wasserstein metric and to establish the optimal sampling iteration complexity bound with respect to dimension as $O(\sqrt{d})$. We show this error can be explicitly controlled by two parts: the Lipschitzness of the push-forward maps of the backward flow which scales independently of the dimension; and a local discretization error scales $O(\sqrt{d})$ in terms of dimension. The former one is related to the existence of Lipschitz changes of variables induced by the (heat) flow. The latter one consists of the regularity of the score function in both spatial and temporal directions.
  These assumptions are valid in the flow-based generative model associated with the Föllmer process and $1$-rectified flow under the Gaussian tail assumption. As a consequence, we show that the sampling iteration complexity grows linearly with the square root of the trace of the covariance operator, which is related to the invariant distribution of the forward process.

</details>


### [433] [A Novel Multimodal RUL Framework for Remaining Useful Life Estimation with Layer-wise Explanations](https://arxiv.org/abs/2512.06708)
*Waleed Razzaq,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: This paper introduces a multimodal framework for accurately estimating the Remaining Useful Life (RUL) of machinery using vibration signals. It integrates image and time-frequency representations, attention mechanisms, and an explainability technique for enhanced performance and transparency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address deficiencies in existing RUL estimation methods such as poor generalization, lack of robustness, high data demands, and limited interpretability. This is critical due to the frequent occurrence of machinery failures caused by rolling-element bearings.

Method: The proposed framework integrates image and time-frequency features of vibration signals and employs a three-branch architecture: two for spatial degradation feature extraction using dilated convolutional blocks and residual connections, and a fusion branch with LSTM and attention mechanisms for temporal modeling. Innovative data transformation techniques and explainability (multimodal-LRP) are also utilized.

Result: The framework was validated on two benchmark datasets (XJTU-SY and PRONOSTIA), outperforming state-of-the-art approaches with ~28%-48% lower training data requirements and strong noise resilience. The explainability mechanism confirmed the transparency and reliability of the model.

Conclusion: The multimodal framework offers enhanced performance, robustness, interpretability, and reduced data demands, making it well-suited for practical industrial applications in machinery prognostics.

Abstract: Estimating the Remaining Useful Life (RUL) of mechanical systems is pivotal in Prognostics and Health Management (PHM). Rolling-element bearings are among the most frequent causes of machinery failure, highlighting the need for robust RUL estimation methods. Existing approaches often suffer from poor generalization, lack of robustness, high data demands, and limited interpretability. This paper proposes a novel multimodal-RUL framework that jointly leverages image representations (ImR) and time-frequency representations (TFR) of multichannel, nonstationary vibration signals. The architecture comprises three branches: (1) an ImR branch and (2) a TFR branch, both employing multiple dilated convolutional blocks with residual connections to extract spatial degradation features; and (3) a fusion branch that concatenates these features and feeds them into an LSTM to model temporal degradation patterns. A multi-head attention mechanism subsequently emphasizes salient features, followed by linear layers for final RUL regression. To enable effective multimodal learning, vibration signals are converted into ImR via the Bresenham line algorithm and into TFR using Continuous Wavelet Transform. We also introduce multimodal Layer-wise Relevance Propagation (multimodal-LRP), a tailored explainability technique that significantly enhances model transparency. The approach is validated on the XJTU-SY and PRONOSTIA benchmark datasets. Results show that our method matches or surpasses state-of-the-art baselines under both seen and unseen operating conditions, while requiring ~28 % less training data on XJTU-SY and ~48 % less on PRONOSTIA. The model exhibits strong noise resilience, and multimodal-LRP visualizations confirm the interpretability and trustworthiness of predictions, making the framework highly suitable for real-world industrial deployment.

</details>


### [434] [A Novel Deep Neural Network Architecture for Real-Time Water Demand Forecasting](https://arxiv.org/abs/2512.06714)
*Tony Salloom,Okyay Kaynak,Wei He*

Main category: cs.LG

TL;DR: The paper proposes a novel method for addressing short-term water demand forecasting using deep learning, specifically targeting the challenge at extreme points and reducing model complexity.


<details>
  <summary>Details</summary>
Motivation: The need for accurate short-term water demand forecasting arises due to its critical role in optimal planning and controlling water supply systems. However, existing deep learning models suffer from high complexity and errors near extreme data points.

Method: The proposed method introduces virtual data within actual data to handle nonlinearity at extreme points and leverages a novel deep learning model combining GRU architecture and K-means for unsupervised classification to enhance prediction accuracy with fewer parameters.

Result: The model reduces complexity sixfold compared to existing methods while maintaining accuracy and decreases error at extreme points by approximately 30%. However, extended datasets increase training time.

Conclusion: The proposed approach successfully balances low model complexity and high forecasting accuracy, particularly at extreme points, proving effective for real-world water demand forecasting applications.

Abstract: Short-term water demand forecasting (StWDF) is the foundation stone in the derivation of an optimal plan for controlling water supply systems. Deep learning (DL) approaches provide the most accurate solutions for this purpose. However, they suffer from complexity problem due to the massive number of parameters, in addition to the high forecasting error at the extreme points. In this work, an effective method to alleviate the error at these points is proposed. It is based on extending the data by inserting virtual data within the actual data to relieve the nonlinearity around them. To our knowledge, this is the first work that considers the problem related to the extreme points. Moreover, the water demand forecasting model proposed in this work is a novel DL model with relatively low complexity. The basic model uses the gated recurrent unit (GRU) to handle the sequential relationship in the historical demand data, while an unsupervised classification method, K-means, is introduced for the creation of new features to enhance the prediction accuracy with less number of parameters. Real data obtained from two different water plants in China are used to train and verify the model proposed. The prediction results and the comparison with the state-of-the-art illustrate that the method proposed reduces the complexity of the model six times of what achieved in the literature while conserving the same accuracy. Furthermore, it is found that extending the data set significantly reduces the error by about 30%. However, it increases the training time.

</details>


### [435] [Decoding Motor Behavior Using Deep Learning and Reservoir Computing](https://arxiv.org/abs/2512.06725)
*Tian Lan*

Main category: cs.LG

TL;DR: This study proposes ESNNet, combining a convolutional neural network and Echo State Network for improved EEG decoding in brain-machine interfaces, achieving superior results on motor-behavior classification tasks.


<details>
  <summary>Details</summary>
Motivation: Conventional convolutional architectures struggle with capturing long-range temporal dependencies in EEG data, a limitation critical for accurate brain-machine interface decoding.

Method: The authors integrate an Echo State Network (ESN) into the CNN-based EEG decoding pipeline to enhance temporal dynamic modeling. Tested on a skateboard-trick dataset, preprocessing was done via the PREP pipeline.

Result: The ESNNet achieved 83.2% within-subject accuracy and 51.3% LOSO accuracy, surpassing traditional CNN-based methods.

Conclusion: Integrating ESNs into EEG decoding enhances performance in brain-machine interface tasks by effectively capturing temporal dynamics, demonstrating significant advantages over existing CNN-only architectures.

Abstract: We present a novel approach to EEG decoding for non-invasive brain machine interfaces (BMIs), with a focus on motor-behavior classification. While conventional convolutional architectures such as EEGNet and DeepConvNet are effective in capturing local spatial patterns, they are markedly less suited for modeling long-range temporal dependencies and nonlinear dynamics. To address this limitation, we integrate an Echo State Network (ESN), a prominent paradigm in reservoir computing into the decoding pipeline. ESNs construct a high-dimensional, sparsely connected recurrent reservoir that excels at tracking temporal dynamics, thereby complementing the spatial representational power of CNNs. Evaluated on a skateboard-trick EEG dataset preprocessed via the PREP pipeline and implemented in MNE-Python, our ESNNet achieves 83.2% within-subject and 51.3% LOSO accuracies, surpassing widely used CNN-based baselines. Code is available at https://github.com/Yutiankunkun/Motion-Decoding-Using-Biosignals

</details>


### [436] [KV-CAR: KV Cache Compression using Autoencoders and KV Reuse in Large Language Models](https://arxiv.org/abs/2512.06727)
*Sourjya Roy,Shrihari Sridharan,Surya Selvam,Anand Raghunathan*

Main category: cs.LG

TL;DR: KV CAR framework reduces KV cache memory usage, facilitating larger sequences/batches during LLM inference.


<details>
  <summary>Details</summary>
Motivation: Large Language Models face memory challenges due to the extensive KV cache needs, limiting batch sizes and sequence lengths.

Method: KV CAR combines a lightweight autoencoder for compressing/retrieving key-value tensors and a similarity-driven reuse mechanism for redundancy reduction.

Result: Achieved up to 47.85% KV cache memory reduction with minimal performance impact on perplexity and zero-shot accuracy.

Conclusion: KV CAR effectively mitigates memory bottlenecks in LLM inference, enabling improved scalability and efficiency.

Abstract: As Large Language Models (LLMs) scale in size and context length, the memory requirements of the key value (KV) cache have emerged as a major bottleneck during autoregressive decoding. The KV cache grows with sequence length and embedding dimension, often exceeding the memory footprint of the model itself and limiting achievable batch sizes and context windows. To address this challenge, we present KV CAR, a unified and architecture agnostic framework that significantly reduces KV cache storage while maintaining model fidelity. KV CAR combines two complementary techniques. First, a lightweight autoencoder learns compact representations of key and value tensors along the embedding dimension, compressing them before they are stored in the KV cache and restoring them upon retrieval. Second, a similarity driven reuse mechanism identifies opportunities to reuse KV tensors of specific attention heads across adjacent layers. Together, these methods reduce the dimensional and structural redundancy in KV tensors without requiring changes to the transformer architecture. Evaluations on GPT 2 and TinyLLaMA models across Wikitext, C4, PIQA, and Winogrande datasets demonstrate that KV CAR achieves up to 47.85 percent KV cache memory reduction with minimal impact on perplexity and zero shot accuracy. System level measurements on an NVIDIA A40 GPU show that the reduced KV footprint directly translates into longer sequence lengths and larger batch sizes during inference. These results highlight the effectiveness of KV CAR in enabling memory efficient LLM inference.

</details>


### [437] [Enhancing Interpretability of AR-SSVEP-Based Motor Intention Recognition via CNN-BiLSTM and SHAP Analysis on EEG Data](https://arxiv.org/abs/2512.06730)
*Lin Yang,Xiang Li,Xin Ma,Xinxin Zhao*

Main category: cs.LG

TL;DR: The paper proposes an AR-based BCI system for motor dysfunction patients, combining an upgraded neural network (MACNN-BiLSTM) and EEG analysis.


<details>
  <summary>Details</summary>
Motivation: To boost patient engagement in rehabilitation and overcome limitations of traditional SSVEP-BCI systems that depend heavily on external visual stimuli.

Method: Developed an EEG-based AR system using HoloLens 2 with data captured from seven subjects. Integrated a refined CNN-BiLSTM architecture enhanced with a multi-head attention mechanism and SHAP interpretability.

Result: Enhanced neural network interpretability and improved real-time motor intention recognition in EEG data.

Conclusion: The proposed AR-SSVEP system provides a more practical and interpretable tool for rehabilitation, supporting better motor recovery outcomes in patients.

Abstract: Patients with motor dysfunction show low subjective engagement in rehabilitation training. Traditional SSVEP-based brain-computer interface (BCI) systems rely heavily on external visual stimulus equipment, limiting their practicality in real-world settings. This study proposes an augmented reality steady-state visually evoked potential (AR-SSVEP) system to address the lack of patient initiative and the high workload on therapists. Firstly, we design four HoloLens 2-based EEG classes and collect EEG data from seven healthy subjects for analysis. Secondly, we build upon the conventional CNN-BiLSTM architecture by integrating a multi-head attention mechanism (MACNN-BiLSTM). We extract ten temporal-spectral EEG features and feed them into a CNN to learn high-level representations. Then, we use BiLSTM to model sequential dependencies and apply a multi-head attention mechanism to highlight motor-intention-related patterns. Finally, the SHAP (SHapley Additive exPlanations) method is applied to visualize EEG feature contributions to the neural network's decision-making process, enhancing the model's interpretability. These findings enhance real-time motor intention recognition and support recovery in patients with motor impairments.

</details>


### [438] [Multi-Scale Protein Structure Modelling with Geometric Graph U-Nets](https://arxiv.org/abs/2512.06752)
*Chang Liu,Vivian Li,Linus Leong,Vladimir Radenkovic,Pietro Liò,Chaitanya K. Joshi*

Main category: cs.LG

TL;DR: This paper introduces Geometric Graph U-Nets, a hierarchical deep learning architecture that models multi-scale protein structural interactions for improved classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Current GNNs and Transformers fail to capture hierarchical interactions in protein structures, which are crucial for understanding protein function.

Method: The paper develops Geometric Graph U-Nets, utilizing recursive graph coarsening and refinement to learn multi-scale representations of protein structures.

Result: Geometric U-Nets outperform invariant and equivariant models in protein fold classification tasks, demonstrating their ability to capture global structural patterns.

Conclusion: The hierarchical approach provides theoretical and practical advantages, establishing a robust framework for multi-scale geometric deep learning in biomolecular applications.

Abstract: Geometric Graph Neural Networks (GNNs) and Transformers have become state-of-the-art for learning from 3D protein structures. However, their reliance on message passing prevents them from capturing the hierarchical interactions that govern protein function, such as global domains and long-range allosteric regulation. In this work, we argue that the network architecture itself should mirror this biological hierarchy. We introduce Geometric Graph U-Nets, a new class of models that learn multi-scale representations by recursively coarsening and refining the protein graph. We prove that this hierarchical design can theoretically more expressive than standard Geometric GNNs. Empirically, on the task of protein fold classification, Geometric U-Nets substantially outperform invariant and equivariant baselines, demonstrating their ability to learn the global structural patterns that define protein folds. Our work provides a principled foundation for designing geometric deep learning architectures that can learn the multi-scale structure of biomolecules.

</details>


### [439] [Optimal Analysis for Bandit Learning in Matching Markets with Serial Dictatorship](https://arxiv.org/abs/2512.06758)
*Zilong Wang,Shuai Li*

Main category: cs.LG

TL;DR: This paper addresses the problem of matching markets with uncertain preferences in online settings and proposes a new algorithm to achieve optimal regret bounds, effectively closing the existing gap.


<details>
  <summary>Details</summary>
Motivation: There is a significant gap between the best-known regret bounds in matching markets with bandits, raising the need to bridge this discrepancy and improve the understanding of performance limits.

Method: The authors introduce a multi-level successive selection algorithm, specifically designed under the assumption of serial dictatorship, to achieve optimal regret bounds.

Result: The proposed algorithm achieves an $O\left( \frac{N\log(T)}{Δ^2} + \frac{K\log(T)}Δ \right)$ regret bound, thereby matching the theoretical lower bound previously identified.

Conclusion: This work is the first to propose an algorithm that achieves optimal regret bounds, aligning with the theoretical lower bounds, under the serial dictatorship assumption in matching markets with bandits.

Abstract: The problem of two-sided matching markets is well-studied in computer science and economics, owing to its diverse applications across numerous domains. Since market participants are usually uncertain about their preferences in various online matching platforms, an emerging line of research is dedicated to the online setting where one-side participants (players) learn their unknown preferences through multiple rounds of interactions with the other side (arms). Sankararaman et al. provide an $Ω\left( \frac{N\log(T)}{Δ^2} + \frac{K\log(T)}Δ \right)$ regret lower bound for this problem under serial dictatorship assumption, where $N$ is the number of players, $K (\geq N)$ is the number of arms, $Δ$ is the minimum reward gap across players and arms, and $T$ is the time horizon. Serial dictatorship assumes arms have the same preferences, which is common in reality when one side participants have a unified evaluation standard. Recently, the work of Kong and Li proposes the ET-GS algorithm and achieves an $O\left( \frac{K\log(T)}{Δ^2} \right)$ regret upper bound, which is the best upper bound attained so far. Nonetheless, a gap between the lower and upper bounds, ranging from $N$ to $K$, persists. It remains unclear whether the lower bound or the upper bound needs to be improved. In this paper, we propose a multi-level successive selection algorithm that obtains an $O\left( \frac{N\log(T)}{Δ^2} + \frac{K\log(T)}Δ \right)$ regret bound when the market satisfies serial dictatorship. To the best of our knowledge, we are the first to propose an algorithm that matches the lower bound in the problem of matching markets with bandits.

</details>


### [440] [Measuring Over-smoothing beyond Dirichlet energy](https://arxiv.org/abs/2512.06782)
*Weiqi Guan,Zihao Shi*

Main category: cs.LG

TL;DR: This paper introduces a generalized approach to measure over-smoothing in Graph Neural Networks using higher-order feature derivatives, extends theoretical analysis of Dirichlet energy decay rates, and highlights the issue in attention-based GNNs.


<details>
  <summary>Details</summary>
Motivation: Current methods using Dirichlet energy only capture first-order feature derivatives, limiting the understanding of over-smoothing in GNNs.

Method: Developing higher-order node similarity measures and analyzing the decay of Dirichlet energy during heat diffusion and discrete aggregation processes.

Result: Empirical and theoretical insights validate the proposed metrics, showing that attention-based GNNs exhibit over-smoothing using these new measures.

Conclusion: The proposed metrics provide a broader framework to evaluate and understand over-smoothing in GNNs, with spectral gap playing a critical role in the decay rate.

Abstract: While Dirichlet energy serves as a prevalent metric for quantifying over-smoothing, it is inherently restricted to capturing first-order feature derivatives. To address this limitation, we propose a generalized family of node similarity measures based on the energy of higher-order feature derivatives. Through a rigorous theoretical analysis of the relationships among these measures, we establish the decay rates of Dirichlet energy under both continuous heat diffusion and discrete aggregation operators. Furthermore, our analysis reveals an intrinsic connection between the over-smoothing decay rate and the spectral gap of the graph Laplacian. Finally, empirical results demonstrate that attention-based Graph Neural Networks (GNNs) suffer from over-smoothing when evaluated under these proposed metrics.

</details>


### [441] [Angular Regularization for Positive-Unlabeled Learning on the Hypersphere](https://arxiv.org/abs/2512.06785)
*Vasileios Sevetlidis,George Pavlidis,Antonios Gasteratos*

Main category: cs.LG

TL;DR: This paper introduces AngularPU, a novel Positive-Unlabeled learning method using cosine similarity and angular margin to address classification issues without explicit negative supervision.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome limitations in current PU learning methods, such as strong distributional assumptions and failure in high-dimensional data scenarios.

Method: AngularPU uses a learnable prototype vector to represent positives and replaces negative modeling with thresholding cosine similarity. An angular regularizer is introduced to disperse unlabeled data over the hypersphere for better separation.

Result: The proposed AngularPU method shows competitive or better performance than state-of-the-art PU methods, especially in cases with limited positives and high-dimensional embeddings.

Conclusion: AngularPU offers a theoretically grounded and interpretable approach for PU learning, outperforming existing methods in challenging settings and providing scalability and geometric insights.

Abstract: Positive-Unlabeled (PU) learning addresses classification problems where only a subset of positive examples is labeled and the remaining data is unlabeled, making explicit negative supervision unavailable. Existing PU methods often rely on negative-risk estimation or pseudo-labeling, which either require strong distributional assumptions or can collapse in high-dimensional settings. We propose AngularPU, a novel PU framework that operates on the unit hypersphere using cosine similarity and angular margin. In our formulation, the positive class is represented by a learnable prototype vector, and classification reduces to thresholding the cosine similarity between an embedding and this prototype-eliminating the need for explicit negative modeling. To counteract the tendency of unlabeled embeddings to cluster near the positive prototype, we introduce an angular regularizer that encourages dispersion of the unlabeled set over the hypersphere, improving separation. We provide theoretical guarantees on the Bayes-optimality of the angular decision rule, consistency of the learned prototype, and the effect of the regularizer on the unlabeled distribution. Experiments on benchmark datasets demonstrate that AngularPU achieves competitive or superior performance compared to state-of-the-art PU methods, particularly in settings with scarce positives and high-dimensional embeddings, while offering geometric interpretability and scalability.

</details>


### [442] [Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games](https://arxiv.org/abs/2512.06791)
*Vedansh Sharma*

Main category: cs.LG

TL;DR: The paper introduces a novel framework, Small-Gain Nash (SGN), that provides convergence guarantees for gradient-based learning in games even when pseudo-gradients are non-monotone in Euclidean geometry.


<details>
  <summary>Details</summary>
Motivation: Gradient-based learning methods often struggle with convergence issues in games featuring strong cross-player couplings, where classical monotonicity conditions fail.

Method: The SGN framework leverages a block-weighted geometry to convert curvature and cross-player coupling bounds into a certificate of contraction. It designs a weighted block metric where pseudo-gradient becomes strongly monotone locally, analyzes continuous and discrete flows, derives step-size bounds, and identifies a TTUR-like timescale band.

Result: SGN certifies convergence in scenarios where Euclidean monotonicity fails, such as quadratic games, and extends applicability to entropy-regularized policy gradient in Markov games. It provides an offline certification pipeline estimating parameters and optimizing weights.

Conclusion: The SGN framework offers a structural and computable approach for convergence certification in non-monotone games, addressing limitations of classical monotonicity-based methods and enabling practical applications in complex game settings.

Abstract: Classical convergence guarantees for gradient-based learning in games require the pseudo-gradient to be (strongly) monotone in Euclidean geometry as shown by rosen(1965), a condition that often fails even in simple games with strong cross-player couplings. We introduce Small-Gain Nash (SGN), a block small-gain condition in a custom block-weighted geometry. SGN converts local curvature and cross-player Lipschitz coupling bounds into a tractable certificate of contraction. It constructs a weighted block metric in which the pseudo-gradient becomes strongly monotone on any region where these bounds hold, even when it is non-monotone in the Euclidean sense. The continuous flow is exponentially contracting in this designed geometry, and projected Euler and RK4 discretizations converge under explicit step-size bounds derived from the SGN margin and a local Lipschitz constant. Our analysis reveals a certified ``timescale band'', a non-asymptotic, metric-based certificate that plays a TTUR-like role: rather than forcing asymptotic timescale separation via vanishing, unequal step sizes, SGN identifies a finite band of relative metric weights for which a single-step-size dynamics is provably contractive. We validate the framework on quadratic games where Euclidean monotonicity analysis fails to predict convergence, but SGN successfully certifies it, and extend the construction to mirror/Fisher geometries for entropy-regularized policy gradient in Markov games. The result is an offline certification pipeline that estimates curvature, coupling, and Lipschitz parameters on compact regions, optimizes block weights to enlarge the SGN margin, and returns a structural, computable convergence certificate consisting of a metric, contraction rate, and safe step-sizes for non-monotone games.

</details>


### [443] [Partial Inverse Design of High-Performance Concrete Using Cooperative Neural Networks for Constraint-Aware Mix Generation](https://arxiv.org/abs/2512.06813)
*Agung Nugraha,Heungjun Im,Jihwan Lee*

Main category: cs.LG

TL;DR: This paper introduces a cooperative neural network framework for partial inverse design of high-performance concrete, addressing a gap in constraint-aware mix design.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the challenges of constraint-aware inverse design in high-performance concrete mix proportioning, a key task often overlooked in traditional data-driven methods.

Method: The authors propose a cooperative neural network framework combining an imputation model and a surrogate model for compressive strength prediction using cooperative learning.

Result: The model achieves high R-squared values (0.87-0.92) and significantly reduces mean squared error compared to autoencoder and Bayesian inference approaches.

Conclusion: The cooperative neural network framework provides an accurate, robust, and efficient solution for constraint-aware high-performance concrete mix design.

Abstract: High-performance concrete offers exceptional strength and durability but requires complex mix designs involving many interdependent variables and practical constraints. While data-driven methods have advanced predictive modeling for forward design, inverse design, which focuses on determining mix compositions that achieve target performance, remains limited, particularly in design situations where some mix variables are fixed by constraints and only the remaining variables must be determined. This study proposes a cooperative neural network framework for the partial inverse design of high-performance concrete. The framework combines two coupled neural network models, an imputation model that infers the undetermined variables and a surrogate model that predicts compressive strength. Through cooperative learning, the model generates valid and performance-consistent mix designs in a single forward pass while accommodating different constraint combinations without retraining. Its performance is compared with both probabilistic and generative approaches, including Bayesian inference based on a Gaussian process surrogate and autoencoder-based models. Evaluated on a benchmark dataset, the proposed model achieves stable and higher R-squared values of 0.87-0.92 and reduces mean squared error by an average of 50 percent compared with autoencoder baselines and by an average of 70 percent compared with Bayesian inference. The results demonstrate that the cooperative neural network provides an accurate, robust, and computationally efficient foundation for constraint-aware, data-driven mix proportioning in concrete engineering.

</details>


### [444] [Neural Factorization-based Bearing Fault Diagnosis](https://arxiv.org/abs/2512.06837)
*Zhenhao Li,Xu Cheng,Yi Zhou*

Main category: cs.LG

TL;DR: The paper proposes a Neural Factorization-based Classification framework for diagnosing bearing faults in high-speed trains, addressing issues of low diagnostic accuracy under complex conditions.


<details>
  <summary>Details</summary>
Motivation: Ensure the safety of train operations by improving the accuracy of bearing fault diagnosis under complex conditions, overcoming limitations of traditional methods.

Method: Developed a Neural Factorization-based Classification (NFC) framework featuring two key points: embedding vibration time series into latent vectors and fusing these vectors via neural factorization principles.

Result: Proposed models (CP-NFC and Tucker-NFC) demonstrated superior fault diagnostic performance compared to traditional machine learning approaches in experiments.

Conclusion: The NFC framework effectively analyzes complex fault patterns, offering a new approach for accurate and reliable bearing fault diagnosis in high-speed train monitoring.

Abstract: This paper studies the key problems of bearing fault diagnosis of high-speed train. As the core component of the train operation system, the health of bearings is directly related to the safety of train operation. The traditional diagnostic methods are facing the challenge of insufficient diagnostic accuracy under complex conditions. To solve these problems, we propose a novel Neural Factorization-based Classification (NFC) framework for bearing fault diagnosis. It is built on two core idea: 1) Embedding vibration time series into multiple mode-wise latent feature vectors to capture diverse fault-related patterns; 2) Leveraging neural factorization principles to fuse these vectors into a unified vibration representation. This design enables effective mining of complex latent fault characteristics from raw time-series data. We further instantiate the framework with two models CP-NFC and Tucker-NFC based on CP and Tucker fusion schemes, respectively. Experimental results show that both models achieve superior diagnostic performance compared with traditional machine learning methods. The comparative analysis provides valuable empirical evidence and practical guidance for selecting effective diagnostic strategies in high-speed train bearing monitoring.

</details>


### [445] [Know your Trajectory -- Trustworthy Reinforcement Learning deployment through Importance-Based Trajectory Analysis](https://arxiv.org/abs/2512.06917)
*Clifford F,Devika Jay,Abhishek Sarkar,Satheesh K Perepu,Santhosh G S,Kaushik Dey,Balaraman Ravindran*

Main category: cs.LG

TL;DR: The paper introduces a framework for explaining reinforcement learning agents' long-term behaviors by ranking and analyzing entire trajectories, using a state-importance metric that captures critical states.


<details>
  <summary>Details</summary>
Motivation: To enhance trust and transparency in real-world applications of reinforcement learning by addressing the lack of explainability in long-term agent behavior analysis.

Method: A framework is proposed that ranks trajectories based on a novel state-importance metric, which combines Q-value differences and an affinity term to the agent's goal. Counterfactual analysis is used to evaluate the optimality of the chosen trajectories.

Result: The metric successfully identifies optimal trajectories and provides robust explanations for the agent's behavior. Validation in OpenAI Gym environments demonstrates its effectiveness over traditional approaches.

Conclusion: The proposed method significantly improves explainability in reinforcement learning by focusing on trajectory-level analysis and state criticality, contributing to developing more trustworthy autonomous systems.

Abstract: As Reinforcement Learning (RL) agents are increasingly deployed in real-world applications, ensuring their behavior is transparent and trustworthy is paramount. A key component of trust is explainability, yet much of the work in Explainable RL (XRL) focuses on local, single-step decisions. This paper addresses the critical need for explaining an agent's long-term behavior through trajectory-level analysis. We introduce a novel framework that ranks entire trajectories by defining and aggregating a new state-importance metric. This metric combines the classic Q-value difference with a "radical term" that captures the agent's affinity to reach its goal, providing a more nuanced measure of state criticality. We demonstrate that our method successfully identifies optimal trajectories from a heterogeneous collection of agent experiences. Furthermore, by generating counterfactual rollouts from critical states within these trajectories, we show that the agent's chosen path is robustly superior to alternatives, thereby providing a powerful "Why this, and not that?" explanation. Our experiments in standard OpenAI Gym environments validate that our proposed importance metric is more effective at identifying optimal behaviors compared to classic approaches, offering a significant step towards trustworthy autonomous systems.

</details>


### [446] [Parent-Guided Semantic Reward Model (PGSRM): Embedding-Based Reward Functions for Reinforcement Learning of Transformer Language Models](https://arxiv.org/abs/2512.06920)
*Alexandr Plashchinsky*

Main category: cs.LG

TL;DR: The paper introduces PGSRM, a framework that uses cosine similarity between embeddings instead of binary correctness or human annotations for RL optimization in language tasks.


<details>
  <summary>Details</summary>
Motivation: To develop a lightweight reward framework for RL in transformer models without relying on human annotations, binary correctness data, or additional model training.

Method: PGSRM utilizes cosine similarity between output embeddings of a parent model and generated outputs of a child model as semantic rewards for RL tasks.

Result: The framework produces smoother reward improvement and more stable PPO dynamics across five language tasks compared to a binary reward baseline.

Conclusion: Embedding-based semantic rewards are a practical alternative to RLHF-style reward modeling for guiding alignment in smaller transformer models.

Abstract: We introduce the Parent-Guided Semantic Reward Model (PGSRM), a lightweight reward framework for reinforcement learning (RL) of transformer language models. PGSRM replaces binary correctness signals, human preference data, and trained reward models with a simple signal: cosine similarity between a parent model's reference output embedding and a child model's generated output for the same input. This yields a dense, semantically meaningful reward with no human annotation or additional model training. We apply PGSRM on five language tasks and find that it produces smoother reward improvement and more stable PPO dynamics than a binary reward baseline, suggesting that embedding-based semantic rewards are a practical alternative to RLHF-style reward modeling for parent-guided alignment in smaller transformer models.

</details>


### [447] [Deep Reinforcement Learning for Phishing Detection with Transformer-Based Semantic Features](https://arxiv.org/abs/2512.06925)
*Aseer Al Faisal*

Main category: cs.LG

TL;DR: The paper introduces a QR-DQN approach combining RoBERTa embeddings and lexical features to achieve highly effective phishing detection with over 99% accuracy, while improving generalization and robustness.


<details>
  <summary>Details</summary>
Motivation: Phishing remains a major cyber threat, causing financial and personal harm, necessitating reliable detection methods.

Method: The study uses QR-DQN, incorporating RoBERTa semantic embeddings with lexical features to detect phishing URLs. Unlike traditional methods, it applies quantile regression to enhance generalization.

Result: The model achieved 99.86% accuracy, 99.75% precision, 99.96% recall, and 99.85% F1-score. It reduced the generalization gap from 1.66% to 0.04% and demonstrated consistent performance during five-fold cross-validation.

Conclusion: The hybrid QR-DQN approach reliably detects phishing attacks, adapts to new strategies, and performs robustly on varied data sources.

Abstract: Phishing is a cybercrime in which individuals are deceived into revealing personal information, often resulting in financial loss. These attacks commonly occur through fraudulent messages, misleading advertisements, and compromised legitimate websites. This study proposes a Quantile Regression Deep Q-Network (QR-DQN) approach that integrates RoBERTa semantic embeddings with handcrafted lexical features to enhance phishing detection while accounting for uncertainties. Unlike traditional DQN methods that estimate single scalar Q-values, QR-DQN leverages quantile regression to model the distribution of returns, improving stability and generalization on unseen phishing data. A diverse dataset of 105,000 URLs was curated from PhishTank, OpenPhish, Cloudflare, and other sources, and the model was evaluated using an 80/20 train-test split. The QR-DQN framework achieved a test accuracy of 99.86%, precision of 99.75%, recall of 99.96%, and F1-score of 99.85%, demonstrating high effectiveness. Compared to standard DQN with lexical features, the hybrid QR-DQN with lexical and semantic features reduced the generalization gap from 1.66% to 0.04%, indicating significant improvement in robustness. Five-fold cross-validation confirmed model reliability, yielding a mean accuracy of 99.90% with a standard deviation of 0.04%. These results suggest that the proposed hybrid approach effectively identifies phishing threats, adapts to evolving attack strategies, and generalizes well to unseen data.

</details>


### [448] [Evaluating the Sensitivity of BiLSTM Forecasting Models to Sequence Length and Input Noise](https://arxiv.org/abs/2512.06926)
*Salma Albelali,Moataz Ahmed*

Main category: cs.LG

TL;DR: This paper analyzes how input sequence length and additive noise affect the performance of Bidirectional Long Short-Term Memory (BiLSTM) models in time-series forecasting.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the underexplored issue of how input data characteristics impact the robustness and generalization of BiLSTM models in time-series forecasting.

Method: A modular forecasting pipeline was developed, and controlled experiments were conducted on real-world datasets with varying sampling frequencies to analyze the effects of input sequence length and additive noise.

Result: Key findings include: (1) longer input sequences increase overfitting risks; (2) additive noise reduces accuracy; and (3) both factors together lead to the greatest decline in model stability.

Conclusion: The study highlights the critical need for data-aware design in deep learning forecasting systems and provides valuable insights for improving their robustness and generalizability.

Abstract: Deep learning (DL) models, a specialized class of multilayer neural networks, have become central to time-series forecasting in critical domains such as environmental monitoring and the Internet of Things (IoT). Among these, Bidirectional Long Short-Term Memory (BiLSTM) architectures are particularly effective in capturing complex temporal dependencies. However, the robustness and generalization of such models are highly sensitive to input data characteristics - an aspect that remains underexplored in existing literature. This study presents a systematic empirical analysis of two key data-centric factors: input sequence length and additive noise. To support this investigation, a modular and reproducible forecasting pipeline is developed, incorporating standardized preprocessing, sequence generation, model training, validation, and evaluation. Controlled experiments are conducted on three real-world datasets with varying sampling frequencies to assess BiLSTM performance under different input conditions. The results yield three key findings: (1) longer input sequences significantly increase the risk of overfitting and data leakage, particularly in data-constrained environments; (2) additive noise consistently degrades predictive accuracy across sampling frequencies; and (3) the simultaneous presence of both factors results in the most substantial decline in model stability. While datasets with higher observation frequencies exhibit greater robustness, they remain vulnerable when both input challenges are present. These findings highlight important limitations in current DL-based forecasting pipelines and underscore the need for data-aware design strategies. This work contributes to a deeper understanding of DL model behavior in dynamic time-series environments and provides practical insights for developing more reliable and generalizable forecasting systems.

</details>


### [449] [Adaptive Normalization Mamba with Multi Scale Trend Decomposition and Patch MoE Encoding](https://arxiv.org/abs/2512.06929)
*MinCheol Jeon*

Main category: cs.LG

TL;DR: This paper introduces AdaMamba, a novel architecture for time series forecasting, which enhances model accuracy and stability through adaptive normalization, multi-scale trend extraction, and a robust sequence modeling approach.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in time series forecasting, such as non-stationarity, multi-scale temporal patterns, and distributional shifts that degrade model performance in real-world data.

Method: The authors propose AdaMamba, which includes an Adaptive Normalization Block for detrending and variance stabilization, a Context Encoder for efficient long-range and local temporal modeling, and a lightweight prediction head for multi-horizon forecasting.

Result: AdaMamba demonstrates consistent improvements in stability and predictive accuracy over traditional Transformer-based baselines across diverse datasets, showcasing its effectiveness in handling real-world forecasting challenges.

Conclusion: The paper concludes that AdaMamba's combination of adaptive normalization, trend extraction, and expert-enhanced modeling offers a robust and extensible solution for time series forecasting, providing reliable and accurate predictions in dynamic environments.

Abstract: Time series forecasting in real world environments faces significant challenges non stationarity, multi scale temporal patterns, and distributional shifts that degrade model stability and accuracy. This study propose AdaMamba, a unified forecasting architecture that integrates adaptive normalization, multi scale trend extraction, and contextual sequence modeling to address these challenges. AdaMamba begins with an Adaptive Normalization Block that removes non stationary components through multi scale convolutional trend extraction and channel wise recalibration, enabling consistent detrending and variance stabilization. The normalized sequence is then processed by a Context Encoder that combines patch wise embeddings, positional encoding, and a Mamba enhanced Transformer layer with a mixture of experts feed forward module, allowing efficient modeling of both long range dependencies and local temporal dynamics. A lightweight prediction head generates multi horizon forecasts, and a denormalization mechanism reconstructs outputs by reintegrating local trends to ensure robustness under varying temporal conditions. AdaMamba provides strong representational capacity with modular extensibility, supporting deterministic prediction and compatibility with probabilistic extensions. Its design effectively mitigates covariate shift and enhances predictive reliability across heterogeneous datasets. Experimental evaluations demonstrate that AdaMamba's combination of adaptive normalization and expert augmented contextual modeling yields consistent improvements in stability and accuracy over conventional Transformer based baselines.

</details>


### [450] [Hidden Leaks in Time Series Forecasting: How Data Leakage Affects LSTM Evaluation Across Configurations and Validation Strategies](https://arxiv.org/abs/2512.06932)
*Salma Albelali,Moataz Ahmed*

Main category: cs.LG

TL;DR: This paper examines the impact of data leakage on LSTM time-series forecasting models. It evaluates validation strategies and highlights configurations to mitigate leakage risks, ensuring accurate performance estimation.


<details>
  <summary>Details</summary>
Motivation: To investigate the impact of data leakage on the evaluation of LSTM-based time-series forecasting models and ensure fair, leakage-resistant performance assessments.

Method: The study evaluates three validation techniques (2-way split, 3-way split, and 10-fold cross-validation) under leaky and clean conditions. The setups differ in how they mitigate data leakage using RMSE Gain as a key metric.

Result: The results show that 10-fold cross-validation is most sensitive to data leakage, yielding RMSE Gain values of up to 20.5%, while 2-way and 3-way splits are more robust (RMSE Gain below 5%). Additionally, model configurations such as input window size and lag step influence leakage sensitivity.

Conclusion: Data leakage significantly affects LSTM performance evaluation, and stricter, leakage-resistant setups should be employed for more reliable analysis. Larger windows and proper validation strategies are recommended to reduce risks.

Abstract: Deep learning models, particularly Long Short-Term Memory (LSTM) networks, are widely used in time series forecasting due to their ability to capture complex temporal dependencies. However, evaluation integrity is often compromised by data leakage, a methodological flaw in which input-output sequences are constructed before dataset partitioning, allowing future information to unintentionally influence training. This study investigates the impact of data leakage on performance, focusing on how validation design mediates leakage sensitivity. Three widely used validation techniques (2-way split, 3-way split, and 10-fold cross-validation) are evaluated under both leaky (pre-split sequence generation) and clean conditions, with the latter mitigating leakage risk by enforcing temporal separation during data splitting prior to sequence construction. The effect of leakage is assessed using RMSE Gain, which measures the relative increase in RMSE caused by leakage, computed as the percentage difference between leaky and clean setups. Empirical results show that 10-fold cross-validation exhibits RMSE Gain values of up to 20.5% at extended lag steps. In contrast, 2-way and 3-way splits demonstrate greater robustness, typically maintaining RMSE Gain below 5% across diverse configurations. Moreover, input window size and lag step significantly influence leakage sensitivity: smaller windows and longer lags increase the risk of leakage, whereas larger windows help reduce it. These findings underscore the need for configuration-aware, leakage-resistant evaluation pipelines to ensure reliable performance estimation.

</details>


### [451] [A Unifying Human-Centered AI Fairness Framework](https://arxiv.org/abs/2512.06944)
*Munshi Mahbubur Rahman,Shimei Pan,James R. Foulds*

Main category: cs.LG

TL;DR: The paper introduces a human-centered fairness framework for AI that systematically covers eight fairness metrics, aids stakeholder decision-making, and demonstrates its application on multiple datasets, revealing trade-offs and enhancing practical deployment.


<details>
  <summary>Details</summary>
Motivation: Concerns about fairness in AI systems, particularly in critical societal domains, due to unequal treatment across sensitive attributes and the challenges in balancing fairness and predictive accuracy.

Method: Developing a unifying fairness framework that encompasses eight fairness metrics, allows stakeholder-driven weighting of objectives, and provides an intuitive structure for practical application. Applied on multiple datasets and through case studies.

Result: The framework reveals nuanced trade-offs between fairness metrics on real-world datasets and informs the value-sensitive deployment of AI through practical case studies.

Conclusion: The framework bridges fairness concepts with practical implementation, helping stakeholders balance values and accuracy for heterogeneous AI deployment contexts.

Abstract: The increasing use of Artificial Intelligence (AI) in critical societal domains has amplified concerns about fairness, particularly regarding unequal treatment across sensitive attributes such as race, gender, and socioeconomic status. While there has been substantial work on ensuring AI fairness, navigating trade-offs between competing notions of fairness as well as predictive accuracy remains challenging, creating barriers to the practical deployment of fair AI systems. To address this, we introduce a unifying human-centered fairness framework that systematically covers eight distinct fairness metrics, formed by combining individual and group fairness, infra-marginal and intersectional assumptions, and outcome-based and equality-of-opportunity (EOO) perspectives. This structure allows stakeholders to align fairness interventions with their values and contextual considerations. The framework uses a consistent and easy-to-understand formulation for all metrics to reduce the learning curve for non-experts. Rather than privileging a single fairness notion, the framework enables stakeholders to assign weights across multiple fairness objectives, reflecting their priorities and facilitating multi-stakeholder compromises. We apply this approach to four real-world datasets: the UCI Adult census dataset for income prediction, the COMPAS dataset for criminal recidivism, the German Credit dataset for credit risk assessment, and the MEPS dataset for healthcare utilization. We show that adjusting weights reveals nuanced trade-offs between different fairness metrics. Finally, through case studies in judicial decision-making and healthcare, we demonstrate how the framework can inform practical and value-sensitive deployment of fair AI systems.

</details>


### [452] [Comparing BFGS and OGR for Second-Order Optimization](https://arxiv.org/abs/2512.06969)
*Adrian Przybysz,Mikołaj Kołek,Franciszek Sobota,Jarek Duda*

Main category: cs.LG

TL;DR: The paper introduces Online Gradient Regression (OGR), an alternative to the traditional BFGS method for estimating the Hessian matrix in neural network training and shows that OGR achieves better convergence, especially in non-convex settings.


<details>
  <summary>Details</summary>
Motivation: To efficiently estimate the Hessian matrix for neural network training in high-dimensional and non-convex settings, addressing the limitations of conventional approaches like BFGS.

Method: The study proposes Online Gradient Regression (OGR), which estimates second derivatives online by performing regression of gradients against positions using an exponential moving average, unlike BFGS which assumes convexity and requires Hessian inversion.

Result: OGR demonstrates faster convergence and better optimization performance over BFGS, especially in non-convex scenarios, through evaluations on standard test functions.

Conclusion: OGR serves as a powerful alternative for Hessian estimation in neural network training, especially suited for non-convex problems and overcoming BFGS's convexity-related limitations.

Abstract: Estimating the Hessian matrix, especially for neural network training, is a challenging problem due to high dimensionality and cost. In this work, we compare the classical Sherman-Morrison update used in the popular BFGS method (Broy-den-Fletcher-Goldfarb-Shanno), which maintains a positive definite Hessian approximation under a convexity assumption, with a novel approach called Online Gradient Regression (OGR). OGR performs regression of gradients against positions using an exponential moving average to estimate second derivatives online, without requiring Hessian inversion. Unlike BFGS, OGR allows estimation of a general (not necessarily positive definite) Hessian and can thus handle non-convex structures. We evaluate both methods across standard test functions and demonstrate that OGR achieves faster convergence and improved loss, particularly in non-convex settings.

</details>


### [453] [LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding](https://arxiv.org/abs/2512.06982)
*Yu Yu,Qian Xie,Nairen Cao,Li Jin*

Main category: cs.LG

TL;DR: The paper introduces an LLM-driven approach for Neural Architecture Search (NAS) to develop composite state encoders in multi-source reinforcement learning. It outperforms traditional baselines and is more sample-efficient.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in designing state encoders for RL with diverse information sources by automating the process, leveraging language-model priors, and improving sample efficiency over manual and existing methods.

Method: A novel pipeline using large language models (LLMs) for composite Neural Architecture Search (NAS), incorporating intermediate-output signals to evaluate and optimize state encoders more effectively.

Result: The method achieves higher-performance architectures in a multi-source RL task (mixed-autonomy traffic control) with fewer candidate evaluations compared to traditional NAS and the GENIUS framework.

Conclusion: An LLM-driven NAS framework can improve state encoder design in multi-source RL, reducing manual effort and enhancing sample efficiency while delivering superior performance.

Abstract: Designing state encoders for reinforcement learning (RL) with multiple information sources -- such as sensor measurements, time-series signals, image observations, and textual instructions -- remains underexplored and often requires manual design. We formalize this challenge as a problem of composite neural architecture search (NAS), where multiple source-specific modules and a fusion module are jointly optimized. Existing NAS methods overlook useful side information from the intermediate outputs of these modules -- such as their representation quality -- limiting sample efficiency in multi-source RL settings. To address this, we propose an LLM-driven NAS pipeline that leverages language-model priors and intermediate-output signals to guide sample-efficient search for high-performing composite state encoders. On a mixed-autonomy traffic control task, our approach discovers higher-performing architectures with fewer candidate evaluations than traditional NAS baselines and the LLM-based GENIUS framework.

</details>


### [454] [OXtal: An All-Atom Diffusion Model for Organic Crystal Structure Prediction](https://arxiv.org/abs/2512.06987)
*Emily Jin,Andrei Cristian Nica,Mikhail Galkin,Jarrid Rector-Brooks,Kin Long Kelvin Lee,Santiago Miret,Frances H. Arnold,Michael Bronstein,Avishek Joey Bose,Alexander Tong,Cheng-Hao Liu*

Main category: cs.LG

TL;DR: The paper introduces OXtal, a diffusion model for predicting 3D molecular crystal structures from 2D chemical graphs, with significant improvements over prior CSP methods.


<details>
  <summary>Details</summary>
Motivation: To address the long-standing challenge of predicting 3D molecular crystal structures (CSP) efficiently, which is crucial for fields like pharmaceuticals and materials science.

Method: The authors developed OXtal, a 100M parameter diffusion model utilizing data augmentation and a lattice-free training scheme called Stoichiometric Stochastic Shell Sampling ($S^4$).

Result: OXtal demonstrated orders-of-magnitude improvements, recovering experimental structures with $	ext{RMSD}_1<0.5$ Å and over 80% packing similarity, outperforming prior methods.

Conclusion: OXtal is a powerful and scalable CSP method capturing both thermodynamic and kinetic aspects of molecular crystallization at reduced computational costs.

Abstract: Accurately predicting experimentally-realizable 3D molecular crystal structures from their 2D chemical graphs is a long-standing open challenge in computational chemistry called crystal structure prediction (CSP). Efficiently solving this problem has implications ranging from pharmaceuticals to organic semiconductors, as crystal packing directly governs the physical and chemical properties of organic solids. In this paper, we introduce OXtal, a large-scale 100M parameter all-atom diffusion model that directly learns the conditional joint distribution over intramolecular conformations and periodic packing. To efficiently scale OXtal, we abandon explicit equivariant architectures imposing inductive bias arising from crystal symmetries in favor of data augmentation strategies. We further propose a novel crystallization-inspired lattice-free training scheme, Stoichiometric Stochastic Shell Sampling ($S^4$), that efficiently captures long-range interactions while sidestepping explicit lattice parametrization -- thus enabling more scalable architectural choices at all-atom resolution. By leveraging a large dataset of 600K experimentally validated crystal structures (including rigid and flexible molecules, co-crystals, and solvates), OXtal achieves orders-of-magnitude improvements over prior ab initio machine learning CSP methods, while remaining orders of magnitude cheaper than traditional quantum-chemical approaches. Specifically, OXtal recovers experimental structures with conformer $\text{RMSD}_1<0.5$ Å and attains over 80\% packing similarity rate, demonstrating its ability to model both thermodynamic and kinetic regularities of molecular crystallization.

</details>


### [455] [Flash Multi-Head Feed-Forward Network](https://arxiv.org/abs/2512.06989)
*Minshen Zhang,Xiang Hu,Jianguo Li,Wei Wu,Kewei Tu*

Main category: cs.LG

TL;DR: This paper introduces FlashMHF, a multi-head mechanism for FFNs in Transformers, enhancing expressivity, efficiency, and scalability.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from improving the expressivity of Transformers by replacing FFN with a multi-head mechanism while addressing challenges like memory consumption and scalability.

Method: The method involves introducing FlashMHF, which incorporates a fused kernel for efficient computation and dynamically weighted parallel sub-networks to balance dimensions.

Result: FlashMHF shows consistent improvements in perplexity and task accuracy, reduces memory usage 3-5x, and increases inference speed by up to 1.08x in models with 128M to 1.3B parameters.

Conclusion: The paper establishes multi-head design as a superior principle for FFNs in Transformers and positions FlashMHF as an effective, efficient alternative to traditional FFNs.

Abstract: We explore Multi-Head FFN (MH-FFN) as a replacement of FFN in the Transformer architecture, motivated by the structural similarity between single-head attention and FFN. While multi-head mechanisms enhance expressivity in attention, naively applying them to FFNs faces two challenges: memory consumption scaling with the head count, and an imbalanced ratio between the growing intermediate size and the fixed head dimension as models scale, which degrades scalability and expressive power. To address these challenges, we propose Flash Multi-Head FFN (FlashMHF), with two key innovations: an I/O-aware fused kernel computing outputs online in SRAM akin to FlashAttention, and a design using dynamically weighted parallel sub-networks to maintain a balanced ratio between intermediate and head dimensions. Validated on models from 128M to 1.3B parameters, FlashMHF consistently improves perplexity and downstream task accuracy over SwiGLU FFNs, while reducing peak memory usage by 3-5x and accelerating inference by up to 1.08x. Our work establishes the multi-head design as a superior architectural principle for FFNs, presenting FlashMHF as a powerful, efficient, and scalable alternative to FFNs in Transformers.

</details>


### [456] [Toward Reliable Machine Unlearning: Theory, Algorithms, and Evaluation](https://arxiv.org/abs/2512.06993)
*Ali Ebrahimpour-Boroojeny*

Main category: cs.LG

TL;DR: This paper introduces advanced unlearning methods, including Adversarial Machine UNlearning (AMUN) and Tilted ReWeighting (TRW), improving over prior approaches for handling forget samples and class unlearning, supported by theoretical insights and new benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve unlearning algorithms to ensure the effective removal of specified samples or classes, while replicating the predictions of a retrained model more accurately than existing methods.

Method: AMUN fine-tunes models using adversarial examples and introduces FastClip for spectral-norm clipping to enhance smoothness. TRW fine-tunes models by approximating the retrained distribution for unlearned classes, using inter-class similarity and a targeted distribution adjustment.

Result: AMUN achieves improved accuracy and robustness for forget samples by outperforming previous unlearning methods using SOTA MIA metrics. TRW also surpasses existing class unlearning approaches by reducing information leakage and closely replicating retrained behavior.

Conclusion: The proposed methods, AMUN and TRW, set new standards in unlearning strategies by addressing key limitations in prior methods, enhancing robustness, and ensuring closer alignment with retrained models.

Abstract: We propose new methodologies for both unlearning random set of samples and class unlearning and show that they outperform existing methods. The main driver of our unlearning methods is the similarity of predictions to a retrained model on both the forget and remain samples. We introduce Adversarial Machine UNlearning (AMUN), which surpasses prior state-of-the-art methods for image classification based on SOTA MIA scores. AMUN lowers the model's confidence on forget samples by fine-tuning on their corresponding adversarial examples. Through theoretical analysis, we identify factors governing AMUN's performance, including smoothness. To facilitate training of smooth models with a controlled Lipschitz constant, we propose FastClip, a scalable method that performs layer-wise spectral-norm clipping of affine layers. In a separate study, we show that increased smoothness naturally improves adversarial example transfer, thereby supporting the second factor above.
  Following the same principles for class unlearning, we show that existing methods fail in replicating a retrained model's behavior by introducing a nearest-neighbor membership inference attack (MIA-NN) that uses the probabilities assigned to neighboring classes to detect unlearned samples and demonstrate the vulnerability of such methods. We then propose a fine-tuning objective that mitigates this leakage by approximating, for forget-class inputs, the distribution over remaining classes that a model retrained from scratch would produce. To construct this approximation, we estimate inter-class similarity and tilt the target model's distribution accordingly. The resulting Tilted ReWeighting(TRW) distribution serves as the desired target during fine-tuning. Across multiple benchmarks, TRW matches or surpasses existing unlearning methods on prior metrics.

</details>


### [457] [Always Keep Your Promises: DynamicLRP, A Model-Agnostic Solution To Layer-Wise Relevance Propagation](https://arxiv.org/abs/2512.07010)
*Kevin Lee,Pablo Millan Arias*

Main category: cs.LG

TL;DR: A novel model-agnostic framework for Layer-wise Relevance Propagation (DynamicLRP) operating at the tensor operation level to ensure architecture agnosticism and extensibility across libraries and architectures.


<details>
  <summary>Details</summary>
Motivation: Existing LRP implementations are limited by architecture-specific propagation rules and require model modifications, reducing generality and sustainability as neural network architectures evolve.

Method: DynamicLRP decomposes attribution to individual tensor operations in computation graphs and introduces the Promise System for deferred activation resolution, creating a framework independent of backpropagation machinery.

Result: DynamicLRP matches or exceeds faithfulness of specialized implementations (e.g., VGG, ViT, RoBERTa-large, and Flan-T5-large) while achieving 99.92% node coverage across diverse architectures without model-specific code.

Conclusion: DynamicLRP provides a sustainable and extensible framework for LRP, surpassing architecture-specific limitations and enabling efficient and robust attributions across evolving AI systems.

Abstract: Layer-wise Relevance Propagation (LRP) provides principled attribution for neural networks through conservation properties and foundations in Deep Taylor Decomposition. However, existing implementations operate at the module level, requiring architecture-specific propagation rules and modifications. These limit the generality of target model and sustainability of implementations as architectures evolve. We introduce DynamicLRP, a model-agnostic LRP framework operating at the tensor operation level. By decomposing attribution to individual operations within computation graphs and introducing a novel mechanism for deferred activation resolution, named the Promise System, our approach achieves true architecture agnosticity while maintaining LRP's theoretical guarantees. This design operates independently of backpropagation machinery, enabling operation on arbitrary computation graphs without model modification and side-by-side execution with gradient backpropagation. Being based on computation graphs, this method is theoretically extensible to other deep learning libraries that support auto-differentiation. We demonstrate faithfulness matching or exceeding specialized implementations (1.77 vs 1.69 ABPC on VGG, equivalent performance on ViT, 93.70\% and 95.06\% top-1 attribution accuracy for explaining RoBERTa-large and Flan-T5-large answers on SQuADv2, respectively) while maintaining practical efficiency on models with hundreds of millions of parameters. We achieved 99.92\% node coverage across 31,465 computation graph nodes from 15 diverse architectures, including state-space models (Mamba), audio transformers (Whisper), and multimodal systems (DePlot) without any model-specific code with rules for 47 fundamental operations implemented. Our operation-level decomposition and Promise System establish a sustainable, extensible foundation for LRP across evolving architectures.

</details>


### [458] [Transferring Clinical Knowledge into ECGs Representation](https://arxiv.org/abs/2512.07021)
*Jose Geraldo Fernandes,Luiz Facury de Souza,Pedro Robles Dutenhefner,Gisele L. Pappa,Wagner Meira*

Main category: cs.LG

TL;DR: This paper introduces a three-stage training approach to improve ECG classification models by leveraging multimodal clinical data during training to create more interpretable and accurate models.


<details>
  <summary>Details</summary>
Motivation: Electrocardiogram (ECG) classification models are highly accurate but lack trust and interpretability, hindering their clinical adoption.

Method: The paper proposes a three-stage training paradigm involving pre-training with multimodal clinical data, creating a self-supervised ECG representation enriched with contextual information, and predicting associated laboratory abnormalities from ECG embeddings.

Result: The proposed model outperformed unimodal baselines in diagnosis classification and nearly achieved the performance of fully multimodal models, evaluated on the MIMIC-IV-ECG dataset.

Conclusion: This method improves ECG model interpretability and accuracy while laying the groundwork for incorporating AI models safely into clinical workflows.

Abstract: Deep learning models have shown high accuracy in classifying electrocardiograms (ECGs), but their black box nature hinders clinical adoption due to a lack of trust and interpretability. To address this, we propose a novel three-stage training paradigm that transfers knowledge from multimodal clinical data (laboratory exams, vitals, biometrics) into a powerful, yet unimodal, ECG encoder. We employ a self-supervised, joint-embedding pre-training stage to create an ECG representation that is enriched with contextual clinical information, while only requiring the ECG signal at inference time. Furthermore, as an indirect way to explain the model's output we train it to also predict associated laboratory abnormalities directly from the ECG embedding. Evaluated on the MIMIC-IV-ECG dataset, our model outperforms a standard signal-only baseline in multi-label diagnosis classification and successfully bridges a substantial portion of the performance gap to a fully multimodal model that requires all data at inference. Our work demonstrates a practical and effective method for creating more accurate and trustworthy ECG classification models. By converting abstract predictions into physiologically grounded \emph{explanations}, our approach offers a promising path toward the safer integration of AI into clinical workflows.

</details>


### [459] [Transformation of Biological Networks into Images via Semantic Cartography for Visual Interpretation and Scalable Deep Analysis](https://arxiv.org/abs/2512.07040)
*Sakib Mostafa,Lei Xing,Md. Tauhidul Islam*

Main category: cs.LG

TL;DR: This paper introduces Graph2Image, a novel method that converts biological networks into 2D images to improve scalability, efficiency, and interpretability for disease analysis and complex network studies.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges like scalability, multimodal integration, oversmoothing, and poor interpretability in analyzing large and complex biological networks with traditional computational approaches.

Method: The method, Graph2Image, spatially transforms network nodes into 2D grid images and uses CNNs to analyze them. This approach makes use of global receptive fields, multi-scale pyramids, and leverages visualization for better interpretation.

Result: Graph2Image improved classification accuracy by up to 67.2% compared to existing methods and enabled the analysis of expansive biological networks on a personal computer. It also delivered biologically meaningful visual patterns.

Conclusion: Graph2Image is a scalable, interpretable, and multimodal-ready framework suited for biological network analysis, offering advancements in disease diagnosis and the exploration of complex biological systems.

Abstract: Complex biological networks are fundamental to biomedical science, capturing interactions among molecules, cells, genes, and tissues. Deciphering these networks is critical for understanding health and disease, yet their scale and complexity represent a daunting challenge for current computational methods. Traditional biological network analysis methods, including deep learning approaches, while powerful, face inherent challenges such as limited scalability, oversmoothing long-range dependencies, difficulty in multimodal integration, expressivity bounds, and poor interpretability. We present Graph2Image, a framework that transforms large biological networks into sets of two-dimensional images by spatially arranging representative network nodes on a 2D grid. This transformation decouples the nodes as images, enabling the use of convolutional neural networks (CNNs) with global receptive fields and multi-scale pyramids, thus overcoming limitations of existing biological network analysis methods in scalability, memory efficiency, and long-range context capture. Graph2Image also facilitates seamless integration with other imaging and omics modalities and enhances interpretability through direct visualization of node-associated images. When applied to several large-scale biological network datasets, Graph2Image improved classification accuracy by up to 67.2% over existing methods and provided interpretable visualizations that revealed biologically coherent patterns. It also allows analysis of very large biological networks (nodes > 1 billion) on a personal computer. Graph2Image thus provides a scalable, interpretable, and multimodal-ready approach for biological network analysis, offering new opportunities for disease diagnosis and the study of complex biological systems.

</details>


### [460] [Self-Supervised Learning on Molecular Graphs: A Systematic Investigation of Masking Design](https://arxiv.org/abs/2512.07064)
*Jiannan Yang,Veronika Thost,Tengfei Ma*

Main category: cs.LG

TL;DR: This paper evaluates the principles behind masking-based self-supervised learning for molecular data, offering practical insights for design choices.


<details>
  <summary>Details</summary>
Motivation: Current practices in masking-based SSL lack principled evaluation, and it is unclear which design choices are genuinely effective for molecular representation learning.

Method: Develop a unified probabilistic framework to evaluate masking strategies and assess three key dimensions: masking distribution, prediction target, and encoder architecture, using controlled experiments and information-theoretic measures.

Result: The study finds that sophisticated masking has no consistent benefit over uniform sampling; instead, semantically richer prediction targets and their synergy with advanced encoders, like Graph Transformers, lead to significant performance gains.

Conclusion: Focus on prediction targets and their compatibility with encoder architectures when designing SSL strategies for molecular graphs, as this has the most impactful benefits.

Abstract: Self-supervised learning (SSL) plays a central role in molecular representation learning. Yet, many recent innovations in masking-based pretraining are introduced as heuristics and lack principled evaluation, obscuring which design choices are genuinely effective. This work cast the entire pretrain-finetune workflow into a unified probabilistic framework, enabling a transparent comparison and deeper understanding of masking strategies. Building on this formalism, we conduct a controlled study of three core design dimensions: masking distribution, prediction target, and encoder architecture, under rigorously controlled settings. We further employ information-theoretic measures to assess the informativeness of pretraining signals and connect them to empirically benchmarked downstream performance. Our findings reveal a surprising insight: sophisticated masking distributions offer no consistent benefit over uniform sampling for common node-level prediction tasks. Instead, the choice of prediction target and its synergy with the encoder architecture are far more critical. Specifically, shifting to semantically richer targets yields substantial downstream improvements, particularly when paired with expressive Graph Transformer encoders. These insights offer practical guidance for developing more effective SSL methods for molecular graphs.

</details>


### [461] [Procrustean Bed for AI-Driven Retrosynthesis: A Unified Framework for Reproducible Evaluation](https://arxiv.org/abs/2512.07079)
*Anton Morgunov,Victor S. Batista*

Main category: cs.LG

TL;DR: The paper introduces RetroCast, an evaluation framework to standardize and benchmark computer-aided synthesis planning models, addressing issues in comparison and metric reliability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of standardized evaluation methods and metrics favoring chemical validity over mere topological completion in computer-aided synthesis planning (CASP).

Method: The authors developed RetroCast, an evaluation framework with a benchmarking pipeline, SynthArena platform for visual inspection, standardized benchmarks, and database to ensure reproducible and statistically meaningful assessment of CASP models.

Result: The study revealed disconnects between solvability and actual chemical validity in models while discovering a 'complexity cliff' where search-based methods deteriorate in reconstructing long synthetic plans compared to sequence-based ones.

Conclusion: The paper emphasizes the importance of standardized evaluations and transparency. It provides tools and benchmarks to drive reproducible and meaningful advancements in CASP research.

Abstract: Progress in computer-aided synthesis planning (CASP) is obscured by the lack of standardized evaluation infrastructure and the reliance on metrics that prioritize topological completion over chemical validity. We introduce RetroCast, a unified evaluation suite that standardizes heterogeneous model outputs into a common schema to enable statistically rigorous, apples-to-apples comparison. The framework includes a reproducible benchmarking pipeline with stratified sampling and bootstrapped confidence intervals, accompanied by SynthArena, an interactive platform for qualitative route inspection. We utilize this infrastructure to evaluate leading search-based and sequence-based algorithms on a new suite of standardized benchmarks. Our analysis reveals a divergence between "solvability" (stock-termination rate) and route quality; high solvability scores often mask chemical invalidity or fail to correlate with the reproduction of experimental ground truths. Furthermore, we identify a "complexity cliff" in which search-based methods, despite high solvability rates, exhibit a sharp performance decay in reconstructing long-range synthetic plans compared to sequence-based approaches. We release the full framework, benchmark definitions, and a standardized database of model predictions to support transparent and reproducible development in the field.

</details>


### [462] [TRACE: A Generalizable Drift Detector for Streaming Data-Driven Optimization](https://arxiv.org/abs/2512.07082)
*Yuan-Ting Zhong,Ting Huang,Xiaolin Xiao,Yue-Jiao Gong*

Main category: cs.LG

TL;DR: TRACE is proposed for detecting distributional changes in streaming data with varying time scales, facilitating adaptive optimization in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in current methods for Streaming Data-Driven Optimization (SDDO), such as restrictive assumptions on drift intervals and environment observability.

Method: TRACE uses a tokenization strategy to extract statistical features and attention-based sequence learning for modeling drift patterns, enabling transferability and robustness in dynamic scenarios.

Result: TRACE effectively detects distributional changes, adapts well to unseen datasets, and significantly improves optimization under unknown drifts as evidenced by experimental benchmarks.

Conclusion: TRACE is a robust and transferable method that enhances the ability to handle streaming data challenges in dynamic, concept-drifting environments.

Abstract: Many optimization tasks involve streaming data with unknown concept drifts, posing a significant challenge as Streaming Data-Driven Optimization (SDDO). Existing methods, while leveraging surrogate model approximation and historical knowledge transfer, are often under restrictive assumptions such as fixed drift intervals and fully environmental observability, limiting their adaptability to diverse dynamic environments. We propose TRACE, a TRAnsferable C}oncept-drift Estimator that effectively detects distributional changes in streaming data with varying time scales. TRACE leverages a principled tokenization strategy to extract statistical features from data streams and models drift patterns using attention-based sequence learning, enabling accurate detection on unseen datasets and highlighting the transferability of learned drift patterns. Further, we showcase TRACE's plug-and-play nature by integrating it into a streaming optimizer, facilitating adaptive optimization under unknown drifts. Comprehensive experimental results on diverse benchmarks demonstrate the superior generalization, robustness, and effectiveness of our approach in SDDO scenarios.

</details>


### [463] [The Geometry of Persona: Disentangling Personality from Reasoning in Large Language Models](https://arxiv.org/abs/2512.07092)
*Zhixiang Wang*

Main category: cs.LG

TL;DR: The paper introduces a framework called the Soul Engine, which solves personalized Large Language Models' challenges by utilizing orthogonal personality subspaces, bypassing the need for fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing methods like Supervised Fine-Tuning (SFT), which degrade reasoning capabilities while aligning LLMs to individual personalization.

Method: The framework is based on the Linear Representation Hypothesis. It uses a dual-head architecture to extract personality vectors from a frozen Qwen-2.5 model without altering the core model weights. Additionally, SoulBench dataset enables personality assessments.

Result: The framework achieves high accuracy (MSE of 0.011), demonstrates geometric orthogonality in personality manifolds, and supports deterministic behavior control through vector arithmetic.

Conclusion: The research demonstrates that fine-tuning is not necessary for personalization. Instead, it uses deterministic latent interventions for precise and safe AI customization and aligns with foundational mathematical principles.

Abstract: Background: The deployment of personalized Large Language Models (LLMs) is currently constrained by the stability-plasticity dilemma. Prevailing alignment methods, such as Supervised Fine-Tuning (SFT), rely on stochastic weight updates that often incur an "alignment tax" -- degrading general reasoning capabilities.
  Methods: We propose the Soul Engine, a framework based on the Linear Representation Hypothesis, which posits that personality traits exist as orthogonal linear subspaces. We introduce SoulBench, a dataset constructed via dynamic contextual sampling. Using a dual-head architecture on a frozen Qwen-2.5 base, we extract disentangled personality vectors without modifying the backbone weights.
  Results: Our experiments demonstrate three breakthroughs. First, High-Precision Profiling: The model achieves a Mean Squared Error (MSE) of 0.011 against psychological ground truth. Second, Geometric Orthogonality: T-SNE visualization confirms that personality manifolds are distinct and continuous, allowing for "Zero-Shot Personality Injection" that maintains original model intelligence. Third, Deterministic Steering: We achieve robust control over behavior via vector arithmetic, validated through extensive ablation studies.
  Conclusion: This work challenges the necessity of fine-tuning for personalization. By transitioning from probabilistic prompting to deterministic latent intervention, we provide a mathematically rigorous foundation for safe, controllable AI personalization.

</details>


### [464] [Dual Refinement Cycle Learning: Unsupervised Text Classification of Mamba and Community Detection on Text Attributed Graph](https://arxiv.org/abs/2512.07100)
*Hong Wang,Yinglong Zhang,Hanhan Guo,Xuewen Xia,Xing Xu*

Main category: cs.LG

TL;DR: DRCL is an unsupervised framework that combines structural and semantic data for community detection in text-attributed networks without the need for labeled data.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of pretrained language models' reliance on labeled data and the neglect of textual semantics in community detection methods.

Method: Introduced DRCL, which integrates a GCN-based Community Detection Module (GCN-CDM) and a Text Semantic Modeling Module (TSMM) to iteratively refine data using pseudo-labels.

Result: DRCL improves both structural and semantic quality across datasets and achieves supervised-level accuracy using unsupervised learning.

Conclusion: DRCL is a practical and effective approach for large-scale systems lacking labeled data, enabling better community detection and text understanding.

Abstract: Pretrained language models offer strong text understanding capabilities but remain difficult to deploy in real-world text-attributed networks due to their heavy dependence on labeled data. Meanwhile, community detection methods typically ignore textual semantics, limiting their usefulness in downstream applications such as content organization, recommendation, and risk monitoring. To overcome these limitations, we present Dual Refinement Cycle Learning (DRCL), a fully unsupervised framework designed for practical scenarios where no labels or category definitions are available.
  DRCL integrates structural and semantic information through a warm-start initialization and a bidirectional refinement cycle between a GCN-based Community Detection Module (GCN-CDM) and a Text Semantic Modeling Module (TSMM). The two modules iteratively exchange pseudo-labels, allowing semantic cues to enhance structural clustering and structural patterns to guide text representation learning without manual supervision.
  Across several text-attributed graph datasets, DRCL consistently improves the structural and semantic quality of discovered communities. Moreover, a Mamba-based classifier trained solely from DRCL's community signals achieves accuracy comparable to supervised models, demonstrating its potential for deployment in large-scale systems where labeled data are scarce or costly.

</details>


### [465] [FOAM: Blocked State Folding for Memory-Efficient LLM Training](https://arxiv.org/abs/2512.07112)
*Ziqing Wen,Jiahuan Wang,Ping Luo,Dongsheng Li,Tao Sun*

Main category: cs.LG

TL;DR: The paper introduces FOAM, a method to mitigate memory bottlenecks in training large language models without compromising performance.


<details>
  <summary>Details</summary>
Motivation: Large language models face memory challenges during training, especially with optimizers like Adam, which can hinder scalability.

Method: FOAM compresses optimizer states using block-wise gradient means and residual correction to recover lost information while maintaining convergence rates similar to Adam.

Result: FOAM reduces training memory by 50%, optimizer state memory by up to 90%, and accelerates convergence, outperforming existing memory-efficient techniques.

Conclusion: FOAM represents a novel and effective solution for optimizing memory usage during large language model training, compatible with existing methods and enhancing scalability.

Abstract: Large language models (LLMs) have demonstrated remarkable performance due to their large parameter counts and extensive training data. However, their scale leads to significant memory bottlenecks during training, especially when using memory-intensive optimizers like Adam. Existing memory-efficient approaches often rely on techniques such as singular value decomposition (SVD), projections, or weight freezing, which can introduce substantial computational overhead, require additional memory for projections, or degrade model performance. In this paper, we propose Folded Optimizer with Approximate Moment (FOAM), a method that compresses optimizer states by computing block-wise gradient means and incorporates a residual correction to recover lost information. Theoretically, FOAM achieves convergence rates equivalent to vanilla Adam under standard non-convex optimization settings. Empirically, FOAM reduces total training memory by approximately 50\%, eliminates up to 90\% of optimizer state memory overhead, and accelerates convergence. Furthermore, FOAM is compatible with other memory-efficient optimizers, delivering performance and throughput that match or surpass both full-rank and existing memory-efficient baselines.

</details>


### [466] [PlantBiMoE: A Bidirectional Foundation Model with SparseMoE for Plant Genomes](https://arxiv.org/abs/2512.07113)
*Kepeng Lin,Qizhe Zhang,Rui Wang,Xuehai Hu,Wei Xu*

Main category: cs.LG

TL;DR: PlantBiMoE is a lightweight model for plant genomic language structure, excelling in bidirectional and parameter-efficient analysis, achieving state-of-the-art results on plant genomic tasks.


<details>
  <summary>Details</summary>
Motivation: Current models such as AgroNT and PDLLMs either have excessive parameters or fail to model the bidirectional nature of DNA strands, necessitating a tool that is both efficient and capable of bidirectional DNA analysis.

Method: PlantBiMoE employs a bidirectional Mamba for structural dependency across DNA strands and a SparseMoE framework to reduce parameters while maintaining modeling capacity. It uses the Modified Plants Genome Benchmark (MPGB) for testing.

Result: PlantBiMoE achieved the best performance on 20 out of 31 datasets and demonstrated superior average performance compared to existing genomic models.

Conclusion: PlantBiMoE is a powerful, efficient model for plant genome analysis, advancing plant genomics applications, including gene editing and synthetic biology.

Abstract: Understanding the underlying linguistic rules of plant genomes remains a fundamental challenge in computational biology. Recent advances including AgroNT and PDLLMs have made notable progress although, they suffer from excessive parameter size and limited ability to model the bidirectional nature of DNA strands respectively. To address these limitations, we propose PlantBiMoE, a lightweight and expressive plant genome language model that integrates bidirectional Mamba and a Sparse Mixture-of-Experts (SparseMoE) framework. The bidirectional Mamba enables the model to effectively capture structural dependencies across both the forward and reverse DNA strands, while SparseMoE significantly reduces the number of active parameters, improving computational efficiency without sacrificing modeling capacity. We evaluated and tested our model on the Modified Plants Genome Benchmark (MPGB), an enhanced genomic benchmark, which consolidates 31 datasets across 11 representative tasks, with input sequence lengths ranging from 50 to 6,000 bp. Experimental results demonstrate that PlantBiMoE achieves the best performance on 20 out of 31 datasets and the average best when comparing with existing models. In summary, all above results demonstrate that our model can effectively represent plant genomic sequences, serving as a robust computational tool for diverse genomic tasks, while making substantive contributions to plant genomics, gene editing, and synthetic biology. The code is available at: https://github.com/HUST-Keep-Lin/PlantBiMoE

</details>


### [467] [FlowLPS: Langevin-Proximal Sampling for Flow-based Inverse Problem Solvers](https://arxiv.org/abs/2512.07150)
*Jonghyun Park,Jong Chul Ye*

Main category: cs.LG

TL;DR: This paper proposes FlowLPS, a training-free framework for inverse problems using latent flow models, which leverages Langevin Proximal Sampling (LPS) to improve reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: Current methods for using latent flow models in inverse problems face convergence and manifold deviation issues, motivating the need for a more robust framework.

Method: The FlowLPS framework combines Langevin dynamics for consistent exploration within latent spaces and proximal optimization for accurate mode-seeking.

Result: FlowLPS achieves superior performance in reconstruction fidelity and perceptual quality in comparison to state-of-the-art methods on FFHQ and DIV2K datasets.

Conclusion: FlowLPS successfully addresses manifold deviation and convergence issues in latent flow models, offering a robust approach to solving inverse problems.

Abstract: Deep generative models have become powerful priors for solving inverse problems, and various training-free methods have been developed. However, when applied to latent flow models, existing methods often fail to converge to the posterior mode or suffer from manifold deviation within latent spaces. To mitigate this, here we introduce a novel training-free framework, FlowLPS, that solves inverse problems with pretrained flow models via a Langevin Proximal Sampling (LPS) strategy. Our method integrates Langevin dynamics for manifold-consistent exploration with proximal optimization for precise mode seeking, achieving a superior balance between reconstruction fidelity and perceptual quality across multiple inverse tasks on FFHQ and DIV2K, outperforming state of the art inverse solvers.

</details>


### [468] [Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration](https://arxiv.org/abs/2512.07173)
*Jucheng Shen,Gaurav Sarkar,Yeonju Ro,Sharath Nittur Sridhar,Zhangyang Wang,Aditya Akella,Souvik Kundu*

Main category: cs.LG

TL;DR: CadLLM is introduced as a method to accelerate diffusion-based large language models (dLLMs) without requiring training, achieving up to 2.28x performance gain over baselines.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency in inference throughput of diffusion-based large language models by leveraging a dynamic and adaptive approach that does not require retraining.

Method: CadLLM dynamically adjusts generation block and step sizes, thresholds based on token unmasking confidence, and reduces softmax overhead by regulating sampling breadth, in a lightweight, model-agnostic, and training-free manner.

Result: The method achieves up to 2.28x improvement in throughput across four tasks while maintaining competitive accuracy.

Conclusion: CadLLM effectively improves inference throughput of diffusion-based LLMs without sacrificing accuracy, representing a flexible and efficient solution.

Abstract: We present CadLLM, a training-free method to accelerate the inference throughput of diffusion-based LLMs (dLLMs). We first investigate the dynamic nature of token unmasking confidence across blocks and steps. Based on this observation, we present a lightweight adaptive approach that controls the generation block size, step size, and threshold based on the average confidence of unmasked tokens. We further reduce softmax overhead by dynamically leveraging a subset of the vocabulary to regulate sampling breadth. CadLLM is a plug-and-play, model-agnostic method compatible with KV-cache-based dLLMs. Extensive experiments on four popular tasks demonstrate that CadLLM yields up to 2.28x throughput improvement over the state-of-the-art baseline with competitive accuracy.

</details>


### [469] [SPACE: Noise Contrastive Estimation Stabilizes Self-Play Fine-Tuning for Large Language Models](https://arxiv.org/abs/2512.07175)
*Yibo Wang,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Lijun Zhang*

Main category: cs.LG

TL;DR: The paper introduces SPACE, a self-play fine-tuning method for LLMs, addressing instability in traditional methods by optimizing absolute reward values through noise contrastive estimation. This approach improves task performance and stability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the adaptation of large language models to downstream tasks with limited real-world data, addressing the instability issues in existing gap-based self-play fine-tuning methods.

Method: The authors propose SPACE, a method leveraging noise contrastive estimation to distinguish synthetic samples from real-world data in a binary classification manner, optimizing absolute rewards for stability and meaningful objectives.

Result: SPACE outperforms traditional supervised fine-tuning and gap-based self-play methods across various tasks by ensuring stable evolution and better alignment with real-world data distribution.

Conclusion: SPACE provides a stable and theoretically sound approach to self-play fine-tuning, achieving superior and more efficient performance in LLM adaptation with limited data.

Abstract: Self-play fine-tuning has demonstrated promising abilities in adapting large language models (LLMs) to downstream tasks with limited real-world data. The basic principle is to iteratively refine the model with real samples and synthetic ones generated from itself. However, the existing methods primarily focus on the relative gaps between the rewards for two types of data, neglecting their absolute values. Through theoretical analysis, we identify that the gap-based methods suffer from unstable evolution, due to the potentially degenerated objectives. To address this limitation, we introduce a novel self-play fine-tuning method, namely Self-PlAy via Noise Contrastive Estimation (SPACE), which leverages noise contrastive estimation to capture the real-world data distribution. Specifically, SPACE treats synthetic samples as auxiliary components, and discriminates them from the real ones in a binary classification manner. As a result, SPACE independently optimizes the absolute reward values for each type of data, ensuring a consistently meaningful objective and thereby avoiding the instability issue. Theoretically, we show that the optimal solution of the objective in SPACE aligns with the underlying distribution of real-world data, and SPACE guarantees a provably stable convergence to the optimal distribution. Empirically, we show that SPACE significantly improves the performance of LLMs over various tasks, and outperforms supervised fine-tuning that employs much more real-world samples. Compared to gap-based self-play fine-tuning methods, SPACE exhibits remarkable superiority and stable evolution.

</details>


### [470] [Pay Less Attention to Function Words for Free Robustness of Vision-Language Models](https://arxiv.org/abs/2512.07222)
*Qiwei Tian,Chenhao Lin,Zhengyu Zhao,Chao Shen*

Main category: cs.LG

TL;DR: The paper introduces Function-word De-Attention (FDA) to enhance robustness in Vision-Language Models (VLMs) by addressing vulnerability caused by function words in adversarial attacks. The method shows significant improvement in robustness with minimal performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the trade-off between robustness and performance in robust Vision-Language Models (VLMs) by mitigating the vulnerability caused by function words in cross-modal adversarial attacks.

Method: The proposed method, FDA, operates by calculating original and function-word cross-attention within attention heads and differentially subtracting the function-word cross-attention from the original to improve alignment and robustness of VLMs.

Result: FDA achieves significant robustness improvements: average ASR reductions of 18/13/53% with minimal performance drops (0.2/0.3/0.6%) in retrieval tasks across three models, and a 90% ASR drop with a 0.3% performance improvement in visual grounding.

Conclusion: FDA effectively improves the robustness of VLMs to adversarial attacks while maintaining strong performance. It demonstrates scalability, generalization, and zero-shot capabilities, backed by extensive experiments and analyses. The code will be publicly released.

Abstract: To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA.

</details>


### [471] [UniDiff: A Unified Diffusion Framework for Multimodal Time Series Forecasting](https://arxiv.org/abs/2512.07184)
*Da Zhang,Bingyu Li,Zhuyuan Zhao,Junyu Gao,Feiping Nie,Xuelong Li*

Main category: cs.LG

TL;DR: The paper proposes UniDiff, a unified diffusion framework for multimodal time series forecasting, leveraging texts and timestamps for state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Time series forecasting with multimodal data, such as texts and timestamps, is challenging due to the lack of methods that effectively utilize cross-modal signals.

Method: The framework tokenizes numerical data into embeddings via an MLP, employs unified cross-attention for multimodal data integration, and introduces classifier-free guidance for enhanced inference control.

Result: Extensive experiments demonstrate UniDiff's state-of-the-art performance on diverse domain benchmarks.

Conclusion: UniDiff effectively addresses the challenges of multimodal time series forecasting, paving the way for robust predictive models utilizing heterogeneous data.

Abstract: As multimodal data proliferates across diverse real-world applications, leveraging heterogeneous information such as texts and timestamps for accurate time series forecasting (TSF) has become a critical challenge. While diffusion models demonstrate exceptional performance in generation tasks, their application to TSF remains largely confined to modeling single-modality numerical sequences, overlooking the abundant cross-modal signals inherent in complex heterogeneous data. To address this gap, we propose UniDiff, a unified diffusion framework for multimodal time series forecasting. To process the numerical sequence, our framework first tokenizes the time series into patches, preserving local temporal dynamics by mapping each patch to an embedding space via a lightweight MLP. At its core lies a unified and parallel fusion module, where a single cross-attention mechanism adaptively weighs and integrates structural information from timestamps and semantic context from texts in one step, enabling a flexible and efficient interplay between modalities. Furthermore, we introduce a novel classifier-free guidance mechanism designed for multi-source conditioning, allowing for decoupled control over the guidance strength of textual and temporal information during inference, which significantly enhances model robustness. Extensive experiments on real-world benchmark datasets across eight domains demonstrate that the proposed UniDiff model achieves state-of-the-art performance.

</details>


### [472] [Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning](https://arxiv.org/abs/2512.07374)
*Yezi Liu,Hanning Chen,Wenjun Huang,Yang Ni,Mohsen Imani*

Main category: cs.LG

TL;DR: Introduces Recover-to-Forget (R2F) framework, an efficient unlearning method for large foundation models without full-model tuning or original training data.


<details>
  <summary>Details</summary>
Motivation: Dynamic updates, data deletion, and behavior correction in LLMs demand practical unlearning solutions beyond full fine-tuning or original data dependency.

Method: The framework uses low-rank LoRA adapters, paraphrased prompts, and a gradient decoder trained on proxy models to reconstruct unlearning gradients effectively.

Result: R2F achieves effective unlearning, preserves general system performance, and eliminates the dependency on retraining or direct model parameter access.

Conclusion: R2F presents a scalable, lightweight approach to unlearning in LLMs, with theoretical support and experimental validation demonstrating its impact.

Abstract: Unlearning in large foundation models (e.g., LLMs) is essential for enabling dynamic knowledge updates, enforcing data deletion rights, and correcting model behavior. However, existing unlearning methods often require full-model fine-tuning or access to the original training data, which limits their scalability and practicality. In this work, we introduce Recover-to-Forget (R2F), a novel framework for efficient unlearning in LLMs based on reconstructing full-model gradient directions from low-rank LoRA adapter updates. Rather than performing backpropagation through the full model, we compute gradients with respect to LoRA parameters using multiple paraphrased prompts and train a gradient decoder to approximate the corresponding full-model gradients. To ensure applicability to larger or black-box models, the decoder is trained on a proxy model and transferred to target models. We provide a theoretical analysis of cross-model generalization and demonstrate that our method achieves effective unlearning while preserving general model performance. Experimental results demonstrate that R2F offers a scalable and lightweight alternative for unlearning in pretrained LLMs without requiring full retraining or access to internal parameters.

</details>


### [473] [Less is More: Non-uniform Road Segments are Efficient for Bus Arrival Prediction](https://arxiv.org/abs/2512.07200)
*Zhen Huang,Jiaxin Deng,Jiayu Xu,Junbiao Pang,Haitao Yu*

Main category: cs.LG

TL;DR: The paper proposes an RL-based method to improve bus arrival time prediction using adaptive, non-uniform road segmentation.


<details>
  <summary>Details</summary>
Motivation: Traditional uniform road segmentation methods fail to account for variable constraints like road conditions and intersections, limiting efficiency in predicting bus arrival times.

Method: Reinforcement Learning is used to create adaptive, non-uniform road segments based on impact scores, followed by a linear model for predictions.

Result: The proposed method outperforms traditional segmentation techniques, achieving better efficiency and prediction performance, even surpassing some complex methods.

Conclusion: This RL-based segmentation strategy optimizes road network segmentation for arrival time prediction, improving both efficiency and accuracy on large-scale benchmarks.

Abstract: In bus arrival time prediction, the process of organizing road infrastructure network data into homogeneous entities is known as segmentation. Segmenting a road network is widely recognized as the first and most critical step in developing an arrival time prediction system, particularly for auto-regressive-based approaches. Traditional methods typically employ a uniform segmentation strategy, which fails to account for varying physical constraints along roads, such as road conditions, intersections, and points of interest, thereby limiting prediction efficiency. In this paper, we propose a Reinforcement Learning (RL)-based approach to efficiently and adaptively learn non-uniform road segments for arrival time prediction. Our method decouples the prediction process into two stages: 1) Non-uniform road segments are extracted based on their impact scores using the proposed RL framework; and 2) A linear prediction model is applied to the selected segments to make predictions. This method ensures optimal segment selection while maintaining computational efficiency, offering a significant improvement over traditional uniform approaches. Furthermore, our experimental results suggest that the linear approach can even achieve better performance than more complex methods. Extensive experiments demonstrate the superiority of the proposed method, which not only enhances efficiency but also improves learning performance on large-scale benchmarks. The dataset and the code are publicly accessible at: https://github.com/pangjunbiao/Less-is-More.

</details>


### [474] [LUNE: Efficient LLM Unlearning via LoRA Fine-Tuning with Negative Examples](https://arxiv.org/abs/2512.07375)
*Yezi Liu,Hanning Chen,Wenjun Huang,Yang Ni,Mohsen Imani*

Main category: cs.LG

TL;DR: A method called LUNE uses low-rank adapters to efficiently unlearn specific language model information without significant computational cost.


<details>
  <summary>Details</summary>
Motivation: Many large language models cannot effectively remove specific information due to the computational constraints of current methods, raising issues in privacy, bias, and knowledge correction.

Method: The paper introduces LUNE, a framework using LoRA-based negative-only unlearning to modify model behavior through lightweight adjustments to low-rank adapters without altering the model backbone.

Result: LUNE delivers performance equivalent to more resource-intensive methods like full fine-tuning, while significantly lowering computational and memory requirements.

Conclusion: LUNE provides an efficient and effective solution for targeted unlearning tasks in large language models, addressing privacy and knowledge correction needs with a localized and computationally practical approach.

Abstract: Large language models (LLMs) possess vast knowledge acquired from extensive training corpora, but they often cannot remove specific pieces of information when needed, which makes it hard to handle privacy, bias mitigation, and knowledge correction. Traditional model unlearning approaches require computationally expensive fine-tuning or direct weight editing, making them impractical for real-world deployment. In this work, we introduce LoRA-based Unlearning with Negative Examples (LUNE), a lightweight framework that performs negative-only unlearning by updating only low-rank adapters while freezing the backbone, thereby localizing edits and avoiding disruptive global changes. Leveraging Low-Rank Adaptation (LoRA), LUNE targets intermediate representations to suppress (or replace) requested knowledge with an order-of-magnitude lower compute and memory than full fine-tuning or direct weight editing. Extensive experiments on multiple factual unlearning tasks show that LUNE: (I) achieves effectiveness comparable to full fine-tuning and memory-editing methods, and (II) reduces computational cost by about an order of magnitude.

</details>


### [475] [Geometric Prior-Guided Federated Prompt Calibration](https://arxiv.org/abs/2512.07208)
*Fei Luo,Ziwei Zhao,Mingxuan Wang,Duoyang Li,Zhe Qian,Jiayi Tuo,Chenyue Zhou,Yanbiao Ma*

Main category: cs.LG

TL;DR: Federated Prompt Learning often suffers from data heterogeneity, leading to biased prompts. The proposed Geometry-Guided Text Prompt Calibration framework offers a privacy-preserving global geometric prior to correct this bias, significantly improving performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of data heterogeneity in Federated Prompt Learning (FPL), which results in biased locally trained prompts and hinders model performance.

Method: Introduced Geometry-Guided Text Prompt Calibration (GGTPC), which uses a global geometric prior derived from the covariance matrix. This prior is reconstructed server-side in a privacy-preserving manner and applied via a Geometry-Prior Calibration Layer to align local feature distributions during training.

Result: GGTPC outperformed state-of-the-art benchmarks, achieving improvements of 2.15% and 9.17% on label-skewed CIFAR-100 datasets, as well as boosting FedAvg's performance by 4.60% on domain-skewed Office-Home datasets.

Conclusion: GGTPC resolves the local training bias caused by data heterogeneity, providing a versatile and effective solution for enhancing Federated Learning algorithms.

Abstract: Federated Prompt Learning (FPL) offers a parameter-efficient solution for collaboratively training large models, but its performance is severely hindered by data heterogeneity, which causes locally trained prompts to become biased. Existing methods, focusing on aggregation or regularization, fail to address this root cause of local training bias. To this end, we propose Geometry-Guided Text Prompt Calibration (GGTPC), a novel framework that directly corrects this bias by providing clients with a global geometric prior. This prior, representing the shape of the global data distribution derived from the covariance matrix, is reconstructed on the server in a privacy-preserving manner. Clients then use a novel Geometry-Prior Calibration Layer (GPCL) to align their local feature distributions with this global prior during training. Extensive experiments show GGTPC's effectiveness. On the label-skewed CIFAR-100 dataset ($β$=0.1), it outperforms the state-of-the-art by 2.15\%. Under extreme skew ($β$=0.01), it improves upon the baseline by 9.17\%. Furthermore, as a plug-and-play module on the domain-skewed Office-Home dataset, it boosts FedAvg's performance by 4.60\%. These results demonstrate that GGTPC effectively mitigates data heterogeneity by correcting the fundamental local training bias, serving as a versatile module to enhance various FL algorithms.

</details>


### [476] [PINE: Pipeline for Important Node Exploration in Attributed Networks](https://arxiv.org/abs/2512.07244)
*Elizaveta Kovtun,Maksim Makarenko,Natalia Semenova,Alexey Zaytsev,Semen Budennyy*

Main category: cs.LG

TL;DR: This paper presents PINE, an unsupervised, attribute-aware attention-based graph model for identifying important nodes using both node attributes and graph structure.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for identifying important nodes in networks ignore rich node attributes, and recent neural network methods require supervision, leading to a gap in unsupervised and attribute-aware approaches.

Method: The authors propose PINE, an attention-based graph model that integrates node semantic features and structural graph properties to assign importance scores based on attention distribution.

Result: PINE demonstrates superior performance across various homogeneous and heterogeneous attributed networks and addresses real-world challenges in large-scale enterprise graphs.

Conclusion: PINE effectively tackles the unsupervised identification of key nodes by leveraging both structural and semantic attributes, offering significant practical utility in various domains.

Abstract: A graph with semantically attributed nodes are a common data structure in a wide range of domains. It could be interlinked web data or citation networks of scientific publications. The essential problem for such a data type is to determine nodes that carry greater importance than all the others, a task that markedly enhances system monitoring and management. Traditional methods to identify important nodes in networks introduce centrality measures, such as node degree or more complex PageRank. However, they consider only the network structure, neglecting the rich node attributes. Recent methods adopt neural networks capable of handling node features, but they require supervision. This work addresses the identified gap--the absence of approaches that are both unsupervised and attribute-aware--by introducing a Pipeline for Important Node Exploration (PINE). At the core of the proposed framework is an attention-based graph model that incorporates node semantic features in the learning process of identifying the structural graph properties. The PINE's node importance scores leverage the obtained attention distribution. We demonstrate the superior performance of the proposed PINE method on various homogeneous and heterogeneous attributed networks. As an industry-implemented system, PINE tackles the real-world challenge of unsupervised identification of key entities within large-scale enterprise graphs.

</details>


### [477] [IFFair: Influence Function-driven Sample Reweighting for Fair Classification](https://arxiv.org/abs/2512.07249)
*Jingran Yang,Min Zhang,Lingfeng Zhang,Zhaohui Wang,Yonggang Zhang*

Main category: cs.LG

TL;DR: This paper proposes IFFair, a pre-processing method using influence function to mitigate bias in machine learning models without altering their structure, data, or decision boundaries.


<details>
  <summary>Details</summary>
Motivation: Machine learning can inherit and exacerbate bias, leading to discriminatory decisions that harm social well-being and hinder application development.

Method: IFFair dynamically adjusts sample weights during training based on influence disparity of training samples across groups, without modifying network structure, data features, or decision boundaries.

Result: Experimental results on real-world datasets demonstrate that IFFair reduces bias across multiple fairness metrics while achieving better trade-offs between fairness and utility compared to previous methods.

Conclusion: IFFair effectively addresses bias issues in machine learning classification tasks, promoting fairness without conflicting metrics or requiring structural changes.

Abstract: Because machine learning has significantly improved efficiency and convenience in the society, it's increasingly used to assist or replace human decision-making. However, the data-based pattern makes related algorithms learn and even exacerbate potential bias in samples, resulting in discriminatory decisions against certain unprivileged groups, depriving them of the rights to equal treatment, thus damaging the social well-being and hindering the development of related applications. Therefore, we propose a pre-processing method IFFair based on the influence function. Compared with other fairness optimization approaches, IFFair only uses the influence disparity of training samples on different groups as a guidance to dynamically adjust the sample weights during training without modifying the network structure, data features and decision boundaries. To evaluate the validity of IFFair, we conduct experiments on multiple real-world datasets and metrics. The experimental results show that our approach mitigates bias of multiple accepted metrics in the classification setting, including demographic parity, equalized odds, equality of opportunity and error rate parity without conflicts. It also demonstrates that IFFair achieves better trade-off between multiple utility and fairness metrics compared with previous pre-processing methods.

</details>


### [478] [Group Representational Position Encoding](https://arxiv.org/abs/2512.07805)
*Yifan Zhang,Zixiang Chen,Yifeng Liu,Zhen Qin,Huizhuo Yuan,Kangping Xu,Yang Yuan,Quanquan Gu,Andrew Chi-Chih Yao*

Main category: cs.LG

TL;DR: The paper introduces GRAPE, a unified framework for positional encoding based on group actions, encompassing multiplicative rotations and additive logit biases, and generalizing prior methods like RoPE and ALiBi.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create a unified, principled framework for positional encodings in long-context models that subsumes and generalizes existing methods, improving their geometric and computational properties.

Method: GRAPE combines two mechanisms: (1) Multiplicative GRAPE using rotations in a group framework (SO) with matrix exponential representations and (2) Additive GRAPE utilizing additive logit biases through unipotent group actions (GL), offering exact relative laws and cacheability.

Result: GRAPE successfully unifies RoPE, ALiBi, and other previous methods into a systematic framework, while extending their capabilities for cross-subspace feature coupling at efficient computational costs.

Conclusion: The framework provides a robust design space for positional encoding in long-context models, offering theoretical clarity and computational advantages by integrating and improving upon existing methods.

Abstract: We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\mathrm{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\mathrm{GL}$. In Multiplicative GRAPE, a position $n \in \mathbb{Z}$ (or $t \in \mathbb{R}$) acts as $\mathbf{G}(n)=\exp(n\,ω\,\mathbf{L})$ with a rank-2 skew generator $\mathbf{L} \in \mathbb{R}^{d \times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.

</details>


### [479] [SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents](https://arxiv.org/abs/2512.07287)
*Sijia Li,Yuchen Huang,Zifan Liu,Zijian Li,Jingjing fu,Lei Song,Jiang Bian,Jun Zhang,Rui Wang*

Main category: cs.LG

TL;DR: The paper introduces the SIT-Graph, a model improving multi-turn tool-use scenarios by leveraging episodic and procedural memory for efficient decision-making.


<details>
  <summary>Details</summary>
Motivation: Current agent systems struggle in multi-turn tool-use scenarios due to evolving intents and states. The lack of efficient methods to reuse partial experience hampers development.

Method: The proposed SIT-Graph combines episodic-like compact state summaries with procedural-like tool dependencies in a graph-based memory structure, facilitating human-like decision-making during tool use.

Result: SIT-Graph achieves superior performance in multi-turn tool-use benchmarks compared to strong baselines, showcasing improved tool selection and experience transfer.

Conclusion: The SIT-Graph offers a promising framework for advancing multi-turn tool-use by enabling balanced episodic recall and procedural execution, enhancing adaptability.

Abstract: Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.

</details>


### [480] [Towards a Relationship-Aware Transformer for Tabular Data](https://arxiv.org/abs/2512.07310)
*Andrei V. Konstantinov,Valerii A. Zuev,Lev V. Utkin*

Main category: cs.LG

TL;DR: The paper introduces modified attention mechanisms that incorporate relationships between data points for tabular data, benchmarking them against existing methods.


<details>
  <summary>Details</summary>
Motivation: To improve tabular data models by leveraging external dependencies between samples, especially in sparse graph scenarios and treatment effect estimation.

Method: Modified attention mechanisms are introduced to account for relationships in the attention matrix. These are evaluated against other models on synthetic, real-world, and a treatment effect dataset.

Result: The proposed models were analyzed in regression tasks and treatment effect estimation to demonstrate their effectiveness.

Conclusion: The proposed solutions highlight the benefits of integrating external dependencies into predictions, presenting an alternative approach to models like gradient boosting decision trees for tabular data.

Abstract: Deep learning models for tabular data typically do not allow for imposing a graph of external dependencies between samples, which can be useful for accounting for relatedness in tasks such as treatment effect estimation. Graph neural networks only consider adjacent nodes, making them difficult to apply to sparse graphs. This paper proposes several solutions based on a modified attention mechanism, which accounts for possible relationships between data points by adding a term to the attention matrix. Our models are compared with each other and the gradient boosting decision trees in a regression task on synthetic and real-world datasets, as well as in a treatment effect estimation task on the IHDP dataset.

</details>


### [481] [Learning-Augmented Ski Rental with Discrete Distributions: A Bayesian Approach](https://arxiv.org/abs/2512.07313)
*Bosun Kang,Hyejun Park,Chenglin Fan*

Main category: cs.LG

TL;DR: This paper revisits the ski rental problem using Bayesian decision-making and machine learning predictions, offering a unified framework that achieves robust performance and adapts to both worst-case and fully-informed scenarios.


<details>
  <summary>Details</summary>
Motivation: To unify traditional worst-case cost minimization and learning-augmented algorithms in online decision-making problems using Bayesian reasoning.

Method: The paper proposes a discrete Bayesian framework that quantifies uncertainty, incorporates expert priors, and achieves competitive guarantees based on prior information.

Result: The framework demonstrates superior empirical performance, achieving near-optimal results with accurate priors while retaining robust guarantees in diverse scenarios.

Conclusion: Bayesian reasoning is shown to be effective for online decision problems with imperfect predictions, enabling extensions to multiple predictions, non-uniform priors, and contextual information.

Abstract: We revisit the classic ski rental problem through the lens of Bayesian decision-making and machine-learned predictions. While traditional algorithms minimize worst-case cost without assumptions, and recent learning-augmented approaches leverage noisy forecasts with robustness guarantees, our work unifies these perspectives. We propose a discrete Bayesian framework that maintains exact posterior distributions over the time horizon, enabling principled uncertainty quantification and seamless incorporation of expert priors. Our algorithm achieves prior-dependent competitive guarantees and gracefully interpolates between worst-case and fully-informed settings. Our extensive experimental evaluation demonstrates superior empirical performance across diverse scenarios, achieving near-optimal results under accurate priors while maintaining robust worst-case guarantees. This framework naturally extends to incorporate multiple predictions, non-uniform priors, and contextual information, highlighting the practical advantages of Bayesian reasoning in online decision problems with imperfect predictions.

</details>


### [482] [Local-Curvature-Aware Knowledge Graph Embedding: An Extended Ricci Flow Approach](https://arxiv.org/abs/2512.07332)
*Zhengquan Luo,Guy Tadmor,Or Amar,David Zeevi,Zhiqiang Xu*

Main category: cs.LG

TL;DR: The paper introduces RicciKGE, a novel KGE model where embedding spaces adapt dynamically to graph structures, overcoming limitations of predefined homogeneous manifolds by co-evolving with local curvatures.


<details>
  <summary>Details</summary>
Motivation: Current KGE models use predefined, homogeneous manifolds to encode graph features, but real-world graphs exhibit varying local curvatures, leading to distortions and reduced expressiveness.

Method: RicciKGE aligns the gradient descent process for embeddings with Ricci flow dynamics, allowing embeddings and manifold geometry to co-evolve toward mutual adaptation.

Result: Experiments reveal RicciKGE's improved performance in link prediction and node classification benchmarks compared to existing methods.

Conclusion: RicciKGE's dynamic adaptation to heterogeneous graph structures enhances its ability to model complex relationships, offering a promising approach for KGE tasks.

Abstract: Knowledge graph embedding (KGE) relies on the geometry of the embedding space to encode semantic and structural relations. Existing methods place all entities on one homogeneous manifold, Euclidean, spherical, hyperbolic, or their product/multi-curvature variants, to model linear, symmetric, or hierarchical patterns. Yet a predefined, homogeneous manifold cannot accommodate the sharply varying curvature that real-world graphs exhibit across local regions. Since this geometry is imposed a priori, any mismatch with the knowledge graph's local curvatures will distort distances between entities and hurt the expressiveness of the resulting KGE. To rectify this, we propose RicciKGE to have the KGE loss gradient coupled with local curvatures in an extended Ricci flow such that entity embeddings co-evolve dynamically with the underlying manifold geometry towards mutual adaptation. Theoretically, when the coupling coefficient is bounded and properly selected, we rigorously prove that i) all the edge-wise curvatures decay exponentially, meaning that the manifold is driven toward the Euclidean flatness; and ii) the KGE distances strictly converge to a global optimum, which indicates that geometric flattening and embedding optimization are promoting each other. Experimental improvements on link prediction and node classification benchmarks demonstrate RicciKGE's effectiveness in adapting to heterogeneous knowledge graph structures.

</details>


### [483] [Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood](https://arxiv.org/abs/2512.07390)
*Gilhyun Nam,Taewon Kim,Joonhyun Jeong,Eunho Yang*

Main category: cs.LG

TL;DR: The paper introduces a framework called SICL for improving calibration during test-time adaptation (TTA), emphasizing robust uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: Existing calibration methods fail to maintain performance in dynamic test conditions during TTA, causing issues in high-stakes domains.

Method: SICL measures instance-wise correctness likelihood through prediction consistency across style-altered data variants without backpropagation.

Result: SICL reduces calibration error significantly by an average of 13 percentage points across multiple baselines, TTA methods, and scenarios.

Conclusion: SICL provides a plug-and-play solution for TTA calibration, ensuring robust uncertainty estimation for dynamic conditions and improving overall reliability.

Abstract: Test-time adaptation (TTA) enables efficient adaptation of deployed models, yet it often leads to poorly calibrated predictive uncertainty - a critical issue in high-stakes domains such as autonomous driving, finance, and healthcare. Existing calibration methods typically assume fixed models or static distributions, resulting in degraded performance under real-world, dynamic test conditions. To address these challenges, we introduce Style Invariance as a Correctness Likelihood (SICL), a framework that leverages style-invariance for robust uncertainty estimation. SICL estimates instance-wise correctness likelihood by measuring prediction consistency across style-altered variants, requiring only the model's forward pass. This makes it a plug-and-play, backpropagation-free calibration module compatible with any TTA method. Comprehensive evaluations across four baselines, five TTA methods, and two realistic scenarios with three model architecture demonstrate that SICL reduces calibration error by an average of 13 percentage points compared to conventional calibration approaches.

</details>


### [484] [Empirical Results for Adjusting Truncated Backpropagation Through Time while Training Neural Audio Effects](https://arxiv.org/abs/2512.07393)
*Yann Bourdin,Pierrick Legrand,Fanny Roche*

Main category: cs.LG

TL;DR: The study optimizes Truncated Backpropagation Through Time (TBPTT) for better training of neural networks in audio effect modeling, focusing on dynamic range compression.


<details>
  <summary>Details</summary>
Motivation: To improve the performance and computational efficiency of TBPTT for training neural networks, specifically in the context of digital audio effect modeling.

Method: The paper evaluates and optimizes key TBPTT hyperparameters such as sequence number, batch size, and sequence length using experiments on convolutional-recurrent architectures with various datasets.

Result: Optimized TBPTT parameters improved model accuracy, training stability, and reduced computational requirements. Objective evaluations and subjective tests verified better performance and perceptual quality.

Conclusion: Carefully tuning TBPTT parameters enhances neural network training for audio effect modeling, achieving both efficient computation and high-quality results.

Abstract: This paper investigates the optimization of Truncated Backpropagation Through Time (TBPTT) for training neural networks in digital audio effect modeling, with a focus on dynamic range compression. The study evaluates key TBPTT hyperparameters -- sequence number, batch size, and sequence length -- and their influence on model performance. Using a convolutional-recurrent architecture, we conduct extensive experiments across datasets with and without conditionning by user controls. Results demonstrate that carefully tuning these parameters enhances model accuracy and training stability, while also reducing computational demands. Objective evaluations confirm improved performance with optimized settings, while subjective listening tests indicate that the revised TBPTT configuration maintains high perceptual quality.

</details>


### [485] [Asymptotic analysis of shallow and deep forgetting in replay with Neural Collapse](https://arxiv.org/abs/2512.07400)
*Giulia Lanzillotta,Damiano Meier,Thomas Hofmann*

Main category: cs.LG

TL;DR: The paper investigates the gap between deep feature-space and shallow classifier-level forgetting in continual learning tasks, analyzing Experience Replay's geometric implications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the paradox in continual learning where neural networks retain separable representations but fail in output predictions due to shallow forgetting.

Method: The method extends the Neural Collapse framework to sequential settings, analyzing geometric drift and statistical artifacts in experience replay buffers.

Result: Findings reveal minimal buffers prevent deep forgetting but exacerbate shallow forgetting due to statistical distortions like rank-deficient covariances.

Conclusion: The study challenges reliance on large buffers, proposing statistical corrections could enable robust continual learning with minimal replay requirements.

Abstract: A persistent paradox in continual learning (CL) is that neural networks often retain linearly separable representations of past tasks even when their output predictions fail. We formalize this distinction as the gap between deep feature-space and shallow classifier-level forgetting. We reveal a critical asymmetry in Experience Replay: while minimal buffers successfully anchor feature geometry and prevent deep forgetting, mitigating shallow forgetting typically requires substantially larger buffer capacities. To explain this, we extend the Neural Collapse framework to the sequential setting. We characterize deep forgetting as a geometric drift toward out-of-distribution subspaces and prove that any non-zero replay fraction asymptotically guarantees the retention of linear separability. Conversely, we identify that the "strong collapse" induced by small buffers leads to rank-deficient covariances and inflated class means, effectively blinding the classifier to true population boundaries. By unifying CL with out-of-distribution detection, our work challenges the prevailing reliance on large buffers, suggesting that explicitly correcting these statistical artifacts could unlock robust performance with minimal replay.

</details>


### [486] [Adaptive Tuning of Parameterized Traffic Controllers via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2512.07417)
*Giray Önür,Azita Dabiri,Bart De Schutter*

Main category: cs.LG

TL;DR: This paper presents a multi-agent reinforcement learning (RL) framework for traffic control, combining RL adaptability with the reactivity of state feedback controllers to improve efficiency and resilience.


<details>
  <summary>Details</summary>
Motivation: Conventional traffic management strategies lack adaptability for handling complex, dynamic, and time-varying traffic conditions.

Method: The study introduces a multi-agent RL system where agents adjust state feedback traffic controller parameters at low frequency, achieving training efficiency and robust adaptability.

Result: In simulations, the framework outperformed no control and fixed-parameter strategies, performed comparably to single-agent adaptive RL, and showed improved resilience to partial failures.

Conclusion: The multi-agent framework enhances adaptability, efficiency, and robustness in traffic control, proving effective for dynamic traffic management challenges.

Abstract: Effective traffic control is essential for mitigating congestion in transportation networks. Conventional traffic management strategies, including route guidance, ramp metering, and traffic signal control, often rely on state feedback controllers, used for their simplicity and reactivity; however, they lack the adaptability required to cope with complex and time-varying traffic dynamics. This paper proposes a multi-agent reinforcement learning framework in which each agent adaptively tunes the parameters of a state feedback traffic controller, combining the reactivity of state feedback controllers with the adaptability of reinforcement learning. By tuning parameters at a lower frequency rather than directly determining control actions at a high frequency, the reinforcement learning agents achieve improved training efficiency while maintaining adaptability to varying traffic conditions. The multi-agent structure further enhances system robustness, as local controllers can operate independently in the event of partial failures. The proposed framework is evaluated on a simulated multi-class transportation network under varying traffic conditions. Results show that the proposed multi-agent framework outperforms the no control and fixed-parameter state feedback control cases, while performing on par with the single-agent RL-based adaptive state feedback control, with a much better resilience to partial failures.

</details>


### [487] [Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models](https://arxiv.org/abs/2512.07419)
*Haidong Kang,Jun Du,Lihong Lin*

Main category: cs.LG

TL;DR: The paper introduces a framework named TAP that uses large language models (LLMs) and reinforcement learning to automate the discovery of proxies for mixed-precision quantization (MPQ), eliminating the need for extensive human expertise and training.


<details>
  <summary>Details</summary>
Motivation: Existing methods for MPQ optimization are either computationally expensive, inflexible, or rely on labor-intensive manual design requiring expert knowledge. This work aims to develop an automatic, efficient, and training-free approach for MPQ optimization.

Method: The authors proposed TAP, a novel framework that leverages LLMs to automatically design proxies for MPQ. A reinforcement-learning-based Direct Policy Optimization (DPO) mechanism is used to optimize prompts, enhancing the reasoning capabilities of LLMs and boosting performance over iterative feedback loops.

Result: The TAP framework outperforms existing mixed-precision quantization methods across various mainstream benchmarks, establishing state-of-the-art efficiency and accuracy.

Conclusion: The proposed TAP framework changes the paradigm for MPQ by significantly reducing reliance on human expertise through automation and achieving high performance. The authors believe this will offer new opportunities and perspectives for future MPQ research.

Abstract: Mixed-Precision Quantization (MPQ) liberates the Deep Neural Networks (DNNs) from the Out-Of-Memory (OOM) bottleneck, which garnered increasing research attention. However, conventional methods either searched from costly differentiable optimization, which is neither efficient nor flexible, or learned a quantized DNN from the proxy (i.e., HAWQ) manually designed by human experts, which is labor-intensive and requires huge expert knowledge. Can we design a proxy without involving any human experts and training? In this paper, we provide an affirmative answer by proposing a novel Large Language Models (LLMs)-driven Training-free Automatic Proxy (dubbed TAP) discovery framework, which reforms the design paradigm of MPQ by utilizing LLMs to find superior TAP tailored for MPQ, automatically. In addition, to bridge the gap between black-box LLMs and the tough MPQ task, we ingeniously propose simple Direct Policy Optimization (DPO) based reinforcement learning to enhance LLMs' reasoning by optimizing prompts, which can construct a positive feedback loop between the LLM and the MPQ task, enabling LLMs to generate better TAP in the next evolution. Extensive experiments on mainstream benchmarks demonstrate that TAP achieves state-of-the-art performance. Finally, we truly believe that our TAP will significantly contribute to the MPQ community by providing a new perspective on LLM-driven design algorithms.

</details>


### [488] [MIDG: Mixture of Invariant Experts with knowledge injection for Domain Generalization in Multimodal Sentiment Analysis](https://arxiv.org/abs/2512.07430)
*Yangle Li,Danli Luo,Haifeng Hu*

Main category: cs.LG

TL;DR: The paper proposes a novel framework for domain generalization in Multimodal Sentiment Analysis (MSA), addressing overlooked inter-modal synergies and fragmented cross-modal knowledge using a Mixture of Invariant Experts model and Cross-Modal Adapter.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to effectively capture rich semantic information from multimodal data due to insufficient focus on inter-modal synergies and fragmented cross-modal knowledge.

Method: A Mixture of Invariant Experts model is introduced for domain-invariant feature extraction, alongside a Cross-Modal Adapter that injects cross-modal knowledge to enrich multimodal representations.

Result: Experimental evaluation on three datasets demonstrates superior performance of the proposed framework compared to existing methods.

Conclusion: The framework enhances multimodal semantic understanding and improves generalization capabilities in MSA, effectively addressing limitations in prior approaches.

Abstract: Existing methods in domain generalization for Multimodal Sentiment Analysis (MSA) often overlook inter-modal synergies during invariant features extraction, which prevents the accurate capture of the rich semantic information within multimodal data. Additionally, while knowledge injection techniques have been explored in MSA, they often suffer from fragmented cross-modal knowledge, overlooking specific representations that exist beyond the confines of unimodal. To address these limitations, we propose a novel MSA framework designed for domain generalization. Firstly, the framework incorporates a Mixture of Invariant Experts model to extract domain-invariant features, thereby enhancing the model's capacity to learn synergistic relationships between modalities. Secondly, we design a Cross-Modal Adapter to augment the semantic richness of multimodal representations through cross-modal knowledge injection. Extensive domain experiments conducted on three datasets demonstrate that the proposed MIDG achieves superior performance.

</details>


### [489] [Mitigating Bias in Graph Hyperdimensional Computing](https://arxiv.org/abs/2512.07433)
*Yezi Liu,William Youngwoo Chung,Yang Ni,Hanning Chen,Mohsen Imani*

Main category: cs.LG

TL;DR: The paper presents FairGHDC, a fairness-aware framework for Graph Hyperdimensional Computing (HDC), reducing bias without sacrificing computational or accuracy benefits.


<details>
  <summary>Details</summary>
Motivation: To address unexamined biases in graph hyperdimensional computing, where unequal treatment arises from data representation and decision rules.

Method: A fairness-aware framework, FairGHDC, incorporates a bias correction term derived from demographic-parity regularizer to debias directly in hypervector space during training.

Result: FairGHDC significantly reduces demographic-parity and equal-opportunity biases across six datasets while maintaining competitive accuracy and achieving up to 10× training speedup on GPU versus baseline methods.

Conclusion: FairGHDC achieves fairness in graph HDC efficiently, maintaining performance with lower computational costs compared to GNN and fairness-aware GNN frameworks.

Abstract: Graph hyperdimensional computing (HDC) has emerged as a promising paradigm for cognitive tasks, emulating brain-like computation with high-dimensional vectors known as hypervectors. While HDC offers robustness and efficiency on graph-structured data, its fairness implications remain largely unexplored. In this paper, we study fairness in graph HDC, where biases in data representation and decision rules can lead to unequal treatment of different groups. We show how hypervector encoding and similarity-based classification can propagate or even amplify such biases, and we propose a fairness-aware training framework, FairGHDC, to mitigate them. FairGHDC introduces a bias correction term, derived from a gap-based demographic-parity regularizer, and converts it into a scalar fairness factor that scales the update of the class hypervector for the ground-truth label. This enables debiasing directly in the hypervector space without modifying the graph encoder or requiring backpropagation. Experimental results on six benchmark datasets demonstrate that FairGHDC substantially reduces demographic-parity and equal-opportunity gaps while maintaining accuracy comparable to standard GNNs and fairness-aware GNNs. At the same time, FairGHDC preserves the computational advantages of HDC, achieving up to about one order of magnitude ($\approx 10\times$) speedup in training time on GPU compared to GNN and fairness-aware GNN baselines.

</details>


### [490] [Forget and Explain: Transparent Verification of GNN Unlearning](https://arxiv.org/abs/2512.07450)
*Imran Ahsan,Hyunwook Yu,Jinsung Kim,Mucheol Kim*

Main category: cs.LG

TL;DR: The paper introduces an explainability-driven verifier for graph neural network (GNN) unlearning, focusing on transparent evidence like attribution shifts and structural changes to verify information deletion.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of ensuring and verifying that GNNs can "forget" information, particularly to comply with privacy regulations like GDPR, as traditional unlearning methods lack transparency.

Method: The method involves a verifier that assesses forgetting in GNNs through five explainability metrics: residual attribution, heatmap shift, explainability score deviation, graph edit distance, and diagnostic graph rule shift.

Result: The study tested GCN and GAT architectures with four unlearning strategies across several benchmarks, finding that Retrain and GNNDelete achieve significant forgetting, GraphEditor achieves partial erasure, and IDEA leaves residual signals.

Conclusion: The verifier's explainability metrics provide transparent, human-readable evidence of forgetting, complemented by membership-inference ROC-AUC for a broader privacy signal.

Abstract: Graph neural networks (GNNs) are increasingly used to model complex patterns in graph-structured data. However, enabling them to "forget" designated information remains challenging, especially under privacy regulations such as the GDPR. Existing unlearning methods largely optimize for efficiency and scalability, yet they offer little transparency, and the black-box nature of GNNs makes it difficult to verify whether forgetting has truly occurred. We propose an explainability-driven verifier for GNN unlearning that snapshots the model before and after deletion, using attribution shifts and localized structural changes (for example, graph edit distance) as transparent evidence. The verifier uses five explainability metrics: residual attribution, heatmap shift, explainability score deviation, graph edit distance, and a diagnostic graph rule shift. We evaluate two backbones (GCN, GAT) and four unlearning strategies (Retrain, GraphEditor, GNNDelete, IDEA) across five benchmarks (Cora, Citeseer, Pubmed, Coauthor-CS, Coauthor-Physics). Results show that Retrain and GNNDelete achieve near-complete forgetting, GraphEditor provides partial erasure, and IDEA leaves residual signals. These explanation deltas provide the primary, human-readable evidence of forgetting; we also report membership-inference ROC-AUC as a complementary, graph-wide privacy signal.

</details>


### [491] [Parallel Algorithms for Combined Regularized Support Vector Machines: Application in Music Genre Classification](https://arxiv.org/abs/2512.07463)
*Rongmei Liang,Zizheng Liu,Xiaofei Wu,Jingwen Tu*

Main category: cs.LG

TL;DR: The paper introduces a unified optimization framework for CR-SVMs, proposing a distributed parallel ADMM algorithm that enhances scalability and efficiency in processing distributed data.


<details>
  <summary>Details</summary>
Motivation: There is a need to address the inefficiency in computing CR-SVMs for distributed-stored big data and improve model handling with non-convex regularization.

Method: A unified optimization framework using consensus structure is proposed, alongside a distributed parallel ADMM algorithm with a Gaussian back-substitution method for efficient computation.

Result: The proposed algorithm maintains computational complexity regardless of different regularization terms and loss functions, showing universality. Experiments verify its reliability, efficiency, and stability.

Conclusion: The developed framework and algorithm are effective for CR-SVMs in distributed data systems, demonstrating promising results for scalability and practical applications such as music information retrieval.

Abstract: In the era of rapid development of artificial intelligence, its applications span across diverse fields, relying heavily on effective data processing and model optimization. Combined Regularized Support Vector Machines (CR-SVMs) can effectively handle the structural information among data features, but there is a lack of efficient algorithms in distributed-stored big data. To address this issue, we propose a unified optimization framework based on consensus structure. This framework is not only applicable to various loss functions and combined regularization terms but can also be effectively extended to non-convex regularization terms, showing strong scalability. Based on this framework, we develop a distributed parallel alternating direction method of multipliers (ADMM) algorithm to efficiently compute CR-SVMs when data is stored in a distributed manner. To ensure the convergence of the algorithm, we also introduce the Gaussian back-substitution method. Meanwhile, for the integrity of the paper, we introduce a new model, the sparse group lasso support vector machine (SGL-SVM), and apply it to music information retrieval. Theoretical analysis confirms that the computational complexity of the proposed algorithm is not affected by different regularization terms and loss functions, highlighting the universality of the parallel algorithm. Experiments on synthetic and free music archiv datasets demonstrate the reliability, stability, and efficiency of the algorithm.

</details>


### [492] [Materium: An Autoregressive Approach for Material Generation](https://arxiv.org/abs/2512.07486)
*Niklas Dobberstein,Jan Hamaekers*

Main category: cs.LG

TL;DR: Materium is a transformer-based model for generating crystal structures quickly and precisely, outperforming diffusion methods in speed and scalability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop a faster and scalable method for generating 3D material representations by bypassing the iterative refinement required in diffusion models.

Method: The method involves encoding crystal structure data (e.g., elements, fractional coordinates, lattice parameters) into token sequences processed by an autoregressive transformer model.

Result: Materium achieves faster training on a single GPU, superior generation speed on CPUs/GPUs, and accurate results across multiple property conditions.

Conclusion: The proposed approach effectively generates crystal structures aligned with predefined property constraints, offering a fast and scalable alternative to traditional methods.

Abstract: We present Materium: an autoregressive transformer for generating crystal structures that converts 3D material representations into token sequences. These sequences include elements with oxidation states, fractional coordinates and lattice parameters. Unlike diffusion approaches, which refine atomic positions iteratively through many denoising steps, Materium places atoms at precise fractional coordinates, enabling fast, scalable generation. With this design, the model can be trained in a few hours on a single GPU and generate samples much faster on GPUs and CPUs than diffusion-based approaches. The model was trained and evaluated using multiple properties as conditions, including fundamental properties, such as density and space group, as well as more practical targets, such as band gap and magnetic density. In both single and combined conditions, the model performs consistently well, producing candidates that align with the requested inputs.

</details>


### [493] [Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent](https://arxiv.org/abs/2512.07490)
*Zhiyu Liu,Zhi Han,Yandong Tang,Jun Fan,Yao Wang*

Main category: cs.LG

TL;DR: The paper introduces an Alternating Preconditioned Gradient Descent (APGD) algorithm for more efficient low-tubal-rank tensor estimation by overcoming limitations of traditional methods, especially under over-parameterization.


<details>
  <summary>Details</summary>
Motivation: Existing methods for low-tubal-rank tensor estimation struggle with computational inefficiency in large-scale tensors and convergence issues when the tensor rank is overestimated.

Method: The paper proposes the APGD algorithm, which incorporates a preconditioning term in gradient descent and alternates updates between factor tensors, ensuring accelerated convergence.

Result: Theoretical results demonstrate that APGD achieves linear convergence regardless of over-parameterization and the tensor condition number. Simulations validate these guarantees.

Conclusion: APGD offers a robust and efficient approach for low-tubal-rank tensor estimation, addressing convergence issues even in over-parameterized settings.

Abstract: The problem of low-tubal-rank tensor estimation is a fundamental task with wide applications across high-dimensional signal processing, machine learning, and image science. Traditional approaches tackle such a problem by performing tensor singular value decomposition, which is computationally expensive and becomes infeasible for large-scale tensors. Recent approaches address this issue by factorizing the tensor into two smaller factor tensors and solving the resulting problem using gradient descent. However, this kind of approach requires an accurate estimate of the tensor rank, and when the rank is overestimated, the convergence of gradient descent and its variants slows down significantly or even diverges. To address this problem, we propose an Alternating Preconditioned Gradient Descent (APGD) algorithm, which accelerates convergence in the over-parameterized setting by adding a preconditioning term to the original gradient and updating these two factors alternately. Based on certain geometric assumptions on the objective function, we establish linear convergence guarantees for more general low-tubal-rank tensor estimation problems. Then we further analyze the specific cases of low-tubal-rank tensor factorization and low-tubal-rank tensor recovery. Our theoretical results show that APGD achieves linear convergence even under over-parameterization, and the convergence rate is independent of the tensor condition number. Extensive simulations on synthetic data are carried out to validate our theoretical assertions.

</details>


### [494] [Exploring possible vector systems for faster training of neural networks with preconfigured latent spaces](https://arxiv.org/abs/2512.07509)
*Nikita Gabdullin*

Main category: cs.LG

TL;DR: This paper explores using vector systems to configure latent spaces in neural networks, enabling classifier training without classification layers, particularly for datasets with numerous classes. It improves speed and efficiency in training.


<details>
  <summary>Details</summary>
Motivation: To improve neural network training efficiency, especially for datasets with a large number of classes. The idea is to exploit predefined vector systems to optimize latent space configurations.

Method: The authors propose leveraging predefined vector systems, like An root system vectors, for configuring latent spaces. These systems enable classifier training without classification layers and minimize dimensional requirements.

Result: The method significantly accelerated training on datasets such as ImageNet-1K and those with 50k-600k classes. Furthermore, reducing latent space dimensions improved convergence and efficiency.

Conclusion: Using vector systems for latent space configurations offers faster convergence, reduced computational resources, and potential benefits for scaling neural networks with large class datasets.

Abstract: The overall neural network (NN) performance is closely related to the properties of its embedding distribution in latent space (LS). It has recently been shown that predefined vector systems, specifically An root system vectors, can be used as targets for latent space configurations (LSC) to ensure the desired LS structure. One of the main LSC advantage is the possibility of training classifier NNs without classification layers, which facilitates training NNs on datasets with extremely large numbers of classes. This paper provides a more general overview of possible vector systems for NN training along with their properties and methods for vector system construction. These systems are used to configure LS of encoders and visual transformers to significantly speed up ImageNet-1K and 50k-600k classes LSC training. It is also shown that using the minimum number of LS dimensions for a specific number of classes results in faster convergence. The latter has potential advantages for reducing the size of vector databases used to store NN embeddings.

</details>


### [495] [Machine Learning: Progress and Prospects](https://arxiv.org/abs/2512.07519)
*Alexander Gammerman*

Main category: cs.LG

TL;DR: This lecture, initially presented in 1996 and updated in 2025, introduces machine learning's historical and theoretical milestones, reflecting its interdisciplinary nature.


<details>
  <summary>Details</summary>
Motivation: To provide an insight into the historical origins, theoretical progress, and interdisciplinary contributions to the field of machine learning.

Method: Offering a historical journey, theoretical advancements, and discussion of subfields, the lecture updates its relevance through additional references reflecting 2025 developments.

Result: The lecture showcases the origins, evolution, and breadth of machine learning's interdisciplinary impact.

Conclusion: Machine learning has deep historical roots and encompasses diverse subfields, influenced by various disciplines, highlighting its complexity and progressiveness.

Abstract: This Inaugural Lecture was given at Royal Holloway University of London in 1996. It covers an introduction to machine learning and describes various theoretical advances and practical projects in the field. The Lecture here is presented in its original format, but a few remarks have been added in 2025 to reflect recent developments, and the list of references has been updated to enhance the convenience and accuracy for readers.
  When did machine learning start? Maybe a good starting point is 1949, when Claude Shannon proposed a learning algorithm for chess-playing programs. Or maybe we should go back to the 1930s when Ronald Fisher developed discriminant analysis - a type of learning where the problem is to construct a decision rule that separates two types of vectors. Or could it be the 18th century when David Hume discussed the idea of induction? Or the 14th century, when William of Ockham formulated the principle of "simplicity" known as "Ockham's razor" (Ockham, by the way, is a small village not far from Royal Holloway). Or it may be that, like almost everything else in Western civilisation and culture, the origin of these ideas lies in the Mediterranean. After all, it was Aristotle who said that "we learn some things only by doing things".
  The field of machine learning has been greatly influenced by other disciplines and the subject is in itself not a very homogeneous discipline, but includes separate, overlapping subfields. There are many parallel lines of research in ML: inductive learning, neural networks, clustering, and theories of learning. They are all part of the more general field of machine learning.

</details>


### [496] [Model-Based Reinforcement Learning Under Confounding](https://arxiv.org/abs/2512.07528)
*Nishanth Venkatesh,Andreas A. Malikopoulos*

Main category: cs.LG

TL;DR: The paper addresses model-based reinforcement learning in settings with unobserved contexts causing confounding, proposing a method that adapts a proximal off-policy evaluation to construct a surrogate MDP for consistent learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable model-based reinforcement learning in contextual MDPs where unobservable contexts cause confounding, rendering standard methods inconsistent.

Method: The paper uses a proximal off-policy evaluation approach, identifying confounded reward expectations via observable variables under mild invertibility conditions. It combines this with a behavior-averaged transition model to define a surrogate MDP.

Result: The authors create a surrogate MDP whose Bellman operator is well-defined and consistent for learning state-based policies in environments with unobserved contextual information.

Conclusion: The proposed approach facilitates robust model-based reinforcement learning in confounded settings, overcoming challenges posed by unobserved or unavailable contextual information.

Abstract: We investigate model-based reinforcement learning in contextual Markov decision processes (C-MDPs) in which the context is unobserved and induces confounding in the offline dataset. In such settings, conventional model-learning methods are fundamentally inconsistent, as the transition and reward mechanisms generated under a behavioral policy do not correspond to the interventional quantities required for evaluating a state-based policy. To address this issue, we adapt a proximal off-policy evaluation approach that identifies the confounded reward expectation using only observable state-action-reward trajectories under mild invertibility conditions on proxy variables. When combined with a behavior-averaged transition model, this construction yields a surrogate MDP whose Bellman operator is well defined and consistent for state-based policies, and which integrates seamlessly with the maximum causal entropy (MaxCausalEnt) model-learning framework. The proposed formulation enables principled model learning and planning in confounded environments where contextual information is unobserved, unavailable, or impractical to collect.

</details>


### [497] [FRWKV:Frequency-Domain Linear Attention for Long-Term Time Series Forecasting](https://arxiv.org/abs/2512.07539)
*Qingyuan Yang,Shizhuo,Dongyue Chen,Da Teng,Zehua Gan*

Main category: cs.LG

TL;DR: The paper introduces FRWKV, a model that combines linear attention and frequency-domain analysis for efficient long-sequence time series forecasting, achieving superior performance on eight datasets.


<details>
  <summary>Details</summary>
Motivation: Transformers struggle with long-sequence time series forecasting due to quadratic complexity and inadequate frequency-domain information utilization.

Method: FRWKV employs a frequency-domain linear-attention framework that achieves linear computational complexity while enhancing temporal feature representations via spectral information.

Result: FRWKV outperforms competitors, securing a first-place average rank across eight real-world datasets, with ablations confirming its design components.

Conclusion: FRWKV demonstrates the effective combination of linear attention and frequency analysis, providing a scalable and efficient modeling approach for long time series data.

Abstract: Traditional Transformers face a major bottleneck in long-sequence time series forecasting due to their quadratic complexity $(\mathcal{O}(T^2))$ and their limited ability to effectively exploit frequency-domain information. Inspired by RWKV's $\mathcal{O}(T)$ linear attention and frequency-domain modeling, we propose FRWKV, a frequency-domain linear-attention framework that overcomes these limitations. Our model integrates linear attention mechanisms with frequency-domain analysis, achieving $\mathcal{O}(T)$ computational complexity in the attention path while exploiting spectral information to enhance temporal feature representations for scalable long-sequence modeling. Across eight real-world datasets, FRWKV achieves a first-place average rank. Our ablation studies confirm the critical roles of both the linear attention and frequency-encoder components. This work demonstrates the powerful synergy between linear attention and frequency analysis, establishing a new paradigm for scalable time series modeling. Code is available at this repository: https://github.com/yangqingyuan-byte/FRWKV.

</details>


### [498] [RRAEDy: Adaptive Latent Linearization of Nonlinear Dynamical Systems](https://arxiv.org/abs/2512.07542)
*Jad Mounayer,Sebastian Rodriguez,Jerome Tomezyk,Chady Ghnatios,Francisco Chinesta*

Main category: cs.LG

TL;DR: RRAEDy introduces a framework to dynamically identify latent dimensions and ensure regularized linear dynamics without requiring manual loss balancing or fixed latent dimensions.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of existing latent-space models such as manual specification of latent dimensions, dependency on complex loss functions, and lack of regularization in latent variables.

Method: RRAEDy is built on Rank-Reduction Autoencoders (RRAEs), employing singular values to rank and prune latent variables while learning a latent Dynamic Mode Decomposition operator for temporal dynamics.

Result: RRAEDy demonstrates accurate and robust predictions across benchmarks like the Van der Pol oscillator, Burgers' equation, and 2D Navier-Stokes. Stability of its operator is theoretically proven.

Conclusion: RRAEDy eliminates prior limitations in latent-space modeling, providing a stable, low-dimensional framework for learning dynamical systems without requiring extensive manual tuning.

Abstract: Most existing latent-space models for dynamical systems require fixing the latent dimension in advance, they rely on complex loss balancing to approximate linear dynamics, and they don't regularize the latent variables. We introduce RRAEDy, a model that removes these limitations by discovering the appropriate latent dimension, while enforcing both regularized and linearized dynamics in the latent space. Built upon Rank-Reduction Autoencoders (RRAEs), RRAEDy automatically rank and prune latent variables through their singular values while learning a latent Dynamic Mode Decomposition (DMD) operator that governs their temporal progression. This structure-free yet linearly constrained formulation enables the model to learn stable and low-dimensional dynamics without auxiliary losses or manual tuning. We provide theoretical analysis demonstrating the stability of the learned operator and showcase the generality of our model by proposing an extension that handles parametric ODEs. Experiments on canonical benchmarks, including the Van der Pol oscillator, Burgers' equation, 2D Navier-Stokes, and Rotating Gaussians, show that RRAEDy achieves accurate and robust predictions. Our code is open-source and available at https://github.com/JadM133/RRAEDy. We also provide a video summarizing the main results at https://youtu.be/ox70mSSMGrM.

</details>


### [499] [ReLaX: Reasoning with Latent Exploration for Large Reasoning Models](https://arxiv.org/abs/2512.07558)
*Shimin Zhang,Xianwei Chen,Yufan Shen,Ziyuan Ye,Jibin Wu*

Main category: cs.LG

TL;DR: The paper introduces ReLaX, a novel reinforcement learning paradigm leveraging latent dynamics to improve exploration-exploitation tradeoff in large reasoning models, showing state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address entropy collapse in reinforcement learning with verifiable rewards (RLVR), which causes premature convergence and hinders effective exploration in large reasoning models (LRMs).

Method: The authors use Koopman operator theory to linearize hidden-state dynamics and introduce Dynamic Spectral Dispersion (DSD) as a metric for policy exploration. They then propose the ReLaX framework that regulates exploration and exploitation based on latent dynamics.

Result: ReLaX outperforms traditional methods in mitigating premature convergence and sets new state-of-the-art results across diverse multimodal and text-only reasoning benchmarks.

Conclusion: Latent dynamics, quantified through DSD, offer a more effective approach for guiding reinforcement learning policies, with ReLaX demonstrating significant improvements in reasoning tasks.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated remarkable potential in enhancing the reasoning capability of Large Reasoning Models (LRMs). However, RLVR often leads to entropy collapse, resulting in premature policy convergence and performance saturation. While manipulating token-level entropy has proven effective for promoting policy exploration, we argue that the latent dynamics underlying token generation encode a far richer computational structure for steering policy optimization toward a more effective exploration-exploitation tradeoff. To enable tractable analysis and intervention of the latent dynamics of LRMs, we leverage Koopman operator theory to obtain a linearized representation of their hidden-state dynamics. This enables us to introduce Dynamic Spectral Dispersion (DSD), a new metric to quantify the heterogeneity of the model's latent dynamics, serving as a direct indicator of policy exploration. Building upon these foundations, we propose Reasoning with Latent eXploration (ReLaX), a paradigm that explicitly incorporates latent dynamics to regulate exploration and exploitation during policy optimization. Comprehensive experiments across a wide range of multimodal and text-only reasoning benchmarks show that ReLaX significantly mitigates premature convergence and consistently achieves state-of-the-art performance.

</details>


### [500] [Weighted Contrastive Learning for Anomaly-Aware Time-Series Forecasting](https://arxiv.org/abs/2512.07569)
*Joel Ekstrand,Tor Mattsson,Zahra Taghiyarrenani,Slawomir Nowaczyk,Jens Lundström,Mikael Lindén*

Main category: cs.LG

TL;DR: The paper introduces Weighted Contrastive Adaptation (WECA), a method to improve time series forecasting under anomalous conditions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of forecasting multivariate time series during anomalous conditions, which disrupt operations like ATM cash logistics due to sudden demand shifts.

Method: The method uses a Weighted Contrastive Adaptation (WECA) objective to align normal and anomaly-augmented representations, preserving anomaly-relevant details while maintaining consistency under benign conditions.

Result: WECA improves forecasting accuracy by 6.1 percentage points on anomaly-affected data compared to the baseline model, with no significant performance loss on normal data.

Conclusion: The approach enhances forecast reliability under anomalies while ensuring robust performance during normal operations.

Abstract: Reliable forecasting of multivariate time series under anomalous conditions is crucial in applications such as ATM cash logistics, where sudden demand shifts can disrupt operations. Modern deep forecasters achieve high accuracy on normal data but often fail when distribution shifts occur. We propose Weighted Contrastive Adaptation (WECA), a Weighted contrastive objective that aligns normal and anomaly-augmented representations, preserving anomaly-relevant information while maintaining consistency under benign variations. Evaluations on a nationwide ATM transaction dataset with domain-informed anomaly injection show that WECA improves SMAPE on anomaly-affected data by 6.1 percentage points compared to a normally trained baseline, with negligible degradation on normal data. These results demonstrate that WECA enhances forecasting reliability under anomalies without sacrificing performance during regular operations.

</details>


### [501] [Time Series Foundation Models for Process Model Forecasting](https://arxiv.org/abs/2512.07624)
*Yongbo Yu,Jari Peeperkorn,Johannes De Smedt,Jochen De Weerdt*

Main category: cs.LG

TL;DR: The paper investigates the use of pre-trained Time Series Foundation Models (TSFMs) for Process Model Forecasting (PMF) and finds that they achieve lower forecasting errors than traditional models, making them highly effective for temporal data transfer.


<details>
  <summary>Details</summary>
Motivation: Current models for Process Model Forecasting (PMF) using machine learning and deep learning show modest improvements over statistical methods due to sparsity and heterogeneity of directly-follow (DF) time series.

Method: The authors utilize pre-trained Time Series Foundation Models (TSFMs) both in zero-shot configurations and fine-tuned versions to compare their efficiency against traditional models using DF time series from real-life event logs.

Result: TSFMs outperform traditional and specialized models in forecasting errors (MAE and RMSE). Fine-tuning offers minor improvements, which can diminish for smaller or complex datasets.

Conclusion: TSFMs demonstrate strong generalization and data efficiency, making zero-shot implementations viable for Process Model Forecasting tasks.

Abstract: Process Model Forecasting (PMF) aims to predict how the control-flow structure of a process evolves over time by modeling the temporal dynamics of directly-follows (DF) relations, complementing predictive process monitoring that focuses on single-case prefixes. Prior benchmarks show that machine learning and deep learning models provide only modest gains over statistical baselines, mainly due to the sparsity and heterogeneity of the DF time series. We investigate Time Series Foundation Models (TSFMs), large pre-trained models for generic time series, as an alternative for PMF. Using DF time series derived from real-life event logs, we compare zero-shot use of TSFMs, without additional training, with fine-tuned variants adapted on PMF-specific data. TSFMs generally achieve lower forecasting errors (MAE and RMSE) than traditional and specialized models trained from scratch on the same logs, indicating effective transfer of temporal structure from non-process domains. While fine-tuning can further improve accuracy, the gains are often small and may disappear on smaller or more complex datasets, so zero-shot use remains a strong default. Our study highlights the generalization capability and data efficiency of TSFMs for process-related time series and, to the best of our knowledge, provides the first systematic evaluation of temporal foundation models for PMF.

</details>


### [502] [A Mathematical Theory of Top-$k$ Sparse Attention via Total Variation Distance](https://arxiv.org/abs/2512.07647)
*Georgios Tzachristas,Lei Deng,Ioannis Tzachristas,Gong Zhang,Renhai Chen*

Main category: cs.LG

TL;DR: The paper develops a unified framework for certified Top-k attention truncation, quantifying approximation error in attention mechanisms for enhanced computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To create precise and robust tools for computing errors in Top-k attention truncation, addressing computational efficiency in widely-used models like transformers.

Method: The researchers introduce a mathematical framework based on total-variation distance and KL divergence to measure errors. They derive deterministic bounds and use probabilistic and experimental validations.

Result: They present new error bounds, propose practical rules for truncation, and validate these methods experimentally, showing improved computational efficiency.

Conclusion: The findings provide a certified way to reduce computational demands in attention mechanisms while meeting error thresholds, contributing to more efficient model implementations.

Abstract: We develop a unified mathematical framework for certified Top-$k$ attention truncation that quantifies approximation error at both the distribution and output levels. For a single attention distribution $P$ and its Top-$k$ truncation $\hat P$, we show that the total-variation distance coincides with the discarded softmax tail mass and satisfies $\mathrm{TV}(P,\hat P)=1-e^{-\mathrm{KL}(\hat P\Vert P)}$, yielding sharp Top-$k$-specific bounds in place of generic inequalities. From this we derive non-asymptotic deterministic bounds -- from a single boundary gap through multi-gap and blockwise variants -- that control $\mathrm{TV}(P,\hat P)$ using only the ordered logits. Using an exact head-tail decomposition, we prove that the output error factorizes as $\|\mathrm{Attn}(q,K,V)-\mathrm{Attn}_k(q,K,V)\|_2=τ\|μ_{\mathrm{tail}}-μ_{\mathrm{head}}\|_2$ with $τ=\mathrm{TV}(P,\hat P)$, yielding a new head-tail diameter bound $\|\mathrm{Attn}(q,K,V)-\mathrm{Attn}_k(q,K,V)\|_2\leτ\,\mathrm{diam}_{H,T}$ and refinements linking the error to $\mathrm{Var}_P(V)$. Under an i.i.d. Gaussian score model $s_i\sim\mathcal N(μ,σ^2)$ we derive closed-form tail masses and an asymptotic rule for the minimal $k_\varepsilon$ ensuring $\mathrm{TV}(P,\hat P)\le\varepsilon$, namely $k_\varepsilon/n\approxΦ_c(σ+Φ^{-1}(\varepsilon))$. Experiments on bert-base-uncased and synthetic logits confirm the predicted scaling of $k_\varepsilon/n$ and show that certified Top-$k$ can reduce scored keys by 2-4$\times$ on average while meeting the prescribed total-variation budget.

</details>


### [503] [Depth-Wise Activation Steering for Honest Language Models](https://arxiv.org/abs/2512.07667)
*Gracjan Góral,Marysia Winkels,Steven Basart*

Main category: cs.LG

TL;DR: The paper introduces a training-free activation steering method using a Gaussian schedule to improve the honesty of language models without retraining. It is shown to enhance truthful reporting in six of seven tested language models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the problem of language models asserting false information despite internally holding the correct answers (honesty failures), which weakens the models' auditability and safety.

Method: The authors propose a Gaussian schedule-based activation steering method that adjusts the intervention strength across the network depth without requiring model retraining.

Result: The approach improves honesty in six out of seven tested models from LLaMA, Qwen, and Mistral families. It also performs better than random, uniform, and box-filter depth allocation in equal-budget ablations on specific LLaMA and Qwen models.

Conclusion: This method offers a simple, model-agnostic, and low-cost tool for enhancing language models' truthful reporting capabilities by leveraging their existing internal representations.

Abstract: Large language models sometimes assert falsehoods despite internally representing the correct answer, failures of honesty rather than accuracy, which undermines auditability and safety. Existing approaches largely optimize factual correctness or depend on retraining and brittle single-layer edits, offering limited leverage over truthful reporting. We present a training-free activation steering method that weights steering strength across network depth using a Gaussian schedule. On the MASK benchmark, which separates honesty from knowledge, we evaluate seven models spanning the LLaMA, Qwen, and Mistral families and find that Gaussian scheduling improves honesty over no-steering and single-layer baselines in six of seven models. Equal-budget ablations on LLaMA-3.1-8B-Instruct and Qwen-2.5-7B-Instruct show the Gaussian schedule outperforms random, uniform, and box-filter depth allocations, indicating that how intervention is distributed across depth materially affects outcomes beyond total strength. The method is simple, model-agnostic, requires no finetuning, and provides a low-cost control knob for eliciting truthful reporting from models' existing capabilities.

</details>


### [504] [In-Context and Few-Shots Learning for Forecasting Time Series Data based on Large Language Models](https://arxiv.org/abs/2512.07705)
*Saroj Gopali,Bipin Chhetri,Deepika Giri,Sima Siami-Namini,Akbar Siami Namin*

Main category: cs.LG

TL;DR: The study compares traditional and foundation models for time series prediction, finding pre-trained models like TimesFM excel in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of pre-trained foundation models like TimesFM and LLMs in outperforming existing time series prediction models.

Method: The authors compare time series forecasting using in-context, zero-shot, and few-shot learning approaches of LLMs (o4-mini and Gemini 2.5), TimesFM, along with TCN and LSTM models.

Result: TimesFM achieves the best performance (lowest RMSE: 0.3023) with competitive inference time (266 seconds), while OpenAI's o4-mini also performs well in Zero Shot learning.

Conclusion: Pre-trained models like TimesFM are promising for real-time time series forecasting due to their accuracy, scalability, and minimal adaptation requirements.

Abstract: Existing data-driven approaches in modeling and predicting time series data include ARIMA (Autoregressive Integrated Moving Average), Transformer-based models, LSTM (Long Short-Term Memory) and TCN (Temporal Convolutional Network). These approaches, and in particular deep learning-based models such as LSTM and TCN, have shown great results in predicting time series data. With the advancement of leveraging pre-trained foundation models such as Large Language Models (LLMs) and more notably Google's recent foundation model for time series data, {\it TimesFM} (Time Series Foundation Model), it is of interest to investigate whether these foundation models have the capability of outperforming existing modeling approaches in analyzing and predicting time series data.
  This paper investigates the performance of using LLM models for time series data prediction. We investigate the in-context learning methodology in the training of LLM models that are specific to the underlying application domain. More specifically, the paper explores training LLMs through in-context, zero-shot and few-shot learning and forecasting time series data with OpenAI {\tt o4-mini} and Gemini 2.5 Flash Lite, as well as the recent Google's Transformer-based TimesFM, a time series-specific foundation model, along with two deep learning models, namely TCN and LSTM networks. The findings indicate that TimesFM has the best overall performance with the lowest RMSE value (0.3023) and the competitive inference time (266 seconds). Furthermore, OpenAI's o4-mini also exhibits a good performance based on Zero Shot learning.
  These findings highlight pre-trained time series foundation models as a promising direction for real-time forecasting, enabling accurate and scalable deployment with minimal model adaptation.

</details>


### [505] [Enabling Delayed-Full Charging Through Transformer-Based Real-Time-to-Departure Modeling for EV Battery Longevity](https://arxiv.org/abs/2512.07723)
*Yonggeon Lee,Jibin Hwang,Alfred Malengo Kondoro,Juhyun Song,Youngtae Noh*

Main category: cs.LG

TL;DR: The paper introduces a Transformer-based model for real-time EV departure prediction to improve battery charging practices and ensure sustainability.


<details>
  <summary>Details</summary>
Motivation: Efficient EV battery management is crucial since lithium-ion batteries degrade rapidly under high SOC. Predicting user departure times accurately can enable optimal charging practices, mitigating this degradation.

Method: The paper proposes a Transformer-based real-time-to-event model that converts daily schedules into grid-based tokens, capturing irregular patterns using both streaming contextual information and historical data.

Result: On real-world data involving 93 users and passive smartphone inputs, the model outperforms baseline models by effectively detecting irregular departure routines.

Conclusion: The proposed method shows practical feasibility for deployment, suggesting it could enhance EV charging systems, prolong battery lifespan, and foster sustainable transportation.

Abstract: Electric vehicles (EVs) are key to sustainable mobility, yet their lithium-ion batteries (LIBs) degrade more rapidly under prolonged high states of charge (SOC). This can be mitigated by delaying full charging \ours until just before departure, which requires accurate prediction of user departure times. In this work, we propose Transformer-based real-time-to-event (TTE) model for accurate EV departure prediction. Our approach represents each day as a TTE sequence by discretizing time into grid-based tokens. Unlike previous methods primarily dependent on temporal dependency from historical patterns, our method leverages streaming contextual information to predict departures. Evaluation on a real-world study involving 93 users and passive smartphone data demonstrates that our method effectively captures irregular departure patterns within individual routines, outperforming baseline models. These results highlight the potential for practical deployment of the \ours algorithm and its contribution to sustainable transportation systems.

</details>


### [506] [A multimodal Bayesian Network for symptom-level depression and anxiety prediction from voice and speech data](https://arxiv.org/abs/2512.07741)
*Agnes Norbury,George Fairs,Alexandra L. Georgescu,Matthew M. Nour,Emilia Molimpakis,Stefano Goria*

Main category: cs.LG

TL;DR: This paper examines the use of Bayesian network modeling to predict depression and anxiety symptoms from voice and speech features, demonstrating its clinical potential with a large dataset.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of integrating multiple nonverbal cues like tone and body language with patient-reported symptoms during psychiatric assessments using advanced AI tools.

Method: The study involves using Bayesian network models to predict mental health symptoms such as depression and anxiety based on large-scale datasets of voice and speech features, while also assessing fairness and clinical relevance.

Result: The model achieved high diagnostic accuracy (depression, anxiety ROC-AUC=0.842,0.831; core symptoms ROC-AUC>0.74) and demonstrated clinical fairness and usefulness, while providing transparent and explainable outputs.

Conclusion: Bayesian network models offer a structured and effective method for integrating multimodal data to support psychiatric assessments and symptom-level analysis, enhancing real-world clinical supervision.

Abstract: During psychiatric assessment, clinicians observe not only what patients report, but important nonverbal signs such as tone, speech rate, fluency, responsiveness, and body language. Weighing and integrating these different information sources is a challenging task and a good candidate for support by intelligence-driven tools - however this is yet to be realized in the clinic. Here, we argue that several important barriers to adoption can be addressed using Bayesian network modelling. To demonstrate this, we evaluate a model for depression and anxiety symptom prediction from voice and speech features in large-scale datasets (30,135 unique speakers). Alongside performance for conditions and symptoms (for depression, anxiety ROC-AUC=0.842,0.831 ECE=0.018,0.015; core individual symptom ROC-AUC>0.74), we assess demographic fairness and investigate integration across and redundancy between different input modality types. Clinical usefulness metrics and acceptability to mental health service users are explored. When provided with sufficiently rich and large-scale multimodal data streams and specified to represent common mental conditions at the symptom rather than disorder level, such models are a principled approach for building robust assessment support tools: providing clinically-relevant outputs in a transparent and explainable format that is directly amenable to expert clinical supervision.

</details>


### [507] [Formalized Hopfield Networks and Boltzmann Machines](https://arxiv.org/abs/2512.07766)
*Matteo Cipollina,Michail Karatarakis,Freek Wiedijk*

Main category: cs.LG

TL;DR: This paper formalizes neural networks using Lean 4, addressing deterministic and stochastic models. Formalizations include Hopfield networks and Boltzmann machines, with proofs of convergence, ergodicity, and Hebbian learning correctness.


<details>
  <summary>Details</summary>
Motivation: To create a rigorous mathematical formalization of neural networks for improved analysis and verification, addressing both deterministic and stochastic models.

Method: The authors use Lean 4 to formalize neural network models, proving properties like convergence, learning rule correctness for Hopfield networks, and ergodicity using the Perron-Frobenius theorem for Boltzmann machines.

Result: The work successfully formalizes neural network models and proves key properties, providing a framework for deeper theoretical understanding and reliability in neural network analysis.

Conclusion: The formalization advances the theoretical rigor in analyzing neural networks, enabling verified, reliable implementations with provable behavior in both deterministic and probabilistic frameworks.

Abstract: Neural networks are widely used, yet their analysis and verification remain challenging. In this work, we present a Lean 4 formalization of neural networks, covering both deterministic and stochastic models. We first formalize Hopfield networks, recurrent networks that store patterns as stable states. We prove convergence and the correctness of Hebbian learning, a training rule that updates network parameters to encode patterns, here limited to the case of pairwise-orthogonal patterns. We then consider stochastic networks, where updates are probabilistic and convergence is to a stationary distribution. As a canonical example, we formalize the dynamics of Boltzmann machines and prove their ergodicity, showing convergence to a unique stationary distribution using a new formalization of the Perron-Frobenius theorem.

</details>


### [508] [GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory](https://arxiv.org/abs/2512.07782)
*Jiaxu Liu,Yuhe Bai,Christos-Savvas Bouganis*

Main category: cs.LG

TL;DR: The paper introduces GatedFWA, a novel attention mechanism for autoregressive models that maintains efficiency while improving memory stability and gradient flow, overcoming limitations in existing methods like SWA and Softmax attention.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and stability issues in current attention mechanisms used in autoregressive models, specifically the unbounded training problem in Sliding Window Attention (SWA) and memory shrinkage/gradient vanishing in Softmax attention.

Method: GatedFWA employs per-token/head gates and a decay bias in attention logits, ensuring memory stability and controllable gradient flow. It also integrates a one-pass gated preprocessing and a FlashAttention-compatible kernel for efficiency and numerical stability.

Result: GatedFWA demonstrates competitive performance in language modeling benchmarks with minimal computational overhead while efficiently utilizing global context. It integrates well with other techniques like NSA and adapts to multiple autoregressive domains.

Conclusion: GatedFWA offers an effective enhancement to attention mechanisms, combining linear efficiency with stable and controllable memory updates, making it a robust choice for advanced autoregressive modeling tasks.

Abstract: Modern autoregressive models rely on attention, yet the Softmax full attention in Transformers scales quadratically with sequence length. Sliding Window Attention (SWA) achieves linear-time encoding/decoding by constraining the attention pattern, but under an \textit{Associative Memory} interpretation, its difference-style update renders the training objective effectively \emph{unbounded}. In contrast, Softmax attention normalizes updates, leading to \emph{memory shrinkage and gradient vanishing}. We propose GatedFWA: a Memory-\underline{Gated} (\underline{F}lash) \underline{W}indowed \underline{A}ttention mechanism that preserves SWAs efficiency while stabilizing memory updates and making gradient flow controllable. In essence, GatedFWA accumulate a per-token/head gate into a decay bias added to the attention logits, acting as a learnable contraction in the memory recurrence. We implement a fused one-pass gate preprocessing and a FlashAttention-compatible kernel that injects the gate under a sliding mask, ensuring I/O efficiency and numerical stability. On language modelling benchmarks, GatedFWA delivers competitive throughput with negligible overhead and better use of global context, and it integrates cleanly with token compression/selection methods such as NSA and generalizes to various autoregressive domains.

</details>


### [509] [The Adoption and Usage of AI Agents: Early Evidence from Perplexity](https://arxiv.org/abs/2512.07828)
*Jeremy Yang,Noah Yonack,Kate Zyskowski,Denis Yarats,Johnny Ho,Jerry Ma*

Main category: cs.LG

TL;DR: This study analyzes the large-scale adoption and usage of AI browser agents, focusing on Perplexity's Comet, using anonymized user data to explore who uses them, their intensity, and their application contexts.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to understand adoption patterns, usage intensity, and specific applications of AI agents in open-world environments, facilitating better design, engagement, and policy-making.

Method: The study analyzes anonymized user interaction data with Comet Assistant, categorizing diverse use cases through a hierarchical agentic taxonomy by topic, subtopic, and task.

Result: Users in high-GDP countries, with advanced education, and in knowledge-intensive fields are more likely to adopt the AI. Most queries focus on Productivity & Workflow and Learning & Research. Personal use dominates, with professional and educational use trailing.

Conclusion: AI agent adoption is influenced by economic, educational, and professional factors. Use cases evolve from general to cognitively complex topics, highlighting important implications for adoption, usage trends, and policymaker considerations.

Abstract: This paper presents the first large-scale field study of the adoption, usage intensity, and use cases of general-purpose AI agents operating in open-world web environments. Our analysis centers on Comet, an AI-powered browser developed by Perplexity, and its integrated agent, Comet Assistant. Drawing on hundreds of millions of anonymized user interactions, we address three fundamental questions: Who is using AI agents? How intensively are they using them? And what are they using them for? Our findings reveal substantial heterogeneity in adoption and usage across user segments. Earlier adopters, users in countries with higher GDP per capita and educational attainment, and individuals working in digital or knowledge-intensive sectors -- such as digital technology, academia, finance, marketing, and entrepreneurship -- are more likely to adopt or actively use the agent. To systematically characterize the substance of agent usage, we introduce a hierarchical agentic taxonomy that organizes use cases across three levels: topic, subtopic, and task. The two largest topics, Productivity & Workflow and Learning & Research, account for 57% of all agentic queries, while the two largest subtopics, Courses and Shopping for Goods, make up 22%. The top 10 out of 90 tasks represent 55% of queries. Personal use constitutes 55% of queries, while professional and educational contexts comprise 30% and 16%, respectively. In the short term, use cases exhibit strong stickiness, but over time users tend to shift toward more cognitively oriented topics. The diffusion of increasingly capable AI agents carries important implications for researchers, businesses, policymakers, and educators, inviting new lines of inquiry into this rapidly emerging class of AI capabilities.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [510] [A Multi-objective Optimization Approach for Feature Selection in Gentelligent Systems](https://arxiv.org/abs/2512.05971)
*Mohammadhossein Ghahramani,Yan Qiao,NaiQi Wu,Mengchu Zhou*

Main category: cs.NE

TL;DR: This paper introduces a "Gentelligent system" utilizing AI for manufacturing, enhancing fault detection and efficiency with a hybrid model, validated by real-world datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to integrate AI into manufacturing to create intelligent systems that improve efficiency, reduce costs, and enhance automation through reliable fault detection and optimized processes.

Method: A hybrid framework using a dominance-based multi-objective evolutionary algorithm was proposed, optimizing feature selection and classification simultaneously, supported by real-world industrial datasets for validation.

Result: The proposed model demonstrated strong generalizability and effectiveness across two real-world industrial datasets, achieving better manufacturing monitoring and optimization.

Conclusion: The study concludes that integrating AI through the "Gentelligent system" offers robust fault detection, optimization, and adaptability, helping manufacturers address multiple conflicting objectives efficiently.

Abstract: The integration of advanced technologies, such as Artificial Intelligence (AI), into manufacturing processes is attracting significant attention, paving the way for the development of intelligent systems that enhance efficiency and automation. This paper uses the term "Gentelligent system" to refer to systems that incorporate inherent component information (akin to genes in bioinformatics-where manufacturing operations are likened to chromosomes in this study) and automated mechanisms. By implementing reliable fault detection methods, manufacturers can achieve several benefits, including improved product quality, increased yield, and reduced production costs. To support these objectives, we propose a hybrid framework with a dominance-based multi-objective evolutionary algorithm. This mechanism enables simultaneous optimization of feature selection and classification performance by exploring Pareto-optimal solutions in a single run. This solution helps monitor various manufacturing operations, addressing a range of conflicting objectives that need to be minimized together. Manufacturers can leverage such predictive methods and better adapt to emerging trends. To strengthen the validation of our model, we incorporate two real-world datasets from different industrial domains. The results on both datasets demonstrate the generalizability and effectiveness of our approach.

</details>


### [511] [SEB-ChOA: An Improved Chimp Optimization Algorithm Using Spiral Exploitation Behavior](https://arxiv.org/abs/2512.05981)
*Leren Qian,Mohammad Khishe,Yiqian Huang,Seyedali Mirjalili*

Main category: cs.NE

TL;DR: The paper proposes an improved variant of the Chimp Optimization Algorithm (ChOA) by incorporating hybrid spiral functions (SEB-ChOA). This adaptation enhances performance on benchmark and real-world problems.


<details>
  <summary>Details</summary>
Motivation: Existing ChOA exhibits slow and premature convergence due to its simplistic modeling of the hunting process, necessitating enhancements for better optimization performance.

Method: Six spiral functions and two novel hybrid spiral functions (SEB-ChOA) are proposed. The SEB-ChOA is tested against benchmarks, constrained engineering problems, and compared to well-known and advanced algorithms.

Result: SEB-ChOA demonstrates top-ranked results in most benchmarks, outperforming traditional and modern optimization algorithms and showing comparable performance to state-of-the-art solutions (jDE100 and DISHchain1e+12).

Conclusion: The proposed SEB-ChOA significantly improves optimization efficiency, offering superior performance over most traditional algorithms and competitive results against cutting-edge techniques.

Abstract: The chimp optimization algorithm (ChOA) is a nature-inspired algorithm that imitates chimpanzees' individual intelligence and hunting behaviors. In this algorithm, the hunting process consists of four steps: driving, blocking, chasing, and attacking. Because of the novelty of ChOA, the steps of the hunting process have been modeled in a simple way, leading to slow and premature convergence similar to other iterative algorithms. This paper proposes six spiral functions and introduces two novel hybrid spiral functions (SEB-ChOA) to address these deficiencies. The performance of SEB-ChOA is evaluated on 23 standard benchmarks, 20 benchmarks of the IEEE CEC-2005 test suite, 10 cases from the IEEE CEC06-2019 test suite, and 12 constrained real-world engineering problems from IEEE CEC-2020. The SEB-ChOA variants are compared with three groups of optimization algorithms, including Particle Swarm Optimization (PSO) and Genetic Algorithm (GA) as well-known optimizers; Slime Mould Algorithm (SMA), Marine Predators Algorithm (MPA), Ant Lion Optimization (ALO), and Henry Gas Solubility Optimization (HGSO) as recently developed optimizers; and jDE100 and DISHchain1e+12, the winners of the IEEE CEC06-2019 competition. Additional comparisons are made with EBOwithCMAR and CIPDE as strong secondary baselines. The SEB-ChOA methods achieve top-ranked results on nearly all benchmarks and show competitive performance compared to jDE100 and DISHchain1e+12. Statistical results indicate that SEB-ChOA outperforms PSO, GA, SMA, MPA, ALO, and HGSO while producing results comparable to those of jDE100 and DISHchain1e+12.

</details>


### [512] [Entropic Regularization in the Deep Linear Network](https://arxiv.org/abs/2512.06137)
*Alan Chen,Tejas Kotwal,Govind Menon*

Main category: cs.NE

TL;DR: The paper investigates the use of entropy-based regularization in deep linear networks (DLN), revealing key properties of equilibria, gradient flow dynamics, and entropy behavior.


<details>
  <summary>Details</summary>
Motivation: To understand the role of entropy in regularization for DLN and study its implications on the optimization landscapes and dynamics in deep learning.

Method: Apply entropy formula from arXiv:2509.09088 to analyze equilibria, gradient flows, and entropy concavity in DLN under Riemannian manifold settings.

Result: Equilibria are minimizers forming an orbit of the orthogonal group, gradient flow reduces to a 1D differential equation for relaxation rates, and entropy is strictly concave in Euclidean geometry but not in DLN's Riemannian geometry.

Conclusion: Entropy-based regularization in DLNs provides novel insights into optimization dynamics and equilibrium structures, useful for theoretical advancements in deep learning.

Abstract: We study regularization for the deep linear network (DLN) using the entropy formula introduced in arXiv:2509.09088. The equilibria and gradient flow of the free energy on the Riemannian manifold of end-to-end maps of the DLN are characterized for energies that depend symmetrically on the singular values of the end-to-end matrix.
  The only equilibria are minimizers and the set of minimizers is an orbit of the orthogonal group. In contrast with random matrix theory there is no singular value repulsion. The corresponding gradient flow reduces to a one-dimensional ordinary differential equation whose solution gives explicit relaxation rates toward the minimizers. We also study the concavity of the entropy in the chamber of singular values. The entropy is shown to be strictly concave in the Euclidean geometry on the chamber but not in the Riemannian geometry defined by the DLN metric.

</details>


### [513] [Neuro-Vesicles: Neuromodulation Should Be a Dynamical System, Not a Tensor Decoration](https://arxiv.org/abs/2512.06966)
*Zilin Li,Weiwei Xu,Vicki Kane*

Main category: cs.NE

TL;DR: The paper proposes Neuro-Vesicles—an event-driven layer for neural networks where vesicles carry information and actions outside traditional tensors. They alter computations by migrating, docking, and decaying.


<details>
  <summary>Details</summary>
Motivation: To enhance neural networks by introducing a missing computational layer that allows dynamic, localized, and adaptive modulations, mimicking biological processes.

Method: The authors propose a dynamical population of mobile vesicles that interact outside tensors, perform operations like docking or migrating, and impact computations through content-dependent actions.

Result: The framework supports dense vesicle approximations for tensor mechanisms and sparse vesicles for rare intervention, with mathematical specifications and reinforcement learning adaptations.

Conclusion: Neuro-Vesicles present a novel interaction layer for neural networks, enabling more adaptive, biologically inspired neuromodulation and potential integration with neuromorphic hardware.

Abstract: We introduce Neuro-Vesicles, a framework that augments conventional neural networks with a missing computational layer: a dynamical population of mobile, discrete vesicles that live alongside the network rather than inside its tensors. Each vesicle is a self contained object v = (c, kappa, l, tau, s) carrying a vector payload, type label, location on the graph G = (V, E), remaining lifetime, and optional internal state. Vesicles are emitted in response to activity, errors, or meta signals; migrate along learned transition kernels; probabilistically dock at nodes; locally modify activations, parameters, learning rules, or external memory through content dependent release operators; and finally decay or are absorbed.
  This event based interaction layer reshapes neuromodulation. Instead of applying the same conditioning tensors on every forward pass, modulation emerges from the stochastic evolution of a vesicle population that can accumulate, disperse, trigger cascades, carve transient pathways, and write structured traces into topological memory. Dense, short lived vesicles approximate familiar tensor mechanisms such as FiLM, hypernetworks, or attention. Sparse, long lived vesicles resemble a small set of mobile agents that intervene only at rare but decisive moments.
  We give a complete mathematical specification of the framework, including emission, migration, docking, release, decay, and their coupling to learning; a continuous density relaxation that yields differentiable reaction diffusion dynamics on the graph; and a reinforcement learning view where vesicle control is treated as a policy optimized for downstream performance. We also outline how the same formalism extends to spiking networks and neuromorphic hardware such as the Darwin3 chip, enabling programmable neuromodulation on large scale brain inspired computers.

</details>


### [514] [Synchrony-Gated Plasticity with Dopamine Modulation for Spiking Neural Networks](https://arxiv.org/abs/2512.07194)
*Yuchen Tian,Samuel Tensingh,Jason Eshraghian,Nhan Duy Truong,Omid Kavehei*

Main category: cs.NE

TL;DR: This paper introduces a biologically inspired learning rule (DA-SSDP) for spiking neural networks that improves training accuracy on major benchmarks without changing model structure or optimization routines.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of integrating biologically inspired local signals into deep spiking neural networks while minimizing memory demands and ensuring alignment with supervised learning objectives.

Method: The authors propose DA-SSDP, a synchrony-based plasticity rule sensitive to task loss. It uses synchrony metrics during a warm-up phase and small weight updates based on spike timing and Gaussian latency, applied after backpropagation.

Result: Testing on CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet-1K showed accuracy improvements of 0.42%, 0.99%, 0.1%, and 0.73%, respectively, with minimal computational overhead.

Conclusion: DA-SSDP enhances the training of spiking neural networks, delivering consistent accuracy improvements as a regularizer, and achieves biologically inspired learning integration without significant complexity.

Abstract: While surrogate backpropagation proves useful for training deep spiking neural networks (SNNs), incorporating biologically inspired local signals on a large scale remains challenging. This difficulty stems primarily from the high memory demands of maintaining accurate spike-timing logs and the potential for purely local plasticity adjustments to clash with the supervised learning goal. To effectively leverage local signals derived from spiking neuron dynamics, we introduce Dopamine-Modulated Spike-Synchrony-Dependent Plasticity (DA-SSDP), a synchrony-based rule that is sensitive to loss and brings a synchrony-based local learning signal to the model. DA-SSDP condenses spike patterns into a synchrony metric at the batch level. An initial brief warm-up phase assesses its relationship to the task loss and sets a fixed gate that subsequently adjusts the local update's magnitude. In cases where synchrony proves unrelated to the task, the gate settles at one, simplifying DA-SSDP to a basic two-factor synchrony mechanism that delivers minor weight adjustments driven by concurrent spike firing and a Gaussian latency function. These small weight updates are only added to the network`s deeper layers following the backpropagation phase, and our tests showed this simplified version did not degrade performance and sometimes gave a small accuracy boost, serving as a regularizer during training. The rule stores only binary spike indicators and first-spike latencies with a Gaussian kernel. Without altering the model structure or optimization routine, evaluations on benchmarks like CIFAR-10 (+0.42\%), CIFAR-100 (+0.99\%), CIFAR10-DVS (+0.1\%), and ImageNet-1K (+0.73\%) demonstrated consistent accuracy gains, accompanied by a minor increase in computational overhead. Our code is available at https://github.com/NeuroSyd/DA-SSDP.

</details>


### [515] [Algorithm-hardware co-design of neuromorphic networks with dual memory pathways](https://arxiv.org/abs/2512.07602)
*Pengfei Sun,Zhe Su,Jascha Achterberg,Giacomo Indiveri,Dan F. M. Goodman,Danyal Akarca*

Main category: cs.NE

TL;DR: The paper introduces a dual memory pathway (DMP) architecture for spiking neural networks that improves accuracy with fewer parameters, alongside a hardware design boosting energy efficiency and throughput.


<details>
  <summary>Details</summary>
Motivation: Spiking neural networks struggle with long-term memory management and hardware efficiency under tight energy and memory budgets.

Method: The authors propose a neural network model inspired by cortical fast-slow organization with a slow memory pathway combined with fast spiking activity and implement near-memory-compute hardware optimized for this architecture.

Result: The proposed architecture achieves competitive accuracy on benchmarks with 40-60% fewer parameters and hardware shows 4x throughput and 5x energy efficiency improvements.

Conclusion: Biological principles can lead to scalable, energy-efficient algorithm-hardware designs for real-time neuromorphic computation and learning.

Abstract: Spiking neural networks excel at event-driven sensing yet maintaining task-relevant context over long timescales. However building these networks in hardware respecting both tight energy and memory budgets, remains a core challenge in the field. We address this challenge through novel algorithm-hardware co-design effort. At the algorithm level, inspired by the cortical fast-slow organization in the brain, we introduce a neural network with an explicit slow memory pathway that, combined with fast spiking activity, enables a dual memory pathway (DMP) architecture in which each layer maintains a compact low-dimensional state that summarizes recent activity and modulates spiking dynamics. This explicit memory stabilizes learning while preserving event-driven sparsity, achieving competitive accuracy on long-sequence benchmarks with 40-60% fewer parameters than equivalent state-of-the-art spiking neural networks. At the hardware level, we introduce a near-memory-compute architecture that fully leverages the advantages of the DMP architecture by retaining its compact shared state while optimizing dataflow, across heterogeneous sparse-spike and dense-memory pathways. We show experimental results that demonstrate more than a 4x increase in throughput and over a 5x improvement in energy efficiency compared with state-of-the-art implementations. Together, these contributions demonstrate that biological principles can guide functional abstractions that are both algorithmically effective and hardware-efficient, establishing a scalable co-design paradigm for real-time neuromorphic computation and learning.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [516] [Predictive Modeling of I/O Performance for Machine Learning Training Pipelines: A Data-Driven Approach to Storage Optimization](https://arxiv.org/abs/2512.06699)
*Karthik Prabhakar*

Main category: cs.PF

TL;DR: The paper addresses I/O bottlenecks in ML training pipelines using XGBoost to predict performance and optimize storage configuration, achieving high prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: ML training often suffers from GPUs being idle due to data I/O bottlenecks. Optimal storage configurations are needed to maximize utilization.

Method: 141 benchmarking observations across storage backends and access patterns were analyzed using seven regression and three classification models, with XGBoost chosen for its superior performance.

Result: XGBoost achieved an R-squared of 0.991, predicting I/O throughput with a 11.8% average error. Batch size and throughput metrics stood out as key drivers of performance.

Conclusion: The approach significantly reduces storage configuration time for ML pipelines, proving extensible and reproducible for other ML resource management. Code and data are provided for further use.

Abstract: Modern machine learning training is increasingly bottlenecked by data I/O rather than compute. GPUs often sit idle at below 50% utilization waiting for data. This paper presents a machine learning approach to predict I/O performance and recommend optimal storage configurations for ML training pipelines. We collected 141 observations through systematic benchmarking across different storage backends (NVMe SSD, network-attached storage, in-memory filesystems), data formats, and access patterns, covering both low-level I/O operations and full training pipelines. After evaluating seven regression models and three classification approaches, XGBoost achieved the best performance with R-squared of 0.991, predicting I/O throughput within 11.8% error on average. Feature importance analysis revealed that throughput metrics and batch size are the primary performance drivers. This data-driven approach can reduce configuration time from days of trial-and-error to minutes of predictive recommendation. The methodology is reproducible and extensible to other resource management problems in ML systems. Code and data are available at https://github.com/knkarthik01/gpu_storage_ml_project

</details>


### [517] [AFarePart: Accuracy-aware Fault-resilient Partitioner for DNN Edge Accelerators](https://arxiv.org/abs/2512.07449)
*Mukta Debnath,Krishnendu Guha,Debasri Saha,Amlan Chakrabarti,Susmita Sur-Kolay*

Main category: cs.PF

TL;DR: This paper presents a fault-resilient DNN partitioning framework using NSGA-II optimization to enhance accuracy in error-prone environments, achieving up to 27.7% improved fault tolerance.


<details>
  <summary>Details</summary>
Motivation: The motivation is addressing the reliability issues in partitioned DNN models under hardware faults and communication errors, particularly for safety-critical applications.

Method: The paper introduces an NSGA-II-based optimization framework that considers accuracy degradation due to faults as a core metric, performs runtime fault injection, and uses a feedback loop for fault-tolerant partitioning.

Result: The framework achieves up to 27.7% improvement in fault tolerance on benchmarks like AlexNet, SqueezeNet, and ResNet18, with minimal performance overhead.

Conclusion: The study highlights the importance of resilience in DNN partitioning for robust AI inference in error-prone distributed environments.

Abstract: Deep Neural Networks (DNNs) are increasingly deployed across distributed and resource-constrained platforms, such as System-on-Chip (SoC) accelerators and edge-cloud systems. DNNs are often partitioned and executed across heterogeneous processing units to optimize latency and energy. However, the reliability of these partitioned models under hardware faults and communication errors remains a critical yet underexplored topic, especially in safety-critical applications. In this paper, we propose an accuracy-aware, fault-resilient DNN partitioning framework targeting multi-objective optimization using NSGA-II, where accuracy degradation under fault conditions is introduced as a core metric alongside energy and latency. Our framework performs runtime fault injection during optimization and utilizes a feedback loop to prioritize fault-tolerant partitioning. We evaluate our approach on benchmark CNNs including AlexNet, SqueezeNet and ResNet18 on hardware accelerators, and demonstrate up to 27.7% improvement in fault tolerance with minimal increase in performance overhead. Our results highlight the importance of incorporating resilience into DNN partitioning, and thereby paving the way for robust AI inference in error-prone environments.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [518] [Nice to Meet You: Synthesizing Practical MLIR Abstract Transformers](https://arxiv.org/abs/2512.06442)
*Xuanyu Peng,Dominic Kennedy,Yuyou Fan,Ben Greenman,John Regehr,Loris D'Antoni*

Main category: cs.PL

TL;DR: This paper introduces NiceToMeetYou, a program synthesis framework to automate the creation of abstract transformers that are sound, precise, and efficient for compilers like LLVM.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges and limitations in creating sound, precise, and efficient abstract transformers for static analysis in compilers, as evidenced by issues like unsoundness and the absence of transformers for many LLVM instructions.

Method: The authors propose a synthesis framework, NiceToMeetYou, that breaks the problem into parts by meeting simpler sound transformers. It allows bulk automation without requiring human-prepared sketches, verified through an SMT dialect of MLIR.

Result: NiceToMeetYou successfully synthesized abstract transformers, with 17% being more precise than those in LLVM, alongside ensuring all are provably sound.

Conclusion: The framework effectively improves the development of abstract transformers, addressing prior limitations and providing automated, precise, and sound solutions for production-level compilers.

Abstract: Static analyses play a fundamental role during compilation: they discover facts that are true in all executions of the code being compiled, and then these facts are used to justify optimizations and diagnostics. Each static analysis is based on a collection of abstract transformers that provide abstract semantics for the concrete instructions that make up a program. It can be challenging to implement abstract transformers that are sound, precise, and efficient, and in fact both LLVM and GCC have suffered from miscompilations caused by unsound abstract transformers. Moreover, even after more than 20 years of development, LLVM lacks abstract transformers for hundreds of instructions in its intermediate representation (IR). We developed NiceToMeetYou, a program synthesis framework for abstract transformers that are aimed at the kinds of non-relational integer abstract domains that are heavily used by today's production compilers. It exploits a simple but novel technique for breaking the synthesis problem into parts: each of our transformers is the meet of a collection of simpler, sound transformers that are synthesized such that each new piece fills a gap in the precision of the final transformer. Our design point is bulk automation: no sketches are required. Transformers are verified by lowering to a previously created SMT dialect of MLIR. Each of our synthesized transformers is provably sound and some (17 percent) are more precise than those provided by LLVM.

</details>


### [519] [PIP: Making Andersen's Points-to Analysis Sound and Practical for Incomplete C Programs](https://arxiv.org/abs/2512.07299)
*Håvard Rognebakke Krogstie,Helge Bahmann,Magnus Själander,Nico Reissmann*

Main category: cs.PL

TL;DR: The paper introduces a fast and sound Andersen-style points-to analysis for incomplete C programs, improving performance and precision while being scalable and practical for compilers.


<details>
  <summary>Details</summary>
Motivation: Existing points-to analyses guarantee sound solutions only for complete programs and are limited by the lack of summary functions in production compilers. A solution is needed for analyzing incomplete programs with efficiency and soundness.

Method: The technique introduces an implicit constraint graph for tracking accessible memory locations and pointers, eliminating the need for explicit pointee tracking. It also employs the Prefer Implicit Pointees (PIP) technique for further performance optimization.

Result: The new analysis achieves a 15x faster constraint solver compared to state-of-the-art techniques with explicit pointee tracking, an additional 1.9x speedup using PIP, reduces MayAlias-responses by 40% compared to LLVM's BasicAA, and demonstrates memory scalability.

Conclusion: This analysis efficiently and soundly addresses incomplete C programs. Its speed, precision, and scalability make it highly practical and usable in real-world optimizing compilers.

Abstract: Compiling files individually lends itself well to parallelization, but forces the compiler to operate on incomplete programs. State-of-the-art points-to analyses guarantee sound solutions only for complete programs, requiring summary functions to describe any missing program parts. Summary functions are rarely available in production compilers, however, where soundness and efficiency are non-negotiable. This paper presents an Andersen-style points-to analysis that efficiently produces sound solutions for incomplete C programs. The analysis accomplishes soundness by tracking memory locations and pointers that are accessible from external modules, and efficiency by performing this tracking implicitly in the constraint graph. We show that implicit pointee tracking makes the constraint solver 15$\times$ faster than any combination of five different state-of-the-art techniques using explicit pointee tracking. We also present the Prefer Implicit Pointees (PIP) technique that further reduces the use of explicit pointees. PIP gives an additional speedup of 1.9$\times$, compared to the fastest solver configuration not benefiting from PIP. The precision of the analysis is evaluated in terms of an alias-analysis client, where it reduces the number of MayAlias-responses by 40% compared to LLVM's BasicAA pass alone. Finally, we show that the analysis is scalable in terms of memory, making it suitable for optimizing compilers in practice.

</details>


### [520] [Canonical bidirectional typechecking](https://arxiv.org/abs/2512.07511)
*Zanzi Mihejevs,Jules Hedges*

Main category: cs.PL

TL;DR: This paper explores the link between bidirectional typechecking, polarised System L, and linear-nonlinear style systems, presenting a unified theoretical framework.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a unified perspective on bidirectional typechecking by linking it with dualities in polarised System L and extending it to both cartesian and linear-nonlinear contexts.

Method: The authors analyze the checkable and synthesisable split in bidirectional typechecking, drawing connections to polarised System L (polarised $\mu\tilde\mu$-calculus) and extending this to cartesian contexts. They employ McBride's co-de Bruijn formulation of scopes to combine these perspectives.

Result: The paper identifies that positive/negative terms and coterms align with checkable/synthesisable distinctions, establishing a three-way correlation between polarised System L, LNL calculi, and bidirectional calculi.

Conclusion: This work unites concepts from polarised System L, LNL calculi, and bidirectional typechecking under a single framework, enhancing understanding of their theoretical and practical relationships.

Abstract: We demonstrate that the checkable/synthesisable split in bidirectional typechecking coincides with existing dualities in polarised System L, also known as polarised $μ\tildeμ$-calculus. Specifically, positive terms and negative coterms are checkable, and negative terms and positive coterms are synthesisable. This combines a standard formulation of bidirectional typechecking with Zeilberger's `cocontextual' variant. We extend this to ordinary `cartesian' System L using Mc Bride's co-de Bruijn formulation of scopes, and show that both can be combined in a linear-nonlinear style, where linear types are positive and cartesian types are negative. This yields a remarkable 3-way coincidence between the shifts of polarised System L, LNL calculi, and bidirectional calculi.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [521] [POrTAL: Plan-Orchestrated Tree Assembly for Lookahead](https://arxiv.org/abs/2512.06002)
*Evan Conway,David Porfirio,David Chan,Mark Roberts,Laura M. Hiatt*

Main category: cs.RO

TL;DR: The paper introduces a new probabilistic planning algorithm, POrTAL, designed for robots dealing with partial observability and computational resource constraints, outperforming existing methods in efficiency and steps required.


<details>
  <summary>Details</summary>
Motivation: Robots in human-robot interaction often operate in partially observable environments, necessitating efficient probabilistic planning methods. Existing algorithms are often resource-intensive or inefficient in achieving goals.

Method: The authors developed POrTAL, a lightweight algorithm combining features from FF-Replan and POMCP. It implements an innovative probabilistic planning approach for robots.

Result: POrTAL demonstrated superior performance in minimizing the number of execution steps compared to baseline algorithms (FF-Replan and POMCP) across various case studies.

Conclusion: POrTAL offers an efficient probabilistic planning solution for robots under partial observability and varying temporal constraints, showcasing its practical applicability and effectiveness.

Abstract: Assigning tasks to robots often involves supplying the robot with an overarching goal, such as through natural language, and then relying on the robot to uncover and execute a plan to achieve that goal. In many settings common to human-robot interaction, however, the world is only partially observable to the robot, requiring that it create plans under uncertainty. Although many probabilistic planning algorithms exist for this purpose, these algorithms can be inefficient if executed with the robot's limited computational resources, or may require more steps than expected to achieve the goal. We thereby created a new, lightweight, probabilistic planning algorithm, Plan-Orchestrated Tree Assembly for Lookahead (POrTAL), that combines the strengths of two baseline planning algorithms, FF-Replan and POMCP. In a series of case studies, we demonstrate POrTAL's ability to quickly arrive at solutions that outperform these baselines in terms of number of steps. We additionally demonstrate how POrTAL performs under varying temporal constraints.

</details>


### [522] [Training-Free Robot Pose Estimation using Off-the-Shelf Foundational Models](https://arxiv.org/abs/2512.06017)
*Laurence Liang*

Main category: cs.RO

TL;DR: This paper explores the use of vision-language models (VLMs) to estimate robot arm joint angles from a single image and establishes a performance baseline.


<details>
  <summary>Details</summary>
Motivation: The need for reliable joint angle estimation of robot arms to enhance safety, performance, and policy training due to their expanding industrial and residential use.

Method: The paper leverages frontier vision-language models to extract joint angles from visual inputs, testing on both synthetic and real-world image data.

Result: The research establishes a baseline for performance using current VLMs and shows that scaling methods alone do not improve accuracy.

Conclusion: Current VLMs can serve as a baseline but enhancements are needed for better joint angle predictions.

Abstract: Pose estimation of a robot arm from visual inputs is a challenging task. However, with the increasing adoption of robot arms for both industrial and residential use cases, reliable joint angle estimation can offer improved safety and performance guarantees, and also be used as a verifier to further train robot policies. This paper introduces using frontier vision-language models (VLMs) as an ``off-the-shelf" tool to estimate a robot arm's joint angles from a single target image. By evaluating frontier VLMs on both synthetic and real-world image-data pairs, this paper establishes a performance baseline attained by current FLMs. In addition, this paper presents empirical results suggesting that test time scaling or parameter scaling alone does not lead to improved joint angle predictions.

</details>


### [523] [Closed-Loop Robotic Manipulation of Transparent Substrates for Self-Driving Laboratories using Deep Learning Micro-Error Correction](https://arxiv.org/abs/2512.06038)
*Kelsey Fontenot,Anjali Gorti,Iva Goel,Tonio Buonassisi,Alexander E. Siemenn*

Main category: cs.RO

TL;DR: The paper introduces Automated Substrate Handling and Exchange (ASHE), a robotic and deep learning method to improve accuracy and reliability in handling substrates for self-driving laboratories.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked challenge of reliably handling and reloading substrates in self-driving laboratories, which is crucial for accurate downstream characterization and automation.

Method: ASHE combines robotics, dual-actuated dispensers, and deep learning-driven computer vision in a closed-loop system to detect and correct errors during substrate manipulation.

Result: ASHE achieved 98.5% first-time placement accuracy in 130 trials, with misplacements detected and successfully corrected, ensuring reliable substrate handling in SDLs.

Conclusion: The method enhances automation in self-driving labs, contributing to increased efficiency and reliability in material and chemical discoveries.

Abstract: Self-driving laboratories (SDLs) have accelerated the throughput and automation capabilities for discovering and improving chemistries and materials. Although these SDLs have automated many of the steps required to conduct chemical and materials experiments, a commonly overlooked step in the automation pipeline is the handling and reloading of substrates used to transfer or deposit materials onto for downstream characterization. Here, we develop a closed-loop method of Automated Substrate Handling and Exchange (ASHE) using robotics, dual-actuated dispensers, and deep learning-driven computer vision to detect and correct errors in the manipulation of fragile and transparent substrates for SDLs. Using ASHE, we demonstrate a 98.5% first-time placement accuracy across 130 independent trials of reloading transparent glass substrates into an SDL, where only two substrate misplacements occurred and were successfully detected as errors and automatically corrected. Through the development of more accurate and reliable methods for handling various types of substrates, we move toward an improvement in the automation capabilities of self-driving laboratories, furthering the acceleration of novel chemical and materials discoveries.

</details>


### [524] [WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving](https://arxiv.org/abs/2512.06112)
*Yifang Xu,Jiahao Cui,Feipeng Cai,Zhihao Zhu,Hanlin Shang,Shan Luan,Mingwang Xu,Neng Zhang,Yaoyi Li,Jia Cai,Siyu Zhu*

Main category: cs.RO

TL;DR: The paper introduces WAM-Flow, a VLA model for ego-trajectory planning using discrete flow matching, achieving superior autonomous driving performance on NAVSIM benchmarks.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving systems need an effective and efficient way to plan ego-trajectory while addressing challenges such as accuracy, safety, and closed-loop competency.

Method: WAM-Flow leverages discrete flow matching in a parallel, denoising manner. It uses numerical tokenizers, a geometry-aware flow objective, and a simulator-guided alignment to optimize trajectory planning.

Result: The model outperforms autoregressive and diffusion-based baselines, achieving high PDMS scores with minimal inference steps on NAVSIM benchmarks.

Conclusion: Discrete flow matching demonstrates a promising new paradigm for end-to-end autonomous driving systems, offering high performance and efficiency.

Abstract: We introduce WAM-Flow, a vision-language-action (VLA) model that casts ego-trajectory planning as discrete flow matching over a structured token space. In contrast to autoregressive decoders, WAM-Flow performs fully parallel, bidirectional denoising, enabling coarse-to-fine refinement with a tunable compute-accuracy trade-off. Specifically, the approach combines a metric-aligned numerical tokenizer that preserves scalar geometry via triplet-margin learning, a geometry-aware flow objective and a simulator-guided GRPO alignment that integrates safety, ego progress, and comfort rewards while retaining parallel generation. A multi-stage adaptation converts a pre-trained auto-regressive backbone (Janus-1.5B) from causal decoding to non-causal flow model and strengthens road-scene competence through continued multimodal pretraining. Thanks to the inherent nature of consistency model training and parallel decoding inference, WAM-Flow achieves superior closed-loop performance against autoregressive and diffusion-based VLA baselines, with 1-step inference attaining 89.1 PDMS and 5-step inference reaching 90.3 PDMS on NAVSIM v1 benchmark. These results establish discrete flow matching as a new promising paradigm for end-to-end autonomous driving. The code will be publicly available soon.

</details>


### [525] [Probabilistic Weapon Engagement Zones for a Turn Constrained Pursuer](https://arxiv.org/abs/2512.06130)
*Grant Stagg,Isaac E. Weintraub,Cameron K. Peterson*

Main category: cs.RO

TL;DR: The paper introduces probabilistic engagement zones to help evaders avoid capture under uncertain parameters from pursuers following curve-straight paths, and evaluates methods for integrating these zones into trajectory optimization.


<details>
  <summary>Details</summary>
Motivation: To help evaders minimize capture risk by understanding spatial regions to avoid when pursuers have uncertain movement parameters.

Method: The authors used a deterministic engagement zone formulation, extended it probabilistically using Monte Carlo sampling, linearization, quadratic approximation, and neural network regression, and integrated the zones into trajectory-optimization algorithms.

Result: The methods were assessed for accuracy and computational cost, showing how probabilistic zones improve trajectory optimization under uncertainty.

Conclusion: Probabilistic engagement zones enhance evader trajectory planning by accounting for pursuer uncertainty, improving safety in dynamic environments.

Abstract: Curve-straight probabilistic engagement zones (CSPEZ) quantify the spatial regions an evader should avoid to reduce capture risk from a turn-rate-limited pursuer following a curve-straight path with uncertain parameters including position, heading, velocity, range, and maximum turn rate. This paper presents methods for generating evader trajectories that minimize capture risk under such uncertainty. We first derive an analytic solution for the deterministic curve-straight basic engagement zone (CSBEZ), then extend this formulation to a probabilistic framework using four uncertainty-propagation approaches: Monte Carlo sampling, linearization, quadratic approximation, and neural-network regression. We evaluate the accuracy and computational cost of each approximation method and demonstrate how CSPEZ constraints can be integrated into a trajectory-optimization algorithm to produce safe paths that explicitly account for pursuer uncertainty.

</details>


### [526] [GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers](https://arxiv.org/abs/2512.06147)
*Hochul Hwang,Soowan Yang,Jahir Sadik Monon,Nicholas A Giudice,Sunghoon Ivan Lee,Joydeep Biswas,Donghyun Kim*

Main category: cs.RO

TL;DR: This paper introduces 'GuideNav,' a vision-based navigation system inspired by guide dog behaviors that enables robots to autonomously follow human-demonstrated routes, validated through human studies and field tests.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the scarcity of direct references for designing robot navigation systems tailored to blind and low-vision individuals, leveraging insights from human studies with guide dog handlers and trainers.

Method: The researchers conducted extensive interviews, observational studies, and field tests to inform the development of 'GuideNav,' designed as a vision-only teach-and-repeat robotic navigation system without reliance on expensive sensors.

Result: GuideNav demonstrated kilometer-scale route-following reliability in varied outdoor environments and feasibility comparable to guide dogs, as validated in user studies with guide dog handlers and trainers.

Conclusion: GuideNav successfully bridges the gap in assistive robot navigation design for BLV individuals, offering a cost-efficient and reliable vision-based system inspired by guide dog navigation behavior.

Abstract: While commendable progress has been made in user-centric research on mobile assistive systems for blind and low-vision (BLV) individuals, references that directly inform robot navigation design remain rare. To bridge this gap, we conducted a comprehensive human study involving interviews with 26 guide dog handlers, four white cane users, nine guide dog trainers, and one O\&M trainer, along with 15+ hours of observing guide dog-assisted walking. After de-identification, we open-sourced the dataset to promote human-centered development and informed decision-making for assistive systems for BLV people. Building on insights from this formative study, we developed GuideNav, a vision-only, teach-and-repeat navigation system. Inspired by how guide dogs are trained and assist their handlers, GuideNav autonomously repeats a path demonstrated by a sighted person using a robot. Specifically, the system constructs a topological representation of the taught route, integrates visual place recognition with temporal filtering, and employs a relative pose estimator to compute navigation actions - all without relying on costly, heavy, power-hungry sensors such as LiDAR. In field tests, GuideNav consistently achieved kilometer-scale route following across five outdoor environments, maintaining reliability despite noticeable scene variations between teach and repeat runs. A user study with 3 guide dog handlers and 1 guide dog trainer further confirmed the system's feasibility, marking (to our knowledge) the first demonstration of a quadruped mobile system retrieving a path in a manner comparable to guide dogs.

</details>


### [527] [Real-Time Spatiotemporal Tubes for Dynamic Unsafe Sets](https://arxiv.org/abs/2512.06151)
*Ratnangshu Das,Siddhartha Upadhyay,Pushpak Jagtap*

Main category: cs.RO

TL;DR: This paper develops a control framework for nonlinear systems to achieve reach-avoid-stay tasks in dynamic environments using a spatiotemporal tube (STT) strategy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of controlling nonlinear systems with unknown dynamics in achieving complex tasks involving safety, time constraints, and adaptability in real-time.

Method: Introduced the real-time spatiotemporal tube (STT) control framework, where an adaptive time-varying tube ensures safety and task adherence. A closed-form control law is derived using real-time sensory input without approximations.

Result: The proposed method guarantees obstacle avoidance and on-time task execution. Effectiveness and scalability are validated through simulations and experiments on mobile robots and aerial vehicles in dynamic environments.

Conclusion: The framework ensures safety and task satisfaction for nonlinear systems in dynamic settings, demonstrating robust real-time adaptability and scalability.

Abstract: This paper presents a real-time control framework for nonlinear pure-feedback systems with unknown dynamics to satisfy reach-avoid-stay tasks within a prescribed time in dynamic environments. To achieve this, we introduce a real-time spatiotemporal tube (STT) framework. An STT is defined as a time-varying ball in the state space whose center and radius adapt online using only real-time sensory input. A closed-form, approximation-free control law is then derived to constrain the system output within the STT, ensuring safety and task satisfaction. We provide formal guarantees for obstacle avoidance and on-time task completion. The effectiveness and scalability of the framework are demonstrated through simulations and hardware experiments on a mobile robot and an aerial vehicle, navigating in cluttered dynamic environments.

</details>


### [528] [Situation-Aware Interactive MPC Switching for Autonomous Driving](https://arxiv.org/abs/2512.06182)
*Shuhao Qi,Qiling Aori,Luyao Zhang,Mircea Lazar,Sofie Haesaert*

Main category: cs.RO

TL;DR: The paper presents a method for autonomous driving in traffic that involves switching between predictive control models based on situation demands to balance performance and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Interactive traffic scenarios can impose challenges in autonomous driving. High-fidelity models can improve decision-making but are computationally costly, prompting investigation into balancing performance with efficiency.

Method: The authors compare various model predictive control (MPC) formulations, develop a hierarchy of interaction capabilities, and create a neural network classifier to switch between different MPC models based on traffic situations.

Result: The proposed approach improves overall performance in critical traffic scenarios and reduces computational load during routine situations.

Conclusion: Situation-aware switching between MPC models is effective in enhancing intelligent behavior while maintaining efficiency in autonomous driving systems.

Abstract: To enable autonomous driving in interactive traffic scenarios, various model predictive control (MPC) formulations have been proposed, each employing different interaction models. While higher-fidelity models enable more intelligent behavior, they incur increased computational cost. Since strong interactions are relatively infrequent in traffic, a practical strategy for balancing performance and computational overhead is to invoke an appropriate controller based on situational demands. To achieve this approach, we first conduct a comparative study to assess and hierarchize the interactive capabilities of different MPC formulations. Furthermore, we develop a neural network-based classifier to enable situation-aware switching among controllers with different levels of interactive capability. We demonstrate that this situation-aware switching can both substantially improve overall performance by activating the most advanced interactive MPC in rare but critical situations, and significantly reduce computational load by using a basic MPC in the majority of scenarios.

</details>


### [529] [REWW-ARM -- Remote Wire-Driven Mobile Robot: Design, Control, and Experimental Validation](https://arxiv.org/abs/2512.06192)
*Takahiro Hattori,Kento Kawaharazuka,Temma Suzuki,Keita Yoneda,Kei Okada*

Main category: cs.RO

TL;DR: This paper introduces a new system called "Remote Wire Drive" for robots, which enhances usability in challenging environments by excluding electronics from the distal robot parts.


<details>
  <summary>Details</summary>
Motivation: Current electronic devices in robots limit their operational range due to environmental challenges. Hence, there is a need for a system offering advanced electronic actuation and control, while eliminating electronics from sensitive parts of the robot.

Method: The authors proposed and developed "Remote Wire Drive," showcased through a proof-of-concept named REWW-ARM, featuring a novel power transmission mechanism, an electronics-free robot, and a motor unit for control.

Result: The REWW-ARM was tested for locomotion, posture control, and object manipulation, demonstrating strong mechanical and control capabilities on land and underwater.

Conclusion: The Remote Wire-Driven system broadens the operational range of robots, highlighting its potential across various applications.

Abstract: Electronic devices are essential for robots but limit their usable environments. To overcome this, methods excluding electronics from the operating environment while retaining advanced electronic control and actuation have been explored. These include the remote hydraulic drive of electronics-free mobile robots, which offer high reachability, and long wire-driven robot arms with motors consolidated at the base, which offer high environmental resistance. To combine the advantages of both, this study proposes a new system, "Remote Wire Drive." As a proof-of-concept, we designed and developed the Remote Wire-Driven robot "REWW-ARM", which consists of the following components: 1) a novel power transmission mechanism, the "Remote Wire Transmission Mechanism" (RWTM), the key technology of the Remote Wire Drive; 2) an electronics-free distal mobile robot driven by it; and 3) a motor-unit that generates power and provides electronic closed-loop control based on state estimation via the RWTM. In this study, we evaluated the mechanical and control performance of REWW-ARM through several experiments, demonstrating its capability for locomotion, posture control, and object manipulation both on land and underwater. This suggests the potential for applying the Remote Wire-Driven system to various types of robots, thereby expanding their operational range.

</details>


### [530] [Cascaded Tightly-Coupled Observer Design for Single-Range-Aided Inertial Navigation](https://arxiv.org/abs/2512.06198)
*Oussama Sifour,Soulaimane Berkane,Abdelhamid Tayebi*

Main category: cs.RO

TL;DR: This study develops a navigation observer using IMU, a vector measurement, and a distance measurement to reconstruct a rigid body's state, demonstrating stability and accuracy.


<details>
  <summary>Details</summary>
Motivation: To create an efficient state reconstruction approach using minimal sensors for autonomous navigation.

Method: Formulates an extended LTV system for state estimation and designs a cascaded observer. Establishes AGAS under uniform observability conditions.

Result: Accurate estimations of position, velocity, and orientation through simulation studies, showcasing robustness and lightweight functionality.

Conclusion: Single-range aiding provides a reliable and efficient solution for autonomous navigation system design.

Abstract: This work introduces a single-range-aided navigation observer that reconstructs the full state of a rigid body using only an Inertial Measurement Unit (IMU), a body-frame vector measurement (e.g., magnetometer), and a distance measurement from a fixed anchor point. The design first formulates an extended linear time-varying (LTV) system to estimate body-frame position, body-frame velocity, and the gravity direction. The recovered gravity direction, combined with the body-frame vector measurement, is then used to reconstruct the full orientation on $\mathrm{SO}(3)$, resulting in a cascaded observer architecture. Almost Global Asymptotic Stability (AGAS) of the cascaded design is established under a uniform observability condition, ensuring robustness to sensor noise and trajectory variations. Simulation studies on three-dimensional trajectories demonstrate accurate estimation of position, velocity, and orientation, highlighting single-range aiding as a lightweight and effective modality for autonomous navigation.

</details>


### [531] [Where to Fly, What to Send: Communication-Aware Aerial Support for Ground Robots](https://arxiv.org/abs/2512.06207)
*Harshil Suthar,Dipankar Maity*

Main category: cs.RO

TL;DR: The paper develops a framework for a multi-robot team operating in limited communication, emphasizing efficient aerial-to-ground data sharing to aid navigation with a VoI-based approach.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of efficient communication and navigation in a multi-robot system operating in bandwidth-constrained environments.

Method: The proposed framework uses Value-of-Information (VoI) for data prioritization, Mixed-Integer Linear Programming (MILP) for transmission optimization, and utility-based strategies for environment exploration.

Result: The analysis provides insights into the trade-off between communication data volume from the aerial agent and the navigation costs faced by ground agents.

Conclusion: The framework facilitates strategic data sharing in a constrained setting, balancing exploration, communication, and goal achievement.

Abstract: In this work we consider a multi-robot team operating in an unknown environment where one aerial agent is tasked to map the environment and transmit (a portion of) the mapped environment to a group of ground agents that are trying to reach their goals. The entire operation takes place over a bandwidth-limited communication channel, which motivates the problem of determining what and how much information the assisting agent should transmit and when while simultaneously performing exploration/mapping. The proposed framework enables the assisting aerial agent to decide what information to transmit based on the Value-of-Information (VoI), how much to transmit using a Mixed-Integer Linear Programming (MILP), and how to acquire additional information through an utility score-based environment exploration strategy. We perform a communication-motion trade-off analysis between the total amount of map data communicated by the aerial agent and the navigation cost incurred by the ground agents.

</details>


### [532] [Safe Model Predictive Diffusion with Shielding](https://arxiv.org/abs/2512.06261)
*Taekyung Kim,Keyvan Majd,Hideki Okamoto,Bardh Hoxha,Dimitra Panagou,Georgios Fainekos*

Main category: cs.RO

TL;DR: This paper introduces Safe Model Predictive Diffusion (Safe MPD), a new diffusion-based trajectory planner for robotics that ensures safety, feasibility, and optimality without requiring post-processing.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of generating trajectories for complex robotic systems that are simultaneously safe, optimal, and feasible while avoiding intractable computational trade-offs.

Method: Safe MPD combines a training-free diffusion framework with a safety shield to enforce safety and feasibility throughout the denoising process, eliminating the need for post-processing corrections.

Result: Safe MPD was tested on non-convex planning problems, demonstrating higher success rates, better safety performance, and faster computation times compared to existing safety strategies.

Conclusion: Safe MPD successfully unifies diffusion-based planning and safety guarantees, offering an efficient and robust solution for robotic trajectory generation in complex environments.

Abstract: Generating safe, kinodynamically feasible, and optimal trajectories for complex robotic systems is a central challenge in robotics. This paper presents Safe Model Predictive Diffusion (Safe MPD), a training-free diffusion planner that unifies a model-based diffusion framework with a safety shield to generate trajectories that are both kinodynamically feasible and safe by construction. By enforcing feasibility and safety on all samples during the denoising process, our method avoids the common pitfalls of post-processing corrections, such as computational intractability and loss of feasibility. We validate our approach on challenging non-convex planning problems, including kinematic and acceleration-controlled tractor-trailer systems. The results show that it substantially outperforms existing safety strategies in success rate and safety, while achieving sub-second computation times.

</details>


### [533] [Leveraging Port-Hamiltonian Theory for Impedance Control Benchmarking](https://arxiv.org/abs/2512.06423)
*Leonardo F. Dos Santos,Elisa G. Vergamini,Cícero Zanette,Lucca Maitan,Thiago Boaventura*

Main category: cs.RO

TL;DR: The paper proposes metrics based on the Port-Hamiltonian (PH) framework for benchmarking impedance control.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create standardized and effective methods for assessing impedance control performance, addressing the need for consistent benchmarking tools in robotics.

Method: The research introduces a causality-consistent PH model for mass-spring-damper systems in Cartesian space and derives a passivity condition independent of force-torque sensing. Additionally, an impedance fidelity metric is defined based on power analysis in free motion.

Result: The proposed metrics were tested through Gazebo simulations with a six-DoF manipulator and a quadruped leg, validating their effectiveness in benchmarking.

Conclusion: The study concludes the PH framework is a robust and reliable approach for standardized impedance control benchmarking in multi-DoF robotic systems.

Abstract: This work proposes PH-based metrics for benchmarking impedance control. A causality-consistent PH model is introduced for mass-spring-damper impedance in Cartesian space. Based on this model, a differentiable, force-torque sensing-independent, n-DoF passivity condition is derived, valid for time-varying references. An impedance fidelity metric is also defined from step-response power in free motion, capturing dynamic decoupling. The proposed metrics are validated in Gazebo simulations with a six-DoF manipulator and a quadruped leg. Results demonstrate the suitability of the PH framework for standardized impedance control benchmarking.

</details>


### [534] [Fault Tolerant Control of Mecanum Wheeled Mobile Robots](https://arxiv.org/abs/2512.06444)
*Xuehui Ma,Shiliang Zhang,Zhiyong Sun*

Main category: cs.RO

TL;DR: The study develops a fault-tolerant control (FTC) strategy for Mecanum wheeled mobile robots (MWMRs) that can handle both complete and partial actuator faults, enhancing performance under various conditions.


<details>
  <summary>Details</summary>
Motivation: MWMRs are prone to actuator faults that diminish performance and risk mission failure. Current FTC approaches often ignore partial faults, necessitating a strategy that ensures robust control under both complete and partial fault scenarios.

Method: The proposed strategy uses posterior probability to estimate real-time fault parameters. It combines probability-weighted control laws for various predefined faults, adapting to different fault levels dynamically.

Result: Simulation results verify that the FTC strategy is effective in maintaining MWMR control performance in diverse fault conditions.

Conclusion: The presented FTC approach enhances the robustness and operational safety of MWMRs by accommodating varying levels of actuator faults.

Abstract: Mecanum wheeled mobile robots (MWMRs) are highly susceptible to actuator faults that degrade performance and risk mission failure. Current fault tolerant control (FTC) schemes for MWMRs target complete actuator failures like motor stall, ignoring partial faults e.g., in torque degradation. We propose an FTC strategy handling both fault types, where we adopt posterior probability to learn real-time fault parameters. We derive the FTC law by aggregating probability-weighed control laws corresponding to predefined faults. This ensures the robustness and safety of MWMR control despite varying levels of fault occurrence. Simulation results demonstrate the effectiveness of our FTC under diverse scenarios.

</details>


### [535] [Entropy-Controlled Intrinsic Motivation Reinforcement Learning for Quadruped Robot Locomotion in Complex Terrains](https://arxiv.org/abs/2512.06486)
*Wanru Gong,Xinyi Zheng,Xiaopeng Yang,Xiaoqing Zhu*

Main category: cs.RO

TL;DR: Entropy-Controlled Intrinsic Motivation (ECIM) improves quadrupedal locomotion across diverse terrains compared to traditional PPO-based reinforcement learning algorithms by addressing premature convergence issues.


<details>
  <summary>Details</summary>
Motivation: Premature convergence in PPO-based reinforcement learning often leads to suboptimal quadrupedal locomotion performance across diverse terrains.

Method: The paper proposes ECIM, an entropy-based reinforcement learning algorithm integrating intrinsic motivation with adaptive exploration to counter premature convergence.

Result: Experiments show ECIM increases task rewards by 4–12%, reduces peak body pitch oscillation by 23–29%, decreases joint acceleration by 20–32%, and lowers joint torque consumption by 11–20%.

Conclusion: ECIM provides more stable and energy-efficient locomotion for quadrupedal robots across terrains, making it superior to PPO-based methods for complex robotic control tasks.

Abstract: Learning is the basis of both biological and artificial systems when it comes to mimicking intelligent behaviors. From the classical PPO (Proximal Policy Optimization), there is a series of deep reinforcement learning algorithms which are widely used in training locomotion policies for quadrupedal robots because of their stability and sample efficiency. However, among all these variants, experiments and simulations often converge prematurely, leading to suboptimal locomotion and reduced task performance. Therefore, in this paper, we introduce Entropy-Controlled Intrinsic Motivation (ECIM), an entropy-based reinforcement learning algorithm in contrast with the PPO series, that can reduce premature convergence by combining intrinsic motivation with adaptive exploration.
  For experiments, in order to parallel with other baselines, we chose to apply it in Isaac Gym across six terrain categories: upward slopes, downward slopes, uneven rough terrain, ascending stairs, descending stairs, and flat ground as widely used. For comparison, our experiments consistently achieve better performance: task rewards increase by 4--12%, peak body pitch oscillation is reduced by 23--29%, joint acceleration decreases by 20--32%, and joint torque consumption declines by 11--20%. Overall, our model ECIM, by combining entropy control and intrinsic motivation control, achieves better results in stability across different terrains for quadrupedal locomotion, and at the same time reduces energetic cost and makes it a practical choice for complex robotic control tasks.

</details>


### [536] [Vision-Guided Grasp Planning for Prosthetic Hands in Unstructured Environments](https://arxiv.org/abs/2512.06517)
*Shifa Sulaiman,Akash Bachhar,Ming Shen,Simon Bøgh*

Main category: cs.RO

TL;DR: The paper presents a vision-guided grasping algorithm for prosthetic hands, integrating advanced perception, planning, and control techniques for object-specific dexterity.


<details>
  <summary>Details</summary>
Motivation: To improve prosthetic hand interaction in dynamic environments by utilizing vision-based intelligent control systems.

Method: A modular pipeline involving a BVH-based vision algorithm for object segmentation, Rapidly-exploring Random Tree Star for grasp planning, and a DLS-based inverse kinematics solver for precise finger joint movements.

Result: The algorithm successfully demonstrated adaptive and object-specific grasping capabilities through simulation and on the Linker Hand O7 platform.

Conclusion: The approach enhances dexterity and autonomy in prosthetic hands through real-time adaptable grasping in unstructured environments.

Abstract: Recent advancements in prosthetic technology have increasingly focused on enhancing dexterity and autonomy through intelligent control systems. Vision-based approaches offer promising results for enabling prosthetic hands to interact more naturally with diverse objects in dynamic environments. Building on this foundation, the paper presents a vision-guided grasping algorithm for a prosthetic hand, integrating perception, planning, and control for dexterous manipulation. A camera mounted on the set up captures the scene, and a Bounding Volume Hierarchy (BVH)-based vision algorithm is employed to segment an object for grasping and define its bounding box. Grasp contact points are then computed by generating candidate trajectories using Rapidly-exploring Random Tree Star algorithm, and selecting fingertip end poses based on the minimum Euclidean distance between these trajectories and the objects point cloud. Each finger grasp pose is determined independently, enabling adaptive, object-specific configurations. Damped Least Square (DLS) based Inverse kinematics solver is used to compute the corresponding joint angles, which are subsequently transmitted to the finger actuators for execution. This modular pipeline enables per-finger grasp planning and supports real-time adaptability in unstructured environments. The proposed method is validated in simulation, and experimental integration on a Linker Hand O7 platform.

</details>


### [537] [TacFinRay: Soft Tactile Fin-Ray Finger with Indirect Tactile Sensing for Robust Grasping](https://arxiv.org/abs/2512.06524)
*Saekwang Nam,Bowen Deng,Loong Yi Lee,Jonathan M. Rossiter,Nathan F. Lepora*

Main category: cs.RO

TL;DR: The paper introduces a new tactile-sensorized Fin-Ray finger that indirectly senses contact location and depth with high accuracy, using a vision-based internal camera system and a convolutional neural network.


<details>
  <summary>Details</summary>
Motivation: To enable accurate and robust tactile sensing for soft robotic structures, especially in scenarios where direct sensing at the contact interface isn't practical or feasible.

Method: The finger integrates a soft Fin-Ray structure with a rigid sensing module via a hinge mechanism, transferring information to an array of marker-tipped pins. Contact data is processed with a convolutional neural network based on deformation patterns captured by a camera.

Result: Achieved sensing accuracies of 0.1 mm for depth and 2 mm for location. Demonstrated generalization to objects of various shapes and sizes. Successfully applied in a pick-and-place task to enhance placement accuracy under uncertain conditions.

Conclusion: The presented sensor is lightweight, flexible, and adaptable for tactile sensing in soft robotics, offering precise, indirect sensing capabilities away from the contact interface.

Abstract: We present a tactile-sensorized Fin-Ray finger that enables simultaneous detection of contact location and indentation depth through an indirect sensing approach. A hinge mechanism is integrated between the soft Fin-Ray structure and a rigid sensing module, allowing deformation and translation information to be transferred to a bottom crossbeam upon which are an array of marker-tipped pins based on the biomimetic structure of the TacTip vision-based tactile sensor. Deformation patterns captured by an internal camera are processed using a convolutional neural network to infer contact conditions without directly sensing the finger surface. The finger design was optimized by varying pin configurations and hinge orientations, achieving 0.1\,mm depth and 2mm location-sensing accuracies. The perception demonstrated robust generalization to various indenter shapes and sizes, which was applied to a pick-and-place task under uncertain picking positions, where the tactile feedback significantly improved placement accuracy. Overall, this work provides a lightweight, flexible, and scalable tactile sensing solution suitable for soft robotic structures where the sensing needs situating away from the contact interface.

</details>


### [538] [Embodied Referring Expression Comprehension in Human-Robot Interaction](https://arxiv.org/abs/2512.06558)
*Md Mofijul Islam,Alexi Gladstone,Sujan Sarker,Ganesh Nanduru,Md Fahim,Keyan Du,Aman Chadha,Tariq Iqbal*

Main category: cs.RO

TL;DR: The paper introduces a new dataset, Refer360, addressing limitations in existing robotic interaction datasets, and proposes MuRes, a multimodal module to enhance robots' comprehension of human instructions.


<details>
  <summary>Details</summary>
Motivation: Robots need to understand human gestures and instructions intuitively for effective human-robot interaction, but existing datasets are inadequate due to biases and limited diversity.

Method: The authors developed Refer360, a dataset covering diverse environments and interactions, and MuRes, a module that extracts and reinforces salient signals for multimodal comprehension.

Result: Experiments showed that multimodal models augmented with MuRes performed better on HRI tasks, including Refer360, highlighting its robustness.

Conclusion: Refer360 serves as a valuable benchmark for embodied interaction study, while MuRes demonstrates potential in enhancing robot comprehension in varied human-centric environments.

Abstract: As robots enter human workspaces, there is a crucial need for them to comprehend embodied human instructions, enabling intuitive and fluent human-robot interaction (HRI). However, accurate comprehension is challenging due to a lack of large-scale datasets that capture natural embodied interactions in diverse HRI settings. Existing datasets suffer from perspective bias, single-view collection, inadequate coverage of nonverbal gestures, and a predominant focus on indoor environments. To address these issues, we present the Refer360 dataset, a large-scale dataset of embodied verbal and nonverbal interactions collected across diverse viewpoints in both indoor and outdoor settings. Additionally, we introduce MuRes, a multimodal guided residual module designed to improve embodied referring expression comprehension. MuRes acts as an information bottleneck, extracting salient modality-specific signals and reinforcing them into pre-trained representations to form complementary features for downstream tasks. We conduct extensive experiments on four HRI datasets, including the Refer360 dataset, and demonstrate that current multimodal models fail to capture embodied interactions comprehensively; however, augmenting them with MuRes consistently improves performance. These findings establish Refer360 as a valuable benchmark and exhibit the potential of guided residual learning to advance embodied referring expression comprehension in robots operating within human environments.

</details>


### [539] [Learning Agile Striker Skills for Humanoid Soccer Robots from Noisy Sensory Input](https://arxiv.org/abs/2512.06571)
*Zifan Xu,Myoungkyu Seo,Dongmyeong Lee,Hao Fu,Jiaheng Hu,Jiaxun Cui,Yuqian Jiang,Zhihan Wang,Anastasiia Brund,Joydeep Biswas,Peter Stone*

Main category: cs.RO

TL;DR: This paper develops a reinforcement learning-based system for humanoid robots to perform robust and adaptive ball-kicking under perceptual noise and uncertainty.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenging problem of enabling humanoid soccer robots to kick balls robustly and adaptively amidst sensory noise, external perturbations, and the demand for coordinated whole-body movements.

Method: The authors utilize a teacher-student reinforcement learning framework alongside a four-stage training process, involving tailored reward functions, realistic noise modeling, and online constrained RL for adaptation.

Result: Experimental results indicate strong performance in terms of kicking accuracy and goal-scoring across diverse configurations on both simulated and real humanoid robots.

Conclusion: The paper establishes a system capable of robust and continual ball-kicking for humanoid robots, underuncertain perception, setting a benchmark for visuomotor skill learning in whole-body humanoid control scenarios.

Abstract: Learning fast and robust ball-kicking skills is a critical capability for humanoid soccer robots, yet it remains a challenging problem due to the need for rapid leg swings, postural stability on a single support foot, and robustness under noisy sensory input and external perturbations (e.g., opponents). This paper presents a reinforcement learning (RL)-based system that enables humanoid robots to execute robust continual ball-kicking with adaptability to different ball-goal configurations. The system extends a typical teacher-student training framework -- in which a "teacher" policy is trained with ground truth state information and the "student" learns to mimic it with noisy, imperfect sensing -- by including four training stages: (1) long-distance ball chasing (teacher); (2) directional kicking (teacher); (3) teacher policy distillation (student); and (4) student adaptation and refinement (student). Key design elements -- including tailored reward functions, realistic noise modeling, and online constrained RL for adaptation and refinement -- are critical for closing the sim-to-real gap and sustaining performance under perceptual uncertainty. Extensive evaluations in both simulation and on a real robot demonstrate strong kicking accuracy and goal-scoring success across diverse ball-goal configurations. Ablation studies further highlight the necessity of the constrained RL, noise modeling, and the adaptation stage. This work presents a system for learning robust continual humanoid ball-kicking under imperfect perception, establishing a benchmark task for visuomotor skill learning in humanoid whole-body control.

</details>


### [540] [Error-Centric PID Untrained Neural-Net (EC-PIDUNN) For Nonlinear Robotics Control](https://arxiv.org/abs/2512.06578)
*Waleed Razzaq*

Main category: cs.RO

TL;DR: The paper introduces EC-PIDUNN, a control architecture combining an untrained neural network with an improved PID controller to tackle nonlinear system challenges efficiently.


<details>
  <summary>Details</summary>
Motivation: Classical PID struggles with nonlinear dynamics and complex systems, causing instability and inefficiency. Existing neural network-enhanced PID models have high computational costs and require intensive data, limiting their practicality.

Method: The proposed EC-PIDUNN integrates an untrained neural network with an enhanced PID controller and stabilizing factors. It bypasses explicit system dynamics, increasing input dimensionality and dynamically adjusting parameters.

Result: EC-PIDUNN outperforms classical PID controllers in stability and convergence, demonstrated in nonlinear applications like unmanned ground vehicles and Pan-Tilt movement systems.

Conclusion: EC-PIDUNN efficiently handles nonlinear systems with improved performance over traditional PID, providing a practical solution for real-world applications.

Abstract: Classical Proportional-Integral-Derivative (PID) control has been widely successful across various industrial systems such as chemical processes, robotics, and power systems. However, as these systems evolved, the increase in the nonlinear dynamics and the complexity of interconnected variables have posed challenges that classical PID cannot effectively handle, often leading to instability, overshooting, or prolonged settling times. Researchers have proposed PIDNN models that combine the function approximation capabilities of neural networks with PID control to tackle these nonlinear challenges. However, these models require extensive, highly refined training data and have significant computational costs, making them less favorable for real-world applications. In this paper, We propose a novel EC-PIDUNN architecture, which integrates an untrained neural network with an improved PID controller, incorporating a stabilizing factor (\(τ\)) to generate the control signal. Like classical PID, our architecture uses the steady-state error \(e_t\) as input bypassing the need for explicit knowledge of the systems dynamics. By forming an input vector from \(e_t\) within the neural network, we increase the dimensionality of input allowing for richer data representation. Additionally, we introduce a vector of parameters \( ρ_t \) to shape the output trajectory and a \textit{dynamic compute} function to adjust the PID coefficients from predefined values. We validate the effectiveness of EC-PIDUNN on multiple nonlinear robotics applications: (1) nonlinear unmanned ground vehicle systems that represent the Ackermann steering mechanism and kinematics control, (2) Pan-Tilt movement system. In both tests, it outperforms classical PID in convergence and stability achieving a nearly critically damped response.

</details>


### [541] [A New Trajectory-Oriented Approach to Enhancing Comprehensive Crowd Navigation Performance](https://arxiv.org/abs/2512.06608)
*Xinyu Zhou,Songhao Piao,Chao Gao,Liguo Chen*

Main category: cs.RO

TL;DR: The paper focuses on improving crowd navigation through deep reinforcement learning (DRL) by addressing gaps in existing metrics, especially regarding trajectory optimization.


<details>
  <summary>Details</summary>
Motivation: Existing research in DRL-based crowd navigation often lacks adequate evaluation of relative priorities among metrics and neglects trajectory-continuity measures like $C^2$ smoothness, leading to suboptimal navigation methods.

Method: The paper introduces a unified evaluation framework to prioritize and jointly assess multiple optimization objectives, along with a novel reward-shaping strategy focusing on trajectory-curvature optimization.

Result: The proposed approach improves trajectory quality and adaptability in various scenarios, outperforming state-of-the-art methods in 2D and 3D experiments.

Conclusion: The paper presents a more comprehensive and effective method for crowd navigation, highlighting the importance of trajectory optimization in achieving natural, comfortable, and energy-efficient navigation systems.

Abstract: Crowd navigation has garnered considerable research interest in recent years, especially with the proliferating application of deep reinforcement learning (DRL) techniques. Many studies, however, do not sufficiently analyze the relative priorities among evaluation metrics, which compromises the fair assessment of methods with divergent objectives. Furthermore, trajectory-continuity metrics, specifically those requiring $C^2$ smoothness, are rarely incorporated. Current DRL approaches generally prioritize efficiency and proximal comfort, often neglecting trajectory optimization or addressing it only through simplistic, unvalidated smoothness reward. Nevertheless, effective trajectory optimization is essential to ensure naturalness, enhance comfort, and maximize the energy efficiency of any navigation system. To address these gaps, this paper proposes a unified framework that enables the fair and transparent assessment of navigation methods by examining the prioritization and joint evaluation of multiple optimization objectives. We further propose a novel reward-shaping strategy that explicitly emphasizes trajectory-curvature optimization. The resulting trajectory quality and adaptability are significantly enhanced across multi-scale scenarios. Through extensive 2D and 3D experiments, we demonstrate that the proposed method achieves superior performance compared to state-of-the-art approaches.

</details>


### [542] [Robust Optimization-based Autonomous Dynamic Soaring with a Fixed-Wing UAV](https://arxiv.org/abs/2512.06610)
*Marvin Harms,Jaeyoung Lim,David Rohr,Friedrich Rockenbauer,Nicholas Lawrance,Roland Siegwart*

Main category: cs.RO

TL;DR: This paper introduces a framework for autonomous dynamic soaring using fixed-wing UAVs, leveraging wind energy for potentially unlimited flight.


<details>
  <summary>Details</summary>
Motivation: To enable fixed-wing UAVs to achieve autonomous dynamic soaring by exploiting wind shear for energy-efficient, sustainable flight.

Method: The framework incorporates explicit wind field representation, robust reference paths, and a robust path-following controller, validated through simulations and real-world tests.

Result: Simulations and tests demonstrate robust dynamic soaring despite varied wind conditions, estimation errors, and disturbances, validating energy predictions and path-following capabilities.

Conclusion: The proposed framework successfully enables autonomous dynamic soaring of UAVs, ensuring robustness and bridging simulation-to-reality gaps.

Abstract: Dynamic soaring is a flying technique to exploit the energy available in wind shear layers, enabling potentially unlimited flight without the need for internal energy sources. We propose a framework for autonomous dynamic soaring with a fixed-wing unmanned aerial vehicle (UAV). The framework makes use of an explicit representation of the wind field and a classical approach for guidance and control of the UAV. Robustness to wind field estimation error is achieved by constructing point-wise robust reference paths for dynamic soaring and the development of a robust path following controller for the fixed-wing UAV. The framework is evaluated in dynamic soaring scenarios in simulation and real flight tests. In simulation, we demonstrate robust dynamic soaring flight subject to varied wind conditions, estimation errors and disturbances. Critical components of the framework, including energy predictions and path-following robustness, are further validated in real flights to assure small sim-to-real gap. Together, our results strongly indicate the ability of the proposed framework to achieve autonomous dynamic soaring flight in wind shear.

</details>


### [543] [MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment](https://arxiv.org/abs/2512.06628)
*Ruicheng Zhang,Mingyang Zhang,Jun Zhou,Zhangrui Guo,Xiaofan Liu,Zunnan Xu,Zhizhou Zhong,Puxin Yan,Haocheng Luo,Xiu Li*

Main category: cs.RO

TL;DR: MIND-V is a framework that synthesizes videos of long-horizon robotic manipulation with logical and physical accuracy, addressing data limitations in embodied imitation learning.


<details>
  <summary>Details</summary>
Motivation: The scarcity of diverse, long-horizon robotic manipulation data limits the progress of embodied imitation learning, and existing models fall short in synthesizing complex, extended actions.

Method: MIND-V combines a vision-language based Semantic Reasoning Hub (SRH) for task planning, a Behavioral Semantic Bridge (BSB) for representation transition, and a Motor Video Generator (MVG) for video rendering. It uses a Staged Visual Future Rollouts strategy and a GRPO RL phase with a Physical Foresight Coherence (PFC) reward for enhancing physical plausibility.

Result: MIND-V achieves state-of-the-art performance in generating long-horizon robotic manipulation videos that are physically plausible and logically coherent.

Conclusion: The proposed framework establishes a scalable, effective approach for generating embodied data, bridging the gap between human-like reasoning and robotic video synthesis.

Abstract: Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.

</details>


### [544] [Statistic-Augmented, Decoupled MoE Routing and Aggregating in Autonomous Driving](https://arxiv.org/abs/2512.06664)
*Wei-Bin Kou,Guangxu Zhu,Jingreng Lei,Chen Zhang,Yik-Chung Wu,Jianping Wang*

Main category: cs.RO

TL;DR: The paper proposes MoE-RAM, a new mechanism to enhance autonomous driving predictions by improving routing and aggregation in Mixture of Experts (MoE).


<details>
  <summary>Details</summary>
Motivation: Autonomous driving faces challenges due to diverse and dynamic scenarios, which cannot be effectively handled by a single deep learning model. The authors aim to address these challenges by improving the efficiency and accuracy of MoE systems.

Method: The authors proposed MoE-RAM, a mechanism involving a statistic-augmented routing strategy to improve expert selection and adaptive reweighting of experts' outputs to optimize aggregation.

Result: Extensive experiments on autonomous driving datasets show MoE-RAM outperforms existing MoE baselines and traditional single-model methods in tasks like semantic segmentation.

Conclusion: The integration of the statistic-augmented routing and aggregating mechanism in MoE-RAM demonstrates superior performance in adapting to diverse autonomous driving conditions, suggesting its potential for broader adoption.

Abstract: Autonomous driving (AD) scenarios are inherently complex and diverse, posing significant challenges for a single deep learning model to effectively cover all possible conditions, such as varying weather, traffic densities, and road types. Large Model (LM)-Driven Mixture of Experts (MoE) paradigm offers a promising solution, where LM serves as the backbone to extract latent features while MoE serves as the downstream head to dynamically select and aggregate specialized experts to adapt to different scenarios. However, routing and aggregating in MoE face intrinsic challenges, including imprecise expert selection due to flawed routing strategy and inefficient expert aggregation leading to suboptimal prediction. To address these issues, we propose a statistic-augmented, decoupled MoE }outing and Aggregating Mechanism (MoE-RAM) driven by LM. Specifically, on the one hand, MoE-RAM enhances expert routing by incorporating statistical retrieval mechanism to match LM-extracted latent features with cached prototypical features of the most relevant experts; on the other hand, MoE-RAM adaptively reweights experts' outputs in fusion by measuring statistical distances of experts' instant features against LM-extracted latent features. Benefiting from the synergy of the statistic-augmented MoE's routing and aggregating, MoE-RAM ultimately improves the prediction performance. We take the AD semantic segmentation task as an example to assess the proposed MoE-RAM. Extensive experiments on AD datasets demonstrate the superiority of MoE-RAM compared to other MoE baselines and conventional single-model approaches.

</details>


### [545] [FedDSR: Federated Deep Supervision and Regularization Towards Autonomous Driving](https://arxiv.org/abs/2512.06676)
*Wei-Bin Kou,Guangxu Zhu,Bingyang Cheng,Chen Zhang,Yik-Chung Wu,Jianping Wang*

Main category: cs.RO

TL;DR: FL faces challenges in autonomous driving due to non-IID data across vehicles. FedDSR introduces intermediate layer supervision and regularization to improve generalization, convergence, and training efficiency.


<details>
  <summary>Details</summary>
Motivation: To address poor generalization and slow convergence in Federated Learning (FL) caused by non-IID data in autonomous driving (AD).

Method: FedDSR uses intermediate layer supervision and regularization by selecting intermediate layers, computing mutual information (MI) and negative entropy (NE) as loss and regularizer, and aggregating models based on these criteria.

Result: The approach improves model generalization and speeds up convergence, achieving up to 8.93% better mIoU and reducing training rounds by 28.57% compared to FL baselines.

Conclusion: FedDSR is a promising strategy to enhance FL for AD systems, making it more practical and effective for real-world deployment.

Abstract: Federated Learning (FL) enables collaborative training of autonomous driving (AD) models across distributed vehicles while preserving data privacy. However, FL encounters critical challenges such as poor generalization and slow convergence due to non-independent and identically distributed (non-IID) data from diverse driving environments. To overcome these obstacles, we introduce Federated Deep Supervision and Regularization (FedDSR), a paradigm that incorporates multi-access intermediate layer supervision and regularization within federated AD system. Specifically, FedDSR comprises following integral strategies: (I) to select multiple intermediate layers based on predefined architecture-agnostic standards. (II) to compute mutual information (MI) and negative entropy (NE) on those selected layers to serve as intermediate loss and regularizer. These terms are integrated into the output-layer loss to form a unified optimization objective, enabling comprehensive optimization across the network hierarchy. (III) to aggregate models from vehicles trained based on aforementioned rules of (I) and (II) to generate the global model on central server. By guiding and penalizing the learning of feature representations at intermediate stages, FedDSR enhances the model generalization and accelerates model convergence for federated AD. We then take the semantic segmentation task as an example to assess FedDSR and apply FedDSR to multiple model architectures and FL algorithms. Extensive experiments demonstrate that FedDSR achieves up to 8.93% improvement in mIoU and 28.57% reduction in training rounds, compared to other FL baselines, making it highly suitable for practical deployment in federated AD ecosystems.

</details>


### [546] [Model-Less Feedback Control of Space-based Continuum Manipulators using Backbone Tension Optimization](https://arxiv.org/abs/2512.06754)
*Shrreya Rajneesh,Nikita Pavle,Rakesh Kumar Sahoo,Manoranjan Sinha*

Main category: cs.RO

TL;DR: The paper introduces a model-less control framework for continuum manipulators to bypass limitations of traditional kinematic modeling, achieving precise and stable control in constrained environments.


<details>
  <summary>Details</summary>
Motivation: Continuum manipulators face challenges stemming from infinite-dimensional backbone deformation, unmodeled internal friction, and configuration-dependent stiffness, affecting kinematic reliability.

Method: A model-less framework using an empirically initialized Jacobian refined online through differential convex updates is proposed. Real-time quadratic programming optimizes the tip motion and incorporates tendon slack avoidance and geometric limits.

Result: The framework demonstrated smooth convergence, stable tension evolution, and sub-millimeter steady-state accuracy across varied trajectory shapes.

Conclusion: The model-less controller provides a scalable, reliable alternative for continuum manipulators in constrained environments without requiring model calibration or parameter identification.

Abstract: Continuum manipulators offer intrinsic dexterity and safe geometric compliance for navigation within confined and obstacle-rich environments. However, their infinite-dimensional backbone deformation, unmodeled internal friction, and configuration-dependent stiffness fundamentally limit the reliability of model-based kinematic formulations, resulting in inaccurate Jacobian predictions, artificial singularities, and unstable actuation behavior. Motivated by these limitations, this work presents a complete model-less control framework that bypasses kinematic modeling by using an empirically initialized Jacobian refined online through differential convex updates. Tip motion is generated via a real-time quadratic program that computes actuator increments while enforcing tendon slack avoidance and geometric limits. A backbone tension optimization term is introduced in this paper to regulate axial loading and suppress co-activation compression. The framework is validated across circular, pentagonal, and square trajectories, demonstrating smooth convergence, stable tension evolution, and sub-millimeter steady-state accuracy without any model calibration or parameter identification. These results establish the proposed controller as a scalable alternative to model-dependent continuum manipulation in a constrained environment.

</details>


### [547] [db-LaCAM: Fast and Scalable Multi-Robot Kinodynamic Motion Planning with Discontinuity-Bounded Search and Lightweight MAPF](https://arxiv.org/abs/2512.06796)
*Akmaral Moldagalieva,Keisuke Okumura,Amanda Prorok,Wolfgang Hönig*

Main category: cs.RO

TL;DR: The paper introduces db-LaCAM, a scalable and fast multi-robot motion planner, achieving up to 10x faster planning for up to 50 robots compared to existing methods, while maintaining solution quality across various robot dynamics.


<details>
  <summary>Details</summary>
Motivation: Addressing the scalability and computational limitations of existing multi-robot kinodynamic motion planners which struggle with large teams of robots.

Method: The db-LaCAM planner uses precomputed motion primitives respecting robot dynamics to create horizon-length motion sequences, allowing user-defined discontinuities while maintaining resolution-completeness for arbitrary robot dynamics.

Result: Experiments demonstrate db-LaCAM's scalability, achieving up to 10x faster runtimes for 50 robots and maintaining comparable solution quality in dynamic 2D and 3D environments.

Conclusion: db-LaCAM effectively combines the strengths of MAPF algorithms and kinodynamic planning, offering a scalable and dynamic-aware solution validated on both simulations and physical robot experiments.

Abstract: State-of-the-art multi-robot kinodynamic motion planners struggle to handle more than a few robots due to high computational burden, which limits their scalability and results in slow planning time.
  In this work, we combine the scalability and speed of modern multi-agent path finding (MAPF) algorithms with the dynamic-awareness of kinodynamic planners to address these limitations.
  To this end, we propose discontinuity-Bounded LaCAM (db-LaCAM), a planner that utilizes a precomputed set of motion primitives that respect robot dynamics to generate horizon-length motion sequences, while allowing a user-defined discontinuity between successive motions.
  The planner db-LaCAM is resolution-complete with respect to motion primitives and supports arbitrary robot dynamics.
  Extensive experiments demonstrate that db-LaCAM scales efficiently to scenarios with up to 50 robots, achieving up to ten times faster runtime compared to state-of-the-art planners, while maintaining comparable solution quality.
  The approach is validated in both 2D and 3D environments with dynamics such as the unicycle and 3D double integrator.
  We demonstrate the safe execution of trajectories planned with db-LaCAM in two distinct physical experiments involving teams of flying robots and car-with-trailer robots.

</details>


### [548] [MagicSkin: Balancing Marker and Markerless Modes in Vision-Based Tactile Sensors with a Translucent Skin](https://arxiv.org/abs/2512.06829)
*Oluwatimilehin Tijani,Zhuo Chen,Jiankang Deng,Shan Luo*

Main category: cs.RO

TL;DR: MagicSkin introduces translucent, tinted markers for Vision-Based Tactile Sensors, balancing the trade-off between marker and markerless tactile skins, enabling improved object classification, texture classification, tangential displacement tracking, and force prediction.


<details>
  <summary>Details</summary>
Motivation: To resolve the trade-off in VBTS between marker designs that obstruct surface details and markerless designs that poorly track tangential displacement, without introducing hardware complexity or software burdens.

Method: MagicSkin employs translucent, tinted markers on tactile skins to balance the advantages of marker and markerless designs, allowing seamless integration into GelSight-family sensors without extra equipment or processing tools.

Result: MagicSkin achieved remarkable performance in object classification (99.17%), texture classification (93.51%), tangential displacement tracking (97% point retention), and force prediction (66% improvement in total force error) compared to traditional designs.

Conclusion: MagicSkin eliminates the trade-off between marker and markerless skins, demonstrating superior multimodal tactile sensing capabilities essential for advancements in tactile robotics.

Abstract: Vision-based tactile sensors (VBTS) face a fundamental trade-off in marker and markerless design on the tactile skin: opaque ink markers enable measurement of force and tangential displacement but completely occlude geometric features necessary for object and texture classification, while markerless skin preserves surface details but struggles in measuring tangential displacements effectively. Current practice to solve the above problem via UV lighting or virtual transfer using learning-based models introduces hardware complexity or computing burdens. This paper introduces MagicSkin, a novel tactile skin with translucent, tinted markers balancing the modes of marker and markerless for VBTS. It enables simultaneous tangential displacement tracking, force prediction, and surface detail preservation. This skin is easy to plug into GelSight-family sensors without requiring additional hardware or software tools. We comprehensively evaluate MagicSkin in downstream tasks. The translucent markers impressively enhance rather than degrade sensing performance compared with traditional markerless and inked marker design: it achieves best performance in object classification (99.17\%), texture classification (93.51\%), tangential displacement tracking (97\% point retention) and force prediction (66\% improvement in total force error). These experimental results demonstrate that translucent skin eliminates the traditional performance trade-off in marker or markerless modes, paving the way for multimodal tactile sensing essential in tactile robotics. See videos at this \href{https://zhuochenn.github.io/MagicSkin_project/}{link}.

</details>


### [549] [Dynamic Visual SLAM using a General 3D Prior](https://arxiv.org/abs/2512.06868)
*Xingguang Zhong,Liren Jin,Marija Popović,Jens Behley,Cyrill Stachniss*

Main category: cs.RO

TL;DR: The paper introduces a monocular visual SLAM system to estimate camera poses robustly in dynamic environments, combining geometric patch-based adjustment with feed-forward models.


<details>
  <summary>Details</summary>
Motivation: Dynamic environments affect camera pose estimation due to scene changes; solving this issue would enhance applications like robotics and AR.

Method: The system combines feed-forward reconstruction for filtering dynamic regions and depth prediction, using patch-based bundle adjustment to address scale ambiguities.

Result: Improved robustness in camera pose estimation in dynamic environments.

Conclusion: Combining patch-based online methods with feed-forward reconstruction offers a practical solution for SLAM in dynamic environments.

Abstract: Reliable incremental estimation of camera poses and 3D reconstruction is key to enable various applications including robotics, interactive visualization, and augmented reality. However, this task is particularly challenging in dynamic natural environments, where scene dynamics can severely deteriorate camera pose estimation accuracy. In this work, we propose a novel monocular visual SLAM system that can robustly estimate camera poses in dynamic scenes. To this end, we leverage the complementary strengths of geometric patch-based online bundle adjustment and recent feed-forward reconstruction models. Specifically, we propose a feed-forward reconstruction model to precisely filter out dynamic regions, while also utilizing its depth prediction to enhance the robustness of the patch-based visual SLAM. By aligning depth prediction with estimated patches from bundle adjustment, we robustly handle the inherent scale ambiguities of the batch-wise application of the feed-forward reconstruction model.

</details>


### [550] [From Zero to High-Speed Racing: An Autonomous Racing Stack](https://arxiv.org/abs/2512.06892)
*Hassan Jardali,Durgakant Pushp,Youwei Yu,Mahmoud Ali,Ihab S. Mohamed,Alejandro Murillo-Gonzalez,Paul D. Coen,Md. Al-Masrur Khan,Reddy Charan Pulivendula,Saeoul Park,Lingchuan Zhou,Lantao Liu*

Main category: cs.RO

TL;DR: This paper introduces the Autonomous Race Stack (ARS) for autonomous racing, analyzing its development, performance, and providing a high-speed dataset.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in autonomous racing such as precise localization, rapid perception, dynamic planning, and real-time control in high-speed environments.

Method: Developed and evolved the modular Autonomous Race Stack (ARS) across three iterations (ARS1, ARS2, and ARS3), tested on multiple tracks, and contrasted performance metrics across environments.

Result: Validated ARS on different tracks, achieving speeds up to 260 km/h, and highlighted insights from high-speed autonomous racing.

Conclusion: The ARS demonstrates advancements in autonomous racing, addressing critical technical challenges and contributing a high-speed, multi-sensor dataset for research.

Abstract: High-speed, head-to-head autonomous racing presents substantial technical and logistical challenges, including precise localization, rapid perception, dynamic planning, and real-time control-compounded by limited track access and costly hardware. This paper introduces the Autonomous Race Stack (ARS), developed by the IU Luddy Autonomous Racing team for the Indy Autonomous Challenge (IAC). We present three iterations of our ARS, each validated on different tracks and achieving speeds up to 260 km/h. Our contributions include: (i) the modular architecture and evolution of the ARS across ARS1, ARS2, and ARS3; (ii) a detailed performance evaluation that contrasts control, perception, and estimation across oval and road-course environments; and (iii) the release of a high-speed, multi-sensor dataset collected from oval and road-course tracks. Our findings highlight the unique challenges and insights from real-world high-speed full-scale autonomous racing.

</details>


### [551] [Control of Powered Ankle-Foot Prostheses on Compliant Terrain: A Quantitative Approach to Stability Enhancement](https://arxiv.org/abs/2512.06896)
*Chrysostomos Karakasis,Camryn Scully,Robert Salati,Panagiotis Artemiadis*

Main category: cs.RO

TL;DR: The paper introduces a control mechanism for powered prostheses to enhance stability on soft terrains, reducing fall risks in amputees.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of walking on compliant terrains for individuals with lower-limb amputations, which increases fall risks.

Method: An admittance-based control strategy was experimentally validated with human subjects, adjusting prosthesis stiffness dynamically to optimize gait stability on compliant surfaces.

Result: Admittance controller demonstrated improved gait stability across all tested compliant conditions compared to traditional controllers.

Conclusion: Adaptive prosthesis control has potential to mitigate fall risks and improve interaction between humans and rehabilitation robotics in real-world scenarios.

Abstract: Walking on compliant terrain presents a substantial challenge for individuals with lower-limb amputation, further elevating their already high risk of falling. While powered ankle-foot prostheses have demonstrated adaptability across speeds and rigid terrains, control strategies optimized for soft or compliant surfaces remain underexplored. This work experimentally validates an admittance-based control strategy that dynamically adjusts the quasi-stiffness of powered prostheses to enhance gait stability on compliant ground. Human subject experiments were conducted with three healthy individuals walking on two bilaterally compliant surfaces with ground stiffness values of 63 and 25 kN/m, representative of real-world soft environments. Controller performance was quantified using phase portraits and two walking stability metrics, offering a direct assessment of fall risk. Compared to a standard phase-variable controller developed for rigid terrain, the proposed admittance controller consistently improved gait stability across all compliant conditions. These results demonstrate the potential of adaptive, stability-aware prosthesis control to reduce fall risk in real-world environments and advance the robustness of human-prosthesis interaction in rehabilitation robotics.

</details>


### [552] [VP-AutoTest: A Virtual-Physical Fusion Autonomous Driving Testing Platform](https://arxiv.org/abs/2512.07507)
*Yiming Cui,Shiyu Fang,Jiarui Zhang,Yan Huang,Chengkai Xu,Bing Zhu,Hao Zhang,Peng Hang,Jian Sun*

Main category: cs.RO

TL;DR: The paper introduces VP-AutoTest, a Virtual-Physical Testing Platform for autonomous vehicles, addressing the limitations of traditional and existing fusion testing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional testing methods for autonomous vehicles face issues like unrealistic conditions, limited scope, and high costs. Virtual-physical fusion testing has the potential to solve these, but also has its own limitations.

Method: VP-AutoTest integrates diverse virtual and physical elements, supports multi-vehicle cooperation testing, uses adversarial testing, and includes AI-driven assessment tools. It ensures seamless V2X cooperation and conducts fidelity self-evaluation through real-world experiment comparison.

Result: VP-AutoTest extends testing capabilities, accelerates fault detection, and achieves high reliability and comprehensive testing coverage.

Conclusion: This platform addresses key challenges in autonomous vehicle testing while enhancing reliability and efficiency. It provides a versatile solution for evaluating real-world traffic scenarios.

Abstract: The rapid development of autonomous vehicles has led to a surge in testing demand. Traditional testing methods, such as virtual simulation, closed-course, and public road testing, face several challenges, including unrealistic vehicle states, limited testing capabilities, and high costs. These issues have prompted increasing interest in virtual-physical fusion testing. However, despite its potential, virtual-physical fusion testing still faces challenges, such as limited element types, narrow testing scope, and fixed evaluation metrics. To address these challenges, we propose the Virtual-Physical Testing Platform for Autonomous Vehicles (VP-AutoTest), which integrates over ten types of virtual and physical elements, including vehicles, pedestrians, and roadside infrastructure, to replicate the diversity of real-world traffic participants. The platform also supports both single-vehicle interaction and multi-vehicle cooperation testing, employing adversarial testing and parallel deduction to accelerate fault detection and explore algorithmic limits, while OBU and Redis communication enable seamless vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) cooperation across all levels of cooperative automation. Furthermore, VP-AutoTest incorporates a multidimensional evaluation framework and AI-driven expert systems to conduct comprehensive performance assessment and defect diagnosis. Finally, by comparing virtual-physical fusion test results with real-world experiments, the platform performs credibility self-evaluation to ensure both the fidelity and efficiency of autonomous driving testing. Please refer to the website for the full testing functionalities on the autonomous driving public service platform OnSite:https://www.onsite.com.cn.

</details>


### [553] [Ground Compliance Improves Retention of Visual Feedback-Based Propulsion Training for Gait Rehabilitation](https://arxiv.org/abs/2512.06897)
*Bradley Hobbs,Panagiotis Artemiadis*

Main category: cs.RO

TL;DR: The study investigates combining ground compliance with visual feedback for gait training, showing improved push-off forces and lasting propulsion strategies.


<details>
  <summary>Details</summary>
Motivation: To explore if adding ground compliance to visual feedback in gait training enhances push-off force rehabilitation, targeting propulsion deficits post-stroke.

Method: Ten participants walked on a split-belt treadmill with real-time visual ground reaction force feedback. Ground compliance was added for one group, while the control group only used visual feedback.

Result: The ground compliance group showed better push-off force improvements, sustained propulsion strategies, and muscle and joint kinematic adaptations post-training compared to the control group.

Conclusion: Using compliant terrain in visual feedback gait training improves propulsion outcomes, making it a promising rehabilitation approach for individuals with propulsion deficits.

Abstract: This study investigates whether adding ground compliance to visual feedback (VF) gait training is more effective at increasing push-off force (POF) compared to using VF alone, with implications for gait rehabilitation. Ten healthy participants walked on a custom split-belt treadmill. All participants received real-time visual feedback of their ground reaction forces. One group also experienced changes in ground compliance, while a control group received only visual feedback. Intentional increases in propulsive ground reaction forces (POF) were successfully achieved and sustained post-intervention, especially in the group that experienced ground compliance. This group also demonstrated lasting after-effects in muscle activity and joint kinematics, indicating a more robust learning of natural strategies to increase propulsion. This work demonstrates how visual and proprioceptive systems coordinate during gait adaptation. It uniquely shows that combining ground compliance with visual feedback enhances the learning of propulsive forces, supporting the potential use of compliant terrain in long-term rehabilitation targeting propulsion deficits, such as those following a stroke.

</details>


### [554] [Energy-Efficient Navigation for Surface Vehicles in Vortical Flow Fields](https://arxiv.org/abs/2512.06912)
*Rushiraj Gadhvi,Sandeep Manjanna*

Main category: cs.RO

TL;DR: The paper introduces a learning-based approach for energy-efficient navigation of autonomous surface vehicles in vortical flow fields, achieving 30-50% energy savings compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of energy-efficient navigation for Autonomous Surface Vehicles during long missions under strict energy constraints, inspired by khalasi's ocean current navigation expertise.

Method: An end-to-end reinforcement learning framework based on Soft Actor-Critic is developed to learn flow-aware navigation using only local velocity measurements.

Result: The method shows significant energy savings (30-50%) and robust adaptability across diverse and unpredictable flow scenarios.

Conclusion: This approach enhances the autonomous navigation of surface vehicles by improving energy efficiency and adaptability, paving the way for long-term operations in ocean environments.

Abstract: For centuries, khalasi have skillfully harnessed ocean currents to navigate vast waters with minimal effort. Emulating this intuition in autonomous systems remains a significant challenge, particularly for Autonomous Surface Vehicles tasked with long duration missions under strict energy budgets. In this work, we present a learning-based approach for energy-efficient surface vehicle navigation in vortical flow fields, where partial observability often undermines traditional path-planning methods. We present an end to end reinforcement learning framework based on Soft Actor Critic that learns flow-aware navigation policies using only local velocity measurements. Through extensive evaluation across diverse and dynamically rich scenarios, our method demonstrates substantial energy savings and robust generalization to previously unseen flow conditions, offering a promising path toward long term autonomy in ocean environments. The navigation paths generated by our proposed approach show an improvement in energy conservation 30 to 50 percent compared to the existing state of the art techniques.

</details>


### [555] [Interconnection and Damping Assignment Passivity-Based Control using Sparse Neural ODEs](https://arxiv.org/abs/2512.06935)
*Nicolò Botteghi,Owen Brook,Urban Fasel,Federico Califano*

Main category: cs.RO

TL;DR: The paper proposes a novel numerical approach to overcome limitations in IDA-PBC implementation, using neural ordinary differential equations and sparse dictionary learning.


<details>
  <summary>Details</summary>
Motivation: IDA-PBC is limited in practical applications due to the complexity of solving matching partial differential equations for enforcing port-Hamiltonian structures in closed-loop systems.

Method: The authors propose an innovative method by reformulating IDA-PBC design as learning of neural ODEs, coupled with sparse dictionary learning and multi-objective optimization.

Result: This approach expands the applicability of IDA-PBC to complex tasks like discovering oscillatory behaviors while enabling derivation of closed-form system expressions.

Conclusion: The numerical method presented effectively addresses IDA-PBC limitations, paving the way for broader applications beyond stabilization tasks.

Abstract: Interconnection and Damping Assignment Passivity-Based Control (IDA-PBC) is a nonlinear control technique that assigns a port-Hamiltonian (pH) structure to a controlled system using a state-feedback law. While IDA-PBC has been extensively studied and applied to many systems, its practical implementation often remains confined to academic examples and, almost exclusively, to stabilization tasks. The main limitation of IDA-PBC stems from the complexity of analytically solving a set of partial differential equations (PDEs), referred to as the matching conditions, which enforce the pH structure of the closed-loop system. However, this is extremely challenging, especially for complex physical systems and tasks.
  In this work, we propose a novel numerical approach for designing IDA-PBC controllers without solving the matching PDEs exactly. We cast the IDA-PBC problem as the learning of a neural ordinary differential equation. In particular, we rely on sparse dictionary learning to parametrize the desired closed-loop system as a sparse linear combination of nonlinear state-dependent functions. Optimization of the controller parameters is achieved by solving a multi-objective optimization problem whose cost function is composed of a generic task-dependent cost and a matching condition-dependent cost. Our numerical results show that the proposed method enables (i) IDA-PBC to be applicable to complex tasks beyond stabilization, such as the discovery of periodic oscillatory behaviors, (ii) the derivation of closed-form expressions of the controlled system, including residual terms

</details>


### [556] [Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge](https://arxiv.org/abs/2512.06951)
*Ilia Larchenko,Gleb Zarin,Akash Karnatak*

Main category: cs.RO

TL;DR: The authors introduce a vision-action policy that outperformed competitors in the 2025 BEHAVIOR Challenge, addressing complex household tasks.


<details>
  <summary>Details</summary>
Motivation: To develop efficient and effective solutions for diverse and long-horizon household tasks in simulation, aiming for improved bimanual manipulation, navigation, and decision-making.

Method: Building on the Pi0.5 architecture, the authors implemented innovations such as correlated noise for flow matching, learnable mixed-layer attention, System 2 stage tracking, multi-sample flow matching for training, and inference techniques like action compression.

Result: Their approach achieved a 26% q-score across all 50 tasks, dominating both public and private leaderboards of the challenge.

Conclusion: The proposed policy, combining architectural and training advancements, proved to be highly effective for handling complex simulation tasks, highlighting its potential for real-world applications.

Abstract: We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.
  Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.
  Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.

</details>


### [557] [VideoVLA: Video Generators Can Be Generalizable Robot Manipulators](https://arxiv.org/abs/2512.06963)
*Yichao Shen,Fangyun Wei,Zhiying Du,Yaobo Liang,Yan Lu,Jiaolong Yang,Nanning Zheng,Baining Guo*

Main category: cs.RO

TL;DR: VideoVLA transforms pre-trained video generation models into robotic manipulators capable of predicting actions and future visual outcomes for enhanced generalization in novel tasks, objects, and settings.


<details>
  <summary>Details</summary>
Motivation: Improve robot manipulation generalization for deployment in diverse environments and advance toward artificial general intelligence.

Method: Uses a multi-modal Diffusion Transformer to jointly model video, language, and action modalities. It predicts actions and visual outcomes based on given instructions and images.

Result: Experiments show high-quality visual imagination correlates with reliable action predictions and task success, enabling generalization to novel objects and skill imitation from other embodiments.

Conclusion: VideoVLA’s dual prediction strategy (actions and visual outcomes) signals a paradigm shift in robot learning, enabling enhanced generalization capabilities in manipulation systems.

Abstract: Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.

</details>


### [558] [Parametric Design of a Cable-Driven Coaxial Spherical Parallel Mechanism for Ultrasound Scans](https://arxiv.org/abs/2512.06995)
*Maryam Seraj,Mohammad Hossein Kamrava,Carlo Tiseo*

Main category: cs.RO

TL;DR: The paper introduces a Cable-Driven Coaxial Spherical Parallel Mechanism (CDC-SPM) to improve rotational motion in haptic interfaces for medical teleoperation.


<details>
  <summary>Details</summary>
Motivation: The need for high-fidelity haptic interfaces in medical teleoperation that effectively balance workspace, dexterity, stiffness, inertia, and bandwidth.

Method: The design and kinematic analysis of a cable-driven interface (CDC-SPM) utilizing coaxial, parallel actuation to achieve decoupled rotational motion with isotropic force and torque transmission.

Result: The CDC-SPM demonstrates improved dynamic responsiveness, minimization of inertial loads, and isotropic motion characteristics validated through simulation and analysis.

Conclusion: The CDC-SPM is suitable for precise and intuitive manipulation tasks such as ultrasound imaging, enhancing safety and responsiveness in medical teleoperation.

Abstract: Haptic interfaces play a critical role in medical teleoperation by enabling surgeons to interact with remote environments through realistic force and motion feedback. Achieving high fidelity in such systems requires balancing performance trade-off among workspace, dexterity, stiffness, inertia, and bandwidth, particularly in applications demanding pure rotational motion. This paper presents the design methodology and kinematic analysis of a Cable-Driven Coaxial Spherical Parallel Mechanism (CDC-SPM) developed to address these challenges. The proposed cable-driven interface design allows for reducing the mass placed at the robot arm end-effector, thereby minimizing inertial loads, enhancing stiffness, and improving dynamic responsiveness. Through parallel and coaxial actuation, the mechanism achieves decoupled rotational degrees of freedom with isotropic force and torque transmission. Simulation and analysis demonstrate that the CDC-SPM provides accurate, responsive, and safe motion characteristics suitable for high-precision haptic applications. These results highlight the mechanism's potential for medical teleoperation tasks such as ultrasound imaging, where precise and intuitive manipulation is essential.

</details>


### [559] [A Hetero-Associative Sequential Memory Model Utilizing Neuromorphic Signals: Validated on a Mobile Manipulator](https://arxiv.org/abs/2512.07032)
*Runcong Wang,Fengyi Wang,Gordon Cheng*

Main category: cs.RO

TL;DR: The paper proposes a neuromorphic memory system for mobile manipulators to make efficient action decisions by associating joint states with tactile observations.


<details>
  <summary>Details</summary>
Motivation: To design a memory system for robots that is both efficient and capable of leveraging tactile input for effective control and decision-making.

Method: The system uses population place coding to encode joint angles and converts tactile forces into spike-rate features via an Izhikevich neuron model. These signals are transformed and combined into binary associations using 3D rotary positional embeddings for improved separability.

Result: Demonstrated on a Toyota Human Support Robot, it enables pseudocompliance control under tactile input and retrieves multi-joint grasp sequences, showcasing generalization and efficiency.

Conclusion: The system offers a quick setup, economical training, and versatile performance, with potential applications in imitation learning, motion planning, and multi-modal integration.

Abstract: This paper presents a hetero-associative sequential memory system for mobile manipulators that learns compact, neuromorphic bindings between robot joint states and tactile observations to produce step-wise action decisions with low compute and memory cost. The method encodes joint angles via population place coding and converts skin-measured forces into spike-rate features using an Izhikevich neuron model; both signals are transformed into bipolar binary vectors and bound element-wise to create associations stored in a large-capacity sequential memory. To improve separability in binary space and inject geometry from touch, we introduce 3D rotary positional embeddings that rotate subspaces as a function of sensed force direction, enabling fuzzy retrieval through a softmax weighted recall over temporally shifted action patterns. On a Toyota Human Support Robot covered by robot skin, the hetero-associative sequential memory system realizes a pseudocompliance controller that moves the link under touch in the direction and with speed correlating to the amplitude of applied force, and it retrieves multi-joint grasp sequences by continuing tactile input. The system sets up quickly, trains from synchronized streams of states and observations, and exhibits a degree of generalization while remaining economical. Results demonstrate single-joint and full-arm behaviors executed via associative recall, and suggest extensions to imitation learning, motion planning, and multi-modal integration.

</details>


### [560] [CERNet: Class-Embedding Predictive-Coding RNN for Unified Robot Motion, Recognition, and Confidence Estimation](https://arxiv.org/abs/2512.07041)
*Hiroki Sawada,Alexandre Pitti,Mathias Quoy*

Main category: cs.RO

TL;DR: The paper introduces CERNet, a hierarchical predictive-coding recurrent neural network model for robust movement generation, real-time intent recognition, and confidence estimation in robots.


<details>
  <summary>Details</summary>
Motivation: To enable robots to simultaneously perform real-time movement generation, infer human intent, and estimate confidence in their inferences for human-robot collaboration.

Method: The CERNet model integrates a class embedding vector within a hierarchical predictive-coding recurrent neural network (PC-RNN). It operates in two modes: generation (which uses class-specific subspaces) and inference (dynamic optimization based on prediction error).

Result: The model demonstrated 76% lower reproduction error compared to a baseline, robust motion fidelity under perturbations, and achieved 68% Top-1 and 81% Top-2 recognition accuracy in trajectory class inference.

Conclusion: CERNet provides a compact, effective framework for motor memory in robots, enabling intent-sensitive human-robot collaboration through real-time recognition, robust generation, and uncertainty estimation.

Abstract: Robots interacting with humans must not only generate learned movements in real-time, but also infer the intent behind observed behaviors and estimate the confidence of their own inferences. This paper proposes a unified model that achieves all three capabilities within a single hierarchical predictive-coding recurrent neural network (PC-RNN) equipped with a class embedding vector, CERNet, which leverages a dynamically updated class embedding vector to unify motor generation and recognition. The model operates in two modes: generation and inference. In the generation mode, the class embedding constrains the hidden state dynamics to a class-specific subspace; in the inference mode, it is optimized online to minimize prediction error, enabling real-time recognition. Validated on a humanoid robot across 26 kinesthetically taught alphabets, our hierarchical model achieves 76% lower trajectory reproduction error than a parameter-matched single-layer baseline, maintains motion fidelity under external perturbations, and infers the demonstrated trajectory class online with 68% Top-1 and 81% Top-2 accuracy. Furthermore, internal prediction errors naturally reflect the model's confidence in its recognition. This integration of robust generation, real-time recognition, and intrinsic uncertainty estimation within a compact PC-RNN framework offers a compact and extensible approach to motor memory in physical robots, with potential applications in intent-sensitive human-robot collaboration.

</details>


### [561] [A Flexible Funnel-Shaped Robotic Hand with an Integrated Single-Sheet Valve for Milligram-Scale Powder Handling](https://arxiv.org/abs/2512.07091)
*Tomoya Takahashi,Yusaku Nakajima,Cristian Camilo Beltran-Hernandez,Yuki Kuroda,Kazutoshi Tanaka,Masashi Hamaya,Kanta Ono,Yoshitaka Ushiku*

Main category: cs.RO

TL;DR: This paper proposes a robotic hand with a controllable valve for precise powder measurement, achieving high accuracy and adaptability in laboratory automation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the challenge of automating precise milligram-scale powder handling due to complex powder flow dynamics and task variations.

Method: The study develops a flexible, funnel-shaped robotic hand with a controllable valve, feedback control, and flow prediction models to adapt to powder dynamics while achieving precise dispensing.

Result: The system demonstrated an 80% success rate within a 2 mg error margin and significantly improved accuracy and convergence speed in comparison to simpler control methods.

Conclusion: The solution enables accurate, flexible, and scalable powder weighing suitable for diverse laboratory automation applications.

Abstract: Laboratory Automation (LA) has the potential to accelerate solid-state materials discovery by enabling continuous robotic operation without human intervention. While robotic systems have been developed for tasks such as powder grinding and X-ray diffraction (XRD) analysis, fully automating powder handling at the milligram scale remains a significant challenge due to the complex flow dynamics of powders and the diversity of laboratory tasks. To address this challenge, this study proposes a novel, funnel-shaped, flexible robotic hand that preserves the softness and conical sheet designs in prior work while incorporating a controllable valve at the cone apex to enable precise, incremental dispensing of milligram-scale powder quantities. The hand is integrated with an external balance through a feedback control system based on a model of powder flow and online parameter identification. Experimental evaluations with glass beads, monosodium glutamate, and titanium dioxide demonstrated that 80% of the trials achieved an error within 2 mg, and the maximum error observed was approximately 20 mg across a target range of 20 mg to 3 g. In addition, by incorporating flow prediction models commonly used for hoppers and performing online parameter identification, the system is able to adapt to variations in powder dynamics. Compared to direct PID control, the proposed model-based control significantly improved both accuracy and convergence speed. These results highlight the potential of the proposed system to enable efficient and flexible powder weighing, with scalability toward larger quantities and applicability to a broad range of laboratory automation tasks.

</details>


### [562] [Surrogate compliance modeling enables reinforcement learned locomotion gaits for soft robots](https://arxiv.org/abs/2512.07114)
*Jue Wang,Mingsong Jiang,Luis A. Ramirez,Bilige Yang,Mujun Zhang,Esteban Figueroa,Wenzhong Yan,Rebecca Kramer-Bottiglio*

Main category: cs.RO

TL;DR: This paper proposes a surrogate compliance modeling approach for simulating soft-material dynamics in rigid-body simulators to achieve adaptive locomotion in morphogenetic robots, demonstrated on an amphibious robotic turtle.


<details>
  <summary>Details</summary>
Motivation: Current simulation tools struggle to accurately and efficiently model soft-body dynamics, creating challenges in developing adaptive morphogenetic robots capable of handling diverse tasks and environments.

Method: The authors utilize indirect variables to represent soft-material deformation in a rigid-body simulator and apply reinforcement learning with extensive randomization for policy learning.

Result: The learned locomotion policies reliably transfer to hardware, showing high sim-to-real fidelity on hard terrain and decent robustness on complex terrains. The system achieves enhanced terrestrial maneuverability and significantly reduced transport costs.

Conclusion: The proposed approach enables reliable adaptation of soft-morphing robotic systems, demonstrating practical implementation across diverse terrains and offering significant performance improvements in locomotion.

Abstract: Adaptive morphogenetic robots adapt their morphology and control policies to meet changing tasks and environmental conditions. Many such systems leverage soft components, which enable shape morphing but also introduce simulation and control challenges. Soft-body simulators remain limited in accuracy and computational tractability, while rigid-body simulators cannot capture soft-material dynamics. Here, we present a surrogate compliance modeling approach: rather than explicitly modeling soft-body physics, we introduce indirect variables representing soft-material deformation within a rigid-body simulator. We validate this approach using our amphibious robotic turtle, a quadruped with soft morphing limbs designed for multi-environment locomotion. By capturing deformation effects as changes in effective limb length and limb center of mass, and by applying reinforcement learning with extensive randomization of these indirect variables, we achieve reliable policy learning entirely in a rigid-body simulation. The resulting gaits transfer directly to hardware, demonstrating high-fidelity sim-to-real performance on hard, flat substrates and robust, though lower-fidelity, transfer on rheologically complex terrains. The learned closed-loop gaits exhibit unprecedented terrestrial maneuverability and achieve an order-of-magnitude reduction in cost of transport compared to open-loop baselines. Field experiments with the robot further demonstrate stable, multi-gait locomotion across diverse natural terrains, including gravel, grass, and mud.

</details>


### [563] [Mimir: Hierarchical Goal-Driven Diffusion with Uncertainty Propagation for End-to-End Autonomous Driving](https://arxiv.org/abs/2512.07130)
*Zebin Xing,Yupeng Zheng,Qichao Zhang,Zhixing Ding,Pengxuan Yang,Songen Gu,Zhongpu Xia,Dongbin Zhao*

Main category: cs.RO

TL;DR: The paper introduces 'Mimir,' a hierarchical framework for autonomous driving, combining uncertainty estimation and multi-rate guidance to address limitations in guidance robustness and computational overhead.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving systems face challenges with high-level guidance inaccuracies and computational inefficiency; this paper aims to improve trajectory robustness and inference speed.

Method: It proposes a hierarchical dual-system framework using Laplace distribution for goal-point uncertainty estimation, combined with a multi-rate guidance mechanism to predict extended goal points.

Result: On benchmarks like Navhard and Navtest, Mimir improved the driving score by 20% and enhanced the inference speed of high-level modules by 1.6 times, maintaining accuracy.

Conclusion: Mimir demonstrated significant gains in autonomous driving efficiency and robustness, addressing key limitations in existing systems. Its upcoming code and model release aim to foster reproducibility and further research.

Abstract: End-to-end autonomous driving has emerged as a pivotal direction in the field of autonomous systems. Recent works have demonstrated impressive performance by incorporating high-level guidance signals to steer low-level trajectory planners. However, their potential is often constrained by inaccurate high-level guidance and the computational overhead of complex guidance modules. To address these limitations, we propose Mimir, a novel hierarchical dual-system framework capable of generating robust trajectories relying on goal points with uncertainty estimation: (1) Unlike previous approaches that deterministically model, we estimate goal point uncertainty with a Laplace distribution to enhance robustness; (2) To overcome the slow inference speed of the guidance system, we introduce a multi-rate guidance mechanism that predicts extended goal points in advance. Validated on challenging Navhard and Navtest benchmarks, Mimir surpasses previous state-of-the-art methods with a 20% improvement in the driving score EPDMS, while achieving 1.6 times improvement in high-level module inference speed without compromising accuracy. The code and models will be released soon to promote reproducibility and further development. The code is available at https://github.com/ZebinX/Mimir-Uncertainty-Driving

</details>


### [564] [Time-Varying Formation Tracking Control of Wheeled Mobile Robots With Region Constraint: A Generalized Udwadia-Kalaba Framework](https://arxiv.org/abs/2512.07137)
*Kang Yijie,Hao Yuqing,Wang Qingyun,Chen Guanrong*

Main category: cs.RO

TL;DR: This paper investigates a control strategy for wheeled mobile robots ensuring safe region constraints during time-varying formation tracking.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of ensuring safe zone navigation for wheeled mobile robots using an advanced control framework.

Method: Using the Udwadia-Kalaba framework, the authors reformulate the tracking objective and transform region constraints using diffeomorphism to design a controller.

Result: The proposed strategy demonstrates safety and effectiveness in simulations for time-varying formation tracking.

Conclusion: Through integration of region constraints, the control strategy ensures robot safety and viability in formation tracking tasks.

Abstract: In this paper, the time-varying formation tracking control of wheeled mobile robots with region constraint is investigated from a generalized Udwadia-Kalaba framework. The communication topology is directed, weighted and has a spanning tree with the leader being the root. By reformulating the time-varying formation tracking control objective as a constrained equation and transforming the region constraint by a diffeomorphism, the time-varying formation tracking controller with the region constraint is designed under the generalized Udwadia-Kalaba framework. Compared with the existing works on time-varying formation tracking control, the region constraint is takeninto account in this paper, which ensures the safety of the robots.Finally, some numerical simulations are presented to illustrate the effectiveness of the proposed control strategy.

</details>


### [565] [Using Vision-Language Models as Proxies for Social Intelligence in Human-Robot Interaction](https://arxiv.org/abs/2512.07177)
*Fanjun Bu,Melina Tsai,Audrey Tjokro,Tapomayukh Bhattacharjee,Jorge Ortiz,Wendy Ju*

Main category: cs.RO

TL;DR: The paper discusses robot interactions in social settings, proposing a method to detect and act on human cues using a two-stage pipeline combining lightweight detectors and video-based vision-language models (VLM).


<details>
  <summary>Details</summary>
Motivation: Robots often struggle with determining when and how to engage with humans, especially since these decisions rely on subtle, time-sensitive nonverbal cues that are challenging to model.

Method: A two-stage pipeline is proposed: lightweight perceptual detectors capture nonverbal cues (like gaze shifts and proxemics) to activate VLMs that guide decisions. Evaluations are conducted using replayed interactions in a field deployment.

Result: The proposed approach better aligns robot behavior with social cues by only using heavy VLMs selectively when meaningful cues are detected, improving responsiveness and efficiency.

Conclusion: Utilizing VLMs as proxies for social reasoning in a targeted manner can enhance robots' ability to interact socially by interpreting natural human signals effectively.

Abstract: Robots operating in everyday environments must often decide when and whether to engage with people, yet such decisions often hinge on subtle nonverbal cues that unfold over time and are difficult to model explicitly. Drawing on a five-day Wizard-of-Oz deployment of a mobile service robot in a university cafe, we analyze how people signal interaction readiness through nonverbal behaviors and how expert wizards use these cues to guide engagement. Motivated by these observations, we propose a two-stage pipeline in which lightweight perceptual detectors (gaze shifts and proxemics) are used to selectively trigger heavier video-based vision-language model (VLM) queries at socially meaningful moments. We evaluate this pipeline on replayed field interactions and compare two prompting strategies. Our findings suggest that selectively using VLMs as proxies for social reasoning enables socially responsive robot behavior, allowing robots to act appropriately by attending to the cues people naturally provide in real-world interactions.

</details>


### [566] [Spatiotemporal Calibration and Ground Truth Estimation for High-Precision SLAM Benchmarking in Extended Reality](https://arxiv.org/abs/2512.07221)
*Zichao Shu,Shitao Bei,Lijun Li,Zetao Chen*

Main category: cs.RO

TL;DR: The paper addresses limitations in SLAM benchmarking for XR applications by proposing a new method to improve trajectory accuracy using MoCap data, IMU integration, and advanced calibration techniques.


<details>
  <summary>Details</summary>
Motivation: Due to increasing standards for XR immersion, SLAM benchmarking requires improved accuracy in trajectory evaluation, yet MoCap-based ground truth has precision limitations that hinder SLAM evaluation, especially for rotation errors and inter-frame jitter.

Method: The authors developed a continuous-time maximum likelihood estimator that compensates for MoCap jitter using IMU data. They also introduced a variable time synchronization technique and a screw congruence constraint for more accurate spatiotemporal calibration.

Result: The proposed approach showed superior precision in experiments compared to existing methods, providing reliable benchmarking datasets for state-of-the-art SLAM algorithms and XR devices.

Conclusion: This method enhances MoCap-based SLAM benchmarking, offering improved tools for XR evaluation. The publicly available code expands access to these advancements for SLAM research in XR.

Abstract: Simultaneous localization and mapping (SLAM) plays a fundamental role in extended reality (XR) applications. As the standards for immersion in XR continue to increase, the demands for SLAM benchmarking have become more stringent. Trajectory accuracy is the key metric, and marker-based optical motion capture (MoCap) systems are widely used to generate ground truth (GT) because of their drift-free and relatively accurate measurements. However, the precision of MoCap-based GT is limited by two factors: the spatiotemporal calibration with the device under test (DUT) and the inherent jitter in the MoCap measurements. These limitations hinder accurate SLAM benchmarking, particularly for key metrics like rotation error and inter-frame jitter, which are critical for immersive XR experiences. This paper presents a novel continuous-time maximum likelihood estimator to address these challenges. The proposed method integrates auxiliary inertial measurement unit (IMU) data to compensate for MoCap jitter. Additionally, a variable time synchronization method and a pose residual based on screw congruence constraints are proposed, enabling precise spatiotemporal calibration across multiple sensors and the DUT. Experimental results demonstrate that our approach outperforms existing methods, achieving the precision necessary for comprehensive benchmarking of state-of-the-art SLAM algorithms in XR applications. Furthermore, we thoroughly validate the practicality of our method by benchmarking several leading XR devices and open-source SLAM algorithms. The code is publicly available at https://github.com/ylab-xrpg/xr-hpgt.

</details>


### [567] [SINRL: Socially Integrated Navigation with Reinforcement Learning using Spiking Neural Networks](https://arxiv.org/abs/2512.07266)
*Florian Tretter,Daniel Flögel,Alexandru Vasilache,Max Grobbel,Jürgen Becker,Sören Hohmann*

Main category: cs.RO

TL;DR: The study presents a hybrid socially integrated DRL approach combining SNNs and ANNs, showing improved social navigation and significant energy savings.


<details>
  <summary>Details</summary>
Motivation: To integrate autonomous robots efficiently into human environments, addressing unstable training in neuromorphic DRL approaches.

Method: Uses a hybrid DRL actor-critic framework, with SNNs in the actor, ANNs in the critic, and a neuromorphic feature extractor to process temporal dynamics and interactions.

Result: Improved social navigation performance and reduced energy consumption by about 1.69 orders of magnitude.

Conclusion: The hybrid approach effectively bridges the gap in using unstable neuromorphic DRL, promoting better and energy-efficient social navigation of robots in human environments.

Abstract: Integrating autonomous mobile robots into human environments requires human-like decision-making and energy-efficient, event-based computation. Despite progress, neuromorphic methods are rarely applied to Deep Reinforcement Learning (DRL) navigation approaches due to unstable training. We address this gap with a hybrid socially integrated DRL actor-critic approach that combines Spiking Neural Networks (SNNs) in the actor with Artificial Neural Networks (ANNs) in the critic and a neuromorphic feature extractor to capture temporal crowd dynamics and human-robot interactions. Our approach enhances social navigation performance and reduces estimated energy consumption by approximately 1.69 orders of magnitude.

</details>


### [568] [Efficient Computation of a Continuous Topological Model of the Configuration Space of Tethered Mobile Robots](https://arxiv.org/abs/2512.07303)
*Gianpietro Battocletti,Dimitris Boskos,Bart De Schutter*

Main category: cs.RO

TL;DR: This paper proposes a novel method for tethered robot path planning by creating a continuous, topological model of the configuration space.


<details>
  <summary>Details</summary>
Motivation: Path planning for tethered robots traditionally relies on discrete configuration space representations and fails to simultaneously capture the tether’s topology and the robot’s continuous position.

Method: The authors link a tethered robot's configuration space to the universal covering space of its polygonal workspace and develop an algorithm to compute a simplicial complex model for it.

Result: The proposed approach enhances algorithmic performance, computes the model faster than traditional methods, and supports various path planning algorithms due to its continuous nature.

Conclusion: This work improves efficiency and flexibility in tethered robot path planning with a novel configuration space model that surpasses current methods.

Abstract: Despite the attention that the problem of path planning for tethered robots has garnered in the past few decades, the approaches proposed to solve it typically rely on a discrete representation of the configuration space and do not exploit a model that can simultaneously capture the topological information of the tether and the continuous location of the robot. In this work, we explicitly build a topological model of the configuration space of a tethered robot starting from a polygonal representation of the workspace where the robot moves. To do so, we first establish a link between the configuration space of the tethered robot and the universal covering space of the workspace, and then we exploit this link to develop an algorithm to compute a simplicial complex model of the configuration space. We show how this approach improves the performances of existing algorithms that build other types of representations of the configuration space. The proposed model can be computed in a fraction of the time required to build traditional homotopy-augmented graphs, and is continuous, allowing to solve the path planning task for tethered robots using a broad set of path planning algorithms.

</details>


### [569] [Model Predictive Control for Cooperative Docking Between Autonomous Surface Vehicles with Disturbance Rejection](https://arxiv.org/abs/2512.07316)
*Gianpietro Battocletti,Dimitris Boskos,Bart De Schutter*

Main category: cs.RO

TL;DR: The paper presents a centralized Model Predictive Control (MPC) method for cooperative docking of two uncrewed surface vehicles (USVs), enabling efficient, disturbance-resistant docking.


<details>
  <summary>Details</summary>
Motivation: Docking between USVs is essential for shared tasks in marine environments. Existing methods often involve one stationary USV, limiting efficiency. A cooperative approach can improve performance.

Method: The authors use a centralized MPC to design trajectories for both USVs, accounting for constraints and disturbances such as water currents to ensure efficient docking.

Result: Simulations show the proposed centralized MPC approach allows faster and more efficient docking compared to existing stationary-target-based methods.

Conclusion: The cooperative docking approach using centralized MPC enables effective and robust performance in dynamic marine environments, outperforming traditional methods.

Abstract: Uncrewed Surface Vehicles (USVs) are a popular and efficient type of marine craft that find application in a large number of water-based tasks. When multiple USVs operate in the same area, they may be required to dock to each other to perform a shared task. Existing approaches for the docking between autonomous USVs generally consider one USV as a stationary target, while the second one is tasked to reach the required docking pose. In this work, we propose a cooperative approach for USV-USV docking, where two USVs work together to dock at an agreed location. We use a centralized Model Predictive Control (MPC) approach to solve the control problem, obtaining feasible trajectories that also guarantee constraint satisfaction. Owing to its model-based nature, this approach allows the rejection of disturbances, inclusive of exogenous inputs, by anticipating their effect on the USVs through the MPC prediction model. This is particularly effective in case of almost-stationary disturbances such as water currents. In simulations, we demonstrate how the proposed approach allows for a faster and more efficient docking with respect to existing approaches.

</details>


### [570] [Multi-Rigid-Body Approximation of Human Hands with Application to Digital Twin](https://arxiv.org/abs/2512.07359)
*Bin Zhao,Yiwen Lu,Haohua Zhu,Xiao Li,Sheng Yi*

Main category: cs.RO

TL;DR: The paper presents a pipeline for creating personalized multi-rigid-body hand models from optical motion capture, suited for real-time physics simulation and validated by reinforcement learning in manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Human hand simulation is crucial for digital twin applications, requiring a balance between anatomical fidelity and computational efficiency.

Method: The authors develop a pipeline to translate motion capture data to a personalized hand model (MANO), construct a URDF representation, and use closed-form and iterative methods for kinematic constraint adjustment in joint rotations.

Result: The approach achieves sub-centimeter reconstruction error and demonstrates successful grasping in various manipulation tasks through reinforcement learning.

Conclusion: The paper validates the feasibility and accuracy of its hand model pipeline for real-time physics and manipulation in digital twin applications.

Abstract: Human hand simulation plays a critical role in digital twin applications, requiring models that balance anatomical fidelity with computational efficiency. We present a complete pipeline for constructing multi-rigid-body approximations of human hands that preserve realistic appearance while enabling real-time physics simulation. Starting from optical motion capture of a specific human hand, we construct a personalized MANO (Multi-Abstracted hand model with Neural Operations) model and convert it to a URDF (Unified Robot Description Format) representation with anatomically consistent joint axes. The key technical challenge is projecting MANO's unconstrained SO(3) joint rotations onto the kinematically constrained joints of the rigid-body model. We derive closed-form solutions for single degree-of-freedom joints and introduce a Baker-Campbell-Hausdorff (BCH)-corrected iterative method for two degree-of-freedom joints that properly handles the non-commutativity of rotations. We validate our approach through digital twin experiments where reinforcement learning policies control the multi-rigid-body hand to replay captured human demonstrations. Quantitative evaluation shows sub-centimeter reconstruction error and successful grasp execution across diverse manipulation tasks.

</details>


### [571] [ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning](https://arxiv.org/abs/2512.07371)
*Byungju Kim,Jinu Pahk,Chungwoo Lee,Jaejoon Kim,Jangha Lee,Theo Taeyeong Kim,Kyuhwan Shim,Jun Ki Lee,Byoung-Tak Zhang*

Main category: cs.RO

TL;DR: The paper introduces ESPADA, a framework to optimize behavior-cloning visuomotor policies by identifying task-critical segments and accelerating non-critical phases, achieving faster manipulation with consistent precision.


<details>
  <summary>Details</summary>
Motivation: The slow tempo of behavior-cloning visuomotor policies—due to reliance on human demonstrations—hinders practical deployment in real-world applications.

Method: ESPADA uses a combination of Vision Language Models (VLMs), Large Language Models (LLMs), and 3D gripper-object relations to segment tasks and apply varying levels of acceleration. It further propagates segment labels across datasets using Dynamic Time Warping on dynamics-only features.

Result: The framework achieves an approximate 2x speed-up in manipulation tasks while retaining the success rates of the original behavior-cloning models.

Conclusion: ESPADA bridges the gap between cautious human demonstrations and efficient robotic control without additional datasets or retraining, enabling faster and precise robotic manipulation.

Abstract: Behavior-cloning based visuomotor policies enable precise manipulation but often inherit the slow, cautious tempo of human demonstrations, limiting practical deployment. However, prior studies on acceleration methods mainly rely on statistical or heuristic cues that ignore task semantics and can fail across diverse manipulation settings. We present ESPADA, a semantic and spatially aware framework that segments demonstrations using a VLM-LLM pipeline with 3D gripper-object relations, enabling aggressive downsampling only in non-critical segments while preserving precision-critical phases, without requiring extra data or architectural modifications, or any form of retraining. To scale from a single annotated episode to the full dataset, ESPADA propagates segment labels via Dynamic Time Warping (DTW) on dynamics-only features. Across both simulation and real-world experiments with ACT and DP baselines, ESPADA achieves approximately a 2x speed-up while maintaining success rates, narrowing the gap between human demonstrations and efficient robot control.

</details>


### [572] [Gait-Adaptive Perceptive Humanoid Locomotion with Real-Time Under-Base Terrain Reconstruction](https://arxiv.org/abs/2512.07464)
*Haolin Song,Hongbo Zhu,Tao Yu,Yan Liu,Mingqi Yuan,Wengang Zhou,Hua Chen,Houqiang Li*

Main category: cs.RO

TL;DR: This paper proposes a reinforcement learning-based framework to enable full-size humanoid robots to traverse challenging terrains by merging terrain sensing, gait regulation, and whole-body control into a unified policy.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in reinforcement learning, humanoid robots still struggle to achieve consistent and reliable locomotion on complex terrains like stairs due to issues with perception, ambiguous cues, and improper gait timing.

Method: A perceptive locomotion framework is introduced, combining a depth camera-powered egocentric terrain map, unified policy reinforcement learning, and a teacher-student training approach. It entails real-time processing of terrain around the robot's feet and adaptive control of joint movements and gait timing.

Result: The proposed method was tested on a 31-degree-of-freedom humanoid robot, achieving robust locomotion in both simulations and real-world scenarios—including ascending and descending complex stairs and crossing large gaps.

Conclusion: The integration of perception, gait regulation, and whole-body control using reinforcement learning provides a robust solution to enhance humanoid robot locomotion on previously challenging terrains.

Abstract: For full-size humanoid robots, even with recent advances in reinforcement learning-based control, achieving reliable locomotion on complex terrains, such as long staircases, remains challenging. In such settings, limited perception, ambiguous terrain cues, and insufficient adaptation of gait timing can cause even a single misplaced or mistimed step to result in rapid loss of balance. We introduce a perceptive locomotion framework that merges terrain sensing, gait regulation, and whole-body control into a single reinforcement learning policy. A downward-facing depth camera mounted under the base observes the support region around the feet, and a compact U-Net reconstructs a dense egocentric height map from each frame in real time, operating at the same frequency as the control loop. The perceptual height map, together with proprioceptive observations, is processed by a unified policy that produces joint commands and a global stepping-phase signal, allowing gait timing and whole-body posture to be adapted jointly to the commanded motion and local terrain geometry. We further adopt a single-stage successive teacher-student training scheme for efficient policy learning and knowledge transfer. Experiments conducted on a 31-DoF, 1.65 m humanoid robot demonstrate robust locomotion in both simulation and real-world settings, including forward and backward stair ascent and descent, as well as crossing a 46 cm gap. Project Page:https://ga-phl.github.io/

</details>


### [573] [Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation](https://arxiv.org/abs/2512.07472)
*Siyu Xu,Zijian Wang,Yunke Wang,Chenghao Xia,Tao Huang,Chang Xu*

Main category: cs.RO

TL;DR: The paper introduces Affordance Field Intervention (AFI), a framework that enhances the robustness of Vision-Language-Action (VLA) models for robotics by integrating 3D Spatial Affordance Fields (SAFs), improving performance under distribution shifts.


<details>
  <summary>Details</summary>
Motivation: VLA models perform well in robotic manipulation but fail under distribution shifts, memorizing trajectories instead of adapting due to a lack of explicit 3D spatial reasoning.

Method: The method incorporates SAFs to provide geometric 3D cues for actions, detects memory traps through proprioception, repositions robots based on affordance regions, selects affordance-driven waypoints, and uses SAF-based scoring to choose high-affordance trajectories.

Result: The proposed AFI framework achieves a 23.5% improvement on VLA backbones π₀ and π₀.₅ in out-of-distribution real-world scenarios and a 20.2% boost on the LIBERO-Pro benchmark.

Conclusion: AFI significantly enhances the adaptability and robustness of VLA models in unfamiliar robotic scenarios through spatial reasoning using SAFs.

Abstract: Vision-Language-Action (VLA) models have shown great performance in robotic manipulation by mapping visual observations and language instructions directly to actions. However, they remain brittle under distribution shifts: when test scenarios change, VLAs often reproduce memorized trajectories instead of adapting to the updated scene, which is a failure mode we refer to as the "Memory Trap". This limitation stems from the end-to-end design, which lacks explicit 3D spatial reasoning and prevents reliable identification of actionable regions in unfamiliar environments. To compensate for this missing spatial understanding, 3D Spatial Affordance Fields (SAFs) can provide a geometric representation that highlights where interactions are physically feasible, offering explicit cues about regions the robot should approach or avoid. We therefore introduce Affordance Field Intervention (AFI), a lightweight hybrid framework that uses SAFs as an on-demand plug-in to guide VLA behavior. Our system detects memory traps through proprioception, repositions the robot to recent high-affordance regions, and proposes affordance-driven waypoints that anchor VLA-generated actions. A SAF-based scorer then selects trajectories with the highest cumulative affordance. Extensive experiments demonstrate that our method achieves an average improvement of 23.5% across different VLA backbones ($π_{0}$ and $π_{0.5}$) under out-of-distribution scenarios on real-world robotic platforms, and 20.2% on the LIBERO-Pro benchmark, validating its effectiveness in enhancing VLA robustness to distribution shifts.

</details>


### [574] [From Real-World Traffic Data to Relevant Critical Scenarios](https://arxiv.org/abs/2512.07482)
*Florian Lüttner,Nicole Neis,Daniel Stadler,Robin Moss,Mirjam Fehling-Kaschek,Matthias Pfriem,Alexander Stolz,Jens Ziehn*

Main category: cs.RO

TL;DR: The paper proposes methods to identify, analyze, and generate safety-relevant lane change scenarios in highway traffic for autonomous and advanced driving systems using real-world and synthetic data.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles and advanced driving systems need reliable and efficient validation methods to ensure safety across numerous complex scenarios. The difficulty in identifying all relevant driving scenarios, especially unknown unsafe ones, poses a key challenge.

Method: The authors analyze lane change scenarios using real-world highway traffic data. Criticality measures are applied on trajectory data to evaluate scenarios. Additionally, synthetic scenarios are generated based on recorded ones to address unknown unsafe situations.

Result: Safety-relevant driving scenarios were identified through a processing chain including data acquisition, criticality evaluation, and synthetic scenario generation. The chain efficiently tackled both known and unknown unsafe scenarios.

Conclusion: The paper demonstrates a robust method to identify and generate critical scenarios, advancing the validation process for autonomous driving systems, especially in the context of lane changes on highways.

Abstract: The reliable operation of autonomous vehicles, automated driving functions, and advanced driver assistance systems across a wide range of relevant scenarios is critical for their development and deployment. Identifying a near-complete set of relevant driving scenarios for such functionalities is challenging due to numerous degrees of freedom involved, each affecting the outcomes of the driving scenario differently. Moreover, with increasing technical complexity of new functionalities, the number of potentially relevant, particularly "unknown unsafe" scenarios is increasing. To enhance validation efficiency, it is essential to identify relevant scenarios in advance, starting with simpler domains like highways before moving to more complex environments such as urban traffic. To address this, this paper focuses on analyzing lane change scenarios in highway traffic, which involve multiple degrees of freedom and present numerous safetyrelevant scenarios. We describe the process of data acquisition and processing of real-world data from public highway traffic, followed by the application of criticality measures on trajectory data to evaluate scenarios, as conducted within the AVEAS project (www.aveas.org). By linking the calculated measures to specific lane change driving scenarios and the conditions under which the data was collected, we facilitate the identification of safetyrelevant driving scenarios for various applications. Further, to tackle the extensive range of "unknown unsafe" scenarios, we propose a way to generate relevant scenarios by creating synthetic scenarios based on recorded ones. Consequently, we demonstrate and evaluate a processing chain that enables the identification of safety-relevant scenarios, the development of data-driven methods for extracting these scenarios, and the generation of synthetic critical scenarios via sampling on highways.

</details>


### [575] [See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations](https://arxiv.org/abs/2512.07582)
*Guangyan Chen,Meiling Wang,Qi Shao,Zichen Zhou,Weixin Mao,Te Cui,Minzhao Zhu,Yinan Deng,Luojie Yang,Zhanqi Zhang,Yi Yang,Hua Chen,Yufeng Yue*

Main category: cs.RO

TL;DR: The paper introduces ViVLA, a robotic manipulation policy capable of learning tasks from a single expert demonstration video by leveraging vision-language-action models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limited generalization of existing Vision-Language-Action models in robotics to learn novel tasks beyond their training distributions, inspired by human ability to learn from one observation.

Method: The method involves leveraging ViVLA, which jointly processes expert demonstration videos and robot observations to predict actions, combined with a scalable data generation pipeline that synthesizes expert-agent paired trajectories from human videos and curated datasets.

Result: ViVLA demonstrated 30% improvement on unseen LIBERO tasks and 35% gains with cross-embodiment videos. Real-world experiments showed over 38% improvement in learning unseen tasks from human videos.

Conclusion: ViVLA offers a robust approach for robotic manipulation, enabling generalization to unfamiliar tasks using minimal expert demonstrations, thus advancing capabilities in efficient task learning.

Abstract: Developing robust and general-purpose manipulation policies represents a fundamental objective in robotics research. While Vision-Language-Action (VLA) models have demonstrated promising capabilities for end-to-end robot control, existing approaches still exhibit limited generalization to tasks beyond their training distributions. In contrast, humans possess remarkable proficiency in acquiring novel skills by simply observing others performing them once. Inspired by this capability, we propose ViVLA, a generalist robotic manipulation policy that achieves efficient task learning from a single expert demonstration video at test time. Our approach jointly processes an expert demonstration video alongside the robot's visual observations to predict both the demonstrated action sequences and subsequent robot actions, effectively distilling fine-grained manipulation knowledge from expert behavior and transferring it seamlessly to the agent. To enhance the performance of ViVLA, we develop a scalable expert-agent pair data generation pipeline capable of synthesizing paired trajectories from easily accessible human videos, further augmented by curated pairs from publicly available datasets. This pipeline produces a total of 892,911 expert-agent samples for training ViVLA. Experimental results demonstrate that our ViVLA is able to acquire novel manipulation skills from only a single expert demonstration video at test time. Our approach achieves over 30% improvement on unseen LIBERO tasks and maintains above 35% gains with cross-embodiment videos. Real-world experiments demonstrate effective learning from human videos, yielding more than 38% improvement on unseen tasks.

</details>


### [576] [Multi-Domain Motion Embedding: Expressive Real-Time Mimicry for Legged Robots](https://arxiv.org/abs/2512.07673)
*Matthias Heyrman,Chenhao Li,Victor Klemm,Dongho Kang,Stelian Coros,Marco Hutter*

Main category: cs.RO

TL;DR: This paper introduces Multi-Domain Motion Embedding (MDME), a method for enabling robots to achieve real-time imitation of diverse human/animal motions by leveraging structured and unstructured representation features.


<details>
  <summary>Details</summary>
Motivation: Existing motion controllers fail to account for inherent patterns in motion, limiting robots’ ability to imitate expressive behaviors in real-time. Representations commonly fail to jointly capture structured periodic patterns and irregular variations in human/animal movement.

Method: The paper presents MDME, a motion representation system combining wavelet-based encoding for structured features and probabilistic embedding for unstructured features. This representation was used for robot control policies to imitate reference motions effectively.

Result: MDME enables robots to accurately imitate diverse motion styles in real-time without needing retargeting or task-specific tuning, outperforming previous approaches in fidelity and generalization. It demonstrated success on humanoid and quadruped platforms.

Conclusion: MDME serves as a scalable, generalized motion-representation framework that enables real-time, high-fidelity robot imitation across multiple styles and morphologies, including zero-shot motion reproduction.

Abstract: Effective motion representation is crucial for enabling robots to imitate expressive behaviors in real time, yet existing motion controllers often ignore inherent patterns in motion. Previous efforts in representation learning do not attempt to jointly capture structured periodic patterns and irregular variations in human and animal movement. To address this, we present Multi-Domain Motion Embedding (MDME), a motion representation that unifies the embedding of structured and unstructured features using a wavelet-based encoder and a probabilistic embedding in parallel. This produces a rich representation of reference motions from a minimal input set, enabling improved generalization across diverse motion styles and morphologies. We evaluate MDME on retargeting-free real-time motion imitation by conditioning robot control policies on the learned embeddings, demonstrating accurate reproduction of complex trajectories on both humanoid and quadruped platforms. Our comparative studies confirm that MDME outperforms prior approaches in reconstruction fidelity and generalizability to unseen motions. Furthermore, we demonstrate that MDME can reproduce novel motion styles in real-time through zero-shot deployment, eliminating the need for task-specific tuning or online retargeting. These results position MDME as a generalizable and structure-aware foundation for scalable real-time robot imitation.

</details>


### [577] [AMBER: Aerial deployable gripping crawler with compliant microspine for canopy manipulation](https://arxiv.org/abs/2512.07680)
*P. A. Wigner,L. Romanello,A. Hammad,P. H. Nguyen,T. Lan,S. F. Armanini,B. B. Kocer,M. Kovac*

Main category: cs.RO

TL;DR: The paper introduces a crawler system for adaptive locomotion and manipulation on tree canopies using unique design features, showcasing its gripping, climbing, and efficient transport capabilities.


<details>
  <summary>Details</summary>
Motivation: To establish a robotic tool that can operate in tree canopies, combining aerial deployment with adaptive crawling to support environmental monitoring and sampling.

Method: The system employs compliant microspine-based tracks, a rotary gripper, and an elastic tail to enable stable attachment and versatile traversal of branches. It is integrated with a drone-tether deployment system.

Result: The crawler grips securely on high inclinations, climbs branches up to 67.5 degrees, achieves up to 0.55 body lengths per second, and is highly energy-efficient compared to hovering aerial robots.

Conclusion: This study bridges the gap between aerial and surface ecological robotics, demonstrating the crawler's low-power and robust capabilities for in-canopy sensing and environmental applications.

Abstract: This paper presents an aerially deployable crawler designed for adaptive locomotion and manipulation within tree canopies. The system combines compliant microspine-based tracks, a dual-track rotary gripper, and an elastic tail, enabling secure attachment and stable traversal across branches of varying curvature and inclination.
  Experiments demonstrate reliable gripping up to 90 degrees of body roll and inclination, while effective climbing on branches inclined up to 67.5 degrees, achieving a maximum speed of 0.55 body lengths per second on horizontal branches. The compliant tracks allow yaw steering of up to 10 degrees, enhancing maneuverability on irregular surfaces.
  Power measurements show efficient operation with a dimensionless cost of transport over an order of magnitude lower than typical hovering power consumption in aerial robots. Integrated within a drone-tether deployment system, the crawler provides a robust, low-power platform for environmental sampling and in-canopy sensing, bridging the gap between aerial and surface-based ecological robotics.

</details>


### [578] [Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks](https://arxiv.org/abs/2512.07697)
*Aileen Liao,Dong-Ki Kim,Max Olan Smith,Ali-akbar Agha-mohammadi,Shayegan Omidshafiei*

Main category: cs.RO

TL;DR: The paper introduces Delay-Aware Diffusion Policy (DA-DP) to address inference delays in robots by incorporating delays into policy learning, improving robustness and generalizing well.


<details>
  <summary>Details</summary>
Motivation: To address the challenge robots face due to inference delays which cause discrepancies between observed and executed states.

Method: The DA-DP framework adjusts zero-delay trajectories to delay-compensated ones and augments policy learning by conditioning on delays.

Result: Experiments show DA-DP achieves higher robustness to inference delays across tasks, robots, and varying delays compared to delay-unaware methods.

Conclusion: DA-DP improves delay-awareness in imitation learning, generalizes across architectures, and emphasizes reporting performance considering latency in evaluations.

Abstract: As a robot senses and selects actions, the world keeps changing. This inference delay creates a gap of tens to hundreds of milliseconds between the observed state and the state at execution. In this work, we take the natural generalization from zero delay to measured delay during training and inference. We introduce Delay-Aware Diffusion Policy (DA-DP), a framework for explicitly incorporating inference delays into policy learning. DA-DP corrects zero-delay trajectories to their delay-compensated counterparts, and augments the policy with delay conditioning. We empirically validate DA-DP on a variety of tasks, robots, and delays and find its success rate more robust to delay than delay-unaware methods. DA-DP is architecture agnostic and transfers beyond diffusion policies, offering a general pattern for delay-aware imitation learning. More broadly, DA-DP encourages evaluation protocols that report performance as a function of measured latency, not just task difficulty.

</details>


### [579] [Toward Seamless Physical Human-Humanoid Interaction: Insights from Control, Intent, and Modeling with a Vision for What Comes Next](https://arxiv.org/abs/2512.07765)
*Gustavo A. Cardona,Shubham S. Kumbhar,Panagiotis Artemiadis*

Main category: cs.RO

TL;DR: This paper reviews the state of Physical Human-Humanoid Interaction (pHHI) research across three key areas and proposes methods for unifying approaches to advance human-robot collaboration.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need to improve humanoids' ability to interact effectively and intuitively in human-centric environments, particularly under physical human-robot interaction scenarios.

Method: The authors surveyed research on three pillars: humanoid modeling/control, human intent estimation, and computational human models, while introducing a taxonomy for interaction types and proposing cross-pillar integration.

Result: The review highlights progress in each domain but reveals significant challenges in creating cohesive interaction frameworks.

Conclusion: Unifying methods across the three pillars is crucial to enabling robust, scalable, and adaptive physical interactions between humanoids and humans in real-world conditions.

Abstract: Physical Human-Humanoid Interaction (pHHI) is a rapidly advancing field with significant implications for deploying robots in unstructured, human-centric environments. In this review, we examine the current state of the art in pHHI through three core pillars: (i) humanoid modeling and control, (ii) human intent estimation, and (iii) computational human models. For each pillar, we survey representative approaches, identify open challenges, and analyze current limitations that hinder robust, scalable, and adaptive interaction. These include the need for whole-body control strategies capable of handling uncertain human dynamics, real-time intent inference under limited sensing, and modeling techniques that account for variability in human physical states. Although significant progress has been made within each domain, integration across pillars remains limited. We propose pathways for unifying methods across these areas to enable cohesive interaction frameworks. This structure enables us not only to map the current landscape but also to propose concrete directions for future research that aim to bridge these domains. Additionally, we introduce a unified taxonomy of interaction types based on modality, distinguishing between direct interactions (e.g., physical contact) and indirect interactions (e.g., object-mediated), and on the level of robot engagement, ranging from assistance to cooperation and collaboration. For each category in this taxonomy, we provide the three core pillars that highlight opportunities for cross-pillar unification. Our goal is to suggest avenues to advance robust, safe, and intuitive physical interaction, providing a roadmap for future research that will allow humanoid systems to effectively understand, anticipate, and collaborate with human partners in diverse real-world settings.

</details>


### [580] [OptMap: Geometric Map Distillation via Submodular Maximization](https://arxiv.org/abs/2512.07775)
*David Thorne,Nathan Chan,Christa S. Robison,Philip R. Osteen,Brett T. Lopez*

Main category: cs.RO

TL;DR: The paper presents OptMap, a real-time geometric map distillation method to optimize LiDAR-based maps for autonomous robots' perception and decision-making.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of creating application-specific, size-constrained geometric maps for autonomous robots while minimizing computational complexity and ensuring optimal data representation.

Method: OptMap employs submodular function maximization, a novel reward function for data informativeness and bias reduction, and a dynamically reordered streaming submodular algorithm to improve solution quality.

Result: OptMap achieves computational efficiency, minimizes input biases, and provides near-optimal map solutions. Open-source packages compatible with LiDAR SLAM are provided.

Conclusion: OptMap generates efficient and application-specific maps in real-time, enhancing the utility of LiDAR data in autonomous systems and addressing long-duration mapping challenges.

Abstract: Autonomous robots rely on geometric maps to inform a diverse set of perception and decision-making algorithms. As autonomy requires reasoning and planning on multiple scales of the environment, each algorithm may require a different map for optimal performance. Light Detection And Ranging (LiDAR) sensors generate an abundance of geometric data to satisfy these diverse requirements, but selecting informative, size-constrained maps is computationally challenging as it requires solving an NP-hard combinatorial optimization. In this work we present OptMap: a geometric map distillation algorithm which achieves real-time, application-specific map generation via multiple theoretical and algorithmic innovations. A central feature is the maximization of set functions that exhibit diminishing returns, i.e., submodularity, using polynomial-time algorithms with provably near-optimal solutions. We formulate a novel submodular reward function which quantifies informativeness, reduces input set sizes, and minimizes bias in sequentially collected datasets. Further, we propose a dynamically reordered streaming submodular algorithm which improves empirical solution quality and addresses input order bias via an online approximation of the value of all scans. Testing was conducted on open-source and custom datasets with an emphasis on long-duration mapping sessions, highlighting OptMap's minimal computation requirements. Open-source ROS1 and ROS2 packages are available and can be used alongside any LiDAR SLAM algorithm.

</details>


### [581] [Inchworm-Inspired Soft Robot with Groove-Guided Locomotion](https://arxiv.org/abs/2512.07813)
*Hari Prakash Thanabalan,Lars Bengtsson,Ugo Lafont,Giovanni Volpe*

Main category: cs.RO

TL;DR: This paper presents a bio-inspired soft robot that uses a single actuator and a patterned substrate for directional control, simplifying design and reducing energy consumption.


<details>
  <summary>Details</summary>
Motivation: To address challenges in soft robot navigation that require multiple actuators, leading to increased complexity and energy use.

Method: The robot uses a single rolled dielectric elastomer actuator combined with groove patterns on a substrate to control direction passively.

Result: Experiments demonstrated that varying groove angles of the substrate can guide precise locomotion direction, ensuring control without complex actuators.

Conclusion: Using patterned substrates for passive control simplifies soft robot designs, reduces energy use, and enables their use in applications like search and rescue, pipe inspection, and planetary exploration.

Abstract: Soft robots require directional control to navigate complex terrains. However, achieving such control often requires multiple actuators, which increases mechanical complexity, complicates control systems, and raises energy consumption. Here, we introduce an inchworm-inspired soft robot whose locomotion direction is controlled passively by patterned substrates. The robot employs a single rolled dielectric elastomer actuator, while groove patterns on a 3D-printed substrate guide its alignment and trajectory. Through systematic experiments, we demonstrate that varying groove angles enables precise control of locomotion direction without the need for complex actuation strategies. This groove-guided approach reduces energy consumption, simplifies robot design, and expands the applicability of bio-inspired soft robots in fields such as search and rescue, pipe inspection, and planetary exploration.

</details>


### [582] [Efficient and Compliant Control Framework for Versatile Human-Humanoid Collaborative Transportation](https://arxiv.org/abs/2512.07819)
*Shubham S. Kumbhar,Abhijeet M. Kulkarni,Panagiotis Artemiadis*

Main category: cs.RO

TL;DR: The paper introduces a control framework for humanoid robots to effectively collaborate with humans during transportation tasks, incorporating a high-level planner, low-level controller, and stiffness modulation.


<details>
  <summary>Details</summary>
Motivation: To improve humanoid robots' ability to collaborate with humans during transportation tasks, addressing both translational and rotational dynamics.

Method: The framework involves an Interaction Linear Inverted Pendulum (I-LIP) for planning, a QP-based whole-body controller, and stiffness modulation to regulate interaction with objects. Real-world experiments and an efficiency metric evaluate performance.

Result: The framework succeeds in collaborative transport tasks, effectively executing diverse motions, and highlights the importance of compliance in robot-human interaction.

Conclusion: The proposed framework facilitates human-robot collaboration during transport with efficient coordination and dynamic feasibility, demonstrated on the Digit humanoid platform.

Abstract: We present a control framework that enables humanoid robots to perform collaborative transportation tasks with a human partner. The framework supports both translational and rotational motions, which are fundamental to co-transport scenarios. It comprises three components: a high-level planner, a low-level controller, and a stiffness modulation mechanism. At the planning level, we introduce the Interaction Linear Inverted Pendulum (I-LIP), which, combined with an admittance model and an MPC formulation, generates dynamically feasible footstep plans. These are executed by a QP-based whole-body controller that accounts for the coupled humanoid-object dynamics. Stiffness modulation regulates robot-object interaction, ensuring convergence to the desired relative configuration defined by the distance between the object and the robot's center of mass. We validate the effectiveness of the framework through real-world experiments conducted on the Digit humanoid platform. To quantify collaboration quality, we propose an efficiency metric that captures both task performance and inter-agent coordination. We show that this metric highlights the role of compliance in collaborative tasks and offers insights into desirable trajectory characteristics across both high- and low-level control layers. Finally, we showcase experimental results on collaborative behaviors, including translation, turning, and combined motions such as semi circular trajectories, representative of naturally occurring co-transportation tasks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [583] [Auto-SPT: Automating Semantic Preserving Transformations for Code](https://arxiv.org/abs/2512.06042)
*Ashish Hooda,Mihai Christodorescu,Chuangang Ren,Aaron Wilson,Kassem Fawaz,Somesh Jha*

Main category: cs.SE

TL;DR: The paper introduces Auto-SPT, an automated framework for generating semantic-preserving transformations (SPTs) for code, improving the robustness of machine learning-based code clone detectors against real-world code changes.


<details>
  <summary>Details</summary>
Motivation: Address the gap between training datasets (clean, structured code) and real-world code that undergoes semantic-preserving transformations which impacts the performance of code clone detection models.

Method: Auto-SPT leverages Large Language Models (LLMs) to generate diverse, semantic-preserving code transformations that alter syntactic structures without affecting functionality. It formalizes and composes these transformations to ensure effectiveness and diversity.

Result: Auto-SPT produces more diverse transformations compared to existing methods and significantly reduces the performance of state-of-the-art clone detectors, demonstrating their vulnerability. It also shows potential in improving robustness of models by augmenting training datasets.

Conclusion: Auto-SPT effectively bridges the gap between training and real-world conditions for code clone detection models, offering a robust framework for generating transformations and enhancing dataset quality for improved model resilience.

Abstract: Machine learning (ML) models for code clone detection determine whether two pieces of code are semantically equivalent, which in turn is a key building block for software-engineering tasks like refactoring and security tasks like vulnerability and malware detection. While these models are predominantly trained on clean, structured code datasets, real-world code often undergoes a variety of semantic-preserving transformations, including refactoring, minification, automated formatting, and compiler optimizations. To address this critical gap between training and test data, we propose Auto-SPT, a novel framework to automatically construct synthetic-data generators for code. Auto-SPT is designed to produce Semantic Preserving Transformations (SPTs) that alter a program's syntactic structure while preserving its functionality and is instantiated on top of Large Language Models (LLMs). In particular, we use LLMs to craft a diverse set of SPTs, generate strong implementations for these SPTs, and compose them to result into strong transformations. Our formal analysis shows that the diversity of SPTs impacts the strength of their composition. We then empirically demonstrate that Auto-SPT generates more diverse SPTs than existing approaches and these SPTs significantly drop the performance of state-of-the-art code clone detectors. Further experiments show Auto-SPT can be used to enhance code datasets for training, to produce code-clone detection models that are robust to real-world, adversarial code transformations.

</details>


### [584] [Beyond Prototyping: Autonomous, Enterprise-Grade Frontend Development from Pixel to Production via a Specialized Multi-Agent Framework](https://arxiv.org/abs/2512.06046)
*Ramprasath Ganesaraja,Swathika N,Saravanan AP,Kamalkumar Rathinasamy,Chetana Amancharla,Rahul Das,Sahil Dilip Panse,Aditya Batwe,Dileep Vijayan,Veena Ashok,Thanushree A P,Kausthubh J Rao,Alden Olivero,Roshan,Rajeshwar Reddy Manthena,Asmitha Yuga Sre A,Harsh Tripathi,Suganya Selvaraj,Vito Chin,Kasthuri Rangan Bhaskar,Kasthuri Rangan Bhaskar,Venkatraman R,Sajit Vijayakumar*

Main category: cs.SE

TL;DR: AI4UI is a framework designed for autonomous front-end development focusing on production-grade requirements, emphasizing secure, compliant, scalable, and maintainable UI code for enterprises.


<details>
  <summary>Details</summary>
Motivation: The motivation behind AI4UI is to address the limitations of general-purpose code assistants which are better suited for rapid prototyping but lack enterprise-readiness. It focuses on producing secure, compliant, and maintainable UI code tailored for enterprise environments.

Method: AI4UI integrates human-in-the-loop interaction during design (to embed Gen-AI-friendly grammar in Figma prototypes) and post-processing stages (to refine outputs). It autonomously converts Figma designs into UI code using methods like domain-aware knowledge graphs, secure integration methods, and architecture templates coordinated by specialized AI agent roles.

Result: AI4UI achieved significant performance in benchmarks, with over 87% code compilation success, 78% feature implementation, and 73.5% code-review quality. It also generates validated UI screens at a much faster rate than traditional methods.

Conclusion: AI4UI demonstrates enterprise-level potency by delivering production-grade UI code efficiently and accurately compared to competitors, showcasing its potential to revolutionize front-end development in enterprise settings.

Abstract: We present AI4UI, a framework of autonomous front-end development agents purpose-built to meet the rigorous requirements of enterprise-grade application delivery. Unlike general-purpose code assistants designed for rapid prototyping, AI4UI focuses on production readiness delivering secure, scalable, compliant, and maintainable UI code integrated seamlessly into enterprise workflows. AI4UI operates with targeted human-in-the-loop involvement: at the design stage, developers embed a Gen-AI-friendly grammar into Figma prototypes to encode requirements for precise interpretation; and at the post processing stage, domain experts refine outputs for nuanced design adjustments, domain-specific optimizations, and compliance needs. Between these stages, AI4UI runs fully autonomously, converting designs into engineering-ready UI code. Technical contributions include a Figma grammar for autonomous interpretation, domain-aware knowledge graphs, a secure abstract/package code integration strategy, expertise driven architecture templates, and a change-oriented workflow coordinated by specialized agent roles. In large-scale benchmarks against industry baselines and leading competitor systems, AI4UI achieved 97.24% platform compatibility, 87.10% compilation success, 86.98% security compliance, 78.00% feature implementation success, 73.50% code-review quality, and 73.36% UI/UX consistency. In blind preference studies with 200 expert evaluators, AI4UI emerged as one of the leaders demonstrating strong competitive standing among leading solutions. Operating asynchronously, AI4UI generates thousands of validated UI screens in weeks rather than months, compressing delivery timeline

</details>


### [585] [Reinforcement Learning Integrated Agentic RAG for Software Test Cases Authoring](https://arxiv.org/abs/2512.06060)
*Mohanakrishnan Hariharan*

Main category: cs.SE

TL;DR: This paper presents a framework integrating reinforcement learning with autonomous agents to continuously improve software test case generation from business requirements, addressing limitations of static systems.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of static test generation systems by enabling adaptive learning for generating superior test cases from QE feedback.

Method: A Reinforcement Infused Agentic RAG system using hybrid knowledge bases and RL algorithms (PPO and DQN) to refine AI test case generation through QE input.

Result: The framework delivered substantive improvements on Apple enterprise projects, achieving a 2.4% increase in test generation accuracy and a 10.8% rise in defect detection rates.

Conclusion: The system introduces a QE-driven knowledge refinement loop that progressively enhances test case quality, complementing human testing efforts.

Abstract: This paper introduces a framework that integrates reinforcement learning (RL) with autonomous agents to enable continuous improvement in the automated process of software test cases authoring from business requirement documents within Quality Engineering (QE) workflows. Conventional systems employing Large Language Models (LLMs) generate test cases from static knowledge bases, which fundamentally limits their capacity to enhance performance over time. Our proposed Reinforcement Infused Agentic RAG (Retrieve, Augment, Generate) framework overcomes this limitation by employing AI agents that learn from QE feedback, assessments, and defect discovery outcomes to automatically improve their test case generation strategies. The system combines specialized agents with a hybrid vector-graph knowledge base that stores and retrieves software testing knowledge. Through advanced RL algorithms, specifically Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN), these agents optimize their behavior based on QE-reported test effectiveness, defect detection rates, and workflow metrics. As QEs execute AI-generated test cases and provide feedback, the system learns from this expert guidance to improve future iterations. Experimental validation on enterprise Apple projects yielded substantive improvements: a 2.4% increase in test generation accuracy (from 94.8% to 97.2%), and a 10.8% improvement in defect detection rates. The framework establishes a continuous knowledge refinement loop driven by QE expertise, resulting in progressively superior test case quality that enhances, rather than replaces, human testing capabilities.

</details>


### [586] [Toward Patch Robustness Certification and Detection for Deep Learning Systems Beyond Consistent Samples](https://arxiv.org/abs/2512.06123)
*Qilin Zhou,Zhengyuan Wei,Haipeng Wang,Zhuo Wang,W. K. Chan*

Main category: cs.SE

TL;DR: This paper introduces HiCert, a method for certified detection to improve robustness against adversarial patch attacks in deep learning systems.


<details>
  <summary>Details</summary>
Motivation: Existing certified detection methods struggle with samples misclassified or with inconsistently predicted mutants, limiting their effectiveness against adversarial patch attacks.

Method: HiCert employs masking-based certified detection, deriving formal relations between harmful samples and benign counterparts, and imposes a confidence-based checking mechanism for comprehensive certification.

Result: HiCert demonstrates state-of-the-art performance by certifying more benign samples—both consistent and inconsistent—with higher accuracy and a significantly reduced false silent ratio.

Conclusion: HiCert advances patch robustness certification by systematically addressing inconsistencies in sample predictions, setting a new benchmark in certified detection efficiency.

Abstract: Patch robustness certification is an emerging kind of provable defense technique against adversarial patch attacks for deep learning systems. Certified detection ensures the detection of all patched harmful versions of certified samples, which mitigates the failures of empirical defense techniques that could (easily) be compromised. However, existing certified detection methods are ineffective in certifying samples that are misclassified or whose mutants are inconsistently pre icted to different labels. This paper proposes HiCert, a novel masking-based certified detection technique. By focusing on the problem of mutants predicted with a label different from the true label with our formal analysis, HiCert formulates a novel formal relation between harmful samples generated by identified loopholes and their benign counterparts. By checking the bound of the maximum confidence among these potentially harmful (i.e., inconsistent) mutants of each benign sample, HiCert ensures that each harmful sample either has the minimum confidence among mutants that are predicted the same as the harmful sample itself below this bound, or has at least one mutant predicted with a label different from the harmful sample itself, formulated after two novel insights. As such, HiCert systematically certifies those inconsistent samples and consistent samples to a large extent. To our knowledge, HiCert is the first work capable of providing such a comprehensive patch robustness certification for certified detection. Our experiments show the high effectiveness of HiCert with a new state-of the-art performance: It certifies significantly more benign samples, including those inconsistent and consistent, and achieves significantly higher accuracy on those samples without warnings and a significantly lower false silent ratio.

</details>


### [587] [Systematically Thinking about the Complexity of Code Structuring Exercises at Introductory Level](https://arxiv.org/abs/2512.06178)
*Georgiana Haldeman,Peter Ohmann,Paul Denny*

Main category: cs.SE

TL;DR: This paper presents a framework to assess the complexity of code structuring tasks in teaching computational thinking, with emphasis on decomposition and abstraction (DA).


<details>
  <summary>Details</summary>
Motivation: The motivation is to explicitly integrate decomposition and abstraction (DA) tasks into introductory programming education as they are often underemphasized, especially in light of AI reducing the need for syntax-focused programming.

Method: The authors propose a framework with three dimensions of task complexity—repetition, code pattern, and data dependency—and provide examples and an interactive tool for generating DA problems.

Result: A systematic framework that categorizes DA task complexity was created along with an interactive tool for educators to generate practical DA problems.

Conclusion: This work aims to enhance DA skill-building in procedural programming through structured educational tasks, benefiting programming education in the era of generative AI.

Abstract: Decomposition and abstraction is an essential component of computational thinking, yet it is not always emphasized in introductory programming courses. In addition, as generative AI further reduces the focus on syntax and increases the importance of higher-level code reasoning, there is renewed opportunity to teach DA explicitly. In this paper, we introduce a framework for systematically assessing the complexity of code structuring tasks, where students must identify and separate meaningful abstractions within existing, unstructured code. The framework defines three dimensions of task complexity, each with multiple levels: repetition, code pattern, and data dependency. To support practical use, we provide example tasks mapped to these levels and offer an interactive tool for generating and exploring DA problems. The framework is designed to support the development of educational tasks that build students' skills with DA in the procedural paradigm.

</details>


### [588] [Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs](https://arxiv.org/abs/2512.06836)
*Weixing Zhang,Regina Hebig,Daniel Strüber*

Main category: cs.SE

TL;DR: The study examines using Large Language Models for co-evolving DSLs and textual instances, focusing on preserving auxiliary information like comments and layouts. Results highlight feasibility for small cases but report scalability challenges.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the gap in techniques for co-evolving DSLs with textual syntax without losing auxiliary information like comments and layout information.

Method: Experiments using Claude-3.5 and GPT-4o across seven case languages to test grammar and instance co-evolution while preserving auxiliary information.

Result: LLMs showed good performance for small-scale cases but suffered scalability challenges with larger instances, limiting their practical application.

Conclusion: While LLMs demonstrate potential for preserving auxiliary information in small-scale textual instance migrations, further work is needed to improve their scalability and applicability to larger cases.

Abstract: Software languages evolve over time for various reasons, such as the addition of new features. When the language's grammar definition evolves, textual instances that originally conformed to the grammar become outdated. For DSLs in a model-driven engineering context, there exists a plethora of techniques to co-evolve models with the evolving metamodel. However, these techniques are not geared to support DSLs with a textual syntax -- applying them to textual language definitions and instances may lead to the loss of information from the original instances, such as comments and layout information, which are valuable for software comprehension and maintenance. This study explores the potential of Large Language Model (LLM)-based solutions in achieving grammar and instance co-evolution, with attention to their ability to preserve auxiliary information when directly processing textual instances. By applying two advanced language models, Claude-3.5 and GPT-4o, and conducting experiments across seven case languages, we evaluated the feasibility and limitations of this approach. Our results indicate a good ability of the considered LLMs for migrating textual instances in small-scale cases with limited instance size, which are representative of a subset of cases encountered in practice. In addition, we observe significant challenges with the scalability of LLM-based solutions to larger instances, leading to insights that are useful for informing future research.

</details>


### [589] [DUET: Agentic Design Understanding via Experimentation and Testing](https://arxiv.org/abs/2512.06247)
*Gus Henry Smith,Sandesh Adhikary,Vineet Thumuluri,Karthik Suresh,Vivek Pandit,Kartik Hegde,Hamid Shojaei,Chandra Bhagavatula*

Main category: cs.SE

TL;DR: The paper introduces DUET, a methodology enabling AI agents to better understand and solve hardware design problems through iterative experimentation and testing.


<details>
  <summary>Details</summary>
Motivation: This paper addresses the challenge that AI agents with large language models face when dealing with low-level, complex SystemVerilog-based RTL hardware design due to their difficulty in interpreting its time-evolving behaviors.

Method: The authors present DUET, a method inspired by how hardware experts iteratively understand designs. DUET utilizes hypothesis generation, testing via EDA tools (e.g., simulation, waveform inspection, verification), and integrates insights from these processes to grasp complex RTL designs.

Result: DUET demonstrates improved performance in hardware design-related tasks, particularly formal verification, outperforming baseline approaches without experimental iterations.

Conclusion: The DUET methodology significantly enhances AI agents' capability in understanding and solving hardware design tasks by mimicking expert iterative experimentation processes.

Abstract: AI agents powered by large language models (LLMs) are being used to solve increasingly complex software engineering challenges, but struggle with hardware design tasks. Register Transfer Level (RTL) code presents a unique challenge for LLMs, as it encodes complex, dynamic, time-evolving behaviors using the low-level language features of SystemVerilog. LLMs struggle to infer these complex behaviors from the syntax of RTL alone, which limits their ability to complete all downstream tasks like code completion, documentation, or verification. In response to this issue, we present DUET: a general methodology for developing Design Understanding via Experimentation and Testing. DUET mimics how hardware design experts develop an understanding of complex designs: not just via a one-off readthrough of the RTL, but via iterative experimentation using a number of tools. DUET iteratively generates hypotheses, tests them with EDA tools (e.g., simulation, waveform inspection, and formal verification), and integrates the results to build a bottom-up understanding of the design. In our evaluations, we show that DUET improves AI agent performance on formal verification, when compared to a baseline flow without experimentation.

</details>


### [590] [CFCEval: Evaluating Security Aspects in Code Generated by Large Language Models](https://arxiv.org/abs/2512.06248)
*Cheng Cheng,Jinqiu Yang*

Main category: cs.SE

TL;DR: This paper introduces CFCEval, a new framework for evaluating the quality and security of code generated by Large Language Models, addressing biases and limitations of existing evaluation methods like CodeBLEU.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in current evaluation methods for code-focused LLMs, such as dataset bias and the inadequacies of metrics like CodeBLEU, to better assess the quality and security of generated code.

Method: The authors propose CFCEval, which mitigates dataset bias using the MLVBench dataset and evaluates code across four dimensions. It also introduces a new metric, ELRM, to improve the relevance assessment between reference and generated code.

Result: CFCEval captures quality and security aspects of LLM-generated code more effectively, and its ELRM metric aligns more closely with human judgments than existing ones like CodeBLEU.

Conclusion: CFCEval establishes a more rigorous and comprehensive framework for evaluating code quality and security, enabling future advancements in code-focused LLMs.

Abstract: Code-focused Large Language Models (LLMs), such as CodeX and Star-Coder, have demonstrated remarkable capabilities in enhancing developer productivity through context-aware code generation. However, evaluating the quality and security of LLM-generated code remains a significant challenge. Existing evaluation protocols for Code LLMs lack both methodological rigor and comprehensive scope. A key limitation is dataset bias, which arises from unintentional overlap between training and testing data. Furthermore, while CodeBLEU, a BLEU-based metric, is widely used to assess code similarity, it suffers from critical shortcomings, including imprecise tokenization, structural limitations, and low reference diversity. To address these challenges, we introduce CFCEval, a novel framework for evaluating the quality and security of code generated by LLMs. CFCEval mitigates dataset bias by creating a new benchmark, MLVBench, and incorporates ELRM, a new metric designed to assess the relevance between reference code and generated code. CFCEval evaluates generated code across four dimensions: programming quality, vulnerability-fixing capability, post-transformation fixing capability, and relevance. Our experiments show that CFCEval not only captures both quality and security aspects of generated code more effectively but also that its ELRM aligns more closely with human judgments than CodeBLEU, thus paving the way for future advancements in Code LLMs evaluation.

</details>


### [591] [LLMCFG-TGen: Using LLM-Generated Control Flow Graphs to Automatically Create Test Cases from Use Cases](https://arxiv.org/abs/2512.06401)
*Zhenzhen Yang,Chenhui Cui,Tao Li,Rubing Huang,Nan Niu,Dave Towey,Shikai Guo*

Main category: cs.SE

TL;DR: The paper introduces LLMCFG-TGen, a method using large language models (LLMs) to generate structured Control Flow Graphs (CFGs) and corresponding test cases from natural language use-case descriptions, achieving complete path coverage and reducing manual effort.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current LLM-based test case generation methods in achieving comprehensive and accurate test coverage from natural language requirements, especially those involving complex conditional logic.

Method: The proposed three-step LLMCFG-TGen approach involves: (1) transforming natural language use-case descriptions into structured CFGs using LLMs, (2) enumerating all execution paths within these CFGs, and (3) generating test cases based on the identified execution paths.

Result: Experimental evaluations show that the approach achieves full path coverage, generates logically consistent and complete test cases, and is superior to baseline methods in terms of completeness and effectiveness.

Conclusion: The LLMCFG-TGen approach demonstrates the value of combining LLMs with structured representation to bridge the gap between natural language requirements and systematic test case generation, improving test coverage while reducing manual effort.

Abstract: Appropriate test case generation is critical in software testing, significantly impacting the quality of the testing. Requirements-Based Test Generation (RBTG) derives test cases from software requirements, aiming to verify whether or not the system's behaviors align with user needs and expectations. Requirements are often documented in Natural Language (NL), with use-case descriptions being a popular method for capturing functional behaviors and interaction flows in a structured form. Large Language Models (LLMs) have shown strong potential for automating test generation directly from NL requirements. However, current LLM-based approaches may not provide comprehensive, non-redundant coverage. They may also fail to capture complex conditional logic in requirements, resulting in incomplete test cases. We propose a new approach that automatically generates test cases from NL use-case descriptions, called Test Generation based on LLM-generated Control Flow Graphs (LLMCFG-TGen). LLMCFG-TGen comprises three main steps: (1) An LLM transforms a use case into a structured CFG that encapsulates all potential branches; (2) The generated CFG is explored, and all complete execution paths are enumerated; and (3) The execution paths are then used to generate the test cases. To evaluate our proposed approach, we conducted a series of experiments. The results show that LLMs can effectively construct well-structured CFGs from NL use cases. Compared with the baseline methods, LLMCFG-TGen achieves full path coverage, improving completeness and ensuring clear and accurate test cases. Practitioner assessments confirm that LLMCFG-TGen produces logically consistent and comprehensive test cases, while substantially reducing manual effort. The findings suggest that coupling LLM-based semantic reasoning with structured modeling effectively bridges the gap between NL requirements and systematic test generation.

</details>


### [592] [Translating PL/I Macro Procedures into Java Using Automatic Templatization and Large Language Models](https://arxiv.org/abs/2512.06448)
*Takaaki Tateishi,Yasuharu Katsuno*

Main category: cs.SE

TL;DR: The paper addresses the difficulty of translating PL/I macro procedures into Java and proposes a novel templatization method using symbolic execution to assist LLMs in improving translation outcomes.


<details>
  <summary>Details</summary>
Motivation: To modernize legacy systems by translating PL/I code into Java while addressing the complexity posed by PL/I macro procedures that generate other code.

Method: The paper introduces templatization, which employs symbolic execution to produce intermediate code templates. These templates assist LLMs in generating more accurate and maintainable Java code.

Result: The proposed method was tested on ten PL/I macro procedures, and the LLM-based translation through templatization accurately replicated the behavior of macro-generated PL/I programs.

Conclusion: The templatization process enhances the capability of LLMs to handle PL/I macro procedures during code translation, resulting in effective conversion to Java that retains functionality and improves readability.

Abstract: Modernizing legacy enterprise systems often involves translating PL/I programs into modern languages such as Java. This task becomes significantly more complex when PL/I macro procedures are involved. The PL/I macro procedures are considered string-manipulating programs that generate PL/I code, and they make automated translation more complex. Recently, large language models (LLMs) have been explored for automated code translation. However, LLM-based code translation struggles to translate the PL/I macro procedures to Java programs that reproduce the behavior of the plain PL/I code generated by the original PL/I macro procedures.
  This paper proposes a novel method called templatization, which uses symbolic execution to generate code templates (code with named placeholders) as an intermediate representation. In this approach, symbolic values are treated as parts of macro-generated code. By symbolically executing macro procedures and generating code templates, our approach facilitates LLMs to generate readable and maintainable Java code. Our preliminary experiment on ten PL/I macro procedures shows that the LLM-based translation through templatization successfully generates Java programs that reproduce the behavior of the macro-generated PL/I programs.

</details>


### [593] [METRION: A Framework for Accurate Software Energy Measurement](https://arxiv.org/abs/2512.06806)
*Benjamin Weigell,Simon Hornung,Bernhard Bauer*

Main category: cs.SE

TL;DR: The paper addresses the environmental impact of IT sector energy usage, presenting a model for measuring application-level energy consumption.


<details>
  <summary>Details</summary>
Motivation: To address the environmental impact of rising energy consumption in IT, particularly in identifying and quantifying energy optimization at the application and infrastructure levels.

Method: The paper introduces METRION, an energy attribution model that estimates CPU and DRAM energy use on a thread level, including cross-platform support (Linux, Intel CPUs), and evaluates its accuracy in various scenarios.

Result: METRION accurately measures CPU energy consumption with a Mean Absolute Percentage Error of 4.2% and DRAM energy consumption with a 16.1% error.

Conclusion: The model facilitates better energy optimization through accurate cross-platform energy attribution in IT applications, with potential to mitigate the sector's environmental impact.

Abstract: The Information and Communication Technology sector accounted for approximately 1.4% of global greenhouse gas emissions and 4% of the world's electricity consumption in 2020, with both expected to rise. To reduce this environmental impact, optimization strategies are employed to reduce energy consumption at the IT infrastructure and application levels. However, effective optimization requires, firstly, the identification of major energy consumers and, secondly, the ability to quantify whether an optimization has achieved the intended energy savings. Accurate determination of application-level energy consumption is thus essential. Therefore, we introduce an energy attribution model that quantifies the energy consumption of applications on CPU and DRAM at the thread level, considering the influence of Simultaneous Multithreading, frequency scaling, multi-socket architectures, and Non-Uniform Memory Access. To ensure cross-platform applicability, we integrate the proposed model into an extensible framework, METRION, including a platform-independent data model and an initial implementation for Linux systems using Intel CPUs. We evaluate METRION across three different workloads and demonstrate that the energy attribution model can accurately capture the CPU energy consumption of applications targeting solely the CPU with a Mean Absolute Percentage Error of 4.2%, and the DRAM energy consumption of applications targeting DRAM with an 16.1% error.

</details>


### [594] [BabelCoder: Agentic Code Translation with Specification Alignment](https://arxiv.org/abs/2512.06902)
*Fazle Rabbi,Soumit Kanti Saha,Tri Minh Triet Pham,Song Wang,Jinqiu Yang*

Main category: cs.SE

TL;DR: The paper proposes BabelCoder, a framework for cross-language code translation using specialized agents, outperforming baselines in accuracy evaluations.


<details>
  <summary>Details</summary>
Motivation: The challenge of translating code across programming languages automatically due to accuracy limitations, lack of context leveraging, and missing structured, collaborative frameworks.

Method: Introduction of BabelCoder, a multi-agentic framework comprising distinct agents for translation, testing, and refinement, each focusing on specific tasks to improve accuracy.

Result: BabelCoder achieved superior performance compared to state-of-the-art methods, showing a 0.5%-13.5% improvement in 94% of cases across benchmarks, with an average accuracy of 94.16%.

Conclusion: BabelCoder's agentic framework demonstrates an effective method for enhancing cross-language code translation through collaboration and specialization among agents, setting a new benchmark in the field.

Abstract: As software systems evolve, developers increasingly work across multiple programming languages and often face the need to migrate code from one language to another. While automatic code translation offers a promising solution, it has long remained a challenging task. Recent advancements in Large Language Models (LLMs) have shown potential for this task, yet existing approaches remain limited in accuracy and fail to effectively leverage contextual and structural cues within the code. Prior work has explored translation and repair mechanisms, but lacks a structured, agentic framework where multiple specialized agents collaboratively improve translation quality. In this work, we introduce BabelCoder, an agentic framework that performs code translation by decomposing the task into specialized agents for translation, testing, and refinement, each responsible for a specific aspect such as generating code, validating correctness, or repairing errors. We evaluate BabelCoder on four benchmark datasets and compare it against four state-of-the-art baselines. BabelCoder outperforms existing methods by 0.5%-13.5% in 94% of cases, achieving an average accuracy of 94.16%.

</details>


### [595] [MINES: Explainable Anomaly Detection through Web API Invariant Inference](https://arxiv.org/abs/2512.06906)
*Wenjie Zhang,Yun Lin,Chun Fung Amos Kwok,Xiwen Teoh,Xiaofei Xie,Frank Liauw,Hongyu Zhang,Jin Song Dong*

Main category: cs.SE

TL;DR: MINES detects web application anomalies by using schema-level API invariants, rather than relying on noisy and similar logs, enhancing anomaly detection precision and explainability.


<details>
  <summary>Details</summary>
Motivation: Web applications are critical infrastructures and are vulnerable to anomalies caused by attacks or errors. Existing anomaly detection methods fail due to noisy log data and insufficient discriminative information in logs.

Method: MINES converts API signatures into enhanced database schema, infers potential database constraints using LLMs, and refines these constraints with log data. The final constraints are translated into Python code to verify runtime logs for anomalies.

Result: MINES outperforms existing methods in anomaly detection on benchmarks, achieving high recall and almost zero false positives.

Conclusion: MINES sets a new standard in anomaly detection by leveraging schema-level API invariants for precise and explainable detection of web application anomalies.

Abstract: Detecting the anomalies of web applications, important infrastructures for running modern companies and governments, is crucial for providing reliable web services. Many modern web applications operate on web APIs (e.g., RESTful, SOAP, and WebSockets), their exposure invites intended attacks or unintended illegal visits, causing abnormal system behaviors. However, such anomalies can share very similar logs with normal logs, missing crucial information (which could be in database) for log discrimination. Further, log instances can be also noisy, which can further mislead the state-of-the-art log learning solutions to learn spurious correlation, resulting superficial models and rules for anomaly detection. In this work, we propose MINES which infers explainable API invariants for anomaly detection from the schema level instead of detailed raw log instances, which can (1) significantly discriminate noise in logs to identify precise normalities and (2) detect abnormal behaviors beyond the instrumented logs. Technically, MINES (1) converts API signatures into table schema to enhance the original database shema; and (2) infers the potential database constraints on the enhanced database schema to capture the potential relationships between APIs and database tables. MINES uses LLM for extracting potential relationship based on two given table structures; and use normal log instances to reject and accept LLM-generated invariants. Finally, MINES translates the inferred constraints into invariants to generate Python code for verifying the runtime logs. We extensively evaluate MINES on web-tamper attacks on the benchmarks of TrainTicket, NiceFish, Gitea, Mastodon, and NextCloud against baselines such as LogRobust, LogFormer, and WebNorm. The results show that MINES achieves high recall for the anomalies while introducing almost zero false positives, indicating a new state-of-the-art.

</details>


### [596] [Multi-Docker-Eval: A `Shovel of the Gold Rush' Benchmark on Automatic Environment Building for Software Engineering](https://arxiv.org/abs/2512.06915)
*Kelin Fu,Tianyu Liu,Zeyu Shang,Yingwei Ma,Jian Yang,Jiaheng Liu,Kaigui Bian*

Main category: cs.SE

TL;DR: The paper introduces the Multi-Docker-Eval benchmark to evaluate the efficiency and success of automated environment configuration in software engineering, revealing low success rates and offering insights on influencing factors.


<details>
  <summary>Details</summary>
Motivation: Automated environment configuration is a crucial challenge in scaling software engineering automation, requiring reliable evaluation metrics to improve the process.

Method: The authors developed the Multi-Docker-Eval benchmark that includes 40 real-world repositories across 9 programming languages. They evaluate success and efficiency of state-of-the-art large language models (LLMs) and agent frameworks under realistic constraints.

Result: Key findings include low overall success rates (maximum F2P of 37.7%) for current models, with environment construction as the major bottleneck. Model size and reasoning length were found not to be decisive, while open-source models and certain programming languages had a significant impact.

Conclusion: The study provides valuable insights and actionable recommendations for building scalable, fully automated software engineering pipelines by focusing on overcoming current limitations in environment configuration.

Abstract: Automated environment configuration is a critical bottleneck in scaling software engineering (SWE) automation. To provide a reliable evaluation standard for this task, we present Multi-Docker-Eval benchmark. It includes 40 real-world repositories spanning 9 programming languages and measures both success in achieving executable states and efficiency under realistic constraints. Our extensive evaluation of state-of-the-art LLMs and agent frameworks reveals key insights: (1) the overall success rate of current models is low (F2P at most 37.7%), with environment construction being the primary bottleneck; (2) model size and reasoning length are not decisive factors, and open-source models like DeepSeek-V3.1 and Kimi-K2 are competitive in both efficiency and effectiveness; (3) agent framework and programming language also have significantly influence on success rate. These findings provide actionable guidelines for building scalable, fully automated SWE pipelines.

</details>


### [597] [Reformulate, Retrieve, Localize: Agents for Repository-Level Bug Localization](https://arxiv.org/abs/2512.07022)
*Genevieve Caumartin,Glaucia Melo*

Main category: cs.SE

TL;DR: The paper explores the use of an LLM-powered agent to enhance bug localization through query reformulation and summarization, achieving significant performance improvements over baseline methods.


<details>
  <summary>Details</summary>
Motivation: Bug localization in large software systems is time-consuming and inaccurate due to noisy bug descriptions. Existing methods struggle to handle this issue effectively, necessitating new approaches for better results.

Method: The authors implemented a non-fine-tuned LLM for extracting key bug report details (e.g., identifiers, code snippets) and performed query reformulation. BM25-based retrieval was orchestrated by the agent using these queries to automate bug localization processes.

Result: The LLM-powered agent improved first-file retrieval ranking by 35% over the BM25 baseline and achieved a 22% file retrieval performance increase compared to SWE-agent.

Conclusion: The study demonstrates that LLM-assisted query reformulation and summarization significantly enhance bug localization, offering a scalable, automated approach for large software repositories.

Abstract: Bug localization remains a critical yet time-consuming challenge in large-scale software repositories. Traditional information retrieval-based bug localization (IRBL) methods rely on unchanged bug descriptions, which often contain noisy information, leading to poor retrieval accuracy. Recent advances in large language models (LLMs) have improved bug localization through query reformulation, yet the effect on agent performance remains unexplored. In this study, we investigate how an LLM-powered agent can improve file-level bug localization via lightweight query reformulation and summarization. We first employ an open-source, non-fine-tuned LLM to extract key information from bug reports, such as identifiers and code snippets, and reformulate queries pre-retrieval. Our agent then orchestrates BM25 retrieval using these preprocessed queries, automating localization workflow at scale. Using the best-performing query reformulation technique, our agent achieves 35% better ranking in first-file retrieval than our BM25 baseline and up to +22% file retrieval performance over SWE-agent.

</details>


### [598] [RisConFix: LLM-based Automated Repair of Risk-Prone Drone Configurations](https://arxiv.org/abs/2512.07122)
*Liping Han,Tingting Nie,Le Yu,Mingzhe Hu,Tao Yue*

Main category: cs.SE

TL;DR: This paper introduces RisConFix, a real-time repair mechanism for drones using a Large Language Model (LLM) to address risk-prone configurations affecting robustness. It achieved a 97% repair success rate in experiments.


<details>
  <summary>Details</summary>
Motivation: Flight control software can lead to unstable behaviors due to certain configurations, highlighting the need for adaptive, real-time corrective mechanisms for drone robustness.

Method: RisConFix uses Large Language Models to monitor drones' flight states, detect anomalies, and iteratively generate corrective configuration updates to restore stability in real time.

Result: Experimental evaluation on ArduPilot demonstrated RisConFix's effectiveness, achieving a repair success rate of 97% and an average of 1.17 repair cycles for faulty configurations.

Conclusion: RisConFix successfully and efficiently addresses unstable configurations in drones, leveraging LLMs for robustness and stability in real-time operations.

Abstract: Flight control software is typically designed with numerous configurable parameters governing multiple functionalities, enabling flexible adaptation to mission diversity and environmental uncertainty. Although developers and manufacturers usually provide recommendations for these parameters to ensure safe and stable operations, certain combinations of parameters with recommended values may still lead to unstable flight behaviors, thereby degrading the drone's robustness. To this end, we propose a Large Language Model (LLM) based approach for real-time repair of risk-prone configurations (named RisConFix) that degrade drone robustness. RisConFix continuously monitors the drone's operational state and automatically triggers a repair mechanism once abnormal flight behaviors are detected. The repair mechanism leverages an LLM to analyze relationships between configuration parameters and flight states, and then generates corrective parameter updates to restore flight stability. To ensure the validity of the updated configuration, RisConFix operates as an iterative process; it continuously monitors the drone's flight state and, if an anomaly persists after applying an update, automatically triggers the next repair cycle. We evaluated RisConFix through a case study of ArduPilot (with 1,421 groups of misconfigurations). Experimental results show that RisConFix achieved a best repair success rate of 97% and an optimal average number of repairs of 1.17, demonstrating its capability to effectively and efficiently repair risk-prone configurations in real time.

</details>


### [599] [Towards Benchmarking Design Pattern Detection Under Obfuscation: Reproducing and Evaluating Attention-Based Detection Method](https://arxiv.org/abs/2512.07193)
*Manthan Shenoy,Andreas Rausch*

Main category: cs.SE

TL;DR: This paper explores the semantic robustness of attention-based classifiers for detecting design patterns, revealing their dependency on superficial syntactic features and proposing an obfuscated corpus for benchmarking.


<details>
  <summary>Details</summary>
Motivation: To assess and improve the robustness of design pattern detection tools in capturing deeper semantic meanings rather than relying on superficial syntactic features.

Method: Reproducing the DPDAtt design pattern detection model, creating an obfuscated version of the corpus by modifying identifiers and preserving semantic structure, and evaluating the model under these conditions.

Result: The study shows that the DPDAtt classifier relies heavily on superficial syntactic features, suffering substantial performance degradation when these features are obfuscated.

Conclusion: The work emphasizes the necessity for more robust detection methods that generalize to deeper semantics in the source code and introduces an obfuscated corpus as a benchmark for this purpose.

Abstract: This paper investigates the semantic robustness of attention-based classifiers for design pattern detection, particularly focusing on their reliance on structural and behavioral semantics. We reproduce the DPDAtt, an attention-based design pattern detection approach using learning-based classifiers, and evaluate its performance under obfuscation. To this end, we curate an obfuscated version of the DPDAtt Corpus, where the name identifiers in code such as class names, method names, etc., and string literals like print statements and comment blocks are replaced while preserving control flow, inheritance, and logic. Our findings reveal that these trained classifiers in DPDAtt depend significantly on superficial syntactic features, leading to substantial misclassification when such cues are removed through obfuscation. This work highlights the need for more robust detection tools capable of capturing deeper semantic meanings in source code. We propose our curated Obfuscated corpus (containing 34 Java source files) as a reusable proof-of-concept benchmark for evaluating state-of-the-art design pattern detectors on their true semantic generalization capabilities.

</details>


### [600] [Automatic Syntax Error Repair for Discrete Controller Synthesis using Large Language Model](https://arxiv.org/abs/2512.07261)
*Yusei Ishimizu,Takuto Yamauchi,Sinan Chen,Jinyu Cai,Jialong Li,Kenji Tei*

Main category: cs.SE

TL;DR: The paper proposes an automated method using Large Language Models (LLMs) to correct syntax errors in Discrete Controller Synthesis (DCS) models, achieving higher accuracy and significant speed improvements.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address practical challenges in adopting Discrete Controller Synthesis (DCS), specifically syntax errors in formal models causing workflow disruptions and reduced productivity.

Method: The method utilizes Large Language Models (LLMs) guided by a knowledge-informed prompting strategy, derived from empirical studies on error patterns and domain-specific data, for syntax error repair in DCS models.

Result: The method demonstrated high effectiveness with improved repair accuracy and 3.46x speedup compared to human developers, validated through a benchmark of systematically corrupted DCS models.

Conclusion: The proposed approach successfully automates syntax error repair in DCS, enhancing productivity and usability, and offers a benchmark for further research in the field.

Abstract: Discrete Controller Synthesis (DCS) is a powerful formal method for automatically generating specifications of discrete event systems. However, its practical adoption is often hindered by the highly specialized nature of formal models written in languages such as FSP and FLTL. In practice, syntax errors in modeling frequently become an important bottleneck for developers-not only disrupting the workflow and reducing productivity, but also diverting attention from higher-level semantic design. To this end, this paper presents an automated approach that leverages Large Language Models (LLMs) to repair syntax errors in DCS models using a well-designed, knowledge-informed prompting strategy. Specifically, the prompting is derived from a systematic empirical study of common error patterns, identified through expert interviews and student workshops. It equips the LLM with DCS-specific domain knowledge, including formal grammar rules and illustrative examples, to guide accurate corrections. To evaluate our method, we constructed a new benchmark by systematically injecting realistic syntax errors into validated DCS models. The quantitative evaluation demonstrates the high effectiveness of the proposed approach in terms of repair accuracy and its practical utility regarding time, achieving a speedup of 3.46 times compared to human developers. The experimental replication suite, including the benchmark and prompts, is available at https://github.com/Uuusay1432/DCSModelRepair.git

</details>


### [601] [The Human Need for Storytelling: Reflections on Qualitative Software Engineering Research With a Focus Group of Experts](https://arxiv.org/abs/2512.07293)
*Roberto Verdecchia,Justus Bogner*

Main category: cs.SE

TL;DR: This paper discusses the evolution, significance, challenges, and future of qualitative research in software engineering through a dialogue with field experts.


<details>
  <summary>Details</summary>
Motivation: To reflect on the role and development of qualitative methods in the predominantly quantitative field of software engineering research.

Method: Focus group discussion with experts, including Rashina Hoda, Carolyn Seaman, and Klaas Stol, to explore various aspects of qualitative software engineering research.

Result: The discussion highlighted the growing importance of qualitative research in software engineering, challenges faced today, and potential future trajectories.

Conclusion: Qualitative research has increasingly contributed to understanding software engineering practices and can complement quantitative approaches, but barriers remain that need addressing for its broader adoption and impact.

Abstract: From its first adoption in the late 80s, qualitative research has slowly but steadily made a name for itself in what was, and perhaps still is, the predominantly quantitative software engineering (SE) research landscape. As part of our regular column on empirical software engineering (ACM SIGSOFT SEN-ESE), we reflect on the state of qualitative SE research with a focus group of experts. Among other things, we discuss why qualitative SE research is important, how it evolved over time, common impediments faced while practicing it today, and what the future of qualitative SE research might look like. Joining the conversation are Rashina Hoda (Monash University, Australia), Carolyn Seaman (University of Maryland, United States), and Klaas Stol (University College Cork, Ireland). The content of this paper is a faithful account of our conversation from October 25, 2025, which we moderated and edited for our column.

</details>


### [602] [Challenges in Developing Secure Software -- Results of an Interview Study in the German Software Industry](https://arxiv.org/abs/2512.07368)
*Alex R. Mattukat,Timo Langstrof,Horst Lichter*

Main category: cs.SE

TL;DR: The study explores challenges in developing secure software through interviews with industry experts, highlighting complexity, awareness issues, unsuitable processes, and skill gaps.


<details>
  <summary>Details</summary>
Motivation: Despite the existence of tools/frameworks for secure software, cybercrime remains a concern, prompting a need to understand development challenges.

Method: An interview study was conducted with 19 industry professionals from 12 companies across various industries.

Result: Identified challenges include complexity, security awareness deficiencies, unsuitable processes, and skill shortages.

Conclusion: The study sheds light on barriers to secure software and proposes research directions to address these issues.

Abstract: The damage caused by cybercrime makes the development of secure software inevitable. Although many tools and frameworks exist to support the development of secure software, statistics on cybercrime show no improvement in recent years. To understand the challenges software companies face in developing secure software, we conducted an interview study with 19 industry experts from 12 cross-industry companies. The results of our study show that the challenges are mainly due to high complexity, a lack of security awareness, and unsuitable processes, which are further exacerbated by an immediate lack of skilled personnel. This article presents our study and the challenges we identified, and derives potential research directions from them.

</details>


### [603] [Do LLMs Trust the Code They Write?](https://arxiv.org/abs/2512.07404)
*Francisco Ribeiro,Claudio Spiess,Prem Devanbu,Sarah Nadi*

Main category: cs.SE

TL;DR: This paper identifies representations of code correctness within LLMs and uses them to improve the selection of high-quality code, surpassing conventional ranking methods.


<details>
  <summary>Details</summary>
Motivation: Code generated by LLMs is often incorrect due to weak correlations between output probabilities and correctness.

Method: Contrast hidden states of LLMs between correct and incorrect code pairs to extract correctness representations, and use these representations to enhance code ranking and selection.

Result: Correctness representations outperform log-likelihood ranking and verbalized confidence in choosing quality code samples.

Conclusion: Leveraging internal correctness representations improves code generation reliability, fostering trust in LLM-generated code.

Abstract: Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.

</details>


### [604] [Systematic Evaluation of Black-Box Checking for Fast Bug Detection](https://arxiv.org/abs/2512.07434)
*Bram Pellen,María Belén Rodríguez,Frits Vaandrager,Petra van den Bos*

Main category: cs.SE

TL;DR: This study evaluates black-box checking (BBC) in detecting protocol and controller bugs, showing BBC's efficiency and effectiveness in bug detection and safety violation identification.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate black-box checking's (BBC) ability to detect bugs earlier and compare its efficiency against traditional methods, especially in identifying safety property violations and deep bugs.

Method: The study uses 77 real-world benchmark models to compare BBC against other approaches. Benchmark models include protocol implementations and controllers with available safety property specifications. Tests focused on BBC's query efficiency and bug detection capabilities, even with incomplete models.

Result: (a) BBC requires only 3.4% of the queries needed by traditional full-model-based methods to spot bugs. (b) Detects 94% of safety property violations in challenging benchmark tests like RERS 2019. (c) More effective than model-based testing (MBT) for deep bug detection in implementations.

Conclusion: BBC is a significantly efficient and effective tool for detecting safety violations and deep bugs, outperforming traditional MBT and full-model approaches.

Abstract: Combinations of active automata learning, model-based testing and model checking have been successfully used in numerous applications, e.g., for spotting bugs in implementations of major network protocols and to support refactoring of embedded controllers. However, in the large majority of these applications, model checking is only used at the very end, when no counterexample can be found anymore for the latest hypothesis model. This contrasts with the original proposal of black-box checking (BBC) by Peled, Vardi & Yannakakis, which applies model checking for all hypotheses, also the intermediate ones. In this article, we present the first systematic evaluation of the ability of BBC to find bugs quickly, based on 77 benchmarks models from real protocol implementations and controllers for which specifications of safety properties are available. Our main finding are: (a) In cases where the full model can be learned, BBC detects violations of the specifications with just 3.4% of the queries needed by an approach in which model checking is only used for the full model. (b) Even when the full model cannot be learned, BBC is still able to detect many violations of the specification. In particular, BBC manages to detect 94% of the safety properties violations in the challenging RERS 2019 industrial LTL benchmarks. (c) Our results also confirm that BBC is way more effective than existing MBT algorithms in finding deep bugs in implementations.

</details>


### [605] [AutoICE: Automatically Synthesizing Verifiable C Code via LLM-driven Evolution](https://arxiv.org/abs/2512.07501)
*Weilin Luo,Xueyi Liang,Haotian Deng,Yanan Liu,Hai Wan*

Main category: cs.SE

TL;DR: AutoICE uses LLM-driven evolutionary search for synthesizing verifiable C code while addressing errors and implicit knowledge, achieving high verification success rates.


<details>
  <summary>Details</summary>
Motivation: To solve the challenges of syntactic/semantic errors and ineffective formalizing of implicit knowledge in autoformalization efforts, especially with the rise of LLMs.

Method: AutoICE uses LLM-driven evolutionary search, including diverse initialization, collaborative crossover for iterative updates, and self-reflective mutation for uncovering implicit knowledge.

Result: AutoICE achieved a 90.36% verification success rate, outperforming the state-of-the-art approach. On a developer-friendly dataset, it achieved 88.33% success compared to 65% for the SOTA.

Conclusion: AutoICE demonstrates significant effectiveness and reliability in synthesizing verifiable C code, marking a considerable advancement over existing SOTA methods.

Abstract: Automatically synthesizing verifiable code from natural language requirements ensures software correctness and reliability while significantly lowering the barrier to adopting the techniques of formal methods. With the rise of large language models (LLMs), long-standing efforts at autoformalization have gained new momentum. However, existing approaches suffer from severe syntactic and semantic errors due to the scarcity of domain-specific pre-training corpora and often fail to formalize implicit knowledge effectively. In this paper, we propose AutoICE, an LLM-driven evolutionary search for synthesizing verifiable C code. It introduces the diverse individual initialization and the collaborative crossover to enable diverse iterative updates, thereby mitigating error propagation inherent in single-agent iterations. Besides, it employs the self-reflective mutation to facilitate the discovery of implicit knowledge. Evaluation results demonstrate the effectiveness of AutoICE: it successfully verifies $90.36$\% of code, outperforming the state-of-the-art (SOTA) approach. Besides, on a developer-friendly dataset variant, AutoICE achieves a $88.33$\% verification success rate, significantly surpassing the $65$\% success rate of the SOTA approach.

</details>


### [606] [Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach](https://arxiv.org/abs/2512.07814)
*Hua Yang,Alejandro Velasco,Sen Fang,Bowen Xu,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: The paper investigates the diverse privacy risks posed by large language models for code (LLM4Code) and demonstrates how different personally identifiable information (PII) types affect leakage likelihood.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns arising from LLM4Code reproducing sensitive personally identifiable information (PII) from open-source repositories and to understand variations in leakage risks among different PII types.

Method: The authors built a diverse dataset of PII types, fine-tuned models of varying sizes, computed training dynamics on real PII data, and applied a structural causal model to assess the causal effect of learnability on leakage.

Result: Leakage risks are type-dependent, with easy-to-learn PII such as IP addresses leaking more frequently, while difficult-to-learn types like keys and passwords leak less frequently. Ambiguous types show mixed patterns.

Conclusion: This study serves as the first causal evidence that leakage risks vary by PII type and proposes developing defenses that are type-aware and learnability-aware to mitigate privacy concerns in LLM4Code.

Abstract: Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.

</details>


### [607] [Studying the Role of Reusing Crowdsourcing Knowledge in Software Development](https://arxiv.org/abs/2512.07824)
*Rabe Abdalkareem*

Main category: cs.SE

TL;DR: The paper studies the effects of using crowdsourced knowledge from platforms like Stack Overflow on software projects, noting both benefits (increased productivity) and drawbacks (dependency issues). The study suggests using improved Continuous Integration (CI) processes to address associated risks.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is to explore the less studied area of how reusing crowdsourced knowledge impacts software quality, addressing gaps such as why developers rely on this knowledge and the risks associated with it.

Method: The researchers conducted large-scale empirical studies using prominent platforms like Stack Overflow and npm, analyzing the effects of crowdsourced knowledge reuse on software projects, and tested quality assurance methods like continuous integration (CI) to mitigate identified risks.

Result: The study found that while reusing crowdsourced knowledge boosts productivity and development efficiency, it introduces challenges like dependency overhead and increased maintenance efforts. Continuous Integration (CI) has been analyzed as a solution to mitigate these risks.

Conclusion: Reusing crowdsourced knowledge has significant potential to aid software development but poses quality challenges. Applying sound data-driven methods, such as improving CI, can help address risks and optimize the benefits for software projects.

Abstract: Crowdsourcing platforms, such as Stack Overflow, have changed and impacted the software development practice. In these platforms, developers share and reuse their software development and programming experience. Therefore, a plethora of research work focused on crowdsourcing in software engineering and showed that, among other things, crowdsourced development tends to increase developers' productivity and reduce time-to-market. However, in crowdsourcing, the empirical studies of software quality are lacking, and simple questions, such as what developers use the crowdsourcing knowledge for, are unanswered.
  Therefore, our research focused on studying the impact of reusing crowdsourcing knowledge on software projects. To do so, we conduct several large-scale empirical studies on some of the well-known crowdsourcing platforms, including Stack Overflow and npm. Our results showed that reusing knowledge from these crowdsourcing platforms has the potential to assist software development practice, specifically in the form of reusing crowdsourced code. However, using such knowledge affects the quality of the software in several aspects, such as making the software projects suffer from dependency overhead and increasing the maintenance effort. Based on these findings, we use the gained knowledge to make sound data-driven decisions where we examine software quality assurance methods to mitigate the risk of relying on crowd sourcing knowledge in software development. We examine the use of continuous integration (CI). Our analysis showed how CI can be improved to increase developers' productivity and save their resources.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [608] [Assessing the Information Content of Individual Spikes in Population-Level Models of Neural Spiking Activity](https://arxiv.org/abs/2512.06280)
*Azar Ghahari,Uri T. Eden*

Main category: q-bio.NC

TL;DR: This paper examines the efficiency of clusterless decoding in neural data analysis, especially focusing on entropy metrics from observed spikes, compared to traditional spike sorting methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore how clusterless decoding methods, which bypass spike sorting, extract information from neural activity and to understand their efficiency using information-theoretic metrics.

Method: The study uses information-theoretic metrics to measure entropy reduction in spatial coding between clusterless and spike-sorted models in the rat hippocampus, both for isolated spikes and spikes with previous prior information.

Result: Low-amplitude spikes provide less information in isolation compared to high-amplitude spikes, but both offer similar informational value when accounting for prior spiking information.

Conclusion: The analysis highlights the advantages of clusterless decoding, capturing more information from neural activity and providing insights into neural computation, particularly when combined with state-space modeling.

Abstract: In the last decade, there have been major advances in clusterless decoding algorithms for neural data analysis. These algorithms use the theory of marked point processes to describe the joint activity of many neurons simultaneously, without the need for spike sorting. In this study, we examine information-theoretic metrics to analyze the information extracted from each observed spike under such clusterless models. In an analysis of spatial coding in the rat hippocampus, we compared the entropy reduction between spike-sorted and clusterless models for both individual spikes observed in isolation and when the prior information from all previously observed spikes is accounted for. Our analysis demonstrates that low-amplitude spikes, which are difficult to cluster and often left out of spike sorting, provide reduced information compared to sortable, high-amplitude spikes when considered in isolation, but the two provide similar levels of information when considering all the prior information available from past spiking. These findings demonstrate the value of combining information measures with state-space modeling and yield new insights into the underlying mechanisms of neural computation.

</details>


### [609] [Quantification of Planar Cortical Magnification with Optimal Transport and Topological Smoothing](https://arxiv.org/abs/2512.06492)
*Yujian Xiong,Negar Jalili Mallak,Yanshuai Tu,Zhong-Lin Lu,Yalin Wang*

Main category: q-bio.NC

TL;DR: This paper introduces a pipeline to accurately quantify cortical magnification factor (CMF) using retinotopic maps, achieving improved anatomical precision through advanced methodologies.


<details>
  <summary>Details</summary>
Motivation: To address limitations in quantifying CMF from retinotopic maps due to low signal-to-noise ratios in fMRI data and inaccuracies in the topological relationships of retinotopic maps.

Method: The study proposed a pipeline using optimal transport for area-preserving 3D-to-2D projections, followed by topological smoothing and estimation of 2D CMF using the 1-ring patch method on retinotopic maps.

Result: The pipeline revealed novel CMF patterns across the visual field and individual variations among subjects using the HCP 7T and NYU 3T datasets, demonstrating reliable and repeatable results.

Conclusion: The proposed method provides enhanced analytical tools for exploring visual processing and uncovers new insights into the cortical magnification factor in humans.

Abstract: The human visual system exhibits non-uniform spatial resolution across the visual field, which is characterized by the cortical magnification factor (CMF) that reflects its anatomical basis. However, current approaches for quantifying CMF using retinotopic maps derived from BOLD functional magnetic resonance imaging (fMRI) are limited by the inherent low signal-to-noise ratio of fMRI data and inaccuracies in the topological relationships of the retinotopic maps. In this study, we introduced a new pipeline to quantify planar CMF from retinotopic maps generated from the population receptive field (pRF) model. The pipeline projected the 3D pRF solutions onto a 2D planar disk, using optimal transport (OT) to preserve local cortical surface areas, and applied topological smoothing to ensure that the resulting retinotopic maps maintain their topology. We then estimated 2D CMF maps from the projected retinotopic maps on the planar disk using the 1-ring patch method. Applying this pipeline to the Human Connectome Project (HCP) 7T dataset, we revealed previously unobserved CMF patterns across the visual field and demonstrated individual differences among the 181 subjects. The pipeline was further validated on the New York University (NYU) 3T dataset, showing reliable and repeatable results. Our study provided new analytical methods and offered novel insights into visual processing.

</details>


### [610] [Visual Function Profiles via Multi-Path Aggregation Reveal Neuron-Level Responses in the Drosophila Brain](https://arxiv.org/abs/2512.06934)
*Jiangping Xie,Ruohan Ren,Xiao Zhou,Ao Zheng,Jiasong Zhu,Wenyu Jiang,Ziran Zhao*

Main category: q-bio.NC

TL;DR: This paper introduces a Multi-Path Aggregation (MPA) framework for accurately predicting Drosophila neuron responses in visual tasks using whole-brain connectome data, achieving superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: The research aims to better understand and predict individual neuron responses and spatial functional properties in complex visual tasks, overcoming limitations of existing connectome models that are either parameter-heavy or lack predictive accuracy.

Method: A Multi-Path Aggregation (MPA) framework was developed, using neural network steady-state theory to integrate visual input features and brain connectome topology. This method leverages adjacency matrix powers and finite-path optimization to predict functional properties, such as ON/OFF polarity and direction selectivity, without redundant parameters.

Result: The model achieved a Pearson correlation of 0.84±0.12 for ON/OFF neuron responses, surpassing alternative methods (0.33±0.59), and demonstrated accuracy in capturing luminance and directional preferences. It also enabled simulations of neuron blockade effects and behavioral navigation tasks in a Drosophila model.

Conclusion: The study presents an innovative framework linking connectome data, functional neuron profiles, and behavioral understanding in Drosophila. This tool not only advances the study of visual computation but also provides insights for brain-inspired AI systems.

Abstract: Accurately predicting individual neurons' responses and spatial functional properties in complex visual tasks remains a key challenge in understanding neural computation. Existing whole-brain connectome models of Drosophila often rely on parameter assumptions or deep learning approaches, yet remain limited in their ability to reliably predict dynamic neuronal responses. We introduce a Multi-Path Aggregation (MPA) framework, based on neural network steady-state theory, to build a whole-brain Visual Function Profiles (VFP) of Drosophila neurons and predict their responses under diverse visual tasks. Unlike conventional methods relying on redundant parameters, MPA combines visual input features with the whole-brain connectome topology. It uses adjacency matrix powers and finite-path optimization to efficiently predict neuronal function, including ON/OFF polarity, direction selectivity, and responses to complex visual stimuli. Our model achieves a Pearson correlation of 0.84+/-0.12 for ON/OFF responses, outperforming existing methods (0.33+/-0.59), and accurately captures neuron functional properties, including luminance and direction preferences, while allowing single-neuron or population-level blockade simulations. Replacing CNN modules with VFP-derived Lobula Columnar(LC) population responses in a Drosophila simulation enables successful navigation and obstacle avoidance, demonstrating the model's effectiveness in guiding embodied behavior. This study establishes a "connectome-functional profile-behavior" framework, offering a whole-brain quantitative tool to study Drosophila visual computation and a neuron-level guide for brain-inspired intelligence.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [611] [Contextual Strongly Convex Simulation Optimization: Optimize then Predict with Inexact Solutions](https://arxiv.org/abs/2512.06270)
*Nifei Lin,Heng Luo,L. Jeff Hong*

Main category: stat.ML

TL;DR: This paper introduces a contextual strongly convex simulation optimization method using an "optimize then predict" approach for real-time decision making. It develops a framework analyzing solution bias and variance impacts, incorporating four smoothing techniques and demonstrating their convergence rates.


<details>
  <summary>Details</summary>
Motivation: To improve real-time decision-making by addressing the gap in understanding the effects of inexact solutions generated by simulation algorithms on the optimality gap.

Method: An "optimize then predict" framework involving offline simulation optimization to approximate solutions and online evaluation using smoothing techniques such as $k$ nearest neighbor, kernel smoothing, linear regression, and kernel ridge regression. A unified analysis framework handles solution bias and variance.

Result: The convergence rates for smoothing techniques are established, optimal computational budget allocation is derived, and convergence rate can potentially achieve $Γ^{-1}$. The framework's theoretical findings are validated through numerical simulations.

Conclusion: The proposed approach effectively improves real-time decision-making, with theoretical validation showing promising results for practical applications.

Abstract: In this work, we study contextual strongly convex simulation optimization and adopt an "optimize then predict" (OTP) approach for real-time decision making. In the offline stage, simulation optimization is conducted across a set of covariates to approximate the optimal-solution function; in the online stage, decisions are obtained by evaluating this approximation at the observed covariate. The central theoretical challenge is to understand how the inexactness of solutions generated by simulation-optimization algorithms affects the optimality gap, which is overlooked in existing studies. To address this, we develop a unified analysis framework that explicitly accounts for both solution bias and variance. Using Polyak-Ruppert averaging SGD as an illustrative simulation-optimization algorithm, we analyze the optimality gap of OTP under four representative smoothing techniques: $k$ nearest neighbor, kernel smoothing, linear regression, and kernel ridge regression. We establish convergence rates, derive the optimal allocation of the computational budget $Γ$ between the number of design covariates and the per-covariate simulation effort, and demonstrate the convergence rate can approximately achieve $Γ^{-1}$ under appropriate smoothing technique and sample-allocation rule. Finally, through a numerical study, we validate the theoretical findings and demonstrate the effectiveness and practical value of the proposed approach.

</details>


### [612] [Modeling Spatio-temporal Extremes via Conditional Variational Autoencoders](https://arxiv.org/abs/2512.06348)
*Xiaoyu Ma,Likun Zhang,Christopher K. Wikle*

Main category: stat.ML

TL;DR: The paper introduces a novel modeling technique for spatio-temporal extremes using a conditional variational autoencoder (cXVAE) with applications in climate data analysis.


<details>
  <summary>Details</summary>
Motivation: The research investigates the need to better model spatio-temporal extremes, especially as their co-occurrence can fluctuate under changing climate conditions.

Method: The authors used a conditional variational autoencoder (cXVAE) with a CNN-embedded decoder to integrate climate indices and model spatial extremes, including extensive simulations and real-world application to Fire Weather Index data.

Result: The cXVAE demonstrated high accuracy in emulating spatial fields, detecting condition-driven shifts, and enabling counterfactual experiments for quantifying changes in climate and joint tail risks.

Conclusion: The proposed cXVAE is scalable, efficient, and acts as a valuable tool for analyzing climate extremes under varying conditions, demonstrated through its application to the Fire Weather Index in eastern Australia.

Abstract: Extreme weather events are widely studied in fields such as agriculture, ecology, and meteorology. The spatio-temporal co-occurrence of extreme events can strengthen or weaken under changing climate conditions. In this paper, we propose a novel approach to model spatio-temporal extremes by integrating climate indices via a conditional variational autoencoder (cXVAE). A convolutional neural network (CNN) is embedded in the decoder to convolve climatological indices with the spatial dependence within the latent space, thereby allowing the decoder to be dependent on the climate variables. There are three main contributions here. First, we demonstrate through extensive simulations that the proposed conditional XVAE accurately emulates spatial fields and recovers spatially and temporally varying extremal dependence with very low computational cost post training. Second, we provide a simple, scalable approach to detecting condition-driven shifts and whether the dependence structure is invariant to the conditioning variable. Third, when dependence is found to be condition-sensitive, the conditional XVAE supports counterfactual experiments allowing intervention on the climate covariate and propagating the associated change through the learned decoder to quantify differences in joint tail risk, co-occurrence ranges, and return metrics. To demonstrate the practical utility and performance of the model in real-world scenarios, we apply our method to analyze the monthly maximum Fire Weather Index (FWI) over eastern Australia from 2014 to 2024 conditioned on the El Niño/Southern Oscillation (ENSO) index.

</details>


### [613] [Canonical Tail Dependence for Soft Extremal Clustering of Multichannel Brain Signals](https://arxiv.org/abs/2512.06435)
*Mara Sherlin Talento,Jordan Richards,Raphael Huser,Hernando Ombao*

Main category: stat.ML

TL;DR: This paper introduces a novel method to analyze extremal dependence between cortical brain regions during extreme signal amplitudes, aiding in seizure identification and risk management.


<details>
  <summary>Details</summary>
Motivation: Existing methods in brain connectivity analysis do not adequately characterize extreme events like seizures, nor do they identify specific channels driving maximal tail dependence.

Method: The authors extended canonical correlation techniques to tail dependence, proposing the Tail Pairwise Dependence Matrix (TPDM) for computational efficiency and soft clustering of signals.

Result: The proposed TPDM method successfully distinguished neonates with seizures from those without, showcasing enhanced discriminatory power for extreme brain events.

Conclusion: By focusing on tail connectivity, the study offers improved mechanisms for seizure detection and risk assessment, addressing limitations in traditional signal processing approaches.

Abstract: We develop a novel characterization of extremal dependence between two cortical regions of the brain when its signals display extremely large amplitudes. We show that connectivity in the tails of the distribution reveals unique features of extreme events (e.g., seizures) that can help to identify their occurrence. Numerous studies have established that connectivity-based features are effective for discriminating brain states. Here, we demonstrate the advantage of the proposed approach: that tail connectivity provides additional discriminatory power, enabling more accurate identification of extreme-related events and improved seizure risk management. Common approaches in tail dependence modeling use pairwise summary measures or parametric models. However, these approaches do not identify channels that drive the maximal tail dependence between two groups of signals -- an information that is useful when analyzing electroencephalography of epileptic patients where specific channels are responsible for seizure occurrences. A familiar approach in traditional signal processing is canonical correlation, which we extend to the tails to develop a visualization of extremal channel-contributions. Through the tail pairwise dependence matrix (TPDM), we develop a computationally-efficient estimator for our canonical tail dependence measure. Our method is then used for accurate frequency-based soft clustering of neonates, distinguishing those with seizures from those without.

</details>


### [614] [Latent Nonlinear Denoising Score Matching for Enhanced Learning of Structured Distributions](https://arxiv.org/abs/2512.06615)
*Kaichen Shen,Wei Zhu*

Main category: stat.ML

TL;DR: The paper introduces LNDSM, a method integrating nonlinear dynamics and latent SGMs for improved score-based generative modeling.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance of score-based generative models by integrating nonlinear forward dynamics with latent SGM frameworks.

Method: Reformulates the cross-entropy term using approximate Gaussian transitions and addresses numerical stability by removing variance-exploding terms.

Result: The method demonstrates faster synthesis, improved learning, better sample quality, and variability on MNIST dataset variants.

Conclusion: LNDSM effectively advances score-based generative modeling by improving speed, learning capability, and sample quality over benchmarks.

Abstract: We present latent nonlinear denoising score matching (LNDSM), a novel training objective for score-based generative models that integrates nonlinear forward dynamics with the VAE-based latent SGM framework. This combination is achieved by reformulating the cross-entropy term using the approximate Gaussian transition induced by the Euler-Maruyama scheme. To ensure numerical stability, we identify and remove two zero-mean but variance exploding terms arising from small time steps. Experiments on variants of the MNIST dataset demonstrate that the proposed method achieves faster synthesis and enhanced learning of inherently structured distributions. Compared to benchmark structure-agnostic latent SGMs, LNDSM consistently attains superior sample quality and variability.

</details>


### [615] [ADAM Optimization with Adaptive Batch Selection](https://arxiv.org/abs/2512.06795)
*Gyu Yeol Kim,Min-hwan Oh*

Main category: stat.ML

TL;DR: The paper introduces AdamCB, an optimizer integrating combinatorial bandit techniques into Adam for adaptive sample selection, achieving superior convergence and performance.


<details>
  <summary>Details</summary>
Motivation: Adam treats each sample equally during neural network training, which can lead to inefficient convergence. Previous attempts to address this using bandit frameworks lacked strong theoretical guarantees.

Method: The authors propose AdamCB, which combines combinatorial bandit sampling methods with Adam. AdamCB utilizes feedback from multiple samples simultaneously to improve convergence and efficiency.

Result: AdamCB demonstrates faster convergence theoretically compared to other Adam-based methods. Numerical experiments reveal that AdamCB consistently outperforms existing optimizers.

Conclusion: AdamCB provides both enhanced theoretical guarantees and practical performance improvements, addressing limitations in previous methods effectively.

Abstract: Adam is a widely used optimizer in neural network training due to its adaptive learning rate. However, because different data samples influence model updates to varying degrees, treating them equally can lead to inefficient convergence. To address this, a prior work proposed adapting the sampling distribution using a bandit framework to select samples adaptively. While promising, the bandit-based variant of Adam suffers from limited theoretical guarantees. In this paper, we introduce Adam with Combinatorial Bandit Sampling (AdamCB), which integrates combinatorial bandit techniques into Adam to resolve these issues. AdamCB is able to fully utilize feedback from multiple samples at once, enhancing both theoretical guarantees and practical performance. Our regret analysis shows that AdamCB achieves faster convergence than Adam-based methods including the previous bandit-based variant. Numerical experiments demonstrate that AdamCB consistently outperforms existing methods.

</details>


### [616] [Symmetric Aggregation of Conformity Scores for Efficient Uncertainty Sets](https://arxiv.org/abs/2512.06945)
*Nabil Alami,Jad Zakharia,Souhaib Ben Taieb*

Main category: stat.ML

TL;DR: The paper introduces SACP, a method for combining prediction uncertainties from multiple models into sharper prediction sets using conformal prediction techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the process of aggregating predictive uncertainties from multiple models, achieving more efficient and reliable uncertainty quantification within conformal prediction contexts.

Method: The proposed method, SACP, transforms nonconformity scores from multiple predictors into e-values, which are then combined using symmetric aggregation functions. This flexible design allows optimization of aggregation strategies.

Result: Theoretical insights validate SACP's approach, and extensive experiments across diverse datasets show that it improves the efficiency of prediction sets while often outperforming current state-of-the-art methods.

Conclusion: SACP is a robust and effective framework for uncertainty aggregation, achieving better performance and sharper prediction sets in multi-model scenarios.

Abstract: Access to multiple predictive models trained for the same task, whether in regression or classification, is increasingly common in many applications. Aggregating their predictive uncertainties to produce reliable and efficient uncertainty quantification is therefore a critical but still underexplored challenge, especially within the framework of conformal prediction (CP). While CP methods can generate individual prediction sets from each model, combining them into a single, more informative set remains a challenging problem. To address this, we propose SACP (Symmetric Aggregated Conformal Prediction), a novel method that aggregates nonconformity scores from multiple predictors. SACP transforms these scores into e-values and combines them using any symmetric aggregation function. This flexible design enables a robust, data-driven framework for selecting aggregation strategies that yield sharper prediction sets. We also provide theoretical insights that help justify the validity and performance of the SACP approach. Extensive experiments on diverse datasets show that SACP consistently improves efficiency and often outperforms state-of-the-art model aggregation baselines.

</details>


### [617] [PARIS: Pruning Algorithm via the Representer theorem for Imbalanced Scenarios](https://arxiv.org/abs/2512.06950)
*Enrico Camporeale*

Main category: stat.ML

TL;DR: The paper introduces PARIS, a dataset pruning framework, to address imbalanced regression by removing uninformative data points efficiently while improving performance in rare-event scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the degradation caused by conventional regression models on rare, high-impact tail events in imbalanced data without introducing distributional distortion or excessive complexity.

Method: PARIS leverages the representer theorem to calculate a closed-form residual for the impact of each training point on validation loss, enabling iterative pruning. This is combined with Cholesky rank-one downdating for computational efficiency.

Result: PARIS reduced training data by up to 75% in a real-world space weather example while maintaining or improving RMSE, outperforming existing techniques like re-weighting and synthetic oversampling.

Conclusion: Representer-guided dataset pruning offers an efficient, interpretable, and effective solution to imbalanced regression scenarios, especially for rare-event modeling.

Abstract: The challenge of \textbf{imbalanced regression} arises when standard Empirical Risk Minimization (ERM) biases models toward high-frequency regions of the data distribution, causing severe degradation on rare but high-impact ``tail'' events. Existing strategies uch as loss re-weighting or synthetic over-sampling often introduce noise, distort the underlying distribution, or add substantial algorithmic complexity.
  We introduce \textbf{PARIS} (Pruning Algorithm via the Representer theorem for Imbalanced Scenarios), a principled framework that mitigates imbalance by \emph{optimizing the training set itself}. PARIS leverages the representer theorem for neural networks to compute a \textbf{closed-form representer deletion residual}, which quantifies the exact change in validation loss caused by removing a single training point \emph{without retraining}. Combined with an efficient Cholesky rank-one downdating scheme, PARIS performs fast, iterative pruning that eliminates uninformative or performance-degrading samples.
  We use a real-world space weather example, where PARIS reduces the training set by up to 75\% while preserving or improving overall RMSE, outperforming re-weighting, synthetic oversampling, and boosting baselines. Our results demonstrate that representer-guided dataset pruning is a powerful, interpretable, and computationally efficient approach to rare-event regression.

</details>


### [618] [Statistical analysis of Inverse Entropy-regularized Reinforcement Learning](https://arxiv.org/abs/2512.06956)
*Denis Belomestny,Alexey Naumov,Sergey Samsonov*

Main category: stat.ML

TL;DR: This paper addresses the inherent ambiguity in traditional Inverse Reinforcement Learning (IRL) through a unique framework called Inverse Entropy-regularized Reinforcement Learning. It combines entropy regularization and least-squares reward reconstruction to resolve non-uniqueness in recovered rewards while providing theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the non-uniqueness problem in IRL, where many reward functions can correspond to the same optimal policy, making the inverse problem ill-posed. There is a need for a framework that ensures a unique and statistically rigorous reward recovery process.

Method: The authors propose combining entropy regularization with a least-squares reward reconstruction approach, using soft Bellman residuals. They model expert demonstrations as a Markov chain and estimate the expert policy through penalized maximum-likelihood methods, accounting for statistical complexity.

Result: The paper derives high-probability bounds for differences between the estimated and true expert policies, leading to non-asymptotic, minimax optimal convergence rates for the identified least-squares reward function. Statistical properties are tied to sample size and model complexity.

Conclusion: The proposed framework resolves the ambiguity in classical IRL, providing a well-defined and unique reward consistent with the expert policy. It advances the theoretical and practical foundations of IRL while linking the method to behavior cloning and statistical learning theory.

Abstract: Inverse reinforcement learning aims to infer the reward function that explains expert behavior observed through trajectories of state--action pairs. A long-standing difficulty in classical IRL is the non-uniqueness of the recovered reward: many reward functions can induce the same optimal policy, rendering the inverse problem ill-posed. In this paper, we develop a statistical framework for Inverse Entropy-regularized Reinforcement Learning that resolves this ambiguity by combining entropy regularization with a least-squares reconstruction of the reward from the soft Bellman residual. This combination yields a unique and well-defined so-called least-squares reward consistent with the expert policy. We model the expert demonstrations as a Markov chain with the invariant distribution defined by an unknown expert policy $π^\star$ and estimate the policy by a penalized maximum-likelihood procedure over a class of conditional distributions on the action space. We establish high-probability bounds for the excess Kullback--Leibler divergence between the estimated policy and the expert policy, accounting for statistical complexity through covering numbers of the policy class. These results lead to non-asymptotic minimax optimal convergence rates for the least-squares reward function, revealing the interplay between smoothing (entropy regularization), model complexity, and sample size. Our analysis bridges the gap between behavior cloning, inverse reinforcement learning, and modern statistical learning theory.

</details>


### [619] [Learning Conditional Independence Differential Graphs From Time-Dependent Data](https://arxiv.org/abs/2512.06960)
*Jitendra K Tugnait*

Main category: stat.ML

TL;DR: The paper investigates methodologies to estimate differences in conditional independence graphs for time series Gaussian graphical models, incorporating time dependencies and using penalized optimization techniques.


<details>
  <summary>Details</summary>
Motivation: The work addresses the challenge of estimating changes in conditional dependencies in time-series data where dependencies evolve over time, addressing limitations in prior methods focused on i.i.d. data.

Method: The paper utilizes a penalized D-trace loss function in the frequency domain and explores both convex (group lasso) and non-convex (log-sum and SCAD group penalties) regularization methods. It employs Wirtinger calculus and an ADMM algorithm for optimization.

Result: The proposed approach achieves consistency in estimating high-dimensional inverse power spectral density and graph recovery. Synthetic results show improved performance using log-sum penalties over lasso methods and traditional i.i.d.-based approaches.

Conclusion: The proposed differential graph learning framework effectively captures changes in time series conditional dependencies, outperforms existing methods, and demonstrates promise in real-world applications.

Abstract: Estimation of differences in conditional independence graphs (CIGs) of two time series Gaussian graphical models (TSGGMs) is investigated where the two TSGGMs are known to have similar structure. The TSGGM structure is encoded in the inverse power spectral density (IPSD) of the time series. In several existing works, one is interested in estimating the difference in two precision matrices to characterize underlying changes in conditional dependencies of two sets of data consisting of independent and identically distributed (i.i.d.) observations. In this paper we consider estimation of the difference in two IPSDs to characterize the underlying changes in conditional dependencies of two sets of time-dependent data. Our approach accounts for data time dependencies unlike past work. We analyze a penalized D-trace loss function approach in the frequency domain for differential graph learning, using Wirtinger calculus. We consider both convex (group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. An alternating direction method of multipliers (ADMM) algorithm is presented to optimize the objective function. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm) and graph recovery. Both synthetic and real data examples are presented in support of the proposed approaches. In synthetic data examples, our log-sum-penalized differential time-series graph estimator significantly outperformed our lasso based differential time-series graph estimator which, in turn, significantly outperformed an existing lasso-penalized i.i.d. modeling approach, with $F_1$ score as the performance metric.

</details>


### [620] [Exact Synthetic Populations for Scalable Societal and Market Modeling](https://arxiv.org/abs/2512.07306)
*Thierry Petit,Arnault Pachot*

Main category: stat.ML

TL;DR: This paper presents a constraint-programming framework to create synthetic populations that precisely match target statistics without needing microdata.


<details>
  <summary>Details</summary>
Motivation: The paper aims to ensure high precision in reproducing statistical targets and maintain consistency within synthetic populations for societal and policy modeling.

Method: They employ constraint-programming to directly encode statistical aggregates and structural relationships without relying on sampled data.

Result: Validated the approach on official demographic sources and assessed impacts on analyses from distribution deviations.

Conclusion: The work showcases the utility of synthetic populations in modeling behaviors, policy exploration, and decision-making, ensuring privacy by not depending on personal data.

Abstract: We introduce a constraint-programming framework for generating synthetic populations that reproduce target statistics with high precision while enforcing full individual consistency. Unlike data-driven approaches that infer distributions from samples, our method directly encodes aggregated statistics and structural relations, enabling exact control of demographic profiles without requiring any microdata. We validate the approach on official demographic sources and study the impact of distributional deviations on downstream analyses. This work is conducted within the Pollitics project developed by Emotia, where synthetic populations can be queried through large language models to model societal behaviors, explore market and policy scenarios, and provide reproducible decision-grade insights without personal data.

</details>


### [621] [Machine learning in an expectation-maximisation framework for nowcasting](https://arxiv.org/abs/2512.07335)
*Paul Wilsens,Katrien Antonio,Gerda Claeskens*

Main category: stat.ML

TL;DR: The paper introduces a machine-learning-based EM framework for nowcasting, leveraging neural networks and XGBoost to address incomplete information caused by delays in event reporting, and demonstrates its effectiveness compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Incomplete information due to reporting delays leads to inaccurate risk assessments. The study aims to improve nowcasting methods by modeling both event occurrence and reporting processes using machine learning.

Method: An EM framework is developed with machine learning techniques (neural networks and XGBoost) for capturing high-dimensional covariate data and nonlinear effects in nowcasting processes.

Result: Simulation experiments confirm the new methodology's superior performance in modeling occurrence and reporting of events over traditional EM approaches using generalized linear models.

Conclusion: The proposed machine-learning-based EM framework is highly effective and outperforms existing models for nowcasting, particularly demonstrated in the context of Argentinian Covid-19 case reporting.

Abstract: Decision making often occurs in the presence of incomplete information, leading to the under- or overestimation of risk. Leveraging the observable information to learn the complete information is called nowcasting. In practice, incomplete information is often a consequence of reporting or observation delays. In this paper, we propose an expectation-maximisation (EM) framework for nowcasting that uses machine learning techniques to model both the occurrence as well as the reporting process of events. We allow for the inclusion of covariate information specific to the occurrence and reporting periods as well as characteristics related to the entity for which events occurred. We demonstrate how the maximisation step and the information flow between EM iterations can be tailored to leverage the predictive power of neural networks and (extreme) gradient boosting machines (XGBoost). With simulation experiments, we show that we can effectively model both the occurrence and reporting of events when dealing with high-dimensional covariate information. In the presence of non-linear effects, we show that our methodology outperforms existing EM-based nowcasting frameworks that use generalised linear models in the maximisation step. Finally, we apply the framework to the reporting of Argentinian Covid-19 cases, where the XGBoost-based approach again is most performant.

</details>


### [622] [High-Dimensional Change Point Detection using Graph Spanning Ratio](https://arxiv.org/abs/2512.07541)
*Youngwen Sun,Katerina Papagiannouli,Vladimir Spokoiny*

Main category: stat.ML

TL;DR: The paper introduces a graph-spanning algorithm effective for detecting data changes across various dimensions, with proven theoretical efficiency and practical advantages.


<details>
  <summary>Details</summary>
Motivation: To create a versatile change detection method applicable to various data structures and distributions, with high accuracy and error control.

Method: A novel graph-spanning algorithm designed to detect changes by leveraging minimax separation rates for both Euclidean and graph-structured data.

Result: The algorithm demonstrates superior accuracy compared to existing methods, even with small observation windows, for both Gaussian and non-Gaussian data.

Conclusion: The approach reliably detects data changes efficiently, proving highly effective, especially in online environments requiring quick results.

Abstract: Inspired by graph-based methodologies, we introduce a novel graph-spanning algorithm designed to identify changes in both offline and online data across low to high dimensions. This versatile approach is applicable to Euclidean and graph-structured data with unknown distributions, while maintaining control over error probabilities. Theoretically, we demonstrate that the algorithm achieves high detection power when the magnitude of the change surpasses the lower bound of the minimax separation rate, which scales on the order of $\sqrt{nd}$. Our method outperforms other techniques in terms of accuracy for both Gaussian and non-Gaussian data. Notably, it maintains strong detection power even with small observation windows, making it particularly effective for online environments where timely and precise change detection is critical.

</details>


### [623] [On Conditional Independence Graph Learning From Multi-Attribute Gaussian Dependent Time Series](https://arxiv.org/abs/2512.07557)
*Jitendra K. Tugnait*

Main category: stat.ML

TL;DR: This paper focuses on estimating conditional independence graphs for high-dimensional multivariate Gaussian time series, using penalized log-likelihood functions in the frequency domain.


<details>
  <summary>Details</summary>
Motivation: Current methods for estimating conditional independence graphs largely consider single-attribute time series, which fail to adequately account for multi-attribute relationships or vector-valued data.

Method: The study formulates a penalized log-likelihood objective function in the frequency domain using the discrete Fourier transform. Both convex and non-convex penalty methods are explored, such as sparse-group lasso, log-sum, and SCAD penalties.

Result: High-dimensional consistency, local convexity (for non-convex penalties), and graph recovery are achieved without the need for incoherence or irrepresentability conditions. Empirical results validate this using tuning parameters and numerical experiments.

Conclusion: The proposed framework offers a robust and theoretically grounded approach for multi-attribute conditional independence graph estimation in high-dimensional time series, widening the scope beyond traditional methods.

Abstract: Estimation of the conditional independence graph (CIG) of high-dimensional multivariate Gaussian time series from multi-attribute data is considered. Existing methods for graph estimation for such data are based on single-attribute models where one associates a scalar time series with each node. In multi-attribute graphical models, each node represents a random vector or vector time series. In this paper we provide a unified theoretical analysis of multi-attribute graph learning for dependent time series using a penalized log-likelihood objective function formulated in the frequency domain using the discrete Fourier transform of the time-domain data. We consider both convex (sparse-group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm), local convexity when using non-convex penalties, and graph recovery. We do not impose any incoherence or irrepresentability condition for our convergence results. We also empirically investigate selection of the tuning parameters based on the Bayesian information criterion, and illustrate our approach using numerical examples utilizing both synthetic and real data.

</details>


### [624] [$φ$-test: Global Feature Selection and Inference for Shapley Additive Explanations](https://arxiv.org/abs/2512.07578)
*Dongseok Kim,Hyoungsun Choi,Mohamed Jismy Aashik Rasool,Gisung Oh*

Main category: stat.ML

TL;DR: The paper introduces the $φ$-test, a global feature-selection and significance method combining Shapley attributions with selective inference for black-box models, yielding interpretable and statistically valid results.


<details>
  <summary>Details</summary>
Motivation: To provide a statistically sound and interpretable global explanation method for black-box predictors by combining modern Shapley-based feature attribution and selective inference.

Method: The $φ$-test utilizes SHAP-guided feature screening, followed by fitting a linear surrogate model on selected features, with tractable selective-inference procedures and outputs statistical measures such as p-values.

Result: Experiments with regression tasks demonstrate that $φ$-test achieves predictive performance similar to original models, using fewer features with high stability across resampling and different model backbones.

Conclusion: $φ$-test serves as an efficient and statistically robust tool for understanding global feature importance, bridging Shapley-based and classical inference.

Abstract: We propose $φ$-test, a global feature-selection and significance procedure for black-box predictors that combines Shapley attributions with selective inference. Given a trained model and an evaluation dataset, $φ$-test performs SHAP-guided screening and fits a linear surrogate on the screened features via a selection rule with a tractable selective-inference form. For each retained feature, it outputs a Shapley-based global score, a surrogate coefficient, and post-selection $p$-values and confidence intervals in a global feature-importance table. Experiments on real tabular regression tasks with tree-based and neural backbones suggest that $φ$-test can retain much of the predictive ability of the original model while using only a few features and producing feature sets that remain fairly stable across resamples and backbone classes. In these settings, $φ$-test acts as a practical global explanation layer linking Shapley-based importance summaries with classical statistical inference.

</details>


### [625] [Physics-Informed Neural Networks for Source Inversion and Parameters Estimation in Atmospheric Dispersion](https://arxiv.org/abs/2512.07755)
*Brenda Anague,Bamdad Hosseini,Issa Karambal,Jean Medard Ngnotchouye*

Main category: stat.ML

TL;DR: The paper introduces a weighted adaptive PINNs method to tackle source inversion and parameter estimation problems in 2D and 3D advection-diffusion equations with unknown coefficients from scarce and noisy data.


<details>
  <summary>Details</summary>
Motivation: Estimating emission source locations and model parameters from limited data is challenging in environmental monitoring tasks.

Method: A weighted adaptive method extending physics-informed neural networks (PINNs), leveraging neural tangent kernels, is applied to jointly recover solutions, sources, and parameters using PDEs as constraints.

Result: Numerical experiments validate the robustness and success of the proposed method in handling practical engineering systems and noisy measurements.

Conclusion: The weighted adaptive PINNs method is effective for addressing ill-posed problems in source inversion and parameter estimation, showcasing reliability even in noisy scenarios.

Abstract: Recent studies have shown the success of deep learning in solving forward and inverse problems in engineering and scientific computing domains, such as physics-informed neural networks (PINNs). In the fields of atmospheric science and environmental monitoring, estimating emission source locations is a central task that further relies on multiple model parameters that dictate velocity profiles and diffusion parameters. Estimating these parameters at the same time as emission sources from scarce data is a difficult task. In this work, we achieve this by leveraging the flexibility and generality of PINNs. We use a weighted adaptive method based on the neural tangent kernels to solve a source inversion problem with parameter estimation on the 2D and 3D advection-diffusion equations with unknown velocity and diffusion coefficients that may vary in space and time. Our proposed weighted adaptive method is presented as an extension of PINNs for forward PDE problems to a highly ill-posed source inversion and parameter estimation problem. The key idea behind our methodology is to attempt the joint recovery of the solution, the sources along with the unknown parameters, thereby using the underlying partial differential equation as a constraint that couples multiple unknown functional parameters, leading to more efficient use of the limited information in the measurements. We present various numerical experiments, using different types of measurements that model practical engineering systems, to show that our proposed method is indeed successful and robust to additional noise in the measurements.

</details>


### [626] [Distribution-informed Online Conformal Prediction](https://arxiv.org/abs/2512.07770)
*Dongjian Hu,Junxi Wu,Shu-Tao Xia,Changliang Zou*

Main category: stat.ML

TL;DR: The paper introduces COP, an online conformal prediction method, that reduces prediction set conservatism by incorporating data patterns, while ensuring valid coverage guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing online conformal prediction methods are too conservative in adversarial environments, lacking adaptability to data patterns.

Method: Introduced COP, utilizing cumulative distribution function estimates for non-conformity scores, to adapt predictions based on underlying data patterns with joint bounds on coverage and regret.

Result: COP delivers valid coverage guarantees, achieves distribution-free, finite-sample coverage, and produces tighter prediction sets compared to baselines.

Conclusion: COP effectively addresses data distribution shifts by balancing conservatism and adaptability in prediction set construction, showing superior experimental performance.

Abstract: Conformal prediction provides a pivotal and flexible technique for uncertainty quantification by constructing prediction sets with a predefined coverage rate. Many online conformal prediction methods have been developed to address data distribution shifts in fully adversarial environments, resulting in overly conservative prediction sets. We propose Conformal Optimistic Prediction (COP), an online conformal prediction algorithm incorporating underlying data pattern into the update rule. Through estimated cumulative distribution function of non-conformity scores, COP produces tighter prediction sets when predictable pattern exists, while retaining valid coverage guarantees even when estimates are inaccurate. We establish a joint bound on coverage and regret, which further confirms the validity of our approach. We also prove that COP achieves distribution-free, finite-sample coverage under arbitrary learning rates and can converge when scores are $i.i.d.$. The experimental results also show that COP can achieve valid coverage and construct shorter prediction intervals than other baselines.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [627] [The Reproducible Research Platform establishes a unified open science environment bridging data and software lifecycles across disciplines, from proposal to publication](https://arxiv.org/abs/2512.06039)
*Andreas P. Cuny,Henry Lütcke,Andrei-Valentin Plamadă,Antti Luomi,John Hennig,Matthew Baker,Fabian Rudolf,Bernd Rinn*

Main category: cs.DL

TL;DR: This paper introduces the Reproducible Research Platform (RRP) to address issues in making data and code FAIR and reproducible, providing seamless execution across domains.


<details>
  <summary>Details</summary>
Motivation: Researchers struggle with achieving FAIR and reproducible research due to disconnected data/code life cycles, lack of executable environments in publications, and high technical skill requirements.

Method: The RRP integrates data management and computational environments through version-controlled, containerized, and shareable projects, offering modular and user-friendly workflows.

Result: The RRP successfully reproduces results from diverse, even decade-old, studies, demonstrating its sustained reproducibility and usability.

Conclusion: The RRP facilitates accessible, reproducible science across domains through minimal user interface, modular tools, and server or local compatibility, advancing FAIR principles.

Abstract: Many research groups aspire to make data and code FAIR and reproducible, yet struggle because the data and code life cycles are disconnected, executable environments are often missing from published work, and technical skill requirements hinder adoption. Existing approaches rarely enable researchers to keep using their preferred tools or support seamless execution across domains. To close this gap, we developed the open-source Reproducible Research Platform (RRP), which unifies research data management with version-controlled, containerized computational environments in modular, shareable projects. RRP enables anyone to execute, reuse, and publish fully documented, FAIR research workflows without manual retrieval or platform-specific setup. We demonstrate RRP's impact by reproducing results from diverse published studies, including work over a decade old, showing sustained reproducibility and usability. With a minimal graphical interface focused on core tasks, modular tool installation, and compatibility with institutional servers or local computers, RRP makes reproducible science broadly accessible across scientific domains.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [628] [Multi-resolution Physics-Aware Recurrent Convolutional Neural Network for Complex Flows](https://arxiv.org/abs/2512.06031)
*Xinlun Cheng,Joseph Choi,H. S. Udaykumar,Stephen Baek*

Main category: physics.flu-dyn

TL;DR: MRPARCv2 is a Multi-resolution Physics-Aware Recurrent CNN designed for complex flow modeling. It improves simulation accuracy and efficiency using hierarchical discretization and multi-resolution features, outperforming its predecessor despite fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Modeling complex flows in turbulent systems efficiently and accurately remains a significant challenge. This paper aims to address the limitations in performance and accuracy of existing machine learning models for physics-informed multi-scale flow dynamics.

Method: MRPARCv2 integrates the structure of advection-diffusion-reaction equations and employs a multi-resolution architecture with hierarchical discretization and cross-resolution feature communication.

Result: MRPARCv2 significantly outperforms baseline models and its predecessor in accuracy, achieving up to 50% better roll-out prediction error and 86% lower spectral error on challenging flow datasets, despite using fewer trainable parameters.

Conclusion: Multi-resolution inductive bias is highly effective for modeling multi-scale flow dynamics, but embedding knowledge of equations of state (EOS) into model architectures is essential for enhanced physical fidelity.

Abstract: We present MRPARCv2, Multi-resolution Physics-Aware Recurrent Convolutional Neural Network, designed to model complex flows by embedding the structure of advection-diffusion-reaction equations and leveraging a multi-resolution architecture. MRPARCv2 introduces hierarchical discretization and cross-resolution feature communication to improve the accuracy and efficiency of flow simulations. We evaluate the model on a challenging 2D turbulent radiative layer dataset from The Well multi-physics benchmark repository and demonstrate significant improvements when compared to the single resolution baseline model, in both Variance Scaled Root Mean Squared Error and physics-driven metrics, including turbulent kinetic energy spectra and mass-temperature distributions. Despite having 30% fewer trainable parameters, MRPARCv2 outperforms its predecessor by up to 50% in roll-out prediction error and 86% in spectral error. A preliminary study on uncertainty quantification was performed, and we also analyzed the model's performance under different levels of abstractions of the flow, specifically on sampling subsets of field variables. We find that the absence of physical constraints on the equation of state (EOS) in the network architecture leads to degraded accuracy. A variable substitution experiment confirms that this issue persists regardless of which physical quantity is predicted directly. Our findings highlight the advantages of multi-resolution inductive bias for capturing multi-scale flow dynamics and suggest the need for future PIML models to embed EOS knowledge to enhance physical fidelity.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [629] [An Index-based Approach for Efficient and Effective Web Content Extraction](https://arxiv.org/abs/2512.06641)
*Yihan Chen,Benfeng Xu,Xiaorui Wang,Zhendong Mao*

Main category: cs.IR

TL;DR: The paper introduces an index-based, efficient web content extraction method tailored for large language models (LLMs) to address issues of high latency and low signal density in existing methods.


<details>
  <summary>Details</summary>
Motivation: The challenge was managing context in large token spaces for web agents analyzing massive web pages, where existing solutions proved inefficient or inadequate.

Method: The paper proposes dividing HTML into structure-aware, addressable segments and predicting indices of relevant content for extraction, instead of traditional token-by-token generation.

Result: The method improves Question Answering (QA) accuracy and demonstrates better accuracy and speed in main content and query-relevant extraction compared to existing approaches.

Conclusion: This index-based approach successfully enhances LLM interaction with web data, addressing a critical bottleneck in efficiency and accuracy for content extraction tasks.

Abstract: As web agents (e.g., Deep Research) routinely consume massive volumes of web pages to gather and analyze information, LLM context management -- under large token budgets and low signal density -- emerges as a foundational, high-importance, and technically challenging problem for agentic and RAG pipelines. Existing solutions for extracting relevant content are inadequate: generative extraction models suffer from high latency, rule-based heuristics lack adaptability, and chunk-and-rerank methods are blind to webpage structure. To overcome these issues, we introduce Index-based Web Content Extraction to reframe the extraction process from slow, token-by-token generation into a highly efficient, discriminative task of index prediction, achieving both effectiveness and efficiency. We partition HTML into structure-aware, addressable segments, and extract only the positional indices of content relevant to a given query. This method decouples extraction latency from content length, enabling rapid, query-relevant extraction. We first evaluate our method as a post-retrieval processing component within an RAG QA system and find that it improves QA accuracy. Then we directly measure its match rate with the target content in two scenarios: main content extraction (ME) and query-relevant extraction (QE). Experimental results show that our method outperforms existing works in both accuracy and speed, effectively bridging the gap between LLMs and the vast webpages.

</details>


### [630] [Towards Efficient Hypergraph and Multi-LLM Agent Recommender Systems](https://arxiv.org/abs/2512.06590)
*Tendai Mukande,Esraa Ali,Annalina Caputo,Ruihai Dong,Noel OConnor*

Main category: cs.IR

TL;DR: The paper introduces HGLMRec, a novel recommender system utilizing multi-LLM agents and a hypergraph encoder for efficient and effective recommendations.


<details>
  <summary>Details</summary>
Motivation: Recommender systems need to evolve to personalize user experiences effectively. Generative retrieval methods powered by large language models face issues such as hallucination and high computational costs.

Method: The paper proposes a new RS framework, HGLMRec, utilizing multi-LLM agents and a hypergraph encoder to capture complex user-item behavior relationships and optimize token retrieval during inference.

Result: Experimental results show that HGLMRec outperforms state-of-the-art baselines in recommendation performance while maintaining lower computational costs.

Conclusion: HGLMRec effectively addresses important challenges in generative recommendation systems, offering improved performance and efficiency.

Abstract: Recommender Systems (RSs) have become the cornerstone of various applications such as e-commerce and social media platforms. The evolution of RSs is paramount in the digital era, in which personalised user experience is tailored to the user's preferences. Large Language Models (LLMs) have sparked a new paradigm - generative retrieval and recommendation. Despite their potential, generative RS methods face issues such as hallucination, which degrades the recommendation performance, and high computational cost in practical scenarios. To address these issues, we introduce HGLMRec, a novel Multi-LLM agent-based RS that incorporates a hypergraph encoder designed to capture complex, multi-behaviour relationships between users and items. The HGLMRec model retrieves only the relevant tokens during inference, reducing computational overhead while enriching the retrieval context. Experimental results show performance improvement by HGLMRec against state-of-the-art baselines at lower computational cost.

</details>


### [631] [WisPaper: Your AI Scholar Search Engine](https://arxiv.org/abs/2512.06879)
*Li Ju,Jun Zhao,Mingxu Chai,Ziyu Shen,Xiangyang Wang,Yage Geng,Chunchun Ma,Hao Peng,Guangbin Li,Tao Li,Chengyong Liao,Fu Wang,Xiaolong Wang,Junshen Chen,Rui Gong,Shijia Liang,Feiyan Li,Ming Zhang,Kexin Tan,Jujie Ye,Zhiheng Xi,Shihan Dou,Tao Gui,Yuankai Ying,Yang Shi,Yue Zhang,Qi Zhang*

Main category: cs.IR

TL;DR: WisPaper is an intelligent platform for academic retrieval and literature management, integrating paper discovery, systematic organization, and relevant recommendations.


<details>
  <summary>Details</summary>
Motivation: To help researchers efficiently locate, organize, and stay updated with rapidly growing scientific publications, reducing time spent on literature management.

Method: Developed a platform combining quick/detailed search (Scholar Search), knowledge base organization (Library), and intelligent recommendation (AI Feeds).

Result: WisPaper enhances efficiency for researchers, offering a multilingual and multidisciplinary system that minimizes time spent on paper screening and management.

Conclusion: WisPaper establishes a seamless, closed-loop workflow for literature-related tasks, benefiting researchers across various disciplines.

Abstract: Researchers struggle to efficiently locate and manage relevant literature within the exponentially growing body of scientific publications. We present \textsc{WisPaper}, an intelligent academic retrieval and literature management platform that addresses this challenge through three integrated capabilities: (1) \textit{Scholar Search}, featuring both quick keyword-based and deep agentic search modes for efficient paper discovery; (2) \textit{Library}, a customizable knowledge base for systematic literature organization; and (3) \textit{AI Feeds}, an intelligent recommendation system that automatically delivers relevant new publications based on user interests. Unlike existing academic tools, \textsc{WisPaper} provides a closed-loop workflow that seamlessly connects literature discovery, management, and continuous tracking of research frontiers. Our multilingual and multidisciplinary system significantly reduces the time researchers from diverse backgrounds spend on paper screening and management, enabling them to focus on their core research activities. The platform is publicly accessible and serves researchers across academia and industry.

</details>


### [632] [Benchmarking Deep Neural Networks for Modern Recommendation Systems](https://arxiv.org/abs/2512.07000)
*Abderaouf Bahi,Ibtissem Gasmi*

Main category: cs.IR

TL;DR: The paper evaluates seven neural network architectures on three datasets, examining their effectiveness and proposing hybrid methods for better recommendation systems.


<details>
  <summary>Details</summary>
Motivation: To improve recommendation systems by assessing various neural network architectures and addressing challenges related to accuracy, diversity, and computational demands.

Method: Evaluating CNN, RNN, GNN, Autoencoder, Transformer, NCF, and Siamese Networks on Retail E-commerce, Amazon Products, and Netflix Prize datasets using metrics like accuracy, recall, F1-score, and diversity.

Result: GNNs manage complex item relationships well, RNNs excel in capturing temporal dynamics for platforms like Netflix, and Siamese Networks enhance recommendation diversification.

Conclusion: Hybrid methods integrating the strengths of different neural networks are recommended to enhance recommendation systems and adapt to digital platform demands.

Abstract: This paper examines the deployment of seven different neural network architectures CNN, RNN, GNN, Autoencoder, Transformer, NCF, and Siamese Networks on three distinct datasets: Retail E-commerce, Amazon Products, and Netflix Prize. It evaluates their effectiveness through metrics such as accuracy, recall, F1-score, and diversity in recommendations. The results demonstrate that GNNs are particularly adept at managing complex item relationships in e-commerce environments, whereas RNNs are effective in capturing the temporal dynamics that are essential for platforms such as Netflix.. Siamese Networks are emphasized for their contribution to the diversification of recommendations, particularly in retail settings. Despite their benefits, issues like computational demands, reliance on extensive data, and the challenge of balancing accurate and diverse recommendations are addressed. The study seeks to inform the advancement of recommendation systems by suggesting hybrid methods that merge the strengths of various models to better satisfy user preferences and accommodate the evolving demands of contemporary digital platforms.

</details>


### [633] [MUSE: A Simple Yet Effective Multimodal Search-Based Framework for Lifelong User Interest Modeling](https://arxiv.org/abs/2512.07216)
*Bin Wu,Feifan Yang,Zhangming Chan,Yu-Ran Gu,Jiawei Feng,Chao Yi,Xiang-Rong Sheng,Han Zhu,Jian Xu,Mang Ye,Bo Zheng*

Main category: cs.IR

TL;DR: The paper introduces MUSE, a multimodal search framework designed for lifelong user interest modeling, focusing on simplicity in behavior retrieval and multimodal sequence modeling in fine-grained systems.


<details>
  <summary>Details</summary>
Motivation: Current recommendation systems rely heavily on ID-based features, which do not perform well for long-tail items or offer sufficient semantic expressiveness. The authors aim to improve recommendation systems by leveraging multimodal signals across both retrieval and fine-grained modeling stages.

Method: The paper systematically analyzes multimodal integration across two stages of long-term modeling frameworks. It introduces MUSE, which uses lightweight cosine similarity for multimodal embeddings in the retrieval stage and richer multimodal fusion in the fine-grained stage for effective performance.

Result: MUSE is deployed in Taobao’s display advertising system, achieving significant improvements in top-line metrics while keeping latency negligible. Additionally, a large-scale dataset with long behavior sequences is presented to the community.

Conclusion: Simplicity in the retrieval stage combined with advanced multimodal sequence modeling in fine-grained stages proves effective for improving recommendation systems. The research bridges industrial innovation and community contributions through open-source data and practices.

Abstract: Lifelong user interest modeling is crucial for industrial recommender systems, yet existing approaches rely predominantly on ID-based features, suffering from poor generalization on long-tail items and limited semantic expressiveness. While recent work explores multimodal representations for behavior retrieval in the General Search Unit (GSU), they often neglect multimodal integration in the fine-grained modeling stage -- the Exact Search Unit (ESU). In this work, we present a systematic analysis of how to effectively leverage multimodal signals across both stages of the two-stage lifelong modeling framework. Our key insight is that simplicity suffices in the GSU: lightweight cosine similarity with high-quality multimodal embeddings outperforms complex retrieval mechanisms. In contrast, the ESU demands richer multimodal sequence modeling and effective ID-multimodal fusion to unlock its full potential. Guided by these principles, we propose MUSE, a simple yet effective multimodal search-based framework. MUSE has been deployed in Taobao display advertising system, enabling 100K-length user behavior sequence modeling and delivering significant gains in top-line metrics with negligible online latency overhead. To foster community research, we share industrial deployment practices and open-source the first large-scale dataset featuring ultra-long behavior sequences paired with high-quality multimodal embeddings. Our code and data is available at https://taobao-mm.github.io.

</details>


### [634] [Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation](https://arxiv.org/abs/2512.07650)
*Fuyuan Lyu,Zhentai Chen,Jingyan Jiang,Lingjie Li,Xing Tang,Xiuqiang He,Xue Liu*

Main category: cs.IR

TL;DR: The paper explores test-time scaling for deep learning recommendation systems (DLRS), proposing two methods to efficiently utilize and scale computational resources during inference.


<details>
  <summary>Details</summary>
Motivation: While scaling of model parameters during training has been explored, the efficient utilization of computational resources during test-time for DLRS is less studied, which can lead to scaling-efficiency improvements.

Method: Two approaches are proposed: leveraging heterogeneity of different architectures, and using initialization randomness under homogeneous architectures.

Result: Experiments with eight models across three benchmarks show test-time scaling effectiveness. Test-time scaling outperforms parameter scaling within the same inference budget and can be accelerated in parallel server deployments.

Conclusion: Test-time scaling offers an efficient approach to scaling and inference, bringing orthogonal improvements without negatively impacting user-side performance online.

Abstract: Inspired by the success of language models (LM), scaling up deep learning recommendation systems (DLRS) has become a recent trend in the community. All previous methods tend to scale up the model parameters during training time. However, how to efficiently utilize and scale up computational resources during test time remains underexplored, which can prove to be a scaling-efficient approach and bring orthogonal improvements in LM domains. The key point in applying test-time scaling to DLRS lies in effectively generating diverse yet meaningful outputs for the same instance. We propose two ways: One is to explore the heterogeneity of different model architectures. The other is to utilize the randomness of model initialization under a homogeneous architecture. The evaluation is conducted across eight models, including both classic and SOTA models, on three benchmarks. Sufficient evidence proves the effectiveness of both solutions. We further prove that under the same inference budget, test-time scaling can outperform parameter scaling. Our test-time scaling can also be seamlessly accelerated with the increase in parallel servers when deployed online, without affecting the inference time on the user side. Code is available.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [635] [AQUILA: A QUIC-Based Link Architecture for Resilient Long-Range UAV Communication](https://arxiv.org/abs/2512.06889)
*Ximing Huang,Yirui Rao*

Main category: cs.NI

TL;DR: The paper introduces AQUILA, a cross-layer communication architecture based on QUIC, to enhance UAV communication in BVLOS scenarios, overcoming issues with TCP, UDP, and cellular networks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing communication protocols (TCP, UDP, and cellular networks) that hinder efficient, reliable, and low-latency communications for autonomous UAVs in BVLOS applications.

Method: The authors developed AQUILA, which integrates QUIC's features to create a unified transport layer with reliable and unreliable data streams, introduces priority scheduling for critical data, and adapts congestion control for UAVs. It also incorporates 0-RTT connection resumption and a global IP-native architecture.

Result: Experimental results show AQUILA outperforms traditional TCP and UDP methods in reducing C2 latency, improving video quality, and ensuring better link resilience under real-world conditions.

Conclusion: AQUILA offers a significant advancement in UAV communication, making it a viable solution for autonomous BVLOS missions by improving latency, reliability, and bandwidth utilization.

Abstract: The proliferation of autonomous Unmanned Aerial Vehicles (UAVs) in Beyond Visual Line of Sight (BVLOS) applications is critically dependent on resilient, high-bandwidth, and low-latency communication links. Existing solutions face critical limitations: TCP's head-of-line blocking stalls time-sensitive data, UDP lacks reliability and congestion control, and cellular networks designed for terrestrial users degrade severely for aerial platforms. This paper introduces AQUILA, a cross-layer communication architecture built on QUIC to address these challenges. AQUILA contributes three key innovations: (1) a unified transport layer using QUIC's reliable streams for MAVLink Command and Control (C2) and unreliable datagrams for video, eliminating head-of-line blocking under unified congestion control; (2) a priority scheduling mechanism that structurally ensures C2 latency remains bounded and independent of video traffic intensity; (3) a UAV-adapted congestion control algorithm extending SCReAM with altitude-adaptive delay targeting and telemetry headroom reservation. AQUILA further implements 0-RTT connection resumption to minimize handover blackouts with application-layer replay protection, deployed over an IP-native architecture enabling global operation. Experimental validation demonstrates that AQUILA significantly outperforms TCP- and UDP-based approaches in C2 latency, video quality, and link resilience under realistic conditions, providing a robust foundation for autonomous BVLOS missions.

</details>


### [636] [Intrusion Detection on Resource-Constrained IoT Devices with Hardware-Aware ML and DL](https://arxiv.org/abs/2512.02272)
*Ali Diab,Adel Chehade,Edoardo Ragusa,Paolo Gastaldo,Rodolfo Zunino,Amer Baghdadi,Mostafa Rizk*

Main category: cs.NI

TL;DR: This paper introduces a hardware-aware intrusion detection system optimized for IoT/IIoT networks, comparing tree-based ML models and compact DNNs for resource-constrained edge devices.


<details>
  <summary>Details</summary>
Motivation: The need for fast, privacy-preserving, and resource-efficient threat detection in IoT/IIoT networks motivated the development of a novel intrusion detection system adapted for edge-device constraints.

Method: The study used constrained grid search for tree-based classifiers and hardware-aware neural architecture search (HW-NAS) for 1D-CNNs to develop optimized models.

Result: On the Edge-IIoTset benchmark, LightGBM achieved 95.3% accuracy with 75 KB flash and 1.2K operations, while the HW-NAS-optimized CNN achieved 97.2% accuracy with 190 KB flash and 840K FLOPs. Deployment on Raspberry Pi 3 B Plus confirmed practical performance.

Conclusion: The results demonstrate the feasibility and benefits of hardware-constrained model design for real-time threat detection in edge devices, balancing accuracy and latency for different use cases.

Abstract: This paper proposes a hardware-aware intrusion detection system (IDS) for Internet of Things (IoT) and Industrial IoT (IIoT) networks; it targets scenarios where classification is essential for fast, privacy-preserving, and resource-efficient threat detection. The goal is to optimize both tree-based machine learning (ML) models and compact deep neural networks (DNNs) within strict edge-device constraints. This allows for a fair comparison and reveals trade-offs between model families. We apply constrained grid search for tree-based classifiers and hardware-aware neural architecture search (HW-NAS) for 1D convolutional neural networks (1D-CNNs). Evaluation on the Edge-IIoTset benchmark shows that selected models meet tight flash, RAM, and compute limits: LightGBM achieves 95.3% accuracy using 75 KB flash and 1.2 K operations, while the HW-NAS-optimized CNN reaches 97.2% with 190 KB flash and 840 K floating-point operations (FLOPs). We deploy the full pipeline on a Raspberry Pi 3 B Plus, confirming that tree-based models operate within 30 ms and that CNNs remain suitable when accuracy outweighs latency. These results highlight the practicality of hardware-constrained model design for real-time IDS at the edge.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [637] [Small Language Models Reshape Higher Education: Courses, Textbooks, and Teaching](https://arxiv.org/abs/2512.06001)
*Jian Zhang,Jia Shao*

Main category: physics.ed-ph

TL;DR: This study highlights the use of small language models (MiniLMs) tailored for efficient and precise educational purposes in atmospheric physics, overcoming the limitations of large language models in higher education particularly through dynamic course design and advanced retrieval systems.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of large language models in higher education, such as inaccuracies and high computational demands, and aims to explore how MiniLMs can offer lightweight and precise solutions for professional education.

Method: The authors established a custom corpus and image library by collecting over 550,000 PDFs from respected journals in Earth and environmental sciences. They extracted over 100 million sentences and 3 million academic images, organized them into a high-dimensional vector library using MiniLMs, and redesigned educational materials and strategies for the "Atmospheric Physics" course.

Result: The "Atmospheric Physics" course was converted into an interdisciplinary system using MiniLMs, breaking traditional subject boundaries. Teaching materials were digitized into a dynamic library, and innovative question-based learning pathways were implemented to encourage active cognitive development.

Conclusion: MiniLMs present an efficient and accurate tool for higher education, transforming static content into engaging, interdisciplinary learning systems, as demonstrated in the revamped "Atmospheric Physics" course.

Abstract: While large language models (LLMs) have introduced novel paradigms in science and education, their adoption in higher education is constrained by inherent limitations. These include a tendency to produce inaccuracies and high computational requirements, which compromise the strict demands for accurate and reliable knowledge essential in higher education. Small language models (MiniLMs), by contrast, offer distinct advantages in professional education due to their lightweight nature and precise retrieval capabilities. This research takes "Atmospheric Physics" as an example. We established a specialized corpus and image repository by gathering over 550,000 full-text PDFs from over 130 international well-respected journals in Earth and environmental science. From this collection, we extracted over 100 million high-quality sentence-level corpus and more than 3 million high-resolution academic images. Using MiniLMs, these resources were organized into a high-dimensional vector library for precise retrieval and efficient utilization of extensive educational content. Consequently, we systematically redesigned the courses, textbooks, and teaching strategies for "Atmospheric Physics" based on MiniLMs. The course is designed as a "interdisciplinary-frontier" system, breaking down traditional boundaries between atmospheric science, space science, hydrology, and remote sensing. Teaching materials are transformed from static, lagging text formats into a dynamic digital resource library powered by MiniLM. For teaching methods, we have designed a question-based learning pathway. This paradigm promotes a shift from passive knowledge transfer to active cognitive development. Consequently, this MiniLM-driven "Atmospheric Physics" course demonstrates a specific avenue for "AI for education".

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [638] [Human Geometry Distribution for 3D Animation Generation](https://arxiv.org/abs/2512.07459)
*Xiangjun Tang,Biao Zhang,Peter Wonka*

Main category: cs.GR

TL;DR: This paper focuses on generating realistic human geometry animations featuring natural clothing dynamics with limited data, presenting a novel two-stage framework involving latent space learning and generative animation modeling.


<details>
  <summary>Details</summary>
Motivation: The research targets the challenges of generating high-quality human animations with detailed clothing dynamics while addressing the issues caused by limited motion data and mapping constraints.

Method: The proposed method consists of a two-stage framework: (1) a compact distribution-based latent space representation for improved geometry generation, and (2) a generative animation model focusing on short-term transitions while ensuring long-term consistency using identity-conditioned design.

Result: The method delivers high-fidelity human geometries with 90% lower Chamfer Distance compared to previous approaches, and generates animations that scored 2.2 times higher in user studies with detailed and natural dynamics.

Conclusion: The proposed framework effectively overcomes challenges in human geometry generation and animation, achieving state-of-the-art results in both quality and usability.

Abstract: Generating realistic human geometry animations remains a challenging task, as it requires modeling natural clothing dynamics with fine-grained geometric details under limited data. To address these challenges, we propose two novel designs. First, we propose a compact distribution-based latent representation that enables efficient and high-quality geometry generation. We improve upon previous work by establishing a more uniform mapping between SMPL and avatar geometries. Second, we introduce a generative animation model that fully exploits the diversity of limited motion data. We focus on short-term transitions while maintaining long-term consistency through an identity-conditioned design. These two designs formulate our method as a two-stage framework: the first stage learns a latent space, while the second learns to generate animations within this latent space. We conducted experiments on both our latent space and animation model. We demonstrate that our latent space produces high-fidelity human geometry surpassing previous methods ($90\%$ lower Chamfer Dist.). The animation model synthesizes diverse animations with detailed and natural dynamics ($2.2 \times$ higher user study score), achieving the best results across all evaluation metrics.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [639] [Deep Neural Network-Based Aerial Transport in the Presence of Cooperative and Uncooperative UAS](https://arxiv.org/abs/2512.06577)
*Muhammad Junayed Hasan Zahed,Hossein Rastgoftar*

Main category: eess.SY

TL;DR: The paper proposes a robust DNN-based framework for decentralized UAS transport and coverage, resilient even with uncooperative agents.


<details>
  <summary>Details</summary>
Motivation: To develop a decentralized system for UAS transport that remains resilient and performative even in the presence of faults or uncooperative agents.

Method: The framework uses layered inter-UAS communication, forward scheduling, adaptive pruning of edges, and feed-forward mentoring for stability and convergence. Sparse linear relations are computed for global set points.

Result: Simulations confirm rapid and robust convergence to target zones under full cooperation and resilience even under partial cooperation scenarios.

Conclusion: The approach ensures stable, scalable, and fault-resilient UAS transport while maintaining computational efficiency and convergence guarantees.

Abstract: We present a resilient deep neural network (DNN) framework for decentralized transport and coverage using uncrewed aerial systems (UAS) operating in $\mathbb{R}^n$. The proposed DNN-based mass-transport architecture constructs a layered inter-UAS communication graph from an initial formation, assigns time-varying communication weights through a forward scheduling mechanism that guides the team from the initial to the final configuration, and ensures stability and convergence of the resulting multi-agent transport dynamics. The framework is explicitly designed to remain robust in the presence of uncooperative agents that deviate from or refuse to follow the prescribed protocol. Our method preserves a fixed feed-forward topology but dynamically prunes edges to uncooperative agents, maintains convex, feedforward mentoring among cooperative agents, and computes global desired set points through a sparse linear relation consistent with leader references. The target set is abstracted by $N$ points that become final desired positions, enabling coverage-optimal transport while keeping computation low and guarantees intact. Extensive simulations demonstrate that, under full cooperation, all agents converge rapidly to the target zone with a 10\% boundary margin and under partial cooperation with uncooperative agents, the system maintains high convergence among cooperative agents with performance degradation localized near the disruptions, evidencing graceful resilience and scalability. These results confirm that forward-weight scheduling, hierarchical mentor--mentee coordination, and on-the-fly DNN restructuring yield robust, provably stable UAS transport in realistic fault scenarios.

</details>


### [640] [Obstacle Avoidance of UAV in Dynamic Environments Using Direction and Velocity-Adaptive Artificial Potential Field](https://arxiv.org/abs/2512.07609)
*Nikita Vaibhav Pavle,Shrreya Rajneesh,Rakesh Kumar Sahoo,Manoranjan Sinha*

Main category: eess.SY

TL;DR: The paper presents a novel method to address the local minima and moving obstacle limitation in Artificial Potential Field (APF) by integrating a dynamic weighting function combined with Model Predictive Control (MPC) for collision-free UAV navigation.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to overcome challenges in UAV autonomous navigation in dynamic and cluttered airspace, specifically addressing the limitations of the conventional Artificial Potential Field (APF).

Method: The study introduces a Direction and Relative Velocity Weighted APF, incorporating a bounded weighting function based on obstacles' direction and velocity, and integrates this within a Model Predictive Control (MPC) framework.

Result: Simulation results showcase the proposed method's ability to resolve local minima, enhance safety through smooth and predictive avoidance maneuvers, and ensure reliable UAV navigation in complex environments.

Conclusion: The novel methodology proves to be efficient and viable for addressing local minima issues in APF and enabling autonomous UAV navigation with high path integrity in dynamic settings.

Abstract: The conventional Artificial Potential Field (APF) is fundamentally limited by the local minima issue and its inability to account for the kinematics of moving obstacles. This paper addresses the critical challenge of autonomous collision avoidance for Unmanned Aerial Vehicles (UAVs) operating in dynamic and cluttered airspace by proposing a novel Direction and Relative Velocity Weighted Artificial Potential Field (APF). In this approach, a bounded weighting function, $ω(θ,v_{e})$, is introduced to dynamically scale the repulsive potential based on the direction and velocity of the obstacle relative to the UAV. This robust APF formulation is integrated within a Model Predictive Control (MPC) framework to generate collision-free trajectories while adhering to kinematic constraints. Simulation results demonstrate that the proposed method effectively resolves local minima and significantly enhances safety by enabling smooth, predictive avoidance maneuvers. The system ensures superior path integrity and reliable performance, confirming its viability for autonomous navigation in complex environments.

</details>


### [641] [A Physics-Aware Attention LSTM Autoencoder for Early Fault Diagnosis of Battery Systems](https://arxiv.org/abs/2512.06809)
*Jiong Yang*

Main category: eess.SY

TL;DR: The paper introduces a Physics-Aware LSTM Autoencoder framework that improves early fault diagnosis in electric vehicle batteries by incorporating battery aging laws into the deep learning pipeline.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in detecting early battery faults caused by subtle anomalies and operational noise, which existing methods struggle with due to lack of physical integration.

Method: A Physics-Aware Attention LSTM Autoencoder (PA-ALSTM-AE) with a multi-stage fusion mechanism integrates battery aging laws into an adaptive feature construction and physics-guided latent fusion module.

Result: Experiments on a real-world dataset show the model significantly outperforms state-of-the-art methods, enhancing early fault recall rates by over 3 times without compromising precision.

Conclusion: The framework offers a reliable and precise solution for industrial battery management systems, improving safety and supporting fault diagnosis effectively.

Abstract: Battery safety is paramount for electric vehicles. Early fault diagnosis remains a challenge due to the subtle nature of anomalies and the interference of dynamic operating noise. Existing data-driven methods often suffer from "physical blindness" leading to missed detections or false alarms. To address this, we propose a Physics-Aware Attention LSTM Autoencoder (PA-ALSTM-AE). This novel framework explicitly integrates battery aging laws (mileage) into the deep learning pipeline through a multi-stage fusion mechanism. Specifically, an adaptive physical feature construction module selects mileage-sensitive features, and a physics-guided latent fusion module dynamically calibrates the memory cells of the LSTM based on the aging state. Extensive experiments on the large-scale Vloong real-world dataset demonstrate that the proposed method significantly outperforms state-of-the-art baselines. Notably, it improves the recall rate of early faults by over 3 times while maintaining high precision, offering a robust solution for industrial battery management systems.

</details>


### [642] [Joint Learning of Feasibility-Aware Signal Temporal Logic and BarrierNet for Robust and Correct Control](https://arxiv.org/abs/2512.06973)
*Shuo Liu,Wenliang Liu,Wei Xiao,Calin A. Belta*

Main category: eess.SY

TL;DR: The paper introduces a learning-based framework for High Order Control Barrier Functions (HOCBFs) integrated with Signal Temporal Logic (STL) to improve safety and adaptability in robotic controllers.


<details>
  <summary>Details</summary>
Motivation: Existing CBF-STL methods rely on static hyperparameters and short-term optimizations, leading to conservative behavior and difficulty in satisfying long-term STL tasks, especially under strict input constraints.

Method: The approach uses a feasibility-aware learning framework incorporating trainable, time-varying HOCBFs into differentiable Quadratic Programs (dQPs), supported by three neural networks for adaptive parameter tuning.

Result: The proposed controller achieves consistent STL satisfaction with strictly feasible dQPs, improves robustness under tight input bounds, and outperforms non-adaptive baselines in simulations.

Conclusion: The framework eliminates manual tuning, reduces conservativeness, and enhances robustness and feasibility in enforcing long-horizon STL specifications under constrained conditions.

Abstract: Control Barrier Functions (CBFs) have emerged as a powerful tool for enforcing safety in optimization-based controllers, and their integration with Signal Temporal Logic (STL) has enabled the specification-driven synthesis of complex robotic behaviors. However, existing CBF-STL approaches typically rely on fixed hyperparameters and myopic, per-time step optimization, which can lead to overly conservative behavior, infeasibility near tight input limits, and difficulty satisfying long-horizon STL tasks. To address these limitations, we propose a feasibility-aware learning framework that embeds trainable, time-varying High Order Control Barrier Functions (HOCBFs) into a differentiable Quadratic Program (dQP). Our approach provides a systematic procedure for constructing time-varying HOCBF constraints for a broad fragment of STL and introduces a unified robustness measure that jointly captures STL satisfaction, QP feasibility, and control-bound compliance. Three neural networks-InitNet, RefNet, and an extended BarrierNet-collaborate to generate reference inputs and adapt constraint-related hyperparameters automatically over time and across initial conditions, reducing conservativeness while maximizing robustness. The resulting controller achieves STL satisfaction with strictly feasible dQPs and requires no manual tuning. Simulation results demonstrate that the proposed framework maintains high STL robustness under tight input bounds and significantly outperforms fixed-parameter and non-adaptive baselines in complex environments.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [643] [Microseismic event classification with a lightweight Fourier Neural Operator model](https://arxiv.org/abs/2512.07425)
*Ayrat Abdullin,Umair bin Waheed,Leo Eisner,Abdullatif Al-Shuhail*

Main category: physics.geo-ph

TL;DR: The paper introduces a lightweight Fourier Neural Operator (FNO)-based model for seismic event classification, achieving high accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models for seismic event monitoring are often hindered by their high computational demand, limiting their real-time application.

Method: A Fourier Neural Operator-based approach is designed for waveform classification, utilizing its computational efficiency and resolution-invariance.

Result: The FNO model achieves an F1 score of 95% on the STEAD dataset and 98% on a real microseismic dataset while requiring significantly less computational power compared to traditional models.

Conclusion: The FNO model provides an efficient and accurate mechanism for real-time monitoring of microseismicity, enabling effective hazard mitigation and seamless implementation of traffic light systems.

Abstract: Real-time monitoring of induced seismicity is crucial for mitigating operational hazards, relying on the rapid and accurate classification of microseismic events from continuous data streams. However, while many deep learning models excel at this task, their high computational requirements often limit their practical application in real-time monitoring systems. To address this limitation, a lightweight model based on the Fourier Neural Operator (FNO) is proposed for microseismic event classification, leveraging its inherent resolution-invariance and computational efficiency for waveform processing. In the STanford EArthquake Dataset (STEAD), a global and large-scale database of seismic waveforms, the FNO-based model demonstrates high effectiveness for trigger classification, with an F1 score of 95% even in the scenario of data sparsity in training. The new FNO model greatly decreases the computer power needed relative to current deep learning models without sacrificing the classification success rate measured by the F1 score. A test on a real microseismic dataset shows a classification success rate with an F1 score of 98%, outperforming many traditional deep-learning techniques. A combination of high success rate and low computational power indicates that the FNO model can serve as a methodology of choice for real-time monitoring of microseismicity for induced seismicity. The method saves computational resources and facilitates both post-processing and real-time seismic processing suitable for the implementation of traffic light systems to prevent undesired induced seismicity.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [644] [A Chunked-Object Pattern for Multi-Region Large Payload Storage in Managed NoSQL Databases](https://arxiv.org/abs/2512.06852)
*Manideep Reddy Chinthareddy*

Main category: cs.DB

TL;DR: The paper proposes a "chunked-object" pattern to address limitations of key-value databases in handling large items, eliminating replication issues and improving consistency.


<details>
  <summary>Details</summary>
Motivation: To solve architectural challenges and race conditions when handling large objects beyond database size limits with the conventional "pointer pattern".

Method: The "chunked-object" pattern stores large entities as ordered chunks within a database instead of offloading to object storage. A reference implementation uses Amazon DynamoDB Global Tables.

Result: The approach eliminates replication lag issues and improves p99 time-to-consistency for large payloads by keeping data within the same consistency domain.

Conclusion: The chunked-object pattern is effective in overcoming size constraints and resolving multi-region consistency issues in key-value databases.

Abstract: Many managed key-value and NoSQL databases - such as Amazon DynamoDB, Azure Cosmos DB, and Google Cloud Firestore - enforce strict maximum item sizes (e.g., 400 KB in DynamoDB). This constraint imposes significant architectural challenges for applications requiring low-latency, multi-region access to objects that exceed these limits. The standard industry recommendation is to offload payloads to object storage (e.g., Amazon S3) while retaining a pointer in the database. While cost-efficient, this "pointer pattern" introduces network overhead and exposes applications to non-deterministic replication lag between the database and the object store, creating race conditions in active-active architectures.
  This paper presents a "chunked-object" pattern that persists large logical entities as sets of ordered chunks within the database itself. We precisely define the pattern and provide a reference implementation using Amazon DynamoDB Global Tables. The design generalizes to any key-value store with per-item size limits and multi-region replication. We evaluate the approach using telemetry from a production system processing over 200,000 transactions per hour. Results demonstrate that the chunked-object pattern eliminates cross-system replication lag hazards and reduces p99 cross-region time-to-consistency for 1 MB payloads by keeping data and metadata within a single consistency domain.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [645] [Coherent Audio-Visual Editing via Conditional Audio Generation Following Video Edits](https://arxiv.org/abs/2512.07209)
*Masato Ishii,Akio Hayakawa,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.MM

TL;DR: The paper proposes a new audio-visual editing pipeline, using a video-to-audio generation model to maintain coherence between edited videos and their corresponding audio.


<details>
  <summary>Details</summary>
Motivation: To improve coherence and alignment between edited video content and its accompanying audio, as conventional methods may fail to seamlessly integrate the two.

Method: The approach involves editing video first and then using a video-to-audio generation model informed by the source audio, target video, and a text prompt. The model adapts audio editing based on the complexity of changes and uses data augmentation strategies for efficient training.

Result: The experimental results show improved audio-visual alignment and content integrity compared to existing methods.

Conclusion: The proposed method achieves superior coherence and alignment in joint audio-visual editing, demonstrating its effectiveness over traditional techniques.

Abstract: We introduce a novel pipeline for joint audio-visual editing that enhances the coherence between edited video and its accompanying audio. Our approach first applies state-of-the-art video editing techniques to produce the target video, then performs audio editing to align with the visual changes. To achieve this, we present a new video-to-audio generation model that conditions on the source audio, target video, and a text prompt. We extend the model architecture to incorporate conditional audio input and propose a data augmentation strategy that improves training efficiency. Furthermore, our model dynamically adjusts the influence of the source audio based on the complexity of the edits, preserving the original audio structure where possible. Experimental results demonstrate that our method outperforms existing approaches in maintaining audio-visual alignment and content integrity.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [646] [Accurate Models of NVIDIA Tensor Cores](https://arxiv.org/abs/2512.07004)
*Faizan A. Khattak,Mantas Mikaitis*

Main category: cs.MS

TL;DR: The paper discusses emulating matrix multipliers' numerical behavior in GPUs for varying input precisions, targeting reproducibility across architectures.


<details>
  <summary>Details</summary>
Motivation: Matrix multiplication is pivotal for neural network training/inference and scientific computing, but differences in numerical behavior across GPU architectures hinder result reproducibility.

Method: The authors present software models for emulating inner product numerical behaviors on various GPUs, focusing on mixed and low precision (8-, 16-, and 19-bit floating-point formats).

Result: The software model enables studying numerical traits like rounding, accumulation, and normalization across GPUs (V100, A100, H100, B200).

Conclusion: The emulation enhances comprehension of numerical features and boosts reliability for mixed-precision algorithm development across GPU hardware.

Abstract: Matrix multiplication is a fundamental operation in for both training of neural networks and inference. To accelerate matrix multiplication, Graphical Processing Units (GPUs) provide it implemented in hardware. Due to the increased throughput over the software-based matrix multiplication, the multipliers are increasingly used outside of AI, to accelerate various applications in scientific computing. However, matrix multipliers targeted at AI are at present not compliant with IEEE 754 floating-point arithmetic behaviour, with different vendors offering different numerical features. This leads to non-reproducible results across different generations of GPU architectures, at the matrix multiply-accumulate instruction level. To study numerical characteristics of matrix multipliers-such as rounding behaviour, accumulator width, normalization points, extra carry bits, and others-test vectors are typically constructed. Yet, these vectors may or may not distinguish between different hardware models, and due to limited hardware availability, their reliability across many different platforms remains largely untested. We present software models for emulating the inner product behavior of low- and mixed-precision matrix multipliers in the V100, A100, H100 and B200 data center GPUs in most supported input formats of interest to mixed-precision algorithm developers: 8-, 16-, and 19-bit floating point.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [647] [An Asynchronous Mixed-Signal Resonate-and-Fire Neuron](https://arxiv.org/abs/2512.07361)
*Giuseppe Leo,Paolo Gibertini,Irem Ilter,Erika Covi,Ole Richter,Elisabetta Chicca*

Main category: eess.SP

TL;DR: This paper introduces a CMOS mixed-signal Resonate-and-Fire neuron circuit emulating biological neuron behavior for advancing low-power edge temporal signal processing.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop energy-efficient, real-time edge processing inspired by biological neurons to address demands like reduced data storage, transmission, and energy usage.

Method: A CMOS mixed-signal Resonate-and-Fire neuron circuit with asynchronous handshake integration is proposed, analyzed for variability, and tested for frequency detection.

Result: The results confirm the circuit's feasibility for large-scale integration and its effectiveness in frequency detection, indicating potential for neuromorphic systems.

Conclusion: The study establishes the utility of bio-inspired circuits, paving the way for efficient, real-time, edge-based temporal signal processing applications in neuromorphic systems.

Abstract: Analog computing at the edge is an emerging strategy to limit data storage and transmission requirements, as well as energy consumption, and its practical implementation is in its initial stages of development. Translating properties of biological neurons into hardware offers a pathway towards low-power, real-time edge processing. Specifically, resonator neurons offer selectivity to specific frequencies as a potential solution for temporal signal processing. Here, we show a fabricated Complementary Metal-Oxide-Semiconductor (CMOS) mixed-signal Resonate-and-Fire (R&F) neuron circuit implementation that emulates the behavior of these neural cells responsible for controlling oscillations within the central nervous system. We integrate the design with asynchronous handshake capabilities, perform comprehensive variability analyses, and characterize its frequency detection functionality. Our results demonstrate the feasibility of large-scale integration within neuromorphic systems, thereby advancing the exploitation of bio-inspired circuits for efficient edge temporal signal processing.

</details>


### [648] [TagLabel: RFID Based Orientation and Material Sensing for Automated Package Inspection](https://arxiv.org/abs/2512.07097)
*David Wang,Jiale Zhang,Pei Zhang*

Main category: eess.SP

TL;DR: TagLabel, an RFID-based system, enables accurate package orientation and contents detection using passive UHF tags with over 80% accuracy.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in identifying counterfeit products, fraudulent returns, and hazardous items, highlighting weaknesses in slow, expensive, and impractical current package screening methods.

Method: Utilizing low-cost RFID UHF tags, RSSI and phase variations are analyzed to detect package content and orientation. Machine learning classifiers are applied to improve accuracy in material sensing and orientation predictions.

Result: The system achieves over 80% accuracy in detecting package orientation and contents using both two and three tag configurations, even in realistic RF environments.

Conclusion: TagLabel's efficient and cost-effective approach enhances package inspection automation for logistics using standard RFID hardware, improving practical applicability.

Abstract: Modern logistics systems face increasing difficulty in identifying counterfeit products, fraudulent returns, and hazardous items concealed within packages, yet current package screening methods remain too slow, expensive, and impractical for widespread use. This paper presents TagLabel, an RFID based system that determines both the orientation and contents of packages using low cost passive UHF tags. By analyzing how materials change RSSI and phase, the system identifies the contents of a package without opening it. Using orientation inferred from phase differences, tag occlusion, and antenna gain patterns, the system selects the tag with the greatest occlusion for accurate material sensing. We evaluate two and three tag configurations, and show that both can deliver high orientation and material sensing performance through the use of machine learning classifiers, even in realistic RF environments. When combined into a unified pipeline, TagLabel achieves more than 80 percent accuracy across all package orientations. Because it requires only standard RFID hardware and offers fast scanning times, this approach provides a practical way to enhance package inspection and improve automation in logistics operations.

</details>


### [649] [Non-negative DAG Learning from Time-Series Data](https://arxiv.org/abs/2512.07267)
*Samuel Rey,Gonzalo Mateos*

Main category: eess.SP

TL;DR: The paper focuses on learning a directed acyclic graph (DAG) with instantaneous and time-lagged dependencies in multivariate time series using a novel convex optimization approach, guaranteeing global solution optimality and outperforming existing methods on synthetic data.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of non-convex optimization in learning DAGs for multivariate time series and to leverage additional assumptions (non-negative edge weights) for improved computational tractability and performance.

Method: The authors propose modeling DAGs with non-negative edge weights to impose acyclicity through a convex constraint, reframing DAG recovery as a convex optimization problem solved via the method of multipliers.

Result: The proposed method achieves global optimality and demonstrates superior performance compared to alternative approaches on synthetic multivariate time-series datasets.

Conclusion: By incorporating non-negative edge weights into the DAG model and leveraging convex optimization, this method offers a globally optimal and efficient approach to DAG recovery, outperforming existing methods on tested data.

Abstract: This work aims to learn the directed acyclic graph (DAG) that captures the instantaneous dependencies underlying a multivariate time series. The observed data follow a linear structural vector autoregressive model (SVARM) with both instantaneous and time-lagged dependencies, where the instantaneous structure is modeled by a DAG to reflect potential causal relationships. While recent continuous relaxation approaches impose acyclicity through smooth constraint functions involving powers of the adjacency matrix, they lead to non-convex optimization problems that are challenging to solve. In contrast, we assume that the underlying DAG has only non-negative edge weights, and leverage this additional structure to impose acyclicity via a convex constraint. This enables us to cast the problem of non-negative DAG recovery from multivariate time-series data as a convex optimization problem in abstract form, which we solve using the method of multipliers. Crucially, the convex formulation guarantees global optimality of the solution. Finally, we assess the performance of the proposed method on synthetic time-series data, where it outperforms existing alternatives.

</details>


### [650] [Verifiable Deep Quantitative Group Testing](https://arxiv.org/abs/2512.07279)
*Shreyas Jayant Grampurohit,Satish Mulleti,Ajit Rajwade*

Main category: eess.SP

TL;DR: The paper presents a neural network-based framework for solving quantitative group testing (QGT) with high accuracy and structural recovery capability.


<details>
  <summary>Details</summary>
Motivation: To improve decoding accuracy and enable structural verifiability in solving the QGT problem where only a small subset of defective items needs to be identified using very few pooled tests.

Method: A multi-layer perceptron is trained to decode noisy measurement vectors into binary defect indicators. It also recovers the underlying pooling structure using the network's Jacobian analysis.

Result: The trained network achieves accurate defect recovery even under noise, implicitly learns the pooling structure, and validates the combinatorial relations governing QGT.

Conclusion: Standard feedforward neural networks can effectively learn verifiable mappings and structural relationships in QGT, going beyond mere memorization of training data.

Abstract: We present a neural network-based framework for solving the quantitative group testing (QGT) problem that achieves both high decoding accuracy and structural verifiability. In QGT, the objective is to identify a small subset of defective items among $N$ candidates using only $M \ll N$ pooled tests, each reporting the number of defectives in the tested subset. We train a multi-layer perceptron to map noisy measurement vectors to binary defect indicators, achieving accurate and robust recovery even under sparse, bounded perturbations. Beyond accuracy, we show that the trained network implicitly learns the underlying pooling structure that links items to tests, allowing this structure to be recovered directly from the network's Jacobian. This indicates that the model does not merely memorize training patterns but internalizes the true combinatorial relationships governing QGT. Our findings reveal that standard feedforward architectures can learn verifiable inverse mappings in structured combinatorial recovery problems.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [651] [Radiance-Field Reinforced Pretraining: Scaling Localization Models with Unlabeled Wireless Signals](https://arxiv.org/abs/2512.07309)
*Guosheng Wang,Shen Wang,Lei Yang*

Main category: cs.IT

TL;DR: This paper introduces Radiance-Field Reinforced Pretraining (RFRP), a self-supervised framework for RF-based indoor localization, tackling cross-scene generalization issues and achieving significant improvements in localization accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing RF-based indoor localization models are limited by their dependence on scene-specific labeled data, restricting their cross-scene generalization and applicability.

Method: The approach involves coupling a localization model with a neural RF radiance field in an asymmetrical autoencoder design to align input and output for effective representation learning. Large-scale unlabeled RF data were used for self-supervised pretraining.

Result: The RFRP-pretrained localization model achieved a 40% reduction in localization error compared to non-pretrained models and a 21% improvement over models pretrained using supervised learning.

Conclusion: Self-supervised RFRP demonstrates considerable potential to advance RF-based indoor localization by improving generalization and accuracy across diverse scenes with minimal effort in data collection.

Abstract: Radio frequency (RF)-based indoor localization offers significant promise for applications such as indoor navigation, augmented reality, and pervasive computing. While deep learning has greatly enhanced localization accuracy and robustness, existing localization models still face major challenges in cross-scene generalization due to their reliance on scene-specific labeled data. To address this, we introduce Radiance-Field Reinforced Pretraining (RFRP). This novel self-supervised pretraining framework couples a large localization model (LM) with a neural radio-frequency radiance field (RF-NeRF) in an asymmetrical autoencoder architecture. In this design, the LM encodes received RF spectra into latent, position-relevant representations, while the RF-NeRF decodes them to reconstruct the original spectra. This alignment between input and output enables effective representation learning using large-scale, unlabeled RF data, which can be collected continuously with minimal effort. To this end, we collected RF samples at 7,327,321 positions across 100 diverse scenes using four common wireless technologies--RFID, BLE, WiFi, and IIoT. Data from 75 scenes were used for training, and the remaining 25 for evaluation. Experimental results show that the RFRP-pretrained LM reduces localization error by over 40% compared to non-pretrained models and by 21% compared to those pretrained using supervised learning.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [652] [AI as "Co-founder": GenAI for Entrepreneurship](https://arxiv.org/abs/2512.06506)
*Junhui Jeff Cai,Xian Gu,Liugang Sheng,Mengjia Xia,Linda Zhao,Wu Zhu*

Main category: econ.GN

TL;DR: The paper investigates the impact of generative AI (GenAI), specifically ChatGPT's release in November 2022, on firm creation, finding it boosts small-business entry while decreasing large-firm entry.


<details>
  <summary>Details</summary>
Motivation: To understand if and how generative AI like ChatGPT lowers startup costs and promotes firm creation, especially for small businesses.

Method: The study uses the release of ChatGPT as a global shock and analyzes geo-coded data with AI-specific human capital metrics to examine firm registrations in China by the end of 2024.

Result: Areas with higher AI-specific human capital experienced a significant increase in small-firm formations, resulting in a 6.0% contribution to national firm entries. Large-firm entries dropped, and new ventures became leaner.

Conclusion: Generative AI fosters competition by reducing barriers for small firms, especially for entrepreneurs using AI and requiring less financing, while shifting focus away from large-scale ventures.

Abstract: This paper studies whether, how, and for whom generative artificial intelligence (GenAI) facilitates firm creation. Our identification strategy exploits the November 2022 release of ChatGPT as a global shock that lowered start-up costs and leverages variations across geo-coded grids with differential pre-existing AI-specific human capital. Using high-resolution and universal data on Chinese firm registrations by the end of 2024, we find that grids with stronger AI-specific human capital experienced a sharp surge in new firm formation$\unicode{x2013}$driven entirely by small firms, contributing to 6.0% of overall national firm entry. Large-firm entry declines, consistent with a shift toward leaner ventures. New firms are smaller in capital, shareholder number, and founding team size, especially among small firms. The effects are strongest among firms with potential AI applications, weaker financing needs, and among first-time entrepreneurs. Overall, our results highlight that GenAI serves as a pro-competitive force by disproportionately boosting small-firm entry.

</details>


<div id='q-bio.MN'></div>

# q-bio.MN [[Back]](#toc)

### [653] [Interpretable Neural Approximation of Stochastic Reaction Dynamics with Guaranteed Reliability](https://arxiv.org/abs/2512.06294)
*Quentin Badolle,Arthur Theuer,Zhou Fang,Ankit Gupta,Mustafa Khammash*

Main category: q-bio.MN

TL;DR: DeepSKA is introduced as a neural framework that enhances computational efficiency, interpretability, and reliability in estimating outputs for Stochastic Reaction Networks (SRNs).


<details>
  <summary>Details</summary>
Motivation: Estimating expected outputs for SRNs is analytically challenging and computationally expensive with existing methods, while current deep learning approaches lack interpretability and reliability for scientific and decision-making applications.

Method: DeepSKA combines mathematically transparent neural representations with a limited number of stochastic simulations to produce unbiased, provably convergent, and lower-variance estimates.

Result: DeepSKA successfully demonstrates improved accuracy and computational efficiency across nine different SRNs, including complex nonlinear models, achieving orders-of-magnitude advancements.

Conclusion: DeepSKA offers an interpretable, reliable, and efficient methodological foundation, which could be extended to analyze other Markovian systems like stochastic differential equations.

Abstract: Stochastic Reaction Networks (SRNs) are a fundamental modeling framework for systems ranging from chemical kinetics and epidemiology to ecological and synthetic biological processes. A central computational challenge is the estimation of expected outputs across initial conditions and times, a task that is rarely solvable analytically and becomes computationally prohibitive with current methods such as Finite State Projection or the Stochastic Simulation Algorithm. Existing deep learning approaches offer empirical scalability, but provide neither interpretability nor reliability guarantees, limiting their use in scientific analysis and in applications where model outputs inform real-world decisions. Here we introduce DeepSKA, a neural framework that jointly achieves interpretability, guaranteed reliability, and substantial computational gains. DeepSKA yields mathematically transparent representations that generalise across states, times, and output functions, and it integrates this structure with a small number of stochastic simulations to produce unbiased, provably convergent, and dramatically lower-variance estimates than classical Monte Carlo. We demonstrate these capabilities across nine SRNs, including nonlinear and non-mass-action models with up to ten species, where DeepSKA delivers accurate predictions and orders-of-magnitude efficiency improvements. This interpretable and reliable neural framework offers a principled foundation for developing analogous methods for other Markovian systems, including stochastic differential equations.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [654] [KidSpeak: A General Multi-purpose LLM for Kids' Speech Recognition and Screening](https://arxiv.org/abs/2512.05994)
*Rohan Sharma,Dancheng Liu,Jingchen Sun,Shijie Zhou,Jiayu Qin,Jinjun Xiong,Changyou Chen*

Main category: eess.AS

TL;DR: This paper introduces KidSpeak, a specialized speech-enhanced AI model for children's speech, and FASA, an alignment tool to improve children's speech data quality.


<details>
  <summary>Details</summary>
Motivation: Current AI models and datasets are poorly equipped to handle unique challenges in children's speech, such as early developmental stages or speech pathologies.

Method: The authors developed KidSpeak, a foundation model with a speech-specific encoder using phonetic knowledge and a two-stage training process. They introduced FASA, an alignment tool, to generate high-quality children's speech data.

Result: KidSpeak achieved 87% average accuracy on four tasks. FASA improved data quality by 13.6x compared to human annotation, validated on the CHILDES dataset.

Conclusion: KidSpeak and FASA constitute the first comprehensive approach tailored for children's speech and language therapy, combining generative/discriminative capabilities with advanced data alignment.

Abstract: With the rapid advancement of conversational and diffusion-based AI, there is a growing adoption of AI in educational services, ranging from grading and assessment tools to personalized learning systems that provide targeted support for students. However, this adaptability has yet to fully extend to the domain of children's speech, where existing models often fail due to their reliance on datasets designed for clear, articulate adult speech. Children, particularly those in early developmental stages or with speech and language pathologies, present unique challenges that current AI models and datasets are ill-equipped to handle. To address this, we introduce KidSpeak, a multi-task speech-enhanced Foundation Model capable of both generative and discriminative tasks specifically tailored to children's speech patterns. Our framework employs a two-stage training process that incorporates phonetic knowledge into the speech encoder, achieving an average accuracy of 87% across four separate tasks. Furthermore, recognizing the limitations of scalable human annotation and existing speech alignment tools, we propose the Flexible and Automatic Speech Aligner (FASA) and leverage the method to construct high quality datasets for training and evaluation. This novel alignment tool significantly improves the quality of aligned children's speech from noisy data, enhancing data quality by 13.6x compared to human annotations, as demonstrated on the CHILDES dataset. To the best of our knowledge, KidSpeak and FASA represent the first comprehensive solution designed for speech and language therapy in children, offering both a multi-purpose speech LLM and a robust alignment tool.

</details>


### [655] [Degrading Voice: A Comprehensive Overview of Robust Voice Conversion Through Input Manipulation](https://arxiv.org/abs/2512.06304)
*Xining Song,Zhihua Wei,Rui Wang,Haixiao Hu,Yanxiang Chen,Meng Han*

Main category: eess.AS

TL;DR: The paper addresses the challenges of robust voice conversion (VC) in real-world scenarios and explores the impact of degraded input speech, such as noise and perturbations, on model performance.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need to address the limitations of VC models in handling degraded input speech, which is a major challenge in real-world applications.

Method: The authors classify existing attack and defense methods related to degraded input speech and systematically evaluate their impact across four dimensions: intelligibility, naturalness, timbre similarity, and subjective perception.

Result: The paper provides insights into how different forms of input degradation influence VC performance, revealing areas for optimization in attack and defense strategies.

Conclusion: This research highlights open issues and suggests future directions to enhance the robustness of VC models, aiming for effective deployment in real-world settings.

Abstract: Identity, accent, style, and emotions are essential components of human speech. Voice conversion (VC) techniques process the speech signals of two input speakers and other modalities of auxiliary information such as prompts and emotion tags. It changes para-linguistic features from one to another, while maintaining linguistic contents. Recently, VC models have made rapid advancements in both generation quality and personalization capabilities. These developments have attracted considerable attention for diverse applications, including privacy preservation, voice-print reproduction for the deceased, and dysarthric speech recovery. However, these models only learn non-robust features due to the clean training data. Subsequently, it results in unsatisfactory performances when dealing with degraded input speech in real-world scenarios, including additional noise, reverberation, adversarial attacks, or even minor perturbation. Hence, it demands robust deployments, especially in real-world settings. Although latest researches attempt to find potential attacks and countermeasures for VC systems, there remains a significant gap in the comprehensive understanding of how robust the VC model is under input manipulation. here also raises many questions: For instance, to what extent do different forms of input degradation attacks alter the expected output of VC models? Is there potential for optimizing these attack and defense strategies? To answer these questions, we classify existing attack and defense methods from the perspective of input manipulation and evaluate the impact of degraded input speech across four dimensions, including intelligibility, naturalness, timbre similarity, and subjective perception. Finally, we outline open issues and future directions.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [656] [DeepSVM: Learning Stochastic Volatility Models with Physics-Informed Deep Operator Networks](https://arxiv.org/abs/2512.07162)
*Kieran A. Malandain,Selim Kalici,Hakob Chakhoyan*

Main category: q-fin.CP

TL;DR: DeepSVM employs a physics-informed deep learning approach to calibrate stochastic volatility models without requiring labeled data, achieving accurate option pricing while identifying areas for improvement in derivative predictions.


<details>
  <summary>Details</summary>
Motivation: To address the computational bottleneck in real-time calibration of stochastic volatility models caused by solving coupled PDEs repeatedly.

Method: The study proposes DeepSVM, a Physics-Informed Deep Operator Network, which enforces terminal payoffs, no-arbitrage constraints, and uses Residual-based Adaptive Refinement to improve training stability.

Result: DeepSVM attains a training loss of $10^{-5}$ and produces accurate option prices, but shows noise in derivative predictions in the at-the-money regime.

Conclusion: The approach succeeds at improving option pricing performance in stochastic volatility models, but requires refinement for better stability in derivative predictions.

Abstract: Real-time calibration of stochastic volatility models (SVMs) is computationally bottlenecked by the need to repeatedly solve coupled partial differential equations (PDEs). In this work, we propose DeepSVM, a physics-informed Deep Operator Network (PI-DeepONet) designed to learn the solution operator of the Heston model across its entire parameter space. Unlike standard data-driven deep learning (DL) approaches, DeepSVM requires no labelled training data. Rather, we employ a hard-constrained ansatz that enforces terminal payoffs and static no-arbitrage conditions by design. Furthermore, we use Residual-based Adaptive Refinement (RAR) to stabilize training in difficult regions subject to high gradients. Overall, DeepSVM achieves a final training loss of $10^{-5}$ and predicts highly accurate option prices across a range of typical market dynamics. While pricing accuracy is high, we find that the model's derivatives (Greeks) exhibit noise in the at-the-money (ATM) regime, highlighting the specific need for higher-order regularization in physics-informed operator learning.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [657] [Physics Enhanced Deep Surrogates for the Phonon Boltzmann Transport Equation](https://arxiv.org/abs/2512.05976)
*Antonio Varagnolo,Giuseppe Romano,Raphaël Pestourie*

Main category: physics.comp-ph

TL;DR: The study presents a Physics-Enhanced Deep Surrogate (PEDS) for efficient design of nano-scale thermal materials, achieving high accuracy and data efficiency by combining a physical Fourier solver and a neural generator.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of accurately designing materials with controlled heat flow at the nano-scale, an essential requirement for microelectronics, thermoelectrics, and energy technologies, which demand efficient and reliable solutions to the complex Boltzmann Transport Equation (BTE).

Method: The authors propose PEDS, which integrates a differentiable Fourier solver as an inductive bias with a neural generator that adjusts for geometry-dependent behaviors. It uses uncertainty-driven active learning to balance accuracy and efficiency while capturing ballistic and diffusive regimes.

Result: PEDS reduces training-data requirements by up to 70%, achieves about 5% fractional error with 300 BTE simulations, and enables the design of porous materials with low average errors (4%). It also recovers the ballistic-diffusive transition and performs well out-of-distribution.

Conclusion: The integration of simple, differentiable physics into surrogate models can significantly enhance data efficiency and interpretability, making this approach practical for repeated optimization in nano-scale thermal-materials design.

Abstract: Designing materials with controlled heat flow at the nano-scale is central to advances in microelectronics, thermoelectrics, and energy-conversion technologies. At these scales, phonon transport follows the Boltzmann Transport Equation (BTE), which captures non-diffusive (ballistic) effects but is too costly to solve repeatedly in inverse-design loops. Existing surrogate approaches trade speed for accuracy: fast macroscopic solvers can overestimate conductivities by hundreds of percent, while recent data-driven operator learners often require thousands of high-fidelity simulations. This creates a need for a fast, data-efficient surrogate that remains reliable across ballistic and diffusive regimes. We introduce a Physics-Enhanced Deep Surrogate (PEDS) that combines a differentiable Fourier solver with a neural generator and couples it with uncertainty-driven active learning. The Fourier solver acts as a physical inductive bias, while the network learns geometry-dependent corrections and a mixing coefficient that interpolates between macroscopic and nano-scale behavior. PEDS reduces training-data requirements by up to 70% compared with purely data-driven baselines, achieves roughly 5% fractional error with only 300 high-fidelity BTE simulations, and enables efficient design of porous geometries spanning 12-85 W m$^{-1}$ K$^{-1}$ with average design errors of 4%. The learned mixing parameter recovers the ballistic-diffusive transition and improves out of distribution robustness. These results show that embedding simple, differentiable low-fidelity physics can dramatically increase surrogate data-efficiency and interpretability, making repeated PDE-constrained optimization practical for nano-scale thermal-materials design.

</details>


### [658] [Optimized Machine Learning Methods for Studying the Thermodynamic Behavior of Complex Spin Systems](https://arxiv.org/abs/2512.07458)
*Dmitrii Kapitan,Pavel Ovchinnikov,Konstantin Soldatov,Petr Andriushchenko,Vitalii Kapitan*

Main category: physics.comp-ph

TL;DR: The paper demonstrates the use of convolutional neural networks (CNNs) to efficiently analyze phase states and critical temperatures in various spin system models on different lattices.


<details>
  <summary>Details</summary>
Motivation: To explore the capabilities of CNNs in understanding and analyzing critical and low-temperature phase states in spin system models, particularly focusing on the spatial and thermodynamic properties of different lattice structures.

Method: The study applies CNN-based classifiers on spin system models, utilizing configurations generated by the Swendsen-Wang cluster algorithm, and compares performance metrics such as RMSE across lattice types to determine phase states.

Result: CNN models achieved better RMSE compared to fully connected architectures, captured complex correlations, and identified critical temperatures, with the kagome lattice's critical temperature determined without retraining.

Conclusion: Convolutional neural networks are effective tools for analyzing phase transitions and thermodynamic properties in complex spin systems, offering accurate and efficient performance across diverse lattice structures.

Abstract: This paper presents a systematic study of the application of convolutional neural networks (CNNs) as an efficient and versatile tool for the analysis of critical and low-temperature phase states in spin system models. The problem of calculating the dependence of the average energy on the spatial distribution of exchange integrals for the Edwards-Anderson model on a square lattice with frustrated interactions is considered. We further construct a single convolutional classifier of phase states of the ferromagnetic Ising model on square, triangular, honeycomb, and kagome lattices, trained on configurations generated by the Swendsen-Wang cluster algorithm. Computed temperature profiles of the averaged posterior probability of the high-temperature phase form clear S-shaped curves that intersect in the vicinity of the theoretical critical temperatures and allow one to determine the critical temperature for the kagome lattice without additional retraining. It is shown that convolutional models substantially reduce the root-mean-square error (RMSE) compared with fully connected architectures and efficiently capture complex correlations between thermodynamic characteristics and the structure of magnetic correlated systems.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [659] [Beyond Lux thresholds: a systematic pipeline for classifying biologically relevant light contexts from wearable data](https://arxiv.org/abs/2512.06181)
*Yanuo Zhou*

Main category: q-bio.QM

TL;DR: This paper introduces a reproducible pipeline for classifying natural vs. artificial light using wearable spectral data. It achieves high performance and provides code for future benchmarking.


<details>
  <summary>Details</summary>
Motivation: To address the lack of reproducible, validated pipelines for classifying natural and artificial light from wearable spectral data.

Method: The study analyzed spectral data from 26 participants using a structured pipeline involving steps such as log transform, normalization, multilevel aggregation, and MLP classifier under participant-wise cross-validation.

Result: The pipeline achieved high classification accuracy (88%) and AUC 0.938 for natural vs. artificial light, while performance on indoor vs. outdoor classification remained limited due to spectral overlap and class imbalances.

Conclusion: The research offers a validated baseline pipeline with actionable design rules; its open-source implementation facilitates reuse and supports further advancements in contextual light classification.

Abstract: Background: Wearable spectrometers enable field quantification of biologically relevant light, yet reproducible pipelines for contextual classification remain under-specified.
  Objective: To establish and validate a subject-wise evaluated, reproducible pipeline and actionable design rules for classifying natural vs. artificial light from wearable spectral data.
  Methods: We analysed ActLumus recordings from 26 participants, each monitored for at least 7 days at 10-second sampling, paired with daily exposure diaries. The pipeline fixes the sequence: domain selection, log-base-10 transform, L2 normalisation excluding total intensity (to avoid brightness shortcuts), hour-level medoid aggregation, sine/cosine hour encoding, and MLP classifier, evaluated under participant-wise cross-validation.
  Results: The proposed sequence consistently achieved high performance on the primary task, with representative configurations reaching AUC = 0.938 (accuracy 88%) for natural vs. artificial classification on the held-out subject split. In contrast, indoor vs. outdoor classification remained at feasibility level due to spectral overlap and class imbalance (best AUC approximately 0.75; majority-class collapse without contextual sensors). Threshold baselines were insufficient on our data, supporting the need for spectral-temporal modelling beyond illuminance cut-offs.
  Conclusions: We provide a reproducible, auditable baseline pipeline and design rules for contextual light classification under subject-wise generalisation. All code, configuration files, and derived artefacts will be openly archived (GitHub + Zenodo DOI) to support reuse and benchmarking.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [660] [Characterizing Lane-Changing Behavior in Mixed Traffic](https://arxiv.org/abs/2512.07219)
*Sungyong Chung,Alireza Talebpour,Samer H. Hamdar*

Main category: cs.MA

TL;DR: This study uses trajectory data from Waymo Open Motion Dataset (WOMD) to investigate lane-changing behavior in mixed traffic involving automated and human-driven vehicles, employing game-theoretic frameworks and clustering approaches.


<details>
  <summary>Details</summary>
Motivation: Understanding lane-changing behavior in mixed traffic (including automated vehicles) is essential for enhancing safety and efficiency on roads.

Method: Game-theoretic analysis using k-means clustering and quantal response equilibrium framework to classify behaviors and study social dilemmas in lane-changing events.

Result: Cooperative behaviors are more common among AVs, social dilemmas appeared in ~4% for active vehicles and ~11% for passive vehicles, and repeated interactions promote cooperation over time.

Conclusion: Repeated interactions improve cooperation among vehicles, irrespective of AV penetration rates, contributing to a deeper understanding of traffic dynamics in mixed AV-HDV scenarios.

Abstract: Characterizing and understanding lane-changing behavior in the presence of automated vehicles (AVs) is crucial to ensuring safety and efficiency in mixed traffic. Accordingly, this study aims to characterize the interactions between the lane-changing vehicle (active vehicle) and the vehicle directly impacted by the maneuver in the target lane (passive vehicle). Utilizing real-world trajectory data from the Waymo Open Motion Dataset (WOMD), this study explores patterns in lane-changing behavior and provides insight into how these behaviors evolve under different AV market penetration rates (MPRs). In particular, we propose a game-theoretic framework to analyze cooperative and defective behaviors in mixed traffic, applied to the 7,636 observed lane-changing events in the WOMD. First, we utilize k-means clustering to classify vehicles as cooperative or defective, revealing that the proportions of cooperative AVs are higher than those of HDVs in both active and passive roles. Next, we jointly estimate the utilities of active and passive vehicles to model their behaviors using the quantal response equilibrium framework. Empirical payoff tables are then constructed based on these utilities. Using these payoffs, we analyze the presence of social dilemmas and examine the evolution of cooperative behaviors using evolutionary game theory. Our results reveal the presence of social dilemmas in approximately 4% and 11% of lane-changing events for active and passive vehicles, respectively, with most classified as Stag Hunt or Prisoner's Dilemma (Chicken Game rarely observed). Moreover, the Monte Carlo simulation results show that repeated lane-changing interactions consistently lead to increased cooperative behavior over time, regardless of the AV penetration rate.

</details>


### [661] [AI-Generated Compromises for Coalition Formation: Modeling, Simulation, and a Textual Case Study](https://arxiv.org/abs/2512.05983)
*Eyal Briman,Ehud Shapiro,Nimrod Talmon*

Main category: cs.MA

TL;DR: This paper focuses on AI-assisted democratic coalition formation and compromise generation for collaborative text editing, using NLP and LLM techniques.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of finding effective compromise proposals in coalition formation for collaborative democratic tasks, particularly under agent uncertainty and bounded rationality.

Method: The authors formalized a holistic model, used NLP and LLMs to create a semantic metric space for text, and developed algorithms to generate compromise proposals in collaborative text editing scenarios.

Result: The proposed algorithms were tested through simulated coalition formation processes, showing AI's potential in facilitating democratic large-scale text editing, like drafting community constitutions.

Conclusion: AI can play a key role in enhancing democratic processes and addressing the limitations of traditional tools in collaborative text document creation, particularly in proposing effective compromises.

Abstract: The challenge of finding compromises between agent proposals is fundamental to AI sub-fields such as argumentation, mediation, and negotiation. Building on this tradition, Elkind et al. (2021) introduced a process for coalition formation that seeks majority-supported proposals preferable to the status quo, using a metric space where each agent has an ideal point. The crucial step in this iterative process involves identifying compromise proposals around which agent coalitions can unite. How to effectively find such compromise proposals, however, remains an open question. We address this gap by formalizing a holistic model that encompasses agent bounded rationality and uncertainty and developing AI models to generate such compromise proposals. We focus on the domain of collaboratively writing text documents -- e.g., to enable the democratic creation of a community constitution. We apply NLP (Natural Language Processing) techniques and utilize LLMs (Large Language Models) to create a semantic metric space for text and develop algorithms to suggest suitable compromise points. To evaluate the effectiveness of our algorithms, we simulate various coalition formation processes and demonstrate the potential of AI to facilitate large-scale democratic text editing, such as collaboratively drafting a constitution, an area where traditional tools are limited.

</details>


### [662] [ChargingBoul: A Competitive Negotiating Agent with Novel Opponent Modeling](https://arxiv.org/abs/2512.06595)
*Joe Shymanski*

Main category: cs.MA

TL;DR: The paper introduces ChargingBoul, a negotiating agent for automated negotiations that successfully placed second in ANAC 2022.


<details>
  <summary>Details</summary>
Motivation: To advance automated negotiation techniques in multiagent systems, particularly for applications like e-commerce and autonomous decision-making.

Method: ChargingBoul employs strategies like opponent classification, dynamic bidding adjustments, and concession policies to maximize negotiation outcomes.

Result: ChargingBoul achieved high negotiation outcomes and placed second in ANAC 2022, with effectiveness verified across diverse opponent strategies.

Conclusion: ChargingBoul is a significant step in automated negotiation research, with room for improvements in opponent modeling and adaptive heuristics.

Abstract: Automated negotiation has emerged as a critical area of research in multiagent systems, with applications spanning e-commerce, resource allocation, and autonomous decision-making. This paper presents ChargingBoul, a negotiating agent that competed in the 2022 Automated Negotiating Agents Competition (ANAC) and placed second in individual utility by an exceptionally narrow margin. ChargingBoul employs a lightweight yet effective strategy that balances concession and opponent modeling to achieve high negotiation outcomes. The agent classifies opponents based on bid patterns, dynamically adjusts its bidding strategy, and applies a concession policy in later negotiation stages to maximize utility while fostering agreements. We evaluate ChargingBoul's performance using competition results and subsequent studies that have utilized the agent in negotiation research. Our analysis highlights ChargingBoul's effectiveness across diverse opponent strategies and its contributions to advancing automated negotiation techniques. We also discuss potential enhancements, including more sophisticated opponent modeling and adaptive bidding heuristics, to improve its performance further.

</details>


### [663] [Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics](https://arxiv.org/abs/2512.07462)
*Trung-Kiet Huynh,Duy-Minh Dao-Sy,Thanh-Bang Cao,Phong-Hao Le,Hong-Dan Nguyen,Phu-Quy Nguyen-Lam,Minh-Luan Nguyen-Vo,Hong-Phat Pham,Phu-Hoa Pham,Thien-Kim Than,Chi-Nguyen Tran,Huy Tran,Gia-Thoai Tran-Le,Alessio Buscemi,Le Hong Trang,The Anh Han*

Main category: cs.MA

TL;DR: This paper evaluates Large Language Models' (LLMs') strategic behavior in social dilemmas, showcasing incentive-sensitive cooperation, cross-linguistic variances, and systematic behavioral intentions.


<details>
  <summary>Details</summary>
Motivation: The study aims to examine LLMs' strategic behavior as they increasingly function as decision-makers in multi-agent systems and human societies, influencing AI-driven infrastructures.

Method: The paper extends the FAIRGAME framework using payoff-scaled Prisoner's Dilemma and multi-agent Public Goods Game to study LLM behavior, alongside supervised classification of game strategies.

Result: The findings highlight consistent behavioral patterns in LLMs, including cooperation biases, divergence across languages, and architectural and linguistic influences on decision-making.

Conclusion: This research provides a methodology for auditing LLMs as strategic agents, with key insights for AI governance, collective decision-making, and design of safe AI systems.

Abstract: As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [664] [From sparse recovery to plug-and-play priors, understanding trade-offs for stable recovery with generalized projected gradient descent](https://arxiv.org/abs/2512.07397)
*Ali Joundi,Yann Traonmilin,Jean-François Aujol*

Main category: eess.IV

TL;DR: This paper addresses recovering low-dimensional vectors from noisy measurements using Generalized Projected Gradient Descent (GPGD), extending its robustness against errors and proposing strategies to improve recovery and stability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome challenges associated with recovering unknown low-dimensional vectors from noisy and underdetermined observations in both sparse recovery and image reconstruction.

Method: The study extends GPGD's robustness and stability by introducing normalized idempotent regularization and generalized back-projection for structured noise, along with theoretical and numerical analyses.

Result: Numerical experiments demonstrate the effectiveness of the proposed methods, particularly in balancing identifiability and stability in sparse recovery and image inverse problems.

Conclusion: The authors show that the proposed strategies enhance the stability, robustness, and recovery accuracy of GPGD in noisy, underdetermined scenarios, offering valuable insights for sparse recovery and image processing applications.

Abstract: We consider the problem of recovering an unknown low-dimensional vector from noisy, underdetermined observations. We focus on the Generalized Projected Gradient Descent (GPGD) framework, which unifies traditional sparse recovery methods and modern approaches using learned deep projective priors. We extend previous convergence results to robustness to model and projection errors. We use these theoretical results to explore ways to better control stability and robustness constants. To reduce recovery errors due to measurement noise, we consider generalized back-projection strategies to adapt GPGD to structured noise, such as sparse outliers. To improve the stability of GPGD, we propose a normalized idempotent regularization for the learning of deep projective priors. We provide numerical experiments in the context of sparse recovery and image inverse problems, highlighting the trade-offs between identifiability and stability that can be achieved with such methods.

</details>


### [665] [Proof of Concept for Mammography Classification with Enhanced Compactness and Separability Modules](https://arxiv.org/abs/2512.06575)
*Fariza Dahes*

Main category: eess.IV

TL;DR: The paper validates and extends a medical image classification framework, exploring its application from Alzheimer MRI to mammography classification using enriched architectures and comprehensive evaluations.


<details>
  <summary>Details</summary>
Motivation: To investigate the adaptability of advanced architectures and modules for improving diagnostic accuracy in mammography classification, specifically distinguishing malignant and benign cases.

Method: It employs ConvNeXt Tiny, InceptionV3 backbones with GAGM and SEVector modules, evaluates macro F1, recall variance, ROC/AUC, and performs feature interpretability analysis using Grad CAM, with an interactive dashboard developed for clinical use.

Result: GAGM and SEVector modules effectively enhanced feature discriminability and reduced false negatives for malignant cases, although Feature Smoothing Loss yielded no measurable improvement under mammography conditions.

Conclusion: The proposed framework shows improved diagnostic performance; however, future work should focus on new strategies to enhance intra-class compactness and inter-class separability for better distinction between malignant and benign cases.

Abstract: This study presents a validation and extension of a recent methodological framework for medical image classification. While an improved ConvNeXt Tiny architecture, integrating Global Average and Max Pooling fusion (GAGM), lightweight channel attention (SEVector), and Feature Smoothing Loss (FSL), demonstrated promising results on Alzheimer MRI under CPU friendly conditions, our work investigates its transposability to mammography classification. Using a Kaggle dataset that consolidates INbreast, MIAS, and DDSM mammography collections, we compare a baseline CNN, ConvNeXt Tiny, and InceptionV3 backbones enriched with GAGM and SEVector modules. Results confirm the effectiveness of GAGM and SEVector in enhancing feature discriminability and reducing false negatives, particularly for malignant cases. In our experiments, however, the Feature Smoothing Loss did not yield measurable improvements under mammography classification conditions, suggesting that its effectiveness may depend on specific architectural and computational assumptions. Beyond validation, our contribution extends the original framework through multi metric evaluation (macro F1, per class recall variance, ROC/AUC), feature interpretability analysis (Grad CAM), and the development of an interactive dashboard for clinical exploration. As a perspective, we highlight the need to explore alternative approaches to improve intra class compactness and inter class separability, with the specific goal of enhancing the distinction between malignant and benign cases in mammography classification.

</details>


### [666] [Physics-Guided Diffusion Priors for Multi-Slice Reconstruction in Scientific Imaging](https://arxiv.org/abs/2512.06977)
*Laurentius Valdy,Richard D. Paul,Alessio Quercia,Zhuo Cao,Xuan Zhao,Hanno Scharr,Arya Bangun*

Main category: eess.IV

TL;DR: This paper proposes a framework to improve multi-slice reconstruction efficiency and quality in imaging modalities by integrating partitioned diffusion priors with physics-based constraints, reducing memory usage and enhancing accuracy on various datasets.


<details>
  <summary>Details</summary>
Motivation: There is a need for faster and more accurate multi-slice reconstructions in medical and scientific imaging, but achieving this is challenging due to the ill-posed nature of the problem and high computational/memory requirements.

Method: The method integrates partitioned diffusion priors with physics-based constraints, aiming to reduce memory usage and improve reconstruction quality.

Result: The framework achieves high reconstruction quality, outperforms existing baselines in MRI and 4D-STEM modalities, and demonstrates strong generalization on both in-distribution and out-of-distribution datasets.

Conclusion: The proposed framework effectively addresses memory and quality challenges in multi-slice reconstruction, offering a promising solution for diverse imaging applications.

Abstract: Accurate multi-slice reconstruction from limited measurement data is crucial to speed up the acquisition process in medical and scientific imaging. However, it remains challenging due to the ill-posed nature of the problem and the high computational and memory demands. We propose a framework that addresses these challenges by integrating partitioned diffusion priors with physics-based constraints. By doing so, we substantially reduce memory usage per GPU while preserving high reconstruction quality, outperforming both physics-only and full multi-slice reconstruction baselines for different modalities, namely Magnetic Resonance Imaging (MRI) and four-dimensional Scanning Transmission Electron Microscopy (4D-STEM). Additionally, we show that the proposed method improves in-distribution accuracy as well as strong generalization to out-of-distribution datasets.

</details>


### [667] [Clinical Interpretability of Deep Learning Segmentation Through Shapley-Derived Agreement and Uncertainty Metrics](https://arxiv.org/abs/2512.07224)
*Tianyi Ren,Daniel Low,Pittra Jaengprajak,Juampablo Heras Rivera,Jacob Ruzevick,Mehmet Kurt*

Main category: eess.IV

TL;DR: This paper focuses on using Shapley values to explain the performance of deep learning models in medical image segmentation, ensuring explainability for clinical adoption.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address the gap in explainability for medical image segmentation models, which is essential for their broader acceptance in clinical practice.

Method: The authors used Shapley values to attribute model performance to MRI image contrasts and proposed two metrics: agreement with clinical imaging ranking and Shapley ranking variance across cross-validation folds.

Result: The results showed that higher-performing cases had greater agreement with clinical rankings, and greater variance in Shapley rankings correlated with poorer performance.

Conclusion: The study demonstrates that Shapley values can provide interpretable metrics, aiding clinicians in understanding and trusting deep learning models for segmentation tasks.

Abstract: Segmentation is the identification of anatomical regions of interest, such as organs, tissue, and lesions, serving as a fundamental task in computer-aided diagnosis in medical imaging. Although deep learning models have achieved remarkable performance in medical image segmentation, the need for explainability remains critical for ensuring their acceptance and integration in clinical practice, despite the growing research attention in this area. Our approach explored the use of contrast-level Shapley values, a systematic perturbation of model inputs to assess feature importance. While other studies have investigated gradient-based techniques through identifying influential regions in imaging inputs, Shapley values offer a broader, clinically aligned approach, explaining how model performance is fairly attributed to certain imaging contrasts over others. Using the BraTS 2024 dataset, we generated rankings for Shapley values for four MRI contrasts across four model architectures. Two metrics were proposed from the Shapley ranking: agreement between model and ``clinician" imaging ranking, and uncertainty quantified through Shapley ranking variance across cross-validation folds. Higher-performing cases (Dice \textgreater0.6) showed significantly greater agreement with clinical rankings. Increased Shapley ranking variance correlated with decreased performance (U-Net: $r=-0.581$). These metrics provide clinically interpretable proxies for model reliability, helping clinicians better understand state-of-the-art segmentation models.

</details>


### [668] [Stronger is not better: Better Augmentations in Contrastive Learning for Medical Image Segmentation](https://arxiv.org/abs/2512.05992)
*Azeez Idris,Abdurahman Ali Mohammed,Samuel Fanijo*

Main category: eess.IV

TL;DR: This paper evaluates the use of data augmentations in self-supervised contrastive learning for semantic segmentation of medical images and finds that current techniques don’t always improve results.


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness of data augmentations in improving self-supervised contrastive learning performance, particularly for semantic segmentation in the context of medical images.

Method: The authors analyze current data augmentation techniques and experiment with alternative augmentations for optimizing performance in medical image segmentation.

Result: Findings reveal that existing data augmentations do not consistently enhance performance and propose alternative augmentations yielding better results.

Conclusion: Standard data augmentation techniques in self-supervised contrastive learning may be suboptimal for medical image segmentation and customized augmentations might be needed for improvement.

Abstract: Self-supervised contrastive learning is among the recent representation learning methods that have shown performance gains in several downstream tasks including semantic segmentation. This paper evaluates strong data augmentation, one of the most important components for self-supervised contrastive learning's improved performance. Strong data augmentation involves applying the composition of multiple augmentation techniques on images. Surprisingly, we find that the existing data augmentations do not always improve performance for semantic segmentation for medical images. We experiment with other augmentations that provide improved performance.

</details>


### [669] [Semantic Temporal Single-photon LiDAR](https://arxiv.org/abs/2512.06008)
*Fang Li,Tonglin Mu,Shuling Li,Junran Guo,Keyuan Li,Jianing Li,Ziyang Luo,Xiaodong Fan,Ye Chen,Yunfeng Liu,Hong Cai,Lip Ket Chin,Jinbei Zhang,Shihai Sun*

Main category: eess.IV

TL;DR: The paper introduces a semantic TSP-LiDAR for imaging-free target recognition with a self-updating semantic knowledge base (SKB), achieving superior performance under low SNR and limited acquisition times, and effectively handling open-set scenarios with unknown targets.


<details>
  <summary>Details</summary>
Motivation: Existing TSP-LiDAR systems struggle with open-set scenarios, where unknown targets emerge, and show poor performance in low SNR conditions and with fewer photons. Thus, an adaptive and robust system is needed without requiring extensive retraining.

Method: The authors propose a semantic TSP-LiDAR integrated with a self-updating SKB, formulating target recognition as a semantic communication process. The SKB dynamically updates semantic features of unknown targets, enabling continuous adaptation.

Result: The proposed system achieves a recognition accuracy of 89% on nine unknown target types compared to 66% when the updating mechanism is absent. It outperforms traditional methods in both simulation and physical experiments under low SNR and limited acquisition times.

Conclusion: The semantic TSP-LiDAR with dynamic SKB demonstrates strong potential for adaptive and robust target recognition in complex and dynamic environments, reducing the need for retraining.

Abstract: Temporal single-photon (TSP-) LiDAR presents a promising solution for imaging-free target recognition over long distances with reduced size, cost, and power consumption. However, existing TSP-LiDAR approaches are ineffective in handling open-set scenarios where unknown targets emerge, and they suffer significant performance degradation under low signal-to-noise ratio (SNR) and short acquisition times (fewer photons). Here, inspired by semantic communication, we propose a semantic TSP-LiDAR based on a self-updating semantic knowledge base (SKB), in which the target recognition processing of TSP-LiDAR is formulated as a semantic communication. The results, both simulation and experiment, demonstrate that our approach surpasses conventional methods, particularly under challenging conditions of low SNR and limited acquisition time. More importantly, our self-updating SKB mechanism can dynamically update the semantic features of newly encountered targets in the SKB, enabling continuous adaptation without the need for extensive retraining of the neural network. In fact, a recognition accuracy of 89% is achieved on nine types of unknown targets in real-world experiments, compared to 66% without the updating mechanism. These findings highlight the potential of our framework for adaptive and robust target recognition in complex and dynamic environments.

</details>


### [670] [R2MF-Net: A Recurrent Residual Multi-Path Fusion Network for Robust Multi-directional Spine X-ray Segmentation](https://arxiv.org/abs/2512.07576)
*Xuecheng Li,Weikuan Jia,Komildzhon Sharipov,Sharipov Hotam Beknazarovich,Farzona S. Ataeva,Qurbonaliev Alisher,Yuanjie Zheng*

Main category: eess.IV

TL;DR: This paper proposes R2MF-Net, a deep learning network designed for automatic segmentation of multi-directional spine X-ray images to enable accurate scoliosis assessments.


<details>
  <summary>Details</summary>
Motivation: Manual segmentation of spinal structures in X-rays for scoliosis assessment is time-consuming, inconsistent, and challenging due to low-contrast images and overlapping tissues.

Method: R2MF-Net employs a two-stage encoder-decoder architecture with coarse and fine segmentation networks. It incorporates unique features such as R2-Jump modules, MC-Skip mechanisms, and SCSE-Lite blocks for improved accuracy and robustness.

Result: The proposed approach was evaluated on a clinical dataset with 228 multi-view spine X-ray images and demonstrated improved segmentation capabilities across imaging directions and contrast conditions.

Conclusion: R2MF-Net effectively automates spine segmentation in multi-directional X-ray images, enhancing reproducibility and efficiency in scoliosis assessment workflows.

Abstract: Accurate segmentation of spinal structures in X-ray images is a prerequisite for quantitative scoliosis assessment, including Cobb angle measurement, vertebral translation estimation and curvature classification. In routine practice, clinicians acquire coronal, left-bending and right-bending radiographs to jointly evaluate deformity severity and spinal flexibility. However, the segmentation step remains heavily manual, time-consuming and non-reproducible, particularly in low-contrast images and in the presence of rib shadows or overlapping tissues. To address these limitations, this paper proposes R2MF-Net, a recurrent residual multi-path encoder--decoder network tailored for automatic segmentation of multi-directional spine X-ray images. The overall design consists of a coarse segmentation network and a fine segmentation network connected in cascade. Both stages adopt an improved Inception-style multi-branch feature extractor, while a recurrent residual jump connection (R2-Jump) module is inserted into skip paths to gradually align encoder and decoder semantics. A multi-scale cross-stage skip (MC-Skip) mechanism allows the fine network to reuse hierarchical representations from multiple decoder levels of the coarse network, thereby strengthening the stability of segmentation across imaging directions and contrast conditions. Furthermore, a lightweight spatial-channel squeeze-and-excitation block (SCSE-Lite) is employed at the bottleneck to emphasize spine-related activations and suppress irrelevant structures and background noise. We evaluate R2MF-Net on a clinical multi-view radiograph dataset comprising 228 sets of coronal, left-bending and right-bending spine X-ray images with expert annotations.

</details>


### [671] [Affine Subspace Models and Clustering for Patch-Based Image Denoising](https://arxiv.org/abs/2512.07259)
*Tharindu Wickremasinghe,Marco F. Duarte*

Main category: eess.IV

TL;DR: The paper studies affine subspace clustering for image tile groups in denoising tasks and proposes a denoising algorithm achieving better clustering and denoising results.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies of linear subspace models in representing non-negative image patches, and improve clustering and denoising performance by exploring affine subspace models.

Method: The authors introduced an affine subspace clustering model for image tiles and implemented a simple denoising algorithm based on least squares projection, comparing various algorithmic approaches for affine subspace clustering.

Result: Experimental results demonstrated better clustering and denoising performance of the affine subspace models over the conventional methods.

Conclusion: Using affine subspace models for image tile clustering matches the geometric characteristics of image data better than linear models, leading to measurable performance improvements in denoising tasks.

Abstract: Image tile-based approaches are popular in many image processing applications such as denoising (e.g., non-local means). A key step in their use is grouping the images into clusters, which usually proceeds iteratively splitting the images into clusters and fitting a model for the images in each cluster. Linear subspaces have emerged as a suitable model for tile clusters; however, they are not well matched to images patches given that images are non-negative and thus not distributed around the origin in the tile vector space. We study the use of affine subspace models for the clusters to better match the geometric structure of the image tile vector space. We also present a simple denoising algorithm that relies on the affine subspace clustering model using least squares projection. We review several algorithmic approaches to solve the affine subspace clustering problem and show experimental results that highlight the performance improvements in clustering and denoising.

</details>


### [672] [Precise Liver Tumor Segmentation in CT Using a Hybrid Deep Learning-Radiomics Framework](https://arxiv.org/abs/2512.07574)
*Xuecheng Li,Weikuan Jia,Komildzhon Sharipov,Alimov Ruslan,Lutfuloev Mazbutdzhon,Ismoilov Shuhratjon,Yuanjie Zheng*

Main category: eess.IV

TL;DR: The paper presents a hybrid framework for automatic liver and liver-tumor segmentation using deep learning and radiomic features to address challenges like low contrast and complex tumor boundaries in contrast-enhanced CT scans.


<details>
  <summary>Details</summary>
Motivation: Manual liver tumor contouring is time-intensive, subjective, and difficult to standardize across medical centers, necessitating an automated and accurate segmentation method.

Method: The proposed framework combines several techniques: an attention-enhanced cascaded U-Net for initial segmentation, temporal consistency refinement, radiomic feature extraction and classification, and a 3D CNN for voxel-level final adjustments.

Result: The framework integrates multiple innovative approaches to improve segmentation quality, especially in handling complex tumor characteristics and reducing the risk of false positives.

Conclusion: The hybrid method demonstrates potential for accurate liver and tumor segmentation, capable of addressing challenges like low contrast and heterogeneous patterns, promising for clinical applications.

Abstract: Accurate three-dimensional delineation of liver tumors on contrast-enhanced CT is a prerequisite for treatment planning, navigation and response assessment, yet manual contouring is slow, observer-dependent and difficult to standardise across centres. Automatic segmentation is complicated by low lesion-parenchyma contrast, blurred or incomplete boundaries, heterogeneous enhancement patterns, and confounding structures such as vessels and adjacent organs. We propose a hybrid framework that couples an attention-enhanced cascaded U-Net with handcrafted radiomics and voxel-wise 3D CNN refinement for joint liver and liver-tumor segmentation. First, a 2.5D two-stage network with a densely connected encoder, sub-pixel convolution decoders and multi-scale attention gates produces initial liver and tumor probability maps from short stacks of axial slices. Inter-slice temporal consistency is then enforced by a simple three-slice refinement rule along the cranio-caudal direction, which restores thin and tiny lesions while suppressing isolated noise. Next, 728 radiomic descriptors spanning intensity, texture, shape, boundary and wavelet feature groups are extracted from candidate lesions and reduced to 20 stable, highly informative features via multi-strategy feature selection; a random forest classifier uses these features to reject false-positive regions. Finally, a compact 3D patch-based CNN derived from AlexNet operates in a narrow band around the tumor boundary to perform voxel-level relabelling and contour smoothing.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [673] [Machine Learning-based Unfolding for Cross Section Measurements in the Presence of Nuisance Parameters](https://arxiv.org/abs/2512.07074)
*Huanbiao Zhu,Krish Desai,Mikael Kuusela,Vinicius Mikuni,Benjamin Nachman,Larry Wasserman*

Main category: stat.AP

TL;DR: The paper proposes a new machine learning algorithm, Profile OmniFold, which advances the OmniFold forward model for high-dimensional data unfolding by incorporating nuisance parameters.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle detector effects and enhance high-dimensional data unfolding accuracy in particle physics, specifically addressing the issue of approximately specified forward models with encoded uncertainty through nuisance parameters.

Method: The methodology involves extending the OmniFold algorithm by incorporating nuisance parameters, demonstrated through Gaussian and CMS Experiment simulated data case studies.

Result: The Profile OmniFold algorithm is successfully developed and demonstrated to work for simulated high-dimensional data, exhibiting its application in particle physics.

Conclusion: Profile OmniFold extends unfolding capabilities by effectively handling nuisance parameters in simulation-based approaches, which can be applied in complex detector systems.

Abstract: Statistically correcting measured cross sections for detector effects is an important step across many applications. In particle physics, this inverse problem is known as \textit{unfolding}. In cases with complex instruments, the distortions they introduce are often known only implicitly through simulations of the detector. Modern machine learning has enabled efficient simulation-based approaches for unfolding high-dimensional data. Among these, one of the first methods successfully deployed on experimental data is the \textsc{OmniFold} algorithm, a classifier-based Expectation-Maximization procedure. In practice, however, the forward model is only approximately specified, and the corresponding uncertainty is encoded through nuisance parameters. Building on the well-studied \textsc{OmniFold} algorithm, we show how to extend machine learning-based unfolding to incorporate nuisance parameters. Our new algorithm, called Profile \textsc{OmniFold}, is demonstrated using a Gaussian example as well as a particle physics case study using simulated data from the CMS Experiment at the Large Hadron Collider.

</details>


### [674] [Forests of Uncertaint(r)ees: Using tree-based ensembles to estimate probability distributions of future conflict](https://arxiv.org/abs/2512.06210)
*Daniel Mittermaier,Tobias Bohne,Martin Hofer,Daniel Racek*

Main category: stat.AP

TL;DR: The paper addresses the challenge of uncertain predictions in violent conflict forecasting by using a novel method for quantifying predictive distributions that outperform traditional conflict prediction benchmarks.


<details>
  <summary>Details</summary>
Motivation: The research aims to improve predictions of fatalities from violent conflicts by addressing high uncertainty due to inherent conflict characteristics and data limitations.

Method: The paper employs a custom auto-ML approach combining tree-based classifiers and distributional regressors, along with spatial ensembles and regional models, to predict full probability distributions for violent conflict fatalities.

Result: The proposed model consistently outperformed benchmarks for predictions up to a year in advance, especially in conflict-prone regions, while smaller regional models maintained performance and held potential for future integration.

Conclusion: The study highlights the importance of understanding predictive metrics in highly zero-inflated datasets and provides methods that enhance predictive capabilities while accommodating data limitations.

Abstract: Predictions of fatalities from violent conflict on the PRIO-GRID-month (pgm) level are characterized by high levels of uncertainty, limiting their usefulness in practical applications. We discuss the two main sources of uncertainty for this prediction task, the nature of violent conflict and data limitations, embedding this in the wider literature on uncertainty quantification in machine learning. We develop a strategy to quantify uncertainty in conflict forecasting, shifting from traditional point predictions to full predictive distributions. Our approach compares and combines multiple tree-based classifiers and distributional regressors in a custom auto-ML setup, estimating distributions for each pgm individually. We also test the integration of regional models in spatial ensembles as a potential avenue to reduce uncertainty. The models are able to consistently outperform a suite of benchmarks derived from conflict history in predictions up to one year in advance, with performance driven by regions where conflict was observed. With our evaluation, we emphasize the need to understand how a metric behaves for a given prediction problem, in our case characterized by extremely high zero-inflatedness. While not resulting in better predictions, the integration of smaller models does not decrease performance for this prediction task, opening avenues to integrate data sources with less spatial coverage in the future.

</details>


### [675] [A Latent Variable Framework for Scaling Laws in Large Language Models](https://arxiv.org/abs/2512.06553)
*Peiyao Cai,Chengyu Cui,Felipe Maia Polo,Seamus Somerstep,Leshem Choshen,Mikhail Yurochkin,Moulinath Banerjee,Yuekai Sun,Kean Ming Tan,Gongjun Xu*

Main category: stat.AP

TL;DR: The paper proposes a statistical framework for analyzing scaling laws in large language models using latent variable modeling, accommodating diverse architectures and benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the need to effectively evaluate diverse LLM families with distinct architectures and training strategies across multiple benchmarks, overcoming the limitations of a single global scaling curve.

Method: A latent variable modeling framework associates each LLM family with latent variables reflecting shared traits, influencing performance on benchmarks through observable features. Statistical properties and numerical estimation procedures are developed.

Result: Empirical evaluation of the proposed framework was conducted on 12 benchmarks from the Open LLM Leaderboard, demonstrating its effectiveness in capturing performance variations.

Conclusion: The framework successfully addresses heterogeneity in LLM architectures and benchmarks, providing an effective way to model scaling laws and evaluate LLM performance across diverse datasets.

Abstract: We propose a statistical framework built on latent variable modeling for scaling laws of large language models (LLMs). Our work is motivated by the rapid emergence of numerous new LLM families with distinct architectures and training strategies, evaluated on an increasing number of benchmarks. This heterogeneity makes a single global scaling curve inadequate for capturing how performance varies across families and benchmarks. To address this, we propose a latent variable modeling framework in which each LLM family is associated with a latent variable that captures the common underlying features in that family. An LLM's performance on different benchmarks is then driven by its latent skills, which are jointly determined by the latent variable and the model's own observable features. We develop an estimation procedure for this latent variable model and establish its statistical properties. We also design efficient numerical algorithms that support estimation and various downstream tasks. Empirically, we evaluate the approach on 12 widely used benchmarks from the Open LLM Leaderboard (v1/v2).

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [676] [PRIMRose: Insights into the Per-Residue Energy Metrics of Proteins with Double InDel Mutations using Deep Learning](https://arxiv.org/abs/2512.06496)
*Stella Brown,Nicolas Preisig,Autumn Davis,Brian Hutchinson,Filip Jagodzinski*

Main category: q-bio.BM

TL;DR: The study introduces PRIMRose, a method for predicting residue-specific energetic impacts of double InDels in protein sequences using a convolutional neural network.


<details>
  <summary>Details</summary>
Motivation: Advancements in computational biology and bioinformatics demand granularity in understanding how mutations, specifically double InDels, influence protein structure and function at the residue level.

Method: A Convolutional Neural Network is developed to analyze energy changes at the residue level, using datasets based on double InDel mutations from nine proteins categorized into three distinct datasets.

Result: The model achieves high accuracy using Rosetta-calculated energy metrics and uncovers factors like solvent accessibility and secondary structure, which influence residue-level mutational impacts.

Conclusion: PRIMRose enables localized insights into protein mutational tolerance, offering more interpretable and biologically meaningful predictions on the effect of double InDels.

Abstract: Understanding how protein mutations affect protein structure is essential for advancements in computational biology and bioinformatics. We introduce PRIMRose, a novel approach that predicts energy values for each residue given a mutated protein sequence. Unlike previous models that assess global energy shifts, our method analyzes the localized energetic impact of double amino acid insertions or deletions (InDels) at the individual residue level, enabling residue-specific insights into structural and functional disruption. We implement a Convolutional Neural Network architecture to predict the energy changes of each residue in a protein mutation. We train our model on datasets constructed from nine proteins, grouped into three categories: one set with exhaustive double InDel mutations, another with approximately 145k randomly sampled double InDel mutations, and a third with approximately 80k randomly sampled double InDel mutations. Our model achieves high predictive accuracy across a range of energy metrics as calculated by the Rosetta molecular modeling suite and reveals localized patterns that influence model performance, such as solvent accessibility and secondary structure context. This per-residue analysis offers new insights into the mutational tolerance of specific regions within proteins and provides higher interpretable and biologically meaningful predictions of InDels' effects.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [677] [Instance Dependent Testing of Samplers using Interval Conditioning](https://arxiv.org/abs/2512.06458)
*Rishiraj Bhattacharyya,Sourav Chakraborty,Yash Pote,Uddalok Sarkar,Sayantan Sen*

Main category: cs.DS

TL;DR: The paper introduces a new sampler verification algorithm that allows testing samplers over natural numbers and demonstrates significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Verifying samplers for correctness is challenging, especially for samplers on infinite domains, which are common in various fields like Astronomy and Finance.

Method: The method involves a novel distance estimation algorithm utilizing interval conditioning to test unknown samplers against known distributions, with a new approach to estimating probability mass in continuous distributions.

Result: The proposed tester achieves major practical benefits, demonstrating up to 1000x speed improvement over current state-of-the-art testers.

Conclusion: The work pioneers the first instance-dependent tester for samplers over infinite domains, substantially advancing efficiency in sampler verification.

Abstract: Sampling algorithms play a pivotal role in probabilistic AI. However, verifying if a sampler program indeed samples from the claimed distribution is a notoriously hard problem. Provably correct testers like Barbarik, Teq, Flash, CubeProbe for testing of different kinds of samplers were proposed only in the last few years. All these testers focus on the worst-case efficiency, and do not support verification of samplers over infinite domains, a case occurring frequently in Astronomy, Finance, Network Security, etc.
  In this work, we design the first tester of samplers with instance-dependent efficiency, allowing us to test samplers over natural numbers. Our tests are developed via a novel distance estimation algorithm between an unknown and a known probability distribution using an interval conditioning framework. The core technical contribution is a new connection with probability mass estimation of a continuous distribution. The practical gains are also substantial: our experiments establish up to 1000x speedup over state-of-the-art testers.

</details>


### [678] [A Broader View on Clustering under Cluster-Aware Norm Objectives](https://arxiv.org/abs/2512.06211)
*Martin G. Herold,Evangelos Kipouridis,Joachim Spoerhase*

Main category: cs.DS

TL;DR: The paper improves approximation algorithms for the $(f,g)$-clustering problem, focusing on general settings and bridging gaps in understanding compared to basic clustering problems.


<details>
  <summary>Details</summary>
Motivation: To address shortcomings in previous approximation bounds for $(f,g)$-clustering and extend the knowledge beyond special cases handled earlier.

Method: Designing new approximation algorithms addressing $(f, L_1)$-clustering and general $(f,g)$-clustering problems, leveraging new parameter-dependent insights.

Result: Achieved $O(\log^2 n)$-approximation for $(f, L_1)$-clustering and $O(k)$-approximation for general $(f,g)$-clustering, improving upon earlier bounds.

Conclusion: The work offers clearer understanding and improved guarantees for $(f,g)$-clustering, bridging previous gaps and providing unified insights across fundamental clustering objectives.

Abstract: We revisit the $(f,g)$-clustering problem that we introduced in a recent work [SODA'25], and which subsumes fundamental clustering problems such as $k$-Center, $k$-Median, Min-Sum of Radii, and Min-Load $k$-Clustering. This problem assigns each of the $k$ clusters a cost determined by the monotone, symmetric norm $f$ applied to the vector distances in the cluster, and aims at minimizing the norm $g$ applied to the vector of cluster costs. Previously, we focused on certain special cases for which we designed constant-factor approximation algorithms. Our bounds for more general settings left, however, large gaps to the known bounds for the basic problems they capture.
  In this work, we provide a clearer picture of the approximability of these more general settings. First, we design an $O(\log^2 n)$-approximation algorithm for $(f, L_{1})$-clustering for any $f$. This improves upon our previous $\widetilde{O}(\sqrt{n})$-approximation. Second, we provide an $O(k)$-approximation for the general $(f,g)$-clustering problem, which improves upon our previous $\widetilde{O}(\sqrt{kn})$-approximation algorithm and matches the best-known upper bound for Min-Load $k$-Clustering.
  We then design an approximation algorithm for $(f,g)$-clustering that interpolates, up to polylog factors, between the best known bounds for $k$-Center, $k$-Median, Min-Sum of Radii, Min-Load $k$-Clustering, (Top, $L_{1}$)-clustering, and $(L_{\infty},g)$-clustering based on a newly defined parameter of $f$ and $g$.

</details>


### [679] [Chromatic Feature Vectors for 2-Trees: Exact Formulas for Partition Enumeration with Network Applications](https://arxiv.org/abs/2512.07120)
*J. Allagan,G. Morgan,S. Langley,R. Lopez-Bonilla,V. Deriglazov*

Main category: cs.DS

TL;DR: The paper establishes efficient closed-form formulas for enumerating chromatic feature vectors for specific graph types under constraints, revealing meaningful structural properties.


<details>
  <summary>Details</summary>
Motivation: The study aims to derive structural features from constrained graph colorings, addressing constraints relevant to distributed systems to prevent complete concentration or isolation of components.

Method: The approach includes deriving closed-form formulas for constrained chromatic properties in 2-trees, Theta graphs, and fan graphs using combinatorial mathematics.

Result: Efficient computation methods are provided for certain graph types, with results including connections to Fibonacci and Stirling numbers, improving analysis efficiency.

Conclusion: The study introduces meaningful structural features beyond classical chromatic polynomials, valuable for various applications in distributed systems and cryptography.

Abstract: We establish closed-form enumeration formulas for chromatic feature vectors of 2-trees under the bichromatic triangle constraint. These efficiently computable structural features derive from constrained graph colorings where each triangle uses exactly two colors, forbidding monochromatic and rainbow triangles, a constraint arising in distributed systems where components avoid complete concentration or isolation. For theta graphs Theta_n, we prove r_k(Theta_n) = S(n-2, k-1) for k >= 3 (Stirling numbers of the second kind) and r_2(Theta_n) = 2^(n-2) + 1, computable in O(n) time. For fan graphs Phi_n, we establish r_2(Phi_n) = F_{n+1} (Fibonacci numbers) and derive explicit formulas r_k(Phi_n) = sum_{t=k-1}^{n-1} a_{n-1,t} * S(t, k-1) with efficiently computable binomial coefficients, achieving O(n^2) computation per component. Unlike classical chromatic polynomials, which assign identical features to all n-vertex 2-trees, bichromatic constraints provide informative structural features. While not complete graph invariants, these features capture meaningful structural properties through connections to Fibonacci polynomials, Bell numbers, and independent set enumeration. Applications include Byzantine fault tolerance in hierarchical networks, VM allocation in cloud computing, and secret-sharing protocols in distributed cryptography.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [680] [From Tail Universality to Bernstein-von Mises: A Unified Statistical Theory of Semi-Implicit Variational Inference](https://arxiv.org/abs/2512.06107)
*Sean Plummer*

Main category: math.ST

TL;DR: The paper develops a unified theory for semi-implicit variational inference (SIVI), analyzing its approximation, optimization, and statistical consistency properties.


<details>
  <summary>Details</summary>
Motivation: To address the theoretical gaps in understanding semi-implicit variational inference (SIVI) concerning its capacity to approximate target distributions and the stability of optimization methods.

Method: The authors analyze SIVI in three aspects: (1) approximation - exploring conditions for when SIVI families are dense and highlighting limitations, (2) optimization - proving convergence and oracle inequalities for empirical objectives, and (3) a combined statistical theory for end-to-end performance.

Result: The study identifies sharp conditions where SIVI succeeds or fails in recovering target distributions, derives consistency results for empirical maximizers, and explains how design choices impact SIVI's asymptotic performance.

Conclusion: This work provides a rigorous theoretical foundation for SIVI, offering insights into its strengths, limitations, and how to structure it for effective use.

Abstract: Semi-implicit variational inference (SIVI) constructs approximate posteriors of the form $q(θ) = \int k(θ| z) r(dz)$, where the conditional kernel is parameterized and the mixing base is fixed and tractable. This paper develops a unified "approximation-optimization-statistics'' theory for such families.
  On the approximation side, we show that under compact L1-universality and a mild tail-dominance condition, semi-implicit families are dense in L1 and can achieve arbitrarily small forward Kullback-Leibler (KL) error. We also identify two sharp obstructions to global approximation: (i) an Orlicz tail-mismatch condition that induces a strictly positive forward-KL gap, and (ii) structural restrictions, such as non-autoregressive Gaussian kernels, that force "branch collapse'' in conditional distributions. For each obstruction we give a minimal structural modification that restores approximability.
  On the optimization side, we establish finite-sample oracle inequalities and prove that the empirical SIVI objectives L(K,n) $Γ$-converge to their population limit as n and K tend to infinity. These results give consistency of empirical maximizers, quantitative control of finite-K surrogate bias, and stability of the resulting variational posteriors.
  Combining the approximation and optimization analyses yields the first general end-to-end statistical theory for SIVI: we characterize precisely when SIVI can recover the target distribution, when it cannot, and how architectural and algorithmic choices govern the attainable asymptotic behavior.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [681] [Simultaneous Heterogeneity and Reduced-rank Learning for Multivariate Response Regression](https://arxiv.org/abs/2512.06514)
*Jie Wu,Bo Zhang,Daoji Li,Zemin Zheng*

Main category: stat.ME

TL;DR: The paper proposes a method to detect subgroups and estimate covariate effects in multivariate regression using rank-constrained fusion without prior knowledge of subgroup memberships.


<details>
  <summary>Details</summary>
Motivation: Subgroup detection in heterogeneous populations, especially with multivariate responses, is critical but underexplored, as most methods target univariate responses.

Method: The method employs a joint framework integrating rank-constrained pairwise fusion and heterogeneity learning, implemented via ADMM, with convergence and asymptotic properties established.

Result: The approach demonstrates robustness and effectiveness through simulations and a real data application, with an introduced criterion for selecting the coefficient matrix rank.

Conclusion: This method effectively addresses subgroup structure detection and covariate effect estimation in heterogeneous multivariate regression settings.

Abstract: Heterogeneous data are now ubiquitous in many applications in which correctly identifying the subgroups from a heterogeneous population is critical. Although there is an increasing body of literature on subgroup detection, existing methods mainly focus on the univariate response setting. In this paper, we propose a joint heterogeneity and reduced-rank learning framework to simultaneously identify the subgroup structure and estimate the covariate effects for heterogeneous multivariate response regression. In particular, our approach uses rank-constrained pairwise fusion penalization and conducts the subgroup analysis without requiring prior knowledge regarding the individual subgroup memberships. We implement the proposed approach by an alternating direction method of multipliers (ADMM) algorithm and show its convergence. We also establish the asymptotic properties for the resulting estimators under mild and interpretable conditions. A predictive information criterion is proposed to select the rank of the coefficient matrix with theoretical support. The effectiveness of the proposed approach is demonstrated through simulation studies and a real data application.

</details>


### [682] [Hierarchical Clustering With Confidence](https://arxiv.org/abs/2512.06522)
*Di Wu,Jacob Bien,Snigdha Panigrahi*

Main category: stat.ME

TL;DR: Hierarchical clustering is improved by introducing a randomization scheme and valid hypothesis testing to overcome sensitivity issues.


<details>
  <summary>Details</summary>
Motivation: Address instability in hierarchical clustering caused by its greedy nature and create reliable hypothesis testing procedures.

Method: Propose a randomization technique and construct valid p-values for dendrogram nodes to control Type I errors.

Result: Simulations reveal enhanced power in hypothesis tests and effective clustering estimation procedures with probabilistic guarantees.

Conclusion: The method improves clustering stability, provides valid statistical procedures, and demonstrates practical utility in estimating and validating clusters.

Abstract: Agglomerative hierarchical clustering is one of the most widely used approaches for exploring how observations in a dataset relate to each other. However, its greedy nature makes it highly sensitive to small perturbations in the data, often producing different clustering results and making it difficult to separate genuine structure from spurious patterns. In this paper, we show how randomizing hierarchical clustering can be useful not just for measuring stability but also for designing valid hypothesis testing procedures based on the clustering results.
  We propose a simple randomization scheme together with a method for constructing a valid p-value at each node of the hierarchical clustering dendrogram that quantifies evidence against performing the greedy merge. Our test controls the Type I error rate, works with any hierarchical linkage without case-specific derivations, and simulations show it is substantially more powerful than existing selective inference approaches. To demonstrate the practical utility of our p-values, we develop an adaptive $α$-spending procedure that estimates the number of clusters, with a probabilistic guarantee on overestimation. Experiments on simulated and real data show that this estimate yields powerful clustering and can be used, for example, to assess clustering stability across multiple runs of the randomized algorithm.

</details>


### [683] [Latency-Response Theory Model: Evaluating Large Language Models via Response Accuracy and Chain-of-Thought Length](https://arxiv.org/abs/2512.07019)
*Zhiyu Xu,Jia Liu,Yixin Wang,Yuqi Gu*

Main category: stat.ME

TL;DR: The paper introduces the Latency-Response Theory (LaRT), a model that incorporates response accuracy and chain of thought length to better evaluate large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for LLMs, like Item Response Theory (IRT), are limited as they primarily consider response accuracy and overlook other factors like reasoning ability indicated by chain of thought (CoT) lengths.

Method: The authors propose LaRT, a model that accounts for both response accuracy and CoT length, incorporating a correlation parameter between latent ability and latent speed. They developed a stochastic approximation Expectation-Maximization algorithm for parameter estimation and provided rigorous theoretical validations of the model.

Result: LaRT demonstrates advantages over IRT, including better estimation accuracy, narrower confidence intervals, and improved evaluation metrics such as predictive power and ranking validity. It produces different and seemingly better LLM rankings compared to IRT when applied to responses on benchmark datasets.

Conclusion: LaRT proves to be a valid and efficient alternative to IRT, enabling more comprehensive evaluations of LLMs by considering both accuracy and CoT length. Its superior evaluation capabilities are empirically validated on benchmark datasets.

Abstract: The proliferation of Large Language Models (LLMs) necessitates valid evaluation methods to provide guidance for both downstream applications and actionable future improvements. The Item Response Theory (IRT) model with Computerized Adaptive Testing has recently emerged as a promising framework for evaluating LLMs via their response accuracy. Beyond simple response accuracy, LLMs' chain of thought (CoT) lengths serve as a vital indicator of their reasoning ability. To leverage the CoT length information to assist the evaluation of LLMs, we propose the Latency-Response Theory (LaRT) model, which jointly models both the response accuracy and CoT length by introducing a key correlation parameter between the latent ability and the latent speed. We derive an efficient stochastic approximation Expectation-Maximization algorithm for parameter estimation. We establish rigorous identifiability results for the latent ability and latent speed parameters to ensure the statistical validity of their estimation. Through both theoretical asymptotic analyses and simulation studies, we demonstrate LaRT's advantages over IRT in terms of superior estimation accuracy and shorter confidence intervals for latent trait estimation. To evaluate LaRT in real data, we collect responses from diverse LLMs on popular benchmark datasets. We find that LaRT yields different LLM rankings than IRT and outperforms IRT across multiple key evaluation metrics including predictive power, item efficiency, ranking validity, and LLM evaluation efficiency. Code and data are available at https://github.com/Toby-X/Latency-Response-Theory-Model.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [684] [Accelerating Materials Discovery: Learning a Universal Representation of Chemical Processes for Cross-Domain Property Prediction](https://arxiv.org/abs/2512.05979)
*Mikhail Tsitsvero,Atsuyuki Nakao,Hisaki Ikebata*

Main category: physics.chem-ph

TL;DR: The paper proposes a machine learning model using process-graph representation and multimodal graph neural networks to generalize across domains and improve materials discovery.


<details>
  <summary>Details</summary>
Motivation: Experimental validation in chemical processes is slow and expensive. Existing data is hard to utilize due to its heterogeneity.

Method: The authors introduce a directed-tree process-graph representation combined with a multi-modal graph neural network, integrating unstructured text, molecular structures, and numeric data.

Result: The pretrained model learns universal representations from 700,000 process graphs and generalizes well across domains with high performance on specialized datasets.

Conclusion: Universal process representations learned at large scale enable effective transfer to domain-specific predictive tasks, reducing dependency on extensive new data.

Abstract: Experimental validation of chemical processes is slow and costly, limiting exploration in materials discovery. Machine learning can prioritize promising candidates, but existing data in patents and literature is heterogeneous and difficult to use. We introduce a universal directed-tree process-graph representation that unifies unstructured text, molecular structures, and numeric measurements into a single machine-readable format. To learn from this structured data, we developed a multi-modal graph neural network with a property-conditioned attention mechanism. Trained on approximately 700,000 process graphs from nearly 9,000 diverse documents, our model learns semantically rich embeddings that generalize across domains. When fine-tuned on compact, domain-specific datasets, the pretrained model achieves strong performance, demonstrating that universal process representations learned at scale transfer effectively to specialized prediction tasks with minimal additional data.

</details>


### [685] [Two-dimensional RMSD projections for reaction path visualization and validation](https://arxiv.org/abs/2512.07329)
*Rohit Goswami*

Main category: physics.chem-ph

TL;DR: The paper introduces a two-dimensional mapping method for analyzing transition state trajectories in computational chemistry, improving visualization and diagnosis compared to traditional 1D methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for analyzing transition states often reduce dimensions, obscuring high-dimensional structural rearrangements and hindering comparisons between optimization methods.

Method: The authors propose mapping trajectories onto a 2D surface using a corrected root mean square deviation from reactant/product configurations, with energy visualized via radial basis function interpolation.

Result: The new method enhances detection of trajectory details, endpoint basins, and convergence issues, as validated on a cycloaddition reaction with machine-learning and density functional theory comparisons.

Conclusion: This two-dimensional approach provides clearer insights into optimization trajectories and energy landscapes, facilitating improved analysis over conventional methods.

Abstract: Transition state or minimum energy path finding methods constitute a routine component of the computational chemistry toolkit. Standard analysis involves trajectories conventionally plotted in terms of the relative energy to the initial state against a cumulative displacement variable, or the image number. These dimensional reductions obscure structural rearrangements in high dimensions and may often be trajectory dependent. This precludes the ability to compare optimization trajectories of different methods beyond the number of calculations, time taken, and final saddle geometry. We present a method mapping trajectories onto a two-dimension surface defined by a permutation corrected root mean square deviation from the reactant and product configurations. Energy is represented as an interpolated color-mapped surface constructed from all optimization steps using radial basis functions. This representation highlights optimization trajectories, identifies endpoint basins, and diagnoses convergence concerns invisible in one-dimensional profiles. We validate the framework on a cycloaddition reaction, showing that a machine-learned potential saddle and density functional theory reference lie on comparable energy contours despite geometric displacements.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [686] [Reasoning about concurrent loops and recursion with rely-guarantee rules](https://arxiv.org/abs/2512.06242)
*Ian J. Hayes,Larissa A. Meinicke,Cliff B. Jones*

Main category: cs.LO

TL;DR: The paper presents verified refinement rules for reasoning about recursive programs and loops in concurrent systems using the rely-guarantee approach.


<details>
  <summary>Details</summary>
Motivation: To provide a robust, mechanically verified framework for reasoning about concurrency without assuming atomic expression evaluation.

Method: Using the rely-guarantee approach, the authors develop fixed-point-based reasoning laws for recursive programs and loops.

Result: The authors establish laws for handling interference in concurrent threads and apply fixed-point reasoning to loops and recursive programs.

Conclusion: The paper contributes verified refinement laws that simplify compositional reasoning about concurrency, particularly for loops and recursion.

Abstract: The objective of this paper is to present general, mechanically verified, refinement rules for reasoning about recursive programs and while loops in the context of concurrency. Unlike many approaches to concurrency, we do not assume that expression evaluation is atomic. We make use of the rely-guarantee approach to concurrency that facilitates reasoning about interference from concurrent threads in a compositional manner. Recursive programs can be defined as fixed points over a lattice of commands and hence we develop laws for reasoning about fixed points. Loops can be defined in terms of fixed points and hence the laws for recursion can be applied to develop laws for loops.

</details>


### [687] [Formal that "Floats" High: Formal Verification of Floating Point Arithmetic](https://arxiv.org/abs/2512.06850)
*Hansa Mohanty,Vaisakh Naduvodi Viswambharan,Deepak Narayan Gadde*

Main category: cs.LO

TL;DR: This paper proposes a scalable methodology for verifying floating-point arithmetic by using direct RTL-to-RTL model checking integrated with AI-driven property generation and refinement.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in verifying floating-point arithmetic, including non-linear behavior, coupling of control and datapath, and limitations of high-level C model-based approaches.

Method: A divide-and-conquer verification approach is applied at the RTL-to-RTL level, using helper assertions, CEX-guided refinement, targeted fault injection, and incorporating AI-guided property generation with human intervention.

Result: The method achieves better coverage efficiency and fewer assertion requirements in RTL-to-RTL verification compared to standalone approaches, especially when using AI-generated and human-refined properties.

Conclusion: Direct RTL-to-RTL model checking with AI augmentation and modular decomposition improves scalability, robustness, and efficiency in verifying floating-point arithmetic designs.

Abstract: Formal verification of floating-point arithmetic remains challenging due to non-linear arithmetic behavior and the tight coupling between control and datapath logic. Existing approaches often rely on high-level C models for equivalence checking against Register Transfer Level (RTL) designs, but this introduces abstraction gaps, translation overhead, and limits scalability at the RTL level. To address these challenges, this paper presents a scalable methodology for verifying floating-point arithmetic using direct RTL-to-RTL model checking against a golden reference model. The approach adopts a divide-and conquer strategy that decomposes verification into modular stages, each captured by helper assertions and lemmas that collectively prove a main correctness theorem. Counterexample (CEX)-guided refinement is used to iteratively localize and resolve implementation defects, while targeted fault injection validates the robustness of the verification process against precision-critical datapath errors. To assess scalability and practicality, the methodology is extended with agentic AI-based formal property generation, integrating large language model (LLM)-driven automation with Human-in-the-Loop (HITL) refinement. Coverage analysis evaluates the effectiveness of the approach by comparing handwritten and AI-generated properties in both RTL-to-RTL model checking and standalone RTL verification settings. Results show that direct RTL-to-RTL model checking achieves higher coverage efficiency and requires fewer assertions than standalone verification, especially when combined with AI-generated properties refined through HITL guidance.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [688] [GPU-Accelerated Optimization Solver for Unit Commitment in Large-Scale Power Grids](https://arxiv.org/abs/2512.06715)
*Hussein Sharadga,Javad Mohammadi*

Main category: math.OC

TL;DR: This paper introduces a GPU-based solver for large-scale unit commitment problems, enhancing efficiency with the PDHG algorithm and achieving faster computation times.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiencies of traditional CPU-based methods in solving large-scale unit commitment problems in power grids.

Method: A GPU-accelerated solver utilizing the Primal-Dual Hybrid Gradient (PDHG) algorithm to efficiently solve relaxed linear subproblems, improving convergence speed and computation.

Result: Substantial speed-ups and maintained solution quality on large-scale networks (4224-, 6049-, 6717-bus) with intensive computational demands.

Conclusion: The GPU-accelerated approach reduces computation time and enhances scalability for solving mixed-integer linear unit commitment problems.

Abstract: This work presents a GPU-accelerated solver for the unit commitment (UC) problem in large-scale power grids. The solver uses the Primal-Dual Hybrid Gradient (PDHG) algorithm to efficiently solve the relaxed linear subproblem, achieving faster bound estimation and improved crossover and branch-and-bound convergence compared to conventional CPU-based methods. These improvements significantly reduce the total computation time for the mixed-integer linear UC problem. The proposed approach is validated on large-scale systems, including 4224-, 6049-, and 6717-bus networks with long control horizons and computationally intensive problems, demonstrating substantial speed-ups while maintaining solution quality.

</details>


### [689] [Optimal and Diffusion Transports in Machine Learning](https://arxiv.org/abs/2512.06797)
*Gabriel Peyré*

Main category: math.OC

TL;DR: The paper surveys time-evolving probability distributions, examining two main approaches (diffusion methods and optimal transport) and their applications in machine learning tasks like sampling, neural network optimization, and analyzing token dynamics in language models.


<details>
  <summary>Details</summary>
Motivation: To explore common mathematical structures and methods underlying various machine learning tasks requiring the analysis of time-evolving probability distributions, enabling better design and understanding of these processes.

Method: The paper shifts focus from the Eulerian representation of densities to a Lagrangian viewpoint using vector fields to describe density evolution. It emphasizes diffusion methods and optimal transport as complementary approaches, analyzing their mathematical frameworks and applications.

Result: The survey shows applications of the highlighted methods in sampling, neural network training, and token dynamics in language models, demonstrating their effectiveness in various machine learning scenarios.

Conclusion: Adopting Lagrangian vector fields for modeling density evolution presents new challenges and opportunities, providing deeper insights and tools for solving machine learning problems related to time-evolving distributions.

Abstract: Several problems in machine learning are naturally expressed as the design and analysis of time-evolving probability distributions. This includes sampling via diffusion methods, optimizing the weights of neural networks, and analyzing the evolution of token distributions across layers of large language models. While the targeted applications differ (samples, weights, tokens), their mathematical descriptions share a common structure. A key idea is to switch from the Eulerian representation of densities to their Lagrangian counterpart through vector fields that advect particles. This dual view introduces challenges, notably the non-uniqueness of Lagrangian vector fields, but also opportunities to craft density evolutions and flows with favorable properties in terms of regularity, stability, and computational tractability. This survey presents an overview of these methods, with emphasis on two complementary approaches: diffusion methods, which rely on stochastic interpolation processes and underpin modern generative AI, and optimal transport, which defines interpolation by minimizing displacement cost. We illustrate how both approaches appear in applications ranging from sampling, neural network optimization, to modeling the dynamics of transformers for large language models.

</details>


### [690] [Unifying Entropy Regularization in Optimal Control: From and Back to Classical Objectives via Iterated Soft Policies and Path Integral Solutions](https://arxiv.org/abs/2512.06109)
*Ajinkya Bhole,Mohammad Mahmoudi Filabadi,Guillaume Crevecoeur,Tom Lefebvre*

Main category: math.OC

TL;DR: The paper develops a generalized stochastic optimal control formulation using Kullback-Leibler regularization, which unifies various control problems and offers computational advantages.


<details>
  <summary>Details</summary>
Motivation: To unify stochastic optimal control formulations and improve tractability through generalization and KL regularization.

Method: A centralized problem formulation with separate KL penalties on policies and transitions, allowing flexibility to recover classical and soft-policy control problems.

Result: Demonstrated majorization of original problems by regularized solutions with iterative recovery of the original solutions and identified structural synchronization enabling favorable computational properties.

Conclusion: This unification extends computational advantages like linear Bellman equations to a broader class of control problems.

Abstract: This paper develops a unified perspective on several stochastic optimal control formulations through the lens of Kullback-Leibler regularization. We propose a central problem that separates the KL penalties on policies and transitions, assigning them independent weights, thereby generalizing the standard trajectory-level KL-regularization commonly used in probabilistic and KL-regularized control. This generalized formulation acts as a generative structure allowing to recover various control problems. These include the classical Stochastic Optimal Control (SOC), Risk-Sensitive Optimal Control (RSOC), and their policy-based KL-regularized counterparts. The latter we refer to as soft-policy SOC and RSOC, facilitating alternative problems with tractable solutions. Beyond serving as regularized variants, we show that these soft-policy formulations majorize the original SOC and RSOC problem. This means that the regularized solution can be iterated to retrieve the original solution. Furthermore, we identify a structurally synchronized case of the risk-seeking soft-policy RSOC formulation, wherein the policy and transition KL-regularization weights coincide. Remarkably, this specific setting gives rise to several powerful properties such as a linear Bellman equation, path integral solution, and, compositionality, thereby extending these computationally favourable properties to a broad class of control problems.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [691] [The Native Spiking Microarchitecture: From Iontronic Primitives to Bit-Exact FP8 Arithmetic](https://arxiv.org/abs/2512.07724)
*Zhengzheng Tang*

Main category: cs.ET

TL;DR: The paper introduces a Native Spiking Microarchitecture to transform stochastic materials, like those derived from MOFs, into deterministic AI platforms with full FP8 precision.


<details>
  <summary>Details</summary>
Motivation: To solve the challenge of utilizing stochastic analog materials such as angstrom-scale channels for deterministic and bit-exact AI workloads like FP8 precision.

Method: The authors develop a Native Spiking Microarchitecture that uses noisy neurons as logic primitives, featuring a Spatial Combinational Pipeline and Sticky-Extra Correction mechanism to achieve AI task precision.

Result: Achieved 100% alignment with FP8 PyTorch computations across all 16,129 pairs, reduced Linear layer latency to O(log N) with a 17x speedup, and demonstrated robustness under extreme membrane leakage conditions.

Conclusion: The proposed architecture bridges the gap between stochastic materials and exact AI workloads, showing feasibility for future deterministic AI systems built on novel materials.

Abstract: The 2025 Nobel Prize in Chemistry for Metal-Organic Frameworks (MOFs) and recent breakthroughs by Huanting Wang's team at Monash University establish angstrom-scale channels as promising post-silicon substrates with native integrate-and-fire (IF) dynamics. However, utilizing these stochastic, analog materials for deterministic, bit-exact AI workloads (e.g., FP8) remains a paradox. Existing neuromorphic methods often settle for approximation, failing Transformer precision standards. To traverse the gap "from stochastic ions to deterministic floats," we propose a Native Spiking Microarchitecture. Treating noisy neurons as logic primitives, we introduce a Spatial Combinational Pipeline and a Sticky-Extra Correction mechanism. Validation across all 16,129 FP8 pairs confirms 100% bit-exact alignment with PyTorch. Crucially, our architecture reduces Linear layer latency to O(log N), yielding a 17x speedup. Physical simulations further demonstrate robustness against extreme membrane leakage (beta approx 0.01), effectively immunizing the system against the stochastic nature of the hardware.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [692] [Future You: Designing and Evaluating Multimodal AI-generated Digital Twins for Strengthening Future Self-Continuity](https://arxiv.org/abs/2512.06106)
*Constanze Albrecht,Chayapatr Archiwaranguprok,Rachel Poonsiriwong,Awu Chen,Peggy Yin,Monchai Lertsutthiwong,Kavin Winson,Hal Hershfield,Pattie Maes,Pat Pataranutaporn*

Main category: cs.HC

TL;DR: The study evaluates the psychological and affective impact of AI-generated future selves in text, voice, and avatar modalities, showing these interventions enhance emotional well-being, motivation, and Future Self-Continuity. Interaction quality significantly affects outcomes.


<details>
  <summary>Details</summary>
Motivation: Exploring how AI-generated future selves can shape present-day decisions and emotional connection to future identities to improve well-being and motivation.

Method: A randomized controlled study (N=92) compared text, voice, and avatar modalities against a control condition and evaluated Claude 4 and other LLMs across psychological and interaction dimensions.

Result: Personalized AI modalities showed improved Future Self-Continuity, emotional well-being, and motivation versus control, while interaction quality was a key factor in effectiveness. Claude 4 outperformed other LLMs in psychological outcomes.

Conclusion: Compelling AI interactions, not modality type, matter most for psychological and affective impact, demonstrating AI's potential to strengthen connections with future selves and improve well-being.

Abstract: What if users could meet their future selves today? AI-generated future selves simulate meaningful encounters with a digital twin decades in the future. As AI systems advance, combining cloned voices, age-progressed facial rendering, and autobiographical narratives, a central question emerges: Does the modality of these future selves alter their psychological and affective impact? How might a text-based chatbot, a voice-only system, or a photorealistic avatar shape present-day decisions and our feeling of connection to the future? We report a randomized controlled study (N=92) evaluating three modalities of AI-generated future selves (text, voice, avatar) against a neutral control condition. We also report a systematic model evaluation between Claude 4 and three other Large Language Models (LLMs), assessing Claude 4 across psychological and interaction dimensions and establishing conversational AI quality as a critical determinant of intervention effectiveness. All personalized modalities strengthened Future Self-Continuity (FSC), emotional well-being, and motivation compared to control, with avatar producing the largest vividness gains, yet with no significant differences between formats. Interaction quality metrics, particularly persuasiveness, realism, and user engagement, emerged as robust predictors of psychological and affective outcomes, indicating that how compelling the interaction feels matters more than the form it takes. Content analysis found thematic patterns: text emphasized career planning, while voice and avatar facilitated personal reflection. Claude 4 outperformed ChatGPT 3.5, Llama 4, and Qwen 3 in enhancing psychological, affective, and FSC outcomes.

</details>


### [693] [Living the Novel: A System for Generating Self-Training Timeline-Aware Conversational Agents from Novels](https://arxiv.org/abs/2512.07474)
*Yifei Huang,Tianyu Yan,Sitong Gong,Xiwei Gao,Caixin Kang,Ruicong Liu,Huchuan Lu,Bo Zheng*

Main category: cs.HC

TL;DR: The paper introduces a system called 'Living Novel' to convert literary works into multi-character conversational experiences, addressing LLM persona drift and narrative constraints failures.


<details>
  <summary>Details</summary>
Motivation: Generic language models (LLMs) struggle with maintaining character fidelity (persona drift) and adhering to narrative coherence and logic, leading to story inconsistency and failures in immersive storytelling.

Method: The two-stage pipeline includes: (1) Deep Persona Alignment (DPA) with reinforcement finetuning for character fidelity. (2) Coherence and Robustness Enhancing (CRE) uses a knowledge graph and retrieval-grounded training to enforce narrative constraints.

Result: Evaluation using Jules Verne's novel showcased superior persona fidelity compared to GPT-4o and nearly perfect coherence and robustness. Further diary and ablation studies supported the effectiveness of the methods.

Conclusion: Character-centric training and enforcing story-time constraints are vital for consistent, believable AI-driven narratives. The system successfully delivers immersive narrative experiences.

Abstract: We present the Living Novel, an end-to-end system that transforms any literary work into an immersive, multi-character conversational experience. This system is designed to solve two fundamental challenges for LLM-driven characters. Firstly, generic LLMs suffer from persona drift, often failing to stay in character. Secondly, agents often exhibit abilities that extend beyond the constraints of the story's world and logic, leading to both narrative incoherence (spoiler leakage) and robustness failures (frame-breaking). To address these challenges, we introduce a novel two-stage training pipeline. Our Deep Persona Alignment (DPA) stage uses data-free reinforcement finetuning to instill deep character fidelity. Our Coherence and Robustness Enhancing (CRE) stage then employs a story-time-aware knowledge graph and a second retrieval-grounded training pass to architecturally enforce these narrative constraints. We validate our system through a multi-phase evaluation using Jules Verne's Twenty Thousand Leagues Under the Sea. A lab study with a detailed ablation of system components is followed by a 5-day in-the-wild diary study. Our DPA pipeline helps our specialized model outperform GPT-4o on persona-specific metrics, and our CRE stage achieves near-perfect performance in coherence and robustness measures. Our study surfaces practical design guidelines for AI-driven narrative systems: we find that character-first self-training is foundational for believability, while explicit story-time constraints are crucial for sustaining coherent, interruption-resilient mobile-web experiences.

</details>


### [694] [Beyond Satisfaction: From Placebic to Actionable Explanations For Enhanced Understandability](https://arxiv.org/abs/2512.06591)
*Joe Shymanski,Jacob Brue,Sandip Sen*

Main category: cs.HC

TL;DR: The paper evaluates current methods of assessing explainability in AI systems, showing that subjective user surveys are insufficient for measuring explanation quality. Objective performance measures should be integrated.


<details>
  <summary>Details</summary>
Motivation: The paper investigates shortcomings in current explainability evaluation methods, particularly the overreliance on subjective surveys that do not adequately measure explanation effectiveness.

Method: Participants performed a Social Security filing task under three conditions: no explanations, placebic explanations, and actionable explanations. Their mental model performance and satisfaction levels were compared.

Result: While actionable explanations improved participants' mental models, subjective satisfaction ratings showed no differentiation between actionable and placebic explanations.

Conclusion: Subjective assessments alone are inadequate to evaluate explanation quality; objective task performance metrics must be included in future evaluations of explainable AI systems' capability.

Abstract: Explainable AI (XAI) presents useful tools to facilitate transparency and trustworthiness in machine learning systems. However, current evaluations of system explainability often rely heavily on subjective user surveys, which may not adequately capture the effectiveness of explanations. This paper critiques the overreliance on user satisfaction metrics and explores whether these can differentiate between meaningful (actionable) and vacuous (placebic) explanations. In experiments involving optimal Social Security filing age selection tasks, participants used one of three protocols: no explanations, placebic explanations, and actionable explanations. Participants who received actionable explanations significantly outperformed the other groups in objective measures of their mental model, but users rated placebic and actionable explanations as equally satisfying. This suggests that subjective surveys alone fail to capture whether explanations truly support users in building useful domain understanding. We propose that future evaluations of agent explanation capabilities should integrate objective task performance metrics alongside subjective assessments to more accurately measure explanation quality. The code for this study can be found at https://github.com/Shymkis/social-security-explainer.

</details>


### [695] [Memory Power Asymmetry in Human-AI Relationships: Preserving Mutual Forgetting in the Digital Age](https://arxiv.org/abs/2512.06616)
*Rasam Dorri,Rami Zwick*

Main category: cs.HC

TL;DR: The paper introduces Memory Power Asymmetry (MPA), a concept describing AI-led power imbalances in relationships due to advanced memory capabilities. It emphasizes the need for equitable memory management.


<details>
  <summary>Details</summary>
Motivation: To address power imbalances and ethical concerns arising from AI systems' ability to store and utilize interaction histories compared to humans' natural forgetting.

Method: Developed a conceptual framework identifying four dimensions of MPA and mechanisms translating memory asymmetry into power. The framework integrates research from diverse disciplines.

Result: Proposed consequences of MPA across levels and offered six design principles to restore balance in human-AI relationships, including intentional forgetting systems and equalized memory access.

Conclusion: Highlighting MPA as a unique construct, the paper argues for prioritizing mutual control over memory as a policy and design goal in AI-driven relationships.

Abstract: As artificial intelligence (AI) becomes embedded in personal and professional relationships, a new kind of power imbalance emerges from asymmetric memory capabilities. Human relationships have historically relied on mutual forgetting, the natural tendency for both parties to forget details over time, as a foundation for psychological safety, forgiveness, and identity change. By contrast, AI systems can record, store, and recombine interaction histories at scale, often indefinitely. We introduce Memory Power Asymmetry (MPA): a structural power imbalance that arises when one relationship partner (typically an AI-enabled firm) possesses a substantially superior capacity to record, retain, retrieve, and integrate the shared history of the relationship, and can selectively deploy that history in ways the other partner (the human) cannot. Drawing on research in human memory, power-dependence theory, AI architecture, and consumer vulnerability, we develop a conceptual framework with four dimensions of MPA (persistence, accuracy, accessibility, integration) and four mechanisms by which memory asymmetry is translated into power (strategic memory deployment, narrative control, dependence asymmetry, vulnerability accumulation). We theorize downstream consequences at individual, relational/firm, and societal levels, formulate boundary-conditioned propositions, and articulate six design principles for restoring a healthier balance of memory in human-AI relationships (e.g., forgetting by design, contextual containment, symmetric access to records). Our analysis positions MPA as a distinct construct relative to information asymmetry, privacy, surveillance, and customer relationship management, and argues that protecting mutual forgetting, or at least mutual control over memory, should become a central design and policy goal in the AI age.

</details>


### [696] [Graph-Based Learning of Spectro-Topographical EEG Representations with Gradient Alignment for Brain-Computer Interfaces](https://arxiv.org/abs/2512.07820)
*Prithila Angkan,Amin Jalali,Paul Hungler,Ali Etemad*

Main category: cs.HC

TL;DR: This paper introduces the GEEGA model, a graph-based EEG representation learning approach, improving brain-computer interface performance by using multi-domain information fusion, gradient alignment strategies, and domain-specific loss functions.


<details>
  <summary>Details</summary>
Motivation: Achieving high inter-class separability in EEG signals is challenging due to their dynamic and subject-sensitive nature. This research aims to improve EEG-based brain-computer interface systems by optimizing their ability to distinguish between different EEG categories.

Method: The proposed GEEGA model utilizes graph convolutional networks to merge frequency-based and time-frequency representations. It incorporates center loss, pairwise difference loss, and a gradient alignment strategy to enhance inter-domain relationship learning while addressing gradient conflicts.

Result: The GEEGA model achieves superior results on three EEG datasets (BCI-2a, CL-Drive, and CLARE), demonstrating its effectiveness and improved performance. Ablation studies confirm the contributions of its various components.

Conclusion: GEEGA advances EEG representation learning by employing graph-based learning, effective loss functions, and gradient alignment strategies, achieving high inter-class separability and strong performance in EEG-based systems.

Abstract: We present a novel graph-based learning of EEG representations with gradient alignment (GEEGA) that leverages multi-domain information to learn EEG representations for brain-computer interfaces. Our model leverages graph convolutional networks to fuse embeddings from frequency-based topographical maps and time-frequency spectrograms, capturing inter-domain relationships. GEEGA addresses the challenge of achieving high inter-class separability, which arises from the temporally dynamic and subject-sensitive nature of EEG signals by incorporating the center loss and pairwise difference loss. Additionally, GEEGA incorporates a gradient alignment strategy to resolve conflicts between gradients from different domains and the fused embeddings, ensuring that discrepancies, where gradients point in conflicting directions, are aligned toward a unified optimization direction. We validate the efficacy of our method through extensive experiments on three publicly available EEG datasets: BCI-2a, CL-Drive and CLARE. Comprehensive ablation studies further highlight the impact of various components of our model.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [697] [Equivariant Diffusion for Crystal Structure Prediction](https://arxiv.org/abs/2512.07289)
*Peijia Lin,Pin Chen,Rui Jiao,Qing Mo,Jianhuan Cen,Wenbing Huang,Yang Liu,Dan Huang,Yutong Lu*

Main category: cond-mat.mtrl-sci

TL;DR: This paper introduces EquiCSP, an equivariant diffusion-based generative model for Crystal Structure Prediction (CSP).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in ensuring permutation, rotation, and periodic translation equivariance during the diffusion process in CSP tasks. Existing symmetry-aware models fail to fully address lattice permutation equivariance and periodic translation equivariance.

Method: EquiCSP employs a novel noising algorithm designed to rigorously maintain periodic translation equivariance during training and inference and tackles lattice permutation equivariance issues overlooked in previous models.

Result: The model outperforms existing approaches in generating accurate crystal structures and shows faster convergence during training.

Conclusion: EquiCSP effectively improves CSP tasks by addressing gaps in equivariance handling, achieving higher accuracy and efficiency than prior models.

Abstract: In addressing the challenge of Crystal Structure Prediction (CSP), symmetry-aware deep learning models, particularly diffusion models, have been extensively studied, which treat CSP as a conditional generation task. However, ensuring permutation, rotation, and periodic translation equivariance during diffusion process remains incompletely addressed. In this work, we propose EquiCSP, a novel equivariant diffusion-based generative model. We not only address the overlooked issue of lattice permutation equivariance in existing models, but also develop a unique noising algorithm that rigorously maintains periodic translation equivariance throughout both training and inference processes. Our experiments indicate that EquiCSP significantly surpasses existing models in terms of generating accurate structures and demonstrates faster convergence during the training process.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [698] [Phase-multiplexed optical computing: Reconfiguring a multi-task diffractive optical processor using illumination phase diversity](https://arxiv.org/abs/2512.06658)
*Xiao Wang,Aydogan Ozcan*

Main category: physics.optics

TL;DR: A study introduces a monochrome multi-task diffractive network that uses phase multiplexing to achieve accurate complex linear transformations and enhance optical computing.


<details>
  <summary>Details</summary>
Motivation: To enable efficient and dynamic implementation of multiple complex-valued linear transformations using a single optical system.

Method: A common diffractive network is optimized with distinct 'phase keys' to activate specific transformations dynamically.

Result: A proof-of-concept achieves T = 512 distinct transformations with negligible error.

Conclusion: Phase-multiplexing offers a scalable and accurate optical computing architecture, advancing transformation accuracy and system capabilities.

Abstract: We report a monochrome multi-task diffractive network architecture that leverages illumination phase multiplexing to dynamically reconfigure its output function and accurately implement a large group of complex-valued linear transformations between an input and output aperture. Each member of the desired group of T unique transformations is encoded and addressed with a distinct 2D illumination phase profile, termed "phase key", which illuminates the input aperture, activating the corresponding transformation at the output field-of-view. A common diffractive optical network, optimized with T phase keys, demultiplexes these encoded inputs and accurately executes any of the T distinct linear transformations at its output. We demonstrate that a diffractive network composed of N = 2 x T x Ni x No optimized diffractive features can realize T distinct complex-valued linear transformations, accurately executed for any complex field at the input aperture, where Ni and No refer to the input/output pixels, respectively. In our proof-of-concept numerical analysis, T = 512 complex-valued transformations are implemented by the same monochrome diffractive network with negligible error using illumination phase diversity. Compared with wavelength-multiplexed diffractive systems, phase-multiplexing architecture significantly lowers the transformation errors, potentially enabling larger-scale optical transformations to be implemented through a monochrome processor. Phase-multiplexed multi-task diffractive networks would enhance the capabilities of optical computing and machine-vision systems.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [699] [MATEX: A Multi-Agent Framework for Explaining Ethereum Transactions](https://arxiv.org/abs/2512.06933)
*Zifan Peng*

Main category: cs.CE

TL;DR: The paper introduces a multi-agent framework to help users understand complex Ethereum transactions by providing faithful, step-by-step explanations.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty users face in understanding multi-hop token flows, nested contract calls, and opaque execution paths in Ethereum transactions.

Method: The paper proposes a cognitive multi-agent framework called 'matex' which models transaction understanding as a collaborative investigation involving hypothesis generation, off-chain knowledge retrieval, evidence-aware synthesis, and adversarial validation.

Result: Faithful, step-wise explanations of Ethereum transactions are produced, enabling better user comprehension and reducing the risk of blind signing.

Conclusion: The framework provides a robust approach to decoding complex Ethereum transactions, benefiting users, developers, and auditors by offering explanations rooted in on-chain evidence and real-world semantics.

Abstract: Understanding a complicated Ethereum transaction remains challenging: multi-hop token flows, nested contract calls, and opaque execution paths routinely lead users to blind signing. Based on interviews with everyday users, developers, and auditors, we identify the need for faithful, step-wise explanations grounded in both on-chain evidence and real-world protocol semantics. To meet this need, we introduce (matex, a cognitive multi-agent framework that models transaction understanding as a collaborative investigation-combining rapid hypothesis generation, dynamic off-chain knowledge retrieval, evidence-aware synthesis, and adversarial validation to produce faithful explanations.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [700] [FlockVote: LLM-Empowered Agent-Based Modeling for Simulating U.S. Presidential Elections](https://arxiv.org/abs/2512.05982)
*Lingfeng Zhou,Yi Xu,Zhenyu Wang,Dequan Wang*

Main category: physics.soc-ph

TL;DR: The paper introduces FlockVote, a framework using LLMs for political simulations, showing high fidelity in replicating voting outcomes while enhancing interpretability.


<details>
  <summary>Details</summary>
Motivation: To address limitations in traditional agent-based models and large-scale statistical models for modeling human behaviors, particularly voter decisions in elections.

Method: FlockVote employs LLM agents with detailed demographic profiles and dynamic contextual data to simulate nuanced voting behavior.

Result: The framework replicated the macro-level results of the 2024 U.S. Presidential Election in key swing states.

Conclusion: FlockVote is a novel and interpretable tool for computational social science, allowing analysis at agent and macro levels without relying on black-box methods.

Abstract: Modeling complex human behavior, such as voter decisions in national elections, is a long-standing challenge for computational social science. Traditional agent-based models (ABMs) are limited by oversimplified rules, while large-scale statistical models often lack interpretability. We introduce FlockVote, a novel framework that uses Large Language Models (LLMs) to build a "computational laboratory" of LLM agents for political simulation. Each agent is instantiated with a high-fidelity demographic profile and dynamic contextual information (e.g. candidate policies), enabling it to perform nuanced, generative reasoning to simulate a voting decision. We deploy this framework as a testbed on the 2024 U.S. Presidential Election, focusing on seven key swing states. Our simulation's macro-level results successfully replicate the real-world outcome, demonstrating the high fidelity of our "virtual society". The primary contribution is not only the prediction, but also the framework's utility as an interpretable research tool. FlockVote moves beyond black-box outputs, allowing researchers to probe agent-level rationale and analyze the stability and sensitivity of LLM-driven social simulations.

</details>


### [701] [Social welfare optimisation in well-mixed and structured populations](https://arxiv.org/abs/2512.07453)
*Van An Nguyen,Vuong Khang Huynh,Ho Nam Duong,Huu Loi Bui,Hai Anh Ha,Quang Dung Le,Le Quoc Dung Ngo,Tan Dat Nguyen,Ngoc Ngu Nguyen,Hoai Thuong Nguyen,Zhao Song,Le Hong Trang,The Anh Han*

Main category: physics.soc-ph

TL;DR: The study explores maximizing social welfare in incentive-driven cooperation among autonomous agents, revealing gaps between cost efficiency and welfare optimization.


<details>
  <summary>Details</summary>
Motivation: Current research prioritizes minimizing incentive costs and maximizing cooperation frequency, but the impact on overall social welfare is underexplored.

Method: The authors use evolutionary game theory models and agent-based simulations in well-mixed and structured populations to analyze interference strategies and their effects.

Result: The study finds significant differences in per-individual incentive costs when prioritizing social welfare over cost efficiency or cooperation frequency.

Conclusion: Prioritizing social welfare in incentive-driven systems offers better outcomes than focusing solely on cost or cooperation frequency benchmarks.

Abstract: Research on promoting cooperation among autonomous, self-regarding agents has often focused on the bi-objective optimisation problem: minimising the total incentive cost while maximising the frequency of cooperation. However, the optimal value of social welfare under such constraints remains largely unexplored. In this work, we hypothesise that achieving maximal social welfare is not guaranteed at the minimal incentive cost required to drive agents to a desired cooperative state. To address this gap, we adopt to a single-objective approach focused on maximising social welfare, building upon foundational evolutionary game theory models that examined cost efficiency in finite populations, in both well-mixed and structured population settings. Our analytical model and agent-based simulations show how different interference strategies, including rewarding local versus global behavioural patterns, affect social welfare and dynamics of cooperation. Our results reveal a significant gap in the per-individual incentive cost between optimising for pure cost efficiency or cooperation frequency and optimising for maximal social welfare. Overall, our findings indicate that incentive design, policy, and benchmarking in multi-agent systems and human societies should prioritise welfare-centric objectives over proxy targets of cost or cooperation frequency.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [702] [Physics-Guided Deepfake Detection for Voice Authentication Systems](https://arxiv.org/abs/2512.06040)
*Alireza Mohammadi,Keshav Sood,Dhananjay Thiruvady,Asef Nazari*

Main category: cs.SD

TL;DR: The paper proposes a framework that integrates physics-based and self-supervised features to counter deepfake attacks and poisoning in federated voice authentication systems.


<details>
  <summary>Details</summary>
Motivation: To address the dual threats of deepfake synthesis attacks and control-plane poisoning affecting voice authentication systems at the network edge.

Method: The framework combines interpretable physics-guided features with self-supervised learning representations in a Multi-Modal Ensemble Architecture, enhanced by Bayesian uncertainty estimation.

Result: The proposed framework demonstrates robustness against advanced deepfake attacks and control-plane poisoning threats.

Conclusion: By integrating physics-based dynamics and uncertainty-aware capabilities, the framework enhances the security and reliability of edge-based voice authentication systems.

Abstract: Voice authentication systems deployed at the network edge face dual threats: a) sophisticated deepfake synthesis attacks and b) control-plane poisoning in distributed federated learning protocols. We present a framework coupling physics-guided deepfake detection with uncertainty-aware in edge learning. The framework fuses interpretable physics features modeling vocal tract dynamics with representations coming from a self-supervised learning module. The representations are then processed via a Multi-Modal Ensemble Architecture, followed by a Bayesian ensemble providing uncertainty estimates. Incorporating physics-based characteristics evaluations and uncertainty estimates of audio samples allows our proposed framework to remain robust to both advanced deepfake attacks and sophisticated control-plane poisoning, addressing the complete threat model for networked voice authentication.

</details>


### [703] [Who Will Top the Charts? Multimodal Music Popularity Prediction via Adaptive Fusion of Modality Experts and Temporal Engagement Modeling](https://arxiv.org/abs/2512.06259)
*Yash Choudhary,Preeti Rao,Pushpak Bhattacharyya*

Main category: cs.SD

TL;DR: The paper addresses the challenge of predicting music popularity before release using GAMENet, an advanced multimodal deep learning framework that effectively combines audio, lyrics, and social metadata, outperforming existing baselines significantly.


<details>
  <summary>Details</summary>
Motivation: The study aims to solve limitations in existing music popularity prediction methods by incorporating temporal features, semantic lyric structure, artist/songs' historical data, and improved multimodal fusion.

Method: GAMENet leverages adaptive gating mechanisms integrating audio features, lyric embeddings from a large language model pipeline, and multi-year artist statistical trajectories for robust predictions.

Result: GAMENet shows a 12% improvement in R^2 over traditional multimodal approaches and increased robustness across datasets like Music4All and SpotGenTrack Popularity Dataset.

Conclusion: GAMENet effectively predicts pre-release music success, addressing crucial gaps in existing models and achieving significant performance gains across multiple benchmarks.

Abstract: Predicting a song's commercial success prior to its release remains an open and critical research challenge for the music industry. Early prediction of music popularity informs strategic decisions, creative planning, and marketing. Existing methods suffer from four limitations:(i) temporal dynamics in audio and lyrics are averaged away; (ii) lyrics are represented as a bag of words, disregarding compositional structure and affective semantics; (iii) artist- and song-level historical performance is ignored; and (iv) multimodal fusion approaches rely on simple feature concatenation, resulting in poorly aligned shared representations. To address these limitations, we introduce GAMENet, an end-to-end multimodal deep learning architecture for music popularity prediction. GAMENet integrates modality-specific experts for audio, lyrics, and social metadata through an adaptive gating mechanism. We use audio features from Music4AllOnion processed via OnionEnsembleAENet, a network of autoencoders designed for robust feature extraction; lyric embeddings derived through a large language model pipeline; and newly introduced Career Trajectory Dynamics (CTD) features that capture multi-year artist career momentum and song-level trajectory statistics. Using the Music4All dataset (113k tracks), previously explored in MIR tasks but not popularity prediction, GAMENet achieves a 12% improvement in R^2 over direct multimodal feature concatenation. Spotify audio descriptors alone yield an R^2 of 0.13. Integrating aggregate CTD features increases this to 0.69, with an additional 7% gain from temporal CTD features. We further validate robustness using the SpotGenTrack Popularity Dataset (100k tracks), achieving a 16% improvement over the previous baseline. Extensive ablations confirm the model's effectiveness and the distinct contribution of each modality.

</details>


### [704] [Protecting Bystander Privacy via Selective Hearing in LALMs](https://arxiv.org/abs/2512.06380)
*Xiao Zhan,Guangzhi Sun,Jose Such,Phil Woodland*

Main category: cs.SD

TL;DR: The paper introduces SH-Bench, a benchmark to evaluate large audio language models' (LALMs) ability to focus on main speakers while protecting bystander privacy. It also proposes a training method called BPFT to enhance model performance on this task.


<details>
  <summary>Details</summary>
Motivation: Address the overlooked privacy risks posed by LALMs capturing speech from unintended bystanders in real-world applications, and provide a way to systematically evaluate and improve privacy protection.

Method: Developed SH-Bench, a dataset and benchmark for evaluating selective hearing in LALMs, and introduced Bystander Privacy Fine-Tuning (BPFT) to improve models' privacy protection capabilities while retaining comprehension abilities.

Result: State-of-the-art LALMs exhibit privacy leakage, failing to sufficiently protect bystander privacy. BPFT improved models' Selective Efficacy (SE) scores by up to 15.9% over the baseline.

Conclusion: Selective hearing is measurable and improvable in LALMs, but significant work remains to achieve optimal bystander privacy protection. SH-Bench and BPFT provide foundational tools for advancing this area.

Abstract: Large audio language models (LALMs) are increasingly deployed in real-world settings where they inevitably capture speech from unintended nearby bystanders, raising privacy risks that existing benchmarks and defences largely overlook. We introduce SH-Bench, the first benchmark designed to evaluate selective hearing: a model's ability to attend to an intended main speaker while refusing to process or reveal information about incidental bystander speech. SH-Bench contains 3,968 multi-speaker audio mixtures spanning both real-world and synthetic scenarios, paired with 77k multiple-choice questions that probe models under general and selective operating modes. We propose Selective Efficacy (SE), a unified metric capturing both multi-speaker comprehension and bystander-privacy protection. Our evaluation of state-of-the-art open-source and proprietary LALMs reveals substantial privacy leakage, with strong audio understanding failing to translate into selective protection of bystander privacy. To mitigate this gap, we introduce Bystander Privacy Fine-Tuning (BPFT), a training pipeline that teaches models to refuse bystander-related queries without degrading main-speaker comprehension. BPFT yields substantial gains which improve SE by up to 15.9% over Gemini 2.5 Pro, demonstrating that selective hearing is learnable but far from achieved in current LALMs. SH-Bench and BPFT provide the first systematic framework for measuring and improving bystander privacy in audio foundation models.

</details>


### [705] [Singing Timbre Popularity Assessment Based on Multimodal Large Foundation Model](https://arxiv.org/abs/2512.06999)
*Zihao Wang,Ruibin Yuan,Ziqi Geng,Hengjia Li,Xingwei Qu,Xinyi Li,Songye Chen,Haoying Fu,Roger B. Dannenberg,Kejun Zhang*

Main category: cs.SD

TL;DR: This paper addresses the limitations in automated singing assessment systems by proposing a reference-free, descriptive evaluation framework and introducing new tools like the Sing-MD dataset, VocalVerse model, and H-TPR benchmark.


<details>
  <summary>Details</summary>
Motivation: Current singing assessment systems rely on reference tracks and simplistic scoring methods, limiting creative expression and failing to capture complex aspects of performances.

Method: The authors created the Sing-MD dataset with expert annotations, proposed the VocalVerse architecture to handle full-length song analysis, and established the H-TPR benchmark for better evaluation metrics.

Result: The study revealed annotation inconsistencies among experts and introduced tools like VocalVerse for effective, reference-free singing evaluations.

Conclusion: This work emphasizes shifting from discriminative to descriptive singing assessments using novel datasets, models, and metrics, enabling more nuanced and creative evaluations.

Abstract: Automated singing assessment is crucial for education and entertainment. However, existing systems face two fundamental limitations: reliance on reference tracks, which stifles creative expression, and the simplification of complex performances into non-diagnostic scores based solely on pitch and rhythm. We advocate for a shift from discriminative to descriptive evaluation, creating a complete ecosystem for reference-free, multi-dimensional assessment. First, we introduce Sing-MD, a large-scale dataset annotated by experts across four dimensions: breath control, timbre quality, emotional expression, and vocal technique. Our analysis reveals significant annotation inconsistencies among experts, challenging the validity of traditional accuracy-based metrics. Second, addressing the memory limitations of Multimodal Large Language Models (MLLMs) in analyzing full-length songs, we propose VocalVerse. This efficient hybrid architecture leverages a lightweight acoustic encoder to model global performance features and long-term dependencies. Third, to address automated metric shortcomings, we establish the H-TPR (Human-in-the-loop Tiered Perceptual Ranking) benchmark, which evaluates a model's ability to generate perceptually valid rankings rather than predicting noisy ground-truth scores.

</details>


### [706] [Multi-Accent Mandarin Dry-Vocal Singing Dataset: Benchmark for Singing Accent Recognition](https://arxiv.org/abs/2512.07005)
*Zihao Wang,Ruibin Yuan,Ziqi Geng,Hengjia Li,Xingwei Qu,Xinyi Li,Songye Chen,Haoying Fu,Roger B. Dannenberg,Kejun Zhang*

Main category: cs.SD

TL;DR: The paper introduces a new dataset, MADVSD, focused on Mandarin singing accents to address a gap in existing research.


<details>
  <summary>Details</summary>
Motivation: There is a lack of research and suitable datasets for studying singing accents, especially with regional accent annotations and audio quality.

Method: The paper introduces MADVSD, which comprises over 670 hours of dry vocal recordings from 4,206 native Mandarin speakers across nine Chinese regions, with specific exercises and songs recorded in the speakers' native accents.

Result: MADVSD was validated through benchmark experiments in singing accent recognition, demonstrating effectiveness for speech model evaluation and analysis of dialectal influences on singing accents.

Conclusion: MADVSD provides a high-quality dataset that facilitates the exploration of singing accent recognition and its dialectal influences, as well as phonetic variation analysis.

Abstract: Singing accent research is underexplored compared to speech accent studies, primarily due to the scarcity of suitable datasets. Existing singing datasets often suffer from detail loss, frequently resulting from the vocal-instrumental separation process. Additionally, they often lack regional accent annotations. To address this, we introduce the Multi-Accent Mandarin Dry-Vocal Singing Dataset (MADVSD). MADVSD comprises over 670 hours of dry vocal recordings from 4,206 native Mandarin speakers across nine distinct Chinese regions. In addition to each participant recording audio of three popular songs in their native accent, they also recorded phonetic exercises covering all Mandarin vowels and a full octave range. We validated MADVSD through benchmark experiments in singing accent recognition, demonstrating its utility for evaluating state-of-the-art speech models in singing contexts. Furthermore, we explored dialectal influences on singing accent and analyzed the role of vowels in accentual variations, leveraging MADVSD's unique phonetic exercises.

</details>


### [707] [JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention](https://arxiv.org/abs/2512.07168)
*Georgios Ioannides,Christos Constantinou,Aman Chadha,Aaron Elkins,Linsey Pang,Ravid Shwartz-Ziv,Yann LeCun*

Main category: cs.SD

TL;DR: The paper introduces a two-stage self-supervised framework for robust speech representation, using JEPA with DAAM and FSQ for efficient audio processing.


<details>
  <summary>Details</summary>
Motivation: To develop a robust and efficient framework for learning and compressing speech representations while maintaining high quality.

Method: The method involves two stages: first, using JEPA with DAAM for semantic audio feature learning via masked prediction; second, tokenizing the features with FSQ and mixed-radix packing for reconstruction by HiFi-GAN.

Result: The model employs adaptive temporal feature selection at 2.5 Hz, providing hierarchical speech structure and tokens with high compression and reversibility.

Conclusion: The approach achieves competitive and efficient representations compared to existing neural audio codecs.

Abstract: We introduce a two-stage self-supervised framework that combines the Joint-Embedding Predictive Architecture (JEPA) with a Density Adaptive Attention Mechanism (DAAM) for learning robust speech representations. Stage~1 uses JEPA with DAAM to learn semantic audio features via masked prediction in latent space, fully decoupled from waveform reconstruction. Stage~2 leverages these representations for efficient tokenization using Finite Scalar Quantization (FSQ) and a mixed-radix packing scheme, followed by high-fidelity waveform reconstruction with a HiFi-GAN decoder. By integrating Gaussian mixture-based density-adaptive gating into the JEPA encoder, the model performs adaptive temporal feature selection and discovers hierarchical speech structure at a low frame rate of 2.5~Hz. The resulting tokens (47.5 tokens/sec) provide a reversible, highly compressed, and language-model-friendly representation that is competitive with, and often more efficient than, existing neural audio codecs.

</details>


### [708] [XM-ALIGN: Unified Cross-Modal Embedding Alignment for Face-Voice Association](https://arxiv.org/abs/2512.06757)
*Zhihua Fang,Shumei Tao,Junxu Wang,Liang He*

Main category: cs.SD

TL;DR: The paper presents XM-ALIGN, a framework improving cross-modal verification for faces and voices with alignment and augmentation methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficient and effective cross-modal verification for both "heard" and "unheard" languages in the FAME challenge.

Method: A framework combining explicit and implicit alignment using MSE-based losses, feature embeddings from encoders, joint optimization, and data augmentation.

Result: Superior cross-modal verification performance shown on the MAV-Celeb dataset.

Conclusion: XM-ALIGN advances cross-modal verification via enhanced alignment and generalization strategies, with code available for further research.

Abstract: This paper introduces our solution, XM-ALIGN (Unified Cross-Modal Embedding Alignment Framework), proposed for the FAME challenge at ICASSP 2026. Our framework combines explicit and implicit alignment mechanisms, significantly improving cross-modal verification performance in both "heard" and "unheard" languages. By extracting feature embeddings from both face and voice encoders and jointly optimizing them using a shared classifier, we employ mean squared error (MSE) as the embedding alignment loss to ensure tight alignment between modalities. Additionally, data augmentation strategies are applied during model training to enhance generalization. Experimental results show that our approach demonstrates superior performance on the MAV-Celeb dataset. The code will be released at https://github.com/PunkMale/XM-ALIGN.

</details>


### [709] [Incorporating Structure and Chord Constraints in Symbolic Transformer-based Melodic Harmonization](https://arxiv.org/abs/2512.07627)
*Maximos Kaliakatsos-Papakostas,Konstantinos Soiledis,Theodoros Tsamis,Dimos Makris,Vassilis Katsouros,Emilios Cambouropoulos*

Main category: cs.SD

TL;DR: The paper explores using transformers for melodic harmonization with predefined chord constraints and introduces an algorithm called B* to address this.


<details>
  <summary>Details</summary>
Motivation: To improve the ability of transformer models in symbolic music generation by incorporating specific user-defined chord constraints during melodic harmonization.

Method: The authors propose the B* algorithm, which integrates beam search, A* algorithms, and backtracking, to ensure transformer-generated harmonizations respect chord constraints.

Result: The study highlights the complexities of chord-constrained harmonization while presenting a brute-force algorithm (B*) as a first step toward solving the issue.

Conclusion: The proposed B* algorithm shows promise and paves the way for further enhancements, facilitating the incorporation of heuristics to optimize transformer-based music generation systems.

Abstract: Transformer architectures offer significant advantages regarding the generation of symbolic music; their capabilities for incorporating user preferences toward what they generate is being studied under many aspects. This paper studies the inclusion of predefined chord constraints in melodic harmonization, i.e., where a desired chord at a specific location is provided along with the melody as inputs and the autoregressive transformer model needs to incorporate the chord in the harmonization that it generates. The peculiarities of involving such constraints is discussed and an algorithm is proposed for tackling this task. This algorithm is called B* and it combines aspects of beam search and A* along with backtracking to force pretrained transformers to satisfy the chord constraints, at the correct onset position within the correct bar. The algorithm is brute-force and has exponential complexity in the worst case; however, this paper is a first attempt to highlight the difficulties of the problem and proposes an algorithm that offers many possibilities for improvements since it accommodates the involvement of heuristics.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [710] [Scheduling in Quantum Satellite Networks: Fairness and Performance Optimization](https://arxiv.org/abs/2512.07108)
*Ashutosh Jayant Dikshit,Naga Lakshmi Anipeddi,Prajit Dhara,Saikat Guha,Deirdre Kilbane,Leandros Tassiulas,Don Towsley,Nitish K. Panigrahy*

Main category: quant-ph

TL;DR: The paper presents an optimization framework for scheduling in quantum satellite networks, considering real-world constraints and trade-offs.


<details>
  <summary>Details</summary>
Motivation: To enable long-distance quantum communication by addressing resource and environmental challenges in quantum satellite networks.

Method: An integer linear programming (ILP)-based optimization framework that incorporates multiple objectives and considers real-world constraints.

Result: The framework effectively balances maximizing entanglement distribution rates and fairness across ground station pairs.

Conclusion: This framework advances optimized scheduling for quantum networks and serves as a benchmark for evaluating other policies.

Abstract: Quantum satellite networks offer a promising solution for achieving long-distance quantum communication by enabling entanglement distribution across global scales. This work formulates and solves the quantum satellite network scheduling problem by optimizing satellite-to-ground station pair assignments under realistic system and environmental constraints. Our framework accounts for limited satellite and ground station resources, fairness, entanglement fidelity thresholds, and real world non-idealities including atmospheric losses, weather and background noise. In addition, we incorporate the complexities of multi-satellite relays enabled via inter-satellite links. We propose an integer linear programming (ILP) based optimization framework that supports multiple scheduling objectives, allowing us to analyze tradeoffs between maximizing total entanglement distribution rate and ensuring fairness across ground station pairs. Our framework can also be used as a benchmark tool to measure the performance of other potential transmission scheduling policies.

</details>


### [711] [A scalable and real-time neural decoder for topological quantum codes](https://arxiv.org/abs/2512.07737)
*Andrew W. Senior,Thomas Edlich,Francisco J. H. Heras,Lei M. Zhang,Oscar Higgott,James S. Spencer,Taylor Applebaum,Sam Blackwell,Justin Ledford,Akvilė Žemgulytė,Augustin Žídek,Noah Shutty,Andrew Cowie,Yin Li,George Holland,Peter Brooks,Charlie Beattie,Michael Newman,Alex Davies,Cody Jones,Sergio Boixo,Hartmut Neven,Pushmeet Kohli,Johannes Bausch*

Main category: quant-ph

TL;DR: This paper introduces a neural-network decoder, AlphaQubit 2, achieving near-optimal logical error rates under realistic noise conditions for surface and color quantum error correction codes.


<details>
  <summary>Details</summary>
Motivation: Current quantum computer error rates are too high for fault-tolerant computing. Quantum error correction decoders must be fast, accurate, and scalable to make quantum computing practical.

Method: The authors developed AlphaQubit 2, a neural-network-based quantum error correction decoder optimized for speed, accuracy, and scalability, applied to surface and color codes.

Result: AlphaQubit 2 achieves near-optimal logical error rates with superior real-time decoding speeds for surface and color codes, outperforming other decoders, particularly for color codes.

Conclusion: AlphaQubit 2 represents a significant step toward practical, scalable, and high-accuracy quantum error correction required for fault-tolerant quantum computing.

Abstract: Fault-tolerant quantum computing will require error rates far below those achievable with physical qubits. Quantum error correction (QEC) bridges this gap, but depends on decoders being simultaneously fast, accurate, and scalable. This combination of requirements has not yet been met by a machine-learning decoder, nor by any decoder for promising resource-efficient codes such as the colour code. Here we introduce AlphaQubit 2, a neural-network decoder that achieves near-optimal logical error rates for both surface and colour codes at large scales under realistic noise. For the colour code, it is orders of magnitude faster than other high-accuracy decoders. For the surface code, we demonstrate real-time decoding faster than 1 microsecond per cycle up to distance 11 on current commercial accelerators with better accuracy than leading real-time decoders. These results support the practical application of a wider class of promising QEC codes, and establish a credible path towards high-accuracy, real-time neural decoding at the scales required for fault-tolerant quantum computation.

</details>


### [712] [LUNA: LUT-Based Neural Architecture for Fast and Low-Cost Qubit Readout](https://arxiv.org/abs/2512.07808)
*M. A. Farooq,G. Di Guglielmo,A. Rajagopala,N. Tran,V. A. Chhabria,A. Arora*

Main category: quant-ph

TL;DR: LUNA is a proposed superconducting qubit readout accelerator utilizing integrators and LUT-based neural networks for efficient and low-latency decoding with reduced resource usage.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the inefficiency and high latency of DNN-based qubit readout accelerators for practical use in quantum error correction and decoding loops.

Method: LUNA combines integrator-based preprocessing for dimensionality reduction with LogicNet-based neural networks synthesized into LUTs, along with an optimization framework for efficient design exploration.

Result: LUNA achieves up to 10.95x reduction in area and 30% lower latency while preserving qubit readout fidelity compared to existing solutions.

Conclusion: This approach provides scalable, low-resource, and high-speed qubit readout, paving the way for larger and more reliable quantum systems.

Abstract: Qubit readout is a critical operation in quantum computing systems, which maps the analog response of qubits into discrete classical states. Deep neural networks (DNNs) have recently emerged as a promising solution to improve readout accuracy . Prior hardware implementations of DNN-based readout are resource-intensive and suffer from high inference latency, limiting their practical use in low-latency decoding and quantum error correction (QEC) loops. This paper proposes LUNA, a fast and efficient superconducting qubit readout accelerator that combines low-cost integrator-based preprocessing with Look-Up Table (LUT) based neural networks for classification. The architecture uses simple integrators for dimensionality reduction with minimal hardware overhead, and employs LogicNets (DNNs synthesized into LUT logic) to drastically reduce resource usage while enabling ultra-low-latency inference. We integrate this with a differential evolution based exploration and optimization framework to identify high-quality design points. Our results show up to a 10.95x reduction in area and 30% lower latency with little to no loss in fidelity compared to the state-of-the-art. LUNA enables scalable, low-footprint, and high-speed qubit readout, supporting the development of larger and more reliable quantum computing systems.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [713] [Reliable agent engineering should integrate machine-compatible organizational principles](https://arxiv.org/abs/2512.07665)
*R. Patrick Xian,Garry A. Gabison,Ahmed Alaa,Christoph Riedl,Grigorios G. Chrysos*

Main category: cs.CY

TL;DR: The paper compares LLM-based AI agents to human organizations and explores organizational science principles to improve agent reliability and effectiveness.


<details>
  <summary>Details</summary>
Motivation: Address concerns of reliability, coordination, and accountability in LLM-based AI agents integrated into society.

Method: Examines parallels between LLM agents and organizational science frameworks to propose principles for enhancing agent design, scaling, and management.

Result: Proposes three organizational principles: balancing agency and capabilities, managing resource constraints, and employing mechanisms for scaling and management.

Conclusion: The study extends the connection between governance of AI systems and social systems to improve integration and reliability of LLM-based agents.

Abstract: As AI agents built on large language models (LLMs) become increasingly embedded in society, issues of coordination, control, delegation, and accountability are entangled with concerns over their reliability. To design and implement LLM agents around reliable operations, we should consider the task complexity in the application settings and reduce their limitations while striving to minimize agent failures and optimize resource efficiency. High-functioning human organizations have faced similar balancing issues, which led to evidence-based theories that seek to understand their functioning strategies. We examine the parallels between LLM agents and the compatible frameworks in organization science, focusing on what the design, scaling, and management of organizations can inform agentic systems towards improving reliability. We offer three preliminary accounts of organizational principles for AI agent engineering to attain reliability and effectiveness, through balancing agency and capabilities in agent design, resource constraints and performance benefits in agent scaling, and internal and external mechanisms in agent management. Our work extends the growing exchanges between the operational and governance principles of AI systems and social systems to facilitate system integration.

</details>


### [714] [Uncovering Students' Inquiry Patterns in GenAI-Supported Clinical Practice: An Integration of Epistemic Network Analysis and Sequential Pattern Mining](https://arxiv.org/abs/2512.06018)
*Jiameng Wei,Dinh Dang,Kaixun Yang,Emily Stokes,Amna Mazeh,Angelina Lim,David Wei Dai,Joel Moore,Yizhou Fan,Danijela Gasevic,Dragan Gasevic,Guanliang Chen*

Main category: cs.CY

TL;DR: The study explores the use of Generative AI (GenAI) and learning analytics to enhance the training of pharmacy students in clinical communication, analyzing patterns in student-GenAI interactions.


<details>
  <summary>Details</summary>
Motivation: Traditional methods of assessing clinical communication in pharmacy education are limited in scalability and feedback detail. There is potential to leverage Generative AI and learning analytics to improve these processes, which is underexplored.

Method: The researchers analyzed 323 students' interaction logs with GenAI virtual patients across Australian and Malaysian institutions, using Epistemic Network Analysis and Sequential Pattern Mining to identify inquiry patterns.

Result: High-performing students exhibited strategic behaviors focused on recognizing clinically relevant information and building rapport, while low performers stuck to routine verification loops. Demographic factors like language background and prior experience influenced patterns.

Conclusion: This study highlights how GenAI can aid in understanding and improving student clinical reasoning, offering methodological insights and guiding the design of adaptive GenAI tools for diverse learners.

Abstract: Assessment of medication history-taking has traditionally relied on human observation, limiting scalability and detailed performance data. While Generative AI (GenAI) platforms enable extensive data collection and learning analytics provide powerful methods for analyzing educational traces, these approaches remain largely underexplored in pharmacy clinical training. This study addresses this gap by applying learning analytics to understand how students develop clinical communication competencies with GenAI-powered virtual patients -- a crucial endeavor given the diversity of student cohorts, varying language backgrounds, and the limited opportunities for individualized feedback in traditional training settings. We analyzed 323 students' interaction logs across Australian and Malaysian institutions, comprising 50,871 coded utterances from 1,487 student-GenAI dialogues. Combining Epistemic Network Analysis to model inquiry co-occurrences with Sequential Pattern Mining to capture temporal sequences, we found that high performers demonstrated strategic deployment of information recognition behaviors. Specifically, high performers centered inquiry on recognizing clinically relevant information, integrating rapport-building and structural organization, while low performers remained in routine question-verification loops. Demographic factors including first-language background, prior pharmacy work experience, and institutional context, also shaped distinct inquiry patterns. These findings reveal inquiry patterns that may indicate clinical reasoning development in GenAI-assisted contexts, providing methodological insights for health professions education assessment and informing adaptive GenAI system design that supports diverse learning pathways.

</details>


### [715] [Why They Disagree: Decoding Differences in Opinions about AI Risk on the Lex Fridman Podcast](https://arxiv.org/abs/2512.06350)
*Nghi Truong,Phanish Puranam,Özgecan Koçak*

Main category: cs.CY

TL;DR: This paper examines societal divisions in AI debates, particularly on risks, by analyzing differences in reasoning and identifying key contentious points.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address persistent disagreements in contemporary AI risk debates, despite a shared goal of benefiting humanity and avoiding catastrophic risks.

Method: Analyzes reasoning chains of opposing AI perspectives ("doomer" and "boomer" camps) using definitional, factual, causal, and moral premises through an ensemble of LLMs.

Result: Identifies the root causes of differing opinions on existential and employment risks, finding they stem from divergent causal premises rather than moral disagreements.

Conclusion: The findings suggest different views of human rationality and causation drive AI risk debates, offering a method to analyze contentious issues more broadly.

Abstract: The emergence of transformative technologies often surfaces deep societal divisions, nowhere more evident than in contemporary debates about artificial intelligence (AI). A striking feature of these divisions is that they persist despite shared interests in ensuring that AI benefits humanity and avoiding catastrophic outcomes. This paper analyzes contemporary debates about AI risk, parsing the differences between the "doomer" and "boomer" perspectives into definitional, factual, causal, and moral premises to identify key points of contention. We find that differences in perspectives about existential risk ("X-risk") arise fundamentally from differences in causal premises about design vs. emergence in complex systems, while differences in perspectives about employment risks ("E-risks") pertain to different causal premises about the applicability of past theories (evolution) vs their inapplicability (revolution). Disagreements about these two forms of AI risk appear to share two properties: neither involves significant disagreements on moral values and both can be described in terms of differing views on the extent of boundedness of human rationality. Our approach to analyzing reasoning chains at scale, using an ensemble of LLMs to parse textual data, can be applied to identify key points of contention in debates about risk to the public in any arena.

</details>


### [716] [Artificial Intelligence and Nuclear Weapons Proliferation: The Technological Arms Race for (In)visibility](https://arxiv.org/abs/2512.07487)
*David M. Allison,Stephen Herzog*

Main category: cs.CY

TL;DR: Emerging technologies are reshaping nuclear risk, increasing challenges for detection and monitoring efforts.


<details>
  <summary>Details</summary>
Motivation: To address how disruptive technologies, particularly AI, influence nuclear proliferation by enabling and detecting technological advances.

Method: The authors developed a formal model based on a Relative Advantage Index (RAI) and ran scenario-based simulations to evaluate technological impacts.

Result: The study reveals how AI accelerates proliferation-enabling technologies faster than detection-enhancing improvements, increasing uncertainty in nuclear detectability.

Conclusion: Policy makers should focus on regulating proliferation-enabling technologies and improving detection strategies to manage nuclear risks effectively.

Abstract: A robust nonproliferation regime has contained the spread of nuclear weapons to just nine states. Yet, emerging and disruptive technologies are reshaping the landscape of nuclear risks, presenting a critical juncture for decision makers. This article lays out the contours of an overlooked but intensifying technological arms race for nuclear (in)visibility, driven by the interplay between proliferation-enabling technologies (PETs) and detection-enhancing technologies (DETs). We argue that the strategic pattern of proliferation will be increasingly shaped by the innovation pace in these domains. Artificial intelligence (AI) introduces unprecedented complexity to this equation, as its rapid scaling and knowledge substitution capabilities accelerate PET development and challenge traditional monitoring and verification methods. To analyze this dynamic, we develop a formal model centered on a Relative Advantage Index (RAI), quantifying the shifting balance between PETs and DETs. Our model explores how asymmetric technological advancement, particularly logistic AI-driven PET growth versus stepwise DET improvements, expands the band of uncertainty surrounding proliferation detectability. Through replicable scenario-based simulations, we evaluate the impact of varying PET growth rates and DET investment strategies on cumulative nuclear breakout risk. We identify a strategic fork ahead, where detection may no longer suffice without broader PET governance. Governments and international organizations should accordingly invest in policies and tools agile enough to keep pace with tomorrow's technology.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [717] [Learning to Hedge Swaptions](https://arxiv.org/abs/2512.06639)
*Zaniar Ahmadi,Frédéric Godin*

Main category: q-fin.RM

TL;DR: The paper explores RL-based deep hedging for swaptions and its superior performance over traditional rho-hedging, even in cases of model misspecification.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the limitations of traditional sensitivity-based hedging methods, encouraging exploration of advanced RL-based techniques for more effective and adaptive risk management.

Method: The study employs a deep hedging framework driven by RL, using three objective functions and a three-factor arbitrage-free Nelson-Siegel model for simulation.

Result: Deep hedging with RL achieves near-optimal hedging effectiveness, outperforms conventional rho-hedging strategies, and shows adaptability even under model misspecifications.

Conclusion: RL-based deep hedging proves to be an efficient, resilient, and dynamically adaptive strategy for swaption hedging compared to traditional methods.

Abstract: This paper investigates the deep hedging framework, based on reinforcement learning (RL), for the dynamic hedging of swaptions, contrasting its performance with traditional sensitivity-based rho-hedging. We design agents under three distinct objective functions (mean squared error, downside risk, and Conditional Value-at-Risk) to capture alternative risk preferences and evaluate how these objectives shape hedging styles. Relying on a three-factor arbitrage-free dynamic Nelson-Siegel model for our simulation experiments, our findings show that near-optimal hedging effectiveness is achieved when using two swaps as hedging instruments. Deep hedging strategies dynamically adapt the hedging portfolio's exposure to risk factors across states of the market. In our experiments, their out-performance over rho-hedging strategies persists even in the presence some of model misspecification. These results highlight RL's potential to deliver more efficient and resilient swaption hedging strategies.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [718] [Web Technologies Security in the AI Era: A Survey of CDN-Enhanced Defenses](https://arxiv.org/abs/2512.06390)
*Mehrab Hosain,Sabbir Alom Shuvo,Matthew Ogbe,Md Shah Jalal Mazumder,Yead Rahman,Md Azizul Hakim,Anukul Pandey*

Main category: cs.CR

TL;DR: The paper surveys AI-enhanced defenses in edge computing, analyzing current solutions, challenges, and proposing a future research agenda.


<details>
  <summary>Details</summary>
Motivation: The need to address evolving AI-assisted attacks on modern web infrastructures and maximize defensive capabilities via edge computing using AI-driven technologies.

Method: The authors conducted a systematic survey, proposed a threat taxonomy, and evaluated edge-centric AI defenses while offering deployment strategies and governance frameworks.

Result: Edge-centric AI defenses show improved detection and mitigation times, reduced data movement, and better compliance but also pose risks related to abuse, poisoning, and governance.

Conclusion: Edge-based AI defenses provide significant advantages but need careful oversight and further research into explainability, robustness, and autonomous defense mechanisms.

Abstract: The modern web stack, which is dominated by browser-based applications and API-first backends, now operates under an adversarial equilibrium where automated, AI-assisted attacks evolve continuously. Content Delivery Networks (CDNs) and edge computing place programmable defenses closest to users and bots, making them natural enforcement points for machine-learning (ML) driven inspection, throttling, and isolation. This survey synthesizes the landscape of AI-enhanced defenses deployed at the edge: (i) anomaly- and behavior-based Web Application Firewalls (WAFs) within broader Web Application and API Protection (WAAP), (ii) adaptive DDoS detection and mitigation, (iii) bot management that resists human-mimicry, and (iv) API discovery, positive security modeling, and encrypted-traffic anomaly analysis. We add a systematic survey method, a threat taxonomy mapped to edge-observable signals, evaluation metrics, deployment playbooks, and governance guidance. We conclude with a research agenda spanning XAI, adversarial robustness, and autonomous multi-agent defense. Our findings indicate that edge-centric AI measurably improves time-to-detect and time-to-mitigate while reducing data movement and enhancing compliance, yet introduces new risks around model abuse, poisoning, and governance.

</details>


### [719] [From Description to Score: Can LLMs Quantify Vulnerabilities?](https://arxiv.org/abs/2512.06781)
*Sima Jafarikhah,Daniel Thompson,Eva Deans,Hossein Siadati,Yi Liu*

Main category: cs.CR

TL;DR: This study explores using large language models (LLMs) to automate CVSS scoring for vulnerabilities, showing promising but varied results.


<details>
  <summary>Details</summary>
Motivation: Address resource-intensive and subjective manual CVSS scoring by leveraging large language models.

Method: Analyzed over 31,000 CVEs using LLMs and examined their performance on various CVSS metrics, identifying strengths and weaknesses.

Result: LLMs outperform baselines in metrics like Availability Impact but show variability across models and metrics; ChatGPT-5 is the most precise.

Conclusion: Improving CVE descriptions with more context can enhance automated vulnerability scoring and reduce workload.

Abstract: Manual vulnerability scoring, such as assigning Common Vulnerability Scoring System (CVSS) scores, is a resource-intensive process that is often influenced by subjective interpretation. This study investigates the potential of general-purpose large language models (LLMs), namely ChatGPT, Llama, Grok, DeepSeek, and Gemini, to automate this process by analyzing over 31{,}000 recent Common Vulnerabilities and Exposures (CVE) entries. The results show that LLMs substantially outperform the baseline on certain metrics (e.g., \textit{Availability Impact}), while offering more modest gains on others (e.g., \textit{Attack Complexity}). Moreover, model performance varies across both LLM families and individual CVSS metrics, with ChatGPT-5 attaining the highest precision. Our analysis reveals that LLMs tend to misclassify many of the same CVEs, and ensemble-based meta-classifiers only marginally improve performance. Further examination shows that CVE descriptions often lack critical context or contain ambiguous phrasing, which contributes to systematic misclassifications. These findings underscore the importance of enhancing vulnerability descriptions and incorporating richer contextual details to support more reliable automated reasoning and alleviate the growing backlog of CVEs awaiting triage.

</details>


### [720] [An Adaptive Multi-Layered Honeynet Architecture for Threat Behavior Analysis via Deep Learning](https://arxiv.org/abs/2512.07827)
*Lukas Johannes Möller*

Main category: cs.CR

TL;DR: The paper introduces ADLAH, an adaptive honeynet using deep learning and reinforcement learning to detect anomalies and maximize threat intelligence capture.


<details>
  <summary>Details</summary>
Motivation: Static honeypots fail to keep up with the sophistication of modern cyber threats, requiring adaptive and intelligence-driven solutions.

Method: ADLAH employs reinforcement learning for real-time decision-making to escalate sessions from low-interaction sensors to high-interaction honeypots, paired with anomaly detection and bot attack chain analysis.

Result: A functional prototype of the decision mechanism is developed, highlighting design trade-offs while providing a roadmap for field-scale empirical evaluation.

Conclusion: ADLAH presents a cost-efficient AI-driven architecture for capturing high-value adversary behavior and actionable threat intelligence despite lacking live data for field-scale validation.

Abstract: The escalating sophistication and variety of cyber threats have rendered static honeypots inadequate, necessitating adaptive, intelligence-driven deception. In this work, ADLAH is introduced: an Adaptive Deep Learning Anomaly Detection Honeynet designed to maximize high-fidelity threat intelligence while minimizing cost through autonomous orchestration of infrastructure. The principal contribution is offered as an end-to-end architectural blueprint and vision for an AI-driven deception platform. Feasibility is evidenced by a functional prototype of the central decision mechanism, in which a reinforcement learning (RL) agent determines, in real time, when sessions should be escalated from low-interaction sensor nodes to dynamically provisioned, high-interaction honeypots. Because sufficient live data were unavailable, field-scale validation is not claimed; instead, design trade-offs and limitations are detailed, and a rigorous roadmap toward empirical evaluation at scale is provided. Beyond selective escalation and anomaly detection, the architecture pursues automated extraction, clustering, and versioning of bot attack chains, a core capability motivated by the empirical observation that exposed services are dominated by automated traffic. Together, these elements delineate a practical path toward cost-efficient capture of high-value adversary behavior, systematic bot versioning, and the production of actionable threat intelligence.

</details>


### [721] [The Road of Adaptive AI for Precision in Cybersecurity](https://arxiv.org/abs/2512.06048)
*Sahil Garg*

Main category: cs.CR

TL;DR: The paper explores the use of Generative AI pipelines in cybersecurity, focusing on adaptation mechanisms for evolving threats and providing practical guidance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in applying adaptable Generative AI technologies for handling dynamic cybersecurity threats.

Method: Insights are derived from real-world deployments of GenAI pipelines, proposing best practices for retrieval- and model-level adaptation.

Result: Demonstrates how different adaptation mechanisms can enhance the effectiveness of Generative AI in cybersecurity systems.

Conclusion: Provides actionable guidance for practitioners and highlights areas for improving robustness, precision, and auditability in GenAI for cybersecurity.

Abstract: Cybersecurity's evolving complexity presents unique challenges and opportunities for AI research and practice. This paper shares key lessons and insights from designing, building, and operating production-grade GenAI pipelines in cybersecurity, with a focus on the continual adaptation required to keep pace with ever-shifting knowledge bases, tooling, and threats. Our goal is to provide an actionable perspective for AI practitioners and industry stakeholders navigating the frontier of GenAI for cybersecurity, with particular attention to how different adaptation mechanisms complement each other in end-to-end systems. We present practical guidance derived from real-world deployments, propose best practices for leveraging retrieval- and model-level adaptation, and highlight open research directions for making GenAI more robust, precise, and auditable in cyber defense.

</details>


### [722] [Ideal Attribution and Faithful Watermarks for Language Models](https://arxiv.org/abs/2512.07038)
*Min Jae Song,Kameron Shahabi*

Main category: cs.CR

TL;DR: The paper introduces a formal framework for ideal attribution mechanisms based on a ledger for deterministic attribution in model-user interactions, framed as watermarking scheme design.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a unified, clear conceptual framework for attribution decisions and watermarking schemes, replacing fragmented probabilistic approaches.

Method: The proposed approach uses an append-only ledger to record interactions and deterministic criteria for attribution, serving as a ground truth for watermarking mechanisms.

Result: The framework enables unified reasoning and clarifies achievable guarantees for watermarking schemes in an idealized setting.

Conclusion: The framework provides a clear roadmap, specifying ideal functionalities as a basis for future watermarking scheme development.

Abstract: We introduce ideal attribution mechanisms, a formal abstraction for reasoning about attribution decisions over strings. At the core of this abstraction lies the ledger, an append-only log of the prompt-response interaction history between a model and its user. Each mechanism produces deterministic decisions based on the ledger and an explicit selection criterion, making it well-suited to serve as a ground truth for attribution. We frame the design goal of watermarking schemes as faithful representation of ideal attribution mechanisms. This novel perspective brings conceptual clarity, replacing piecemeal probabilistic statements with a unified language for stating the guarantees of each scheme. It also enables precise reasoning about desiderata for future watermarking schemes, even when no current construction achieves them, since the ideal functionalities are specified first. In this way, the framework provides a roadmap that clarifies which guarantees are attainable in an idealized setting and worth pursuing in practice.

</details>


### [723] [Beyond Model Jailbreak: Systematic Dissection of the "Ten DeadlySins" in Embodied Intelligence](https://arxiv.org/abs/2512.06387)
*Yuhang Huang,Junchao Li,Boyang Ma,Xuelong Dai,Minghui Xu,Kaidi Xu,Yue Zhang,Jianping Wang,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: This paper analyzes the security vulnerabilities of the Unitree Go2 embodied AI platform, identifying ten major cross-layer weaknesses.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked security challenges in the broader system stack of embodied intelligence, beyond just language model safeguards.

Method: The researchers employed BLE sniffing, traffic interception, APK reverse engineering, cloud API testing, and hardware probing to identify vulnerabilities across wireless provisioning, core modules, and external interfaces.

Result: They uncovered ten critical vulnerabilities, allowing potential adversaries to hijack devices, extract sensitive information, or gain full physical control.

Conclusion: Securing embodied AI necessitates attention to the entire software and hardware ecosystem, with system-level recommendations for robust protections.

Abstract: Embodied AI systems integrate language models with real world sensing, mobility, and cloud connected mobile apps. Yet while model jailbreaks have drawn significant attention, the broader system stack of embodied intelligence remains largely unexplored. In this work, we conduct the first holistic security analysis of the Unitree Go2 platform and uncover ten cross layer vulnerabilities the "Ten Sins of Embodied AI Security." Using BLE sniffing, traffic interception, APK reverse engineering, cloud API testing, and hardware probing, we identify systemic weaknesses across three architectural layers: wireless provisioning, core modules, and external interfaces. These include hard coded keys, predictable handshake tokens, WiFi credential leakage, missing TLS validation, static SSH password, multilingual safety bypass behavior, insecure local relay channels, weak binding logic, and unrestricted firmware access. Together, they allow adversaries to hijack devices, inject arbitrary commands, extract sensitive information, or gain full physical control.Our findings show that securing embodied AI requires far more than aligning the model itself. We conclude with system level lessons learned and recommendations for building embodied platforms that remain robust across their entire software hardware ecosystem.

</details>


### [724] [DEFEND: Poisoned Model Detection and Malicious Client Exclusion Mechanism for Secure Federated Learning-based Road Condition Classification](https://arxiv.org/abs/2512.06172)
*Sheng Liu,Panos Papadimitratos*

Main category: cs.CR

TL;DR: The study introduces DEFEND, a solution for Federated Learning in ITS to secure model performance against poisoning attacks like Targeted Label-Flipping Attacks (TLFAs), outperforming state-of-the-art countermeasures by 15.78% or more.


<details>
  <summary>Details</summary>
Motivation: Existing Federated Learning approaches for ITS do not effectively handle TLFAs, jeopardizing transportation safety due to poisoned data misguiding model predictions.

Method: DEFEND employs neuron-wise magnitude analysis and Gaussian Mixture Model (GMM)-based clustering for detecting poisoned models, and systematically excludes malicious clients based on their ratings.

Result: DEFEND demonstrated superior resilience against TLFAs, maintaining model performance comparable to attack-free scenarios, outperforming seven other countermeasures.

Conclusion: DEFEND significantly strengthens FL-based ITS models against adversarial attacks, advancing privacy-preserved, safe transportation solutions with reliable model performance.

Abstract: Federated Learning (FL) has drawn the attention of the Intelligent Transportation Systems (ITS) community. FL can train various models for ITS tasks, notably camera-based Road Condition Classification (RCC), in a privacy-preserving collaborative way. However, opening up to collaboration also opens FL-based RCC systems to adversaries, i.e., misbehaving participants that can launch Targeted Label-Flipping Attacks (TLFAs) and threaten transportation safety. Adversaries mounting TLFAs poison training data to misguide model predictions, from an actual source class (e.g., wet road) to a wrongly perceived target class (e.g., dry road). Existing countermeasures against poisoning attacks cannot maintain model performance under TLFAs close to the performance level in attack-free scenarios, because they lack specific model misbehavior detection for TLFAs and neglect client exclusion after the detection. To close this research gap, we propose DEFEND, which includes a poisoned model detection strategy that leverages neuron-wise magnitude analysis for attack goal identification and Gaussian Mixture Model (GMM)-based clustering. DEFEND discards poisoned model contributions in each round and adapts accordingly client ratings, eventually excluding malicious clients. Extensive evaluation involving various FL-RCC models and tasks shows that DEFEND can thwart TLFAs and outperform seven baseline countermeasures, with at least 15.78% improvement, with DEFEND remarkably achieving under attack the same performance as in attack-free scenarios.

</details>


### [725] [Look Twice before You Leap: A Rational Agent Framework for Localized Adversarial Anonymization](https://arxiv.org/abs/2512.06713)
*Donghang Duan,Xu Zheng*

Main category: cs.CR

TL;DR: The paper addresses issues in current LLM-based anonymization due to reliance on untrusted services and inadequacies of local small models, proposing a novel framework called RLAA that achieves better privacy-utility trade-offs.


<details>
  <summary>Details</summary>
Motivation: The authors aim to resolve the 'privacy paradox' where users must share sensitive data with untrusted third parties for anonymization and to address performance issues with local small-scale models.

Method: The paper introduces Rational Localized Adversarial Anonymization (RLAA), using an Attacker-Arbitrator-Anonymizer (A-A-A) architecture with an arbitrator ensuring rational strategies by balancing privacy gain and utility cost.

Result: RLAA demonstrated superior privacy-utility trade-offs across datasets, outperforming state-of-the-art methods under certain conditions.

Conclusion: RLAA offers a localized and training-free solution to enhance privacy-utility performance without relying on untrusted remote services, challenging existing frameworks' approaches.

Abstract: Current LLM-based text anonymization frameworks usually rely on remote API services from powerful LLMs, which creates an inherent "privacy paradox": users must somehow disclose data to untrusted third parties for superior privacy preservation. Moreover, directly migrating these frameworks to local small-scale models (LSMs) offers a suboptimal solution with catastrophic collapse in utility based on our core findings. Our work argues that this failure stems not merely from the capability deficits of LSMs, but from the inherent irrationality of the greedy adversarial strategies employed by current state-of-the-art (SoTA) methods. We model the anonymization process as a trade-off between Marginal Privacy Gain (MPG) and Marginal Utility Cost (MUC), and demonstrate that greedy strategies inevitably drift into an irrational state. To address this, we propose Rational Localized Adversarial Anonymization (RLAA), a fully localized and training-free framework featuring an Attacker-Arbitrator-Anonymizer (A-A-A) architecture. RLAA introduces an arbitrator that acts as a rationality gatekeeper, validating the attacker's inference to filter out feedback providing negligible benefits on privacy preservation. This mechanism enforces a rational early-stopping criterion, and systematically prevents utility collapse. Extensive experiments on different datasets demonstrate that RLAA achieves the best privacy-utility trade-off, and in some cases even outperforms SoTA on the Pareto principle. Our code and datasets will be released upon acceptance.

</details>


### [726] [AgenticCyber: A GenAI-Powered Multi-Agent System for Multimodal Threat Detection and Adaptive Response in Cybersecurity](https://arxiv.org/abs/2512.06396)
*Shovan Roy*

Main category: cs.CR

TL;DR: The paper presents AgenticCyber, a generative AI-powered multi-agent system for multimodal threat detection and adaptive cybersecurity management.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of cyber threats in distributed environments necessitates advanced, real-time, and integrated detection and response systems.

Method: The system uses AI agents orchestrated by multimodal language models like Google’s Gemini and LangChain to analyze cloud logs, surveillance videos, and environmental audio, achieving high performance benchmarks.

Result: AgenticCyber achieves a 96.2% F1-score in threat detection, reduces response latency to 420 ms, and decreases MTTR by 65%, outperforming standard systems on benchmark datasets.

Conclusion: AgenticCyber provides a scalable and modular architecture that integrates cross-modal reasoning to enhance cybersecurity across enterprise networks and IoT ecosystems.

Abstract: The increasing complexity of cyber threats in distributed environments demands advanced frameworks for real-time detection and response across multimodal data streams. This paper introduces AgenticCyber, a generative AI powered multi-agent system that orchestrates specialized agents to monitor cloud logs, surveillance videos, and environmental audio concurrently. The solution achieves 96.2% F1-score in threat detection, reduces response latency to 420 ms, and enables adaptive security posture management using multimodal language models like Google's Gemini coupled with LangChain for agent orchestration. Benchmark datasets, such as AWS CloudTrail logs, UCF-Crime video frames, and UrbanSound8K audio clips, show greater performance over standard intrusion detection systems, reducing mean time to respond (MTTR) by 65% and improving situational awareness. This work introduces a scalable, modular proactive cybersecurity architecture for enterprise networks and IoT ecosystems that overcomes siloed security technologies with cross-modal reasoning and automated remediation.

</details>


### [727] [BEACON: A Unified Behavioral-Tactical Framework for Explainable Cybercrime Analysis with Large Language Models](https://arxiv.org/abs/2512.06555)
*Arush Sachdeva,Rajendraprasad Saravanan,Gargi Sarkar,Kavita Vemuri,Sandeep Kumar Shukla*

Main category: cs.CR

TL;DR: This paper introduces BEACON, a framework combining behavioral psychology and cybercrime tactics for improved analysis and classification.


<details>
  <summary>Details</summary>
Motivation: Existing analytical frameworks for cybercrime lack focus on psychological manipulation and primarily emphasize operational aspects.

Method: BEACON incorporates psychological manipulation categories, derived from Prospect Theory and Cialdini's persuasion principles, into a fourteen-stage tactical lifecycle. A fine-tuned large language model performs multi-label classification and explanation generation.

Result: BEACON achieved 20% higher accuracy in classification compared to baseline, with improved reasoning quality, validated on real-world and synthetically augmented cybercrime data.

Conclusion: The framework automates behavioral and operational analysis of victim narratives, aiding investigation, case linkage, and scam prediction.

Abstract: Cybercrime increasingly exploits human cognitive biases in addition to technical vulnerabilities, yet most existing analytical frameworks focus primarily on operational aspects and overlook psychological manipulation. This paper proposes BEACON, a unified dual-dimension framework that integrates behavioral psychology with the tactical lifecycle of cybercrime to enable structured, interpretable, and scalable analysis of cybercrime. We formalize six psychologically grounded manipulation categories derived from Prospect Theory and Cialdini's principles of persuasion, alongside a fourteen-stage cybercrime tactical lifecycle spanning reconnaissance to final impact. A single large language model is fine-tuned using parameter-efficient learning to perform joint multi-label classification across both psychological and tactical dimensions while simultaneously generating human-interpretable explanations. Experiments conducted on a curated dataset of real-world and synthetically augmented cybercrime narratives demonstrate a 20 percent improvement in overall classification accuracy over the base model, along with substantial gains in reasoning quality measured using ROUGE and BERTScore. The proposed system enables automated decomposition of unstructured victim narratives into structured behavioral and operational intelligence, supporting improved cybercrime investigation, case linkage, and proactive scam detection.

</details>


### [728] [Securing the Model Context Protocol: Defending LLMs Against Tool Poisoning and Adversarial Attacks](https://arxiv.org/abs/2512.06556)
*Saeid Jamshidi,Kawser Wazed Nafi,Arghavan Moradi Dakhel,Negar Shahabi,Foutse Khomh,Naser Ezzati-Jivan*

Main category: cs.CR

TL;DR: The paper proposes security measures for MCP systems with semantic attack vulnerabilities using methods like manifest signing, semantic vetting, and heuristic guardrails.


<details>
  <summary>Details</summary>
Motivation: To address the security gap in MCP systems exposed to semantic attacks that are not covered by existing defenses like prompt-injection protections.

Method: A layered security framework comprising RSA-based signing, semantic vetting by Large Language Models, and runtime heuristic guardrails.

Result: Varied performance across models: GPT-4 balances speed and safety, DeepSeek excels in Shadowing attack defense but is slower, and Llama-3.5 is quick but less robust.

Conclusion: The proposed framework effectively mitigates unsafe tool invocation rates without requiring modifications to internal model structures.

Abstract: The Model Context Protocol (MCP) enables Large Language Models to integrate external tools through structured descriptors, increasing autonomy in decision-making, task execution, and multi-agent workflows. However, this autonomy creates a largely overlooked security gap. Existing defenses focus on prompt-injection attacks and fail to address threats embedded in tool metadata, leaving MCP-based systems exposed to semantic manipulation. This work analyzes three classes of semantic attacks on MCP-integrated systems: (1) Tool Poisoning, where adversarial instructions are hidden in tool descriptors; (2) Shadowing, where trusted tools are indirectly compromised through contaminated shared context; and (3) Rug Pulls, where descriptors are altered after approval to subvert behavior. To counter these threats, we introduce a layered security framework with three components: RSA-based manifest signing to enforce descriptor integrity, LLM-on-LLM semantic vetting to detect suspicious tool definitions, and lightweight heuristic guardrails that block anomalous tool behavior at runtime. Through evaluation of GPT-4, DeepSeek, and Llama-3.5 across eight prompting strategies, we find that security performance varies widely by model architecture and reasoning method. GPT-4 blocks about 71 percent of unsafe tool calls, balancing latency and safety. DeepSeek shows the highest resilience to Shadowing attacks but with greater latency, while Llama-3.5 is fastest but least robust. Our results show that the proposed framework reduces unsafe tool invocation rates without model fine-tuning or internal modification.

</details>


### [729] [Towards Small Language Models for Security Query Generation in SOC Workflows](https://arxiv.org/abs/2512.06660)
*Saleha Muzammil,Rahul Reddy,Vishal Kamalakrishnan,Hadi Ahmadi,Wajih Ul Hassan*

Main category: cs.CR

TL;DR: The paper explores if Small Language Models (SLMs) can effectively translate natural language into KQL for security operations, offering a cost-effective and scalable solution.


<details>
  <summary>Details</summary>
Motivation: Security analysts often rely on KQL for querying data, but correct usage requires specialized knowledge, which limits scaling. The study investigates if SLMs can simplify this process.

Method: A three-part framework is proposed: error-aware prompting for parser failures, LoRA fine-tuning with rationale distillation to transfer reasoning while keeping models compact, and a two-stage architecture for candidate generation and schema-aware refinement.

Result: The proposed approach achieves high accuracy (0.987 syntax and 0.906 semantic accuracy on Defender Evaluation dataset), generalizes well to a different dataset (0.964 syntax and 0.831 semantic accuracy), and reduces token costs significantly compared to GPT-5.

Conclusion: SLMs are effective for NL-to-KQL translation in enterprise security, achieving high accuracy and scalability while lowering costs and maintaining practicality.

Abstract: Analysts in Security Operations Centers routinely query massive telemetry streams using Kusto Query Language (KQL). Writing correct KQL requires specialized expertise, and this dependency creates a bottleneck as security teams scale. This paper investigates whether Small Language Models (SLMs) can enable accurate, cost-effective natural-language-to-KQL translation for enterprise security. We propose a three-knob framework targeting prompting, fine-tuning, and architecture design. First, we adapt existing NL2KQL framework for SLMs with lightweight retrieval and introduce error-aware prompting that addresses common parser failures without increasing token count. Second, we apply LoRA fine-tuning with rationale distillation, augmenting each NLQ-KQL pair with a brief chain-of-thought explanation to transfer reasoning from a teacher model while keeping the SLM compact. Third, we propose a two-stage architecture that uses an SLM for candidate generation and a low-cost LLM judge for schema-aware refinement and selection. We evaluate nine models (five SLMs and four LLMs) across syntax correctness, semantic accuracy, table selection, and filter precision, alongside latency and token cost. On Microsoft's NL2KQL Defender Evaluation dataset, our two-stage approach achieves 0.987 syntax and 0.906 semantic accuracy. We further demonstrate generalizability on Microsoft Sentinel data, reaching 0.964 syntax and 0.831 semantic accuracy. These results come at up to 10x lower token cost than GPT-5, establishing SLMs as a practical, scalable foundation for natural-language querying in security operations.

</details>


### [730] [PrivLLMSwarm: Privacy-Preserving LLM-Driven UAV Swarms for Secure IoT Surveillance](https://arxiv.org/abs/2512.06747)
*Jifar Wakuma Ayana,Huang Qiming*

Main category: cs.CR

TL;DR: The paper introduces PrivLLMSwarm, a privacy-preserving framework using Secure Multi-Party Computation (MPC) for secure LLM inference in UAV swarms within IoT environments.


<details>
  <summary>Details</summary>
Motivation: There is a need to secure and preserve privacy when using Large Language Models for UAV swarm coordination in IoT environments, as plaintext data processing poses security risks.

Method: The framework uses Secure Multi-Party Computation (MPC) with optimized transformer components for encrypted inference. It employs a fine-tuned GPT-based command generator enhanced by reinforcement learning to ensure reliable and confidential instructions.

Result: Experimental tests demonstrate that PrivLLMSwarm achieves high semantic accuracy, low latency, and robust control under privacy constraints. It achieves a better privacy-utility balance than differential privacy, federated learning, and plaintext baselines.

Conclusion: PrivLLMSwarm provides a practical and secure solution for privacy-sensitive UAV swarm coordination in IoT applications like smart cities and emergency response. The implementation is open-source for reproducibility and further studies.

Abstract: Large Language Models (LLMs) are emerging as powerful enablers for autonomous reasoning and natural-language coordination in unmanned aerial vehicle (UAV) swarms operating within Internet of Things (IoT) environments. However, existing LLM-driven UAV systems process sensitive operational data in plaintext, exposing them to privacy and security risks. This work introduces PrivLLMSwarm, a privacy-preserving framework that performs secure LLM inference for UAV swarm coordination through Secure Multi-Party Computation (MPC). The framework incorporates MPC-optimized transformer components with efficient approximations of nonlinear activations, enabling practical encrypted inference on resource-constrained aerial platforms. A fine-tuned GPT-based command generator, enhanced through reinforcement learning in simulation, provides reliable instructions while maintaining confidentiality. Experimental evaluation in urban-scale simulations demonstrates that PrivLLMSwarm achieves high semantic accuracy, low encrypted inference latency, and robust formation control under privacy constraints. Comparative analysis shows PrivLLMSwarm offers a superior privacy-utility balance compared to differential privacy, federated learning, and plaintext baselines. To support reproducibility, the full implementation including source code, MPC components, and a synthetic dataset is publicly available. PrivLLMSwarm establishes a practical foundation for secure, LLM-enabled UAV swarms in privacy-sensitive IoT applications including smart-city monitoring and emergency response.

</details>


### [731] [SoK: Trust-Authorization Mismatch in LLM Agent Interactions](https://arxiv.org/abs/2512.06914)
*Guanquan Shi,Haohua Du,Zhiqiang Wang,Xiaoyu Liang,Weiwenpei Liu,Song Bian,Zhenyu Guan*

Main category: cs.CR

TL;DR: The paper addresses security challenges faced in autonomous agents like Large Language Models (LLMs), providing a new risk analysis framework to study trust and authorization mismatches while proposing systematic research directions.


<details>
  <summary>Details</summary>
Motivation: Security mechanisms for deterministic traditional systems fail in handling AI agents due to unpredictable natural language-driven decision-making, creating challenges in trust and enforcing least privilege, which are poorly understood in current research.

Method: The authors introduce a risk analysis model focused on the mismatch between trust and authorization. They use this model to classify known attacks and defenses and highlight research gaps in agent-interaction security.

Result: They identified key security threats arising from a trust-authorization mismatch, provided a structured classification of these threats, and presented critical gaps in existing security approaches.

Conclusion: The study offers a formal framework that unifies the field of agent-interaction security and calls for systematic research into trusted AI agents and dynamic authorization systems.

Abstract: Large Language Models (LLMs) are rapidly evolving into autonomous agents capable of interacting with the external world, significantly expanding their capabilities through standardized interaction protocols. However, this paradigm revives the classic cybersecurity challenges of agency and authorization in a novel and volatile context. As decision-making shifts from deterministic code logic to probabilistic inference driven by natural language, traditional security mechanisms designed for deterministic behavior fail. It is fundamentally challenging to establish trust for unpredictable AI agents and to enforce the Principle of Least Privilege (PoLP) when instructions are ambiguous. Despite the escalating threat landscape, the academic community's understanding of this emerging domain remains fragmented, lacking a systematic framework to analyze its root causes. This paper provides a unifying formal lens for agent-interaction security.
  We observed that most security threats in this domain stem from a fundamental mismatch between trust evaluation and authorization policies. We introduce a novel risk analysis model centered on this trust-authorization gap. Using this model as a unifying lens, we survey and classify the implementation paths of existing, often seemingly isolated, attacks and defenses. This new framework not only unifies the field but also allows us to identify critical research gaps. Finally, we leverage our analysis to suggest a systematic research direction toward building robust, trusted agents and dynamic authorization mechanisms.

</details>


### [732] [A Comprehensive Study of Supervised Machine Learning Models for Zero-Day Attack Detection: Analyzing Performance on Imbalanced Data](https://arxiv.org/abs/2512.07030)
*Zahra Lotfi,Mostafa Lotfi*

Main category: cs.CR

TL;DR: The paper evaluates five supervised ML models for detecting zero-day cyberattacks, identifying XG Boost as a top performer due to speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Zero-day attacks are problematic as they remain undetected by traditional security systems due to unknown patterns, requiring improved detection methods.

Method: Five supervised ML models are tested on highly imbalanced data, applying oversampling techniques, dimensionality reduction, and grid search for optimization.

Result: Random Forest achieved the highest accuracy detection but had longer processing times, while XG Boost balanced high accuracy with faster processing.

Conclusion: XG Boost is proposed as the optimal model for detecting zero-day attacks efficiently, balancing accuracy and speed.

Abstract: Among the various types of cyberattacks, identifying zero-day attacks is problematic because they are unknown to security systems as their pattern and characteristics do not match known blacklisted attacks. There are many Machine Learning (ML) models designed to analyze and detect network attacks, especially using supervised models. However, these models are designed to classify samples (normal and attacks) based on the patterns they learn during the training phase, so they perform inefficiently on unseen attacks. This research addresses this issue by evaluating five different supervised models to assess their performance and execution time in predicting zero-day attacks and find out which model performs accurately and quickly. The goal is to improve the performance of these supervised models by not only proposing a framework that applies grid search, dimensionality reduction and oversampling methods to overcome the imbalance problem, but also comparing the effectiveness of oversampling on ml model metrics, in particular the accuracy. To emulate attack detection in real life, this research applies a highly imbalanced data set and only exposes the classifiers to zero-day attacks during the testing phase, so the models are not trained to flag the zero-day attacks. Our results show that Random Forest (RF) performs best under both oversampling and non-oversampling conditions, this increased effectiveness comes at the cost of longer processing times. Therefore, we selected XG Boost (XGB) as the top model due to its fast and highly accurate performance in detecting zero-day attacks.

</details>


### [733] [ThinkTrap: Denial-of-Service Attacks against Black-box LLM Services via Infinite Thinking](https://arxiv.org/abs/2512.07086)
*Yunzhe Li,Jianan Wang,Hongzi Zhu,James Lin,Shan Chang,Minyi Guo*

Main category: cs.CR

TL;DR: The paper explores denial-of-service (DoS) attacks targeting large language models (LLMs) through specialized inputs causing infinite generation loops. They present ThinkTrap, an optimization framework for devising such adversarial prompts in black-box environments.


<details>
  <summary>Details</summary>
Motivation: As LLMs are deployed as cloud-based services, adversaries can exploit computational vulnerabilities, causing service disruptions with DoS attacks.

Method: The ThinkTrap framework optimizes adversarial prompts by mapping tokens into continuous embedding spaces, followed by efficient black-box optimization in a low-dimensional subspace.

Result: The proposed method successfully launches DoS attacks across several commercial LLM services, degrading their throughput by up to 99% and causing service failure within restrictive request limits.

Conclusion: ThinkTrap highlights the vulnerability of LLM services to DoS attacks, emphasizing the need for better security measures in black-box deployment settings.

Abstract: Large Language Models (LLMs) have become foundational components in a wide range of applications, including natural language understanding and generation, embodied intelligence, and scientific discovery. As their computational requirements continue to grow, these models are increasingly deployed as cloud-based services, allowing users to access powerful LLMs via the Internet. However, this deployment model introduces a new class of threat: denial-of-service (DoS) attacks via unbounded reasoning, where adversaries craft specially designed inputs that cause the model to enter excessively long or infinite generation loops. These attacks can exhaust backend compute resources, degrading or denying service to legitimate users. To mitigate such risks, many LLM providers adopt a closed-source, black-box setting to obscure model internals. In this paper, we propose ThinkTrap, a novel input-space optimization framework for DoS attacks against LLM services even in black-box environments. The core idea of ThinkTrap is to first map discrete tokens into a continuous embedding space, then undertake efficient black-box optimization in a low-dimensional subspace exploiting input sparsity. The goal of this optimization is to identify adversarial prompts that induce extended or non-terminating generation across several state-of-the-art LLMs, achieving DoS with minimal token overhead. We evaluate the proposed attack across multiple commercial, closed-source LLM services. Our results demonstrate that, even far under the restrictive request frequency limits commonly enforced by these platforms, typically capped at ten requests per minute (10 RPM), the attack can degrade service throughput to as low as 1% of its original capacity, and in some cases, induce complete service failure.

</details>


### [734] [VulnLLM-R: Specialized Reasoning LLM with Agent Scaffold for Vulnerability Detection](https://arxiv.org/abs/2512.07533)
*Yuzhou Nie,Hongwei Li,Chengquan Guo,Ruizhe Jiang,Zhun Wang,Bo Li,Dawn Song,Wenbo Guo*

Main category: cs.CR

TL;DR: VulnLLM-R is the first reasoning-focused LLM for vulnerability detection, outperforming SOTA tools and models while discovering zero-day vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Existing vulnerability detection methods rely on simple pattern matching, lacking generalizability. Current reasoning LLMs are either ultra-large, closed, or suboptimal for vulnerability detection.

Method: A novel training recipe involving data selection, reasoning generation, filtering, correction, and testing optimization was devised to develop VulnLLM-R, a specialized reasoning model with 7 billion parameters.

Result: VulnLLM-R achieves superior effectiveness in vulnerability detection across multiple programming languages, outperforming SOTA tools, while also identifying zero-day vulnerabilities in real-world projects.

Conclusion: The study demonstrates the practical capability of AI agents backed by specialized reasoning models for vulnerability detection in real-world applications, showcasing a breakthrough in the field.

Abstract: We propose VulnLLM-R, the~\emph{first specialized reasoning LLM} for vulnerability detection. Our key insight is that LLMs can reason about program states and analyze the potential vulnerabilities, rather than simple pattern matching. This can improve the model's generalizability and prevent learning shortcuts. However, SOTA reasoning LLMs are typically ultra-large, closed-source, or have limited performance in vulnerability detection. To address this, we propose a novel training recipe with specialized data selection, reasoning data generation, reasoning data filtering and correction, and testing-phase optimization. Using our proposed methodology, we train a reasoning model with seven billion parameters. Through extensive experiments on SOTA datasets across Python, C/C++, and Java, we show that VulnLLM-R has superior effectiveness and efficiency than SOTA static analysis tools and both open-source and commercial large reasoning models. We further conduct a detailed ablation study to validate the key designs in our training recipe. Finally, we construct an agent scaffold around our model and show that it outperforms CodeQL and AFL++ in real-world projects. Our agent further discovers a set of zero-day vulnerabilities in actively maintained repositories. This work represents a pioneering effort to enable real-world, project-level vulnerability detection using AI agents powered by specialized reasoning models. The code is available at~\href{https://github.com/ucsb-mlsec/VulnLLM-R}{github}.

</details>


### [735] [OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation](https://arxiv.org/abs/2512.06589)
*Xiaojun Jia,Jie Liao,Qi Guo,Teng Ma,Simeng Qin,Ranjie Duan,Tianlin Li,Yihao Huang,Zhitao Zeng,Dongxian Wu,Yiming Li,Wenqi Ren,Xiaochun Cao,Yang Liu*

Main category: cs.CR

TL;DR: The study introduces OmniSafeBench-MM, a comprehensive toolbox for evaluating and addressing multi-modal jailbreak attacks on large language models.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the lack of comprehensive benchmarks and toolboxes to evaluate multi-modal vulnerabilities and defenses effectively, given the increasing prevalence of jailbreak attacks in large language models.

Method: The paper presents OmniSafeBench-MM, which integrates 13 attack methods, 15 defense strategies, and a diverse dataset categorized across 9 risk domains and 50 fine-grained categories. It also establishes a three-dimensional evaluation protocol assessing harmfulness, intent alignment, and response detail.

Result: The researchers conducted experiments on 18 large language models (10 open-source, 8 closed-source), uncovering vulnerabilities in their multi-modal jailbreak defenses. The toolbox serves as a standardized reference for analysis.

Conclusion: OmniSafeBench-MM is a robust, reproducible, and open-source platform that consolidates datasets, methodologies, and evaluations to advance research and improve safety measures in multi-modal large language models.

Abstract: Recent advances in multi-modal large language models (MLLMs) have enabled unified perception-reasoning capabilities, yet these systems remain highly vulnerable to jailbreak attacks that bypass safety alignment and induce harmful behaviors. Existing benchmarks such as JailBreakV-28K, MM-SafetyBench, and HADES provide valuable insights into multi-modal vulnerabilities, but they typically focus on limited attack scenarios, lack standardized defense evaluation, and offer no unified, reproducible toolbox. To address these gaps, we introduce OmniSafeBench-MM, which is a comprehensive toolbox for multi-modal jailbreak attack-defense evaluation. OmniSafeBench-MM integrates 13 representative attack methods, 15 defense strategies, and a diverse dataset spanning 9 major risk domains and 50 fine-grained categories, structured across consultative, imperative, and declarative inquiry types to reflect realistic user intentions. Beyond data coverage, it establishes a three-dimensional evaluation protocol measuring (1) harmfulness, distinguished by a granular, multi-level scale ranging from low-impact individual harm to catastrophic societal threats, (2) intent alignment between responses and queries, and (3) response detail level, enabling nuanced safety-utility analysis. We conduct extensive experiments on 10 open-source and 8 closed-source MLLMs to reveal their vulnerability to multi-modal jailbreak. By unifying data, methodology, and evaluation into an open-source, reproducible platform, OmniSafeBench-MM provides a standardized foundation for future research. The code is released at https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM.

</details>


### [736] [PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning](https://arxiv.org/abs/2512.07342)
*Chen Gong,Zheng Liu,Kecen Li,Tianhao Wang*

Main category: cs.CR

TL;DR: The paper proposes PrivORL, a method to synthesize private offline RL datasets using differential privacy principles.


<details>
  <summary>Details</summary>
Motivation: Concerns about privacy leakage in offline RL datasets necessitate an approach to safeguard sensitive information while maintaining data utility.

Method: The authors developed PrivORL, which leverages diffusion models, diffusion transformers, differential privacy stochastic gradient descent (DP-SGD), and curiosity-driven pre-training to synthesize private transitions and trajectories.

Result: PrivORL demonstrates superior utility and dataset fidelity compared to baseline methods, validated through experiments on five sensitive offline RL datasets.

Conclusion: PrivORL effectively synthesizes private offline RL datasets under differential privacy, enabling secure data sharing for downstream research while preserving utility and diversity.

Abstract: Recently, offline reinforcement learning (RL) has become a popular RL paradigm. In offline RL, data providers share pre-collected datasets -- either as individual transitions or sequences of transitions forming trajectories -- to enable the training of RL models (also called agents) without direct interaction with the environments. Offline RL saves interactions with environments compared to traditional RL, and has been effective in critical areas, such as navigation tasks. Meanwhile, concerns about privacy leakage from offline RL datasets have emerged.
  To safeguard private information in offline RL datasets, we propose the first differential privacy (DP) offline dataset synthesis method, PrivORL, which leverages a diffusion model and diffusion transformer to synthesize transitions and trajectories, respectively, under DP. The synthetic dataset can then be securely released for downstream analysis and research. PrivORL adopts the popular approach of pre-training a synthesizer on public datasets, and then fine-tuning on sensitive datasets using DP Stochastic Gradient Descent (DP-SGD). Additionally, PrivORL introduces curiosity-driven pre-training, which uses feedback from the curiosity module to diversify the synthetic dataset and thus can generate diverse synthetic transitions and trajectories that closely resemble the sensitive dataset. Extensive experiments on five sensitive offline RL datasets show that our method achieves better utility and fidelity in both DP transition and trajectory synthesis compared to baselines. The replication package is available at the GitHub repository.

</details>
